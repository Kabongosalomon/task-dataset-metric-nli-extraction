<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingshuang</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lip-reading aims to infer the speech content from the lip movement sequence and can be seen as a typical sequence-to-sequence (seq2seq) problem which translates the input image sequence of lip movements to the text sequence of the speech content. However, the traditional learning process of seq2seq models always suffers from two problems: the exposure bias resulted from the strategy of "teacher-forcing", and the inconsistency between the discriminative optimization target (usually the cross-entropy loss) and the final evaluation metric (usually the character/word error rate). In this paper, we propose a novel pseudo-convolutional policy gradient (PCPG) based method to address these two problems. On the one hand, we introduce the evaluation metric (refers to the character error rate in this paper) as a form of reward to optimize the model together with the original discriminative target. On the other hand, inspired by the local perception property of convolutional operation, we perform a pseudo-convolutional operation on the reward and loss dimension, so as to take more context around each time step into account to generate a robust reward and loss for the whole optimization. Finally, we perform a thorough comparison and evaluation on both the word-level and sentencelevel benchmarks. The results show a significant improvement over other related methods, and report either a new stateof-the-art performance or a competitive accuracy on all these challenging benchmarks, which clearly proves the advantages of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Lip-reading is an appealing tool for intelligent humancomputer interaction and has gained increasing attention in recent years. It aims to infer the speech content by using the visual information <ref type="bibr" target="#b7">[8]</ref> like the lip movements, and so is robust to the ubiquitous acoustic noises. This appealing property makes it play an important role as the complement of the audio-based speech recognition system, especially in a noisy environment. At the same time, lipreading is also crucial for several other potential applications, such as transcribing and re-dubbing archival silent films, sound-source localization, liveness verification and so on <ref type="bibr" target="#b6">[7]</ref>. Benefiting from the vigorous development of deep learning (DL) and the emergence of several large-scale lip-reading datasets, such as GRID <ref type="bibr" target="#b10">[11]</ref>, LRW <ref type="bibr" target="#b7">[8]</ref>, LRW-1000 <ref type="bibr" target="#b28">[29]</ref>, LRS <ref type="bibr" target="#b6">[7]</ref> and so on, lip-reading has made great progress over the past two years.</p><p>Typically, lip-reading can be seen as a sequence-tosequence (seq2seq) problem to translate the lip movement sequence to a character or word sequence, as shown in <ref type="figure" target="#fig_0">Fig.  1</ref>. Several previous work have tried successfully to introduce seq2seq models for lip-reading <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, most seq2seq-based methods suffer from two drawbacks. The first problem is the exposure bias resulted from the strategy of "teacher-forcing". Most current seq2seq models are learned in a way to obtain a correct prediction based on providing the ground-truth word at the previous time-step, which is named "teacher-forcing <ref type="bibr" target="#b21">[22]</ref> and used widely in lip-reading <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b0">[1]</ref>. This strategy is much favorable to make the model converge at a fast speed. But the optimized model learned in this way has a heavy dependence on the ground-truth words at the previous time-steps. Therefore, it is always difficult to obtain a consistent performance when using the optimized model for the actual test process, where no ground-truth is available and the model has to make a correct prediction based on the previous predictions <ref type="bibr" target="#b20">[21]</ref>. This discrepancy would inevitably yield inaccurate results and even errors. Furthermore, this discrepancy would make the errors accumulate quickly along the sequence, leading to worse and worse predictions in the end. The second problem is the inconsistency between the optimized discriminative target and the final non-differentiable evaluation metric. For most lip-reading models, Cross-Entropy minimization (CE loss) is always applied as the discriminative optimization target. The CE loss is always used for each time step and the average over all time steps is finally used as the measure of the quality of the prediction results. This would always lead to two problems. Firstly, the optimized model may be not able to perform well when coming to test due to the inconsistency between the CE loss and the evaluation metric of WER/CER (word/character error rate). Secondly, the optimization process computes the cost at each time step independently, giving little consideration to the predictions before and after each time-step. This could probably lead to the case that only a single time step or just a few time steps are predicted well if they have a larger loss compared with the time steps nearby.</p><p>Inspired by the popular convolutional operation which has the appealing properties of local perception and weight sharing, we propose a novel pseudo convolutional policy gradient (PCPG) based seq2seq model to solve the above two problems for robust lip-reading. On the one hand, we introduce reinforcement learning (RL) into the seq2seq model to connect the optimized discriminative target and the evaluation metric of WER/CER directly. On the other hand, we mimic the computation process of traditional convolutional operation to consider more time steps nearby when computing the reward and loss at each time step. By a thorough evaluation and comparison on several lip-reading benchmarks, we demonstrate both the effectiveness and the generalization ability of our proposed model. Finally, we report either new state-of-the-art performance or competitive accuracy on both the word-level and sentence-level benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we firstly give a brief review of the previous work for the lip-reading task. Then we discuss the work related with seq2seq models and reinforcement learning involved in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Lip-reading</head><p>Lip-reading has made several substantial progress since the emergence of deep learning (DL) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b13">[14]</ref>. These contempts can be divided into two strands according to their particular task.</p><p>The first one mainly focuses on word-level lip-reading. This type of method aims to classify the whole image sequence into a single word class, no matter how many frames in the sequence. For example, the work in <ref type="bibr" target="#b8">[9]</ref> proposed to extract frame-level features by the VGG network and then combine all the features of all the frames at several different stages to obtain the final prediction of the whole sequence. In <ref type="bibr" target="#b23">[24]</ref>, the authors proposed an end-to-end deep learning architecture by combining a spatiotemporal convolutional module, the ResNet module and the final bidirectional LSTM for word-level lipreading, and have obtained the state-of-theart performance when they were published. In this paper, we would keep a similar front-end module with them and would present details in the next section.</p><p>The second strand pays more attention to sentence-level lip-reading task. Different from the word-level which considers the whole image sequence as a single category, the sentence-level method has to decode the character or word at each time step to output a sentence. The LipNet model presented in <ref type="bibr" target="#b3">[4]</ref> is the first end-to-end sentence-level lipreading model. It employes the CTC loss to predict different characters at different time steps, which would then compose the final sentence-level predictions. Then, the work in <ref type="bibr" target="#b2">[3]</ref> evaluated not only the CTC loss but also the beam search based process <ref type="bibr" target="#b27">[28]</ref> for lip-reading. However, the CTC based methods are all based on the assumption that all the predictions at each time step are independent of each other, which we think is not proper in a sequence-based task. In the meantime, the CTC loss has also a limitation that the length of the input image sequence has to be larger than the length of the speech content sequence. Therefore, seq2seq models are gradually introduced to the sentence-level lip-reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Seq2Seq models</head><p>Sequence-to-sequence (seq2seq) models always contain an RNN-based encoder to encode the input image sequence into a vector and an RNN-based decoder to generate the prediction result at each time step. In the decoding process, attention mechanism is always introduced to make the prediction operation at each output's time step being able to "observe" all the input sequence's time steps. For example, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b0">[1]</ref> have successfully introduced seq2seq models for sentence-level lip-reading.</p><p>However, the usual seq2seq models always suffer from two main limitations. Firstly, most current seq2seq models are trained in the way of "teacher forcing", which would provide the ground-truth word at the previous time step as a condition to predict the results at the next time step. In fact, the models have to predict each output based on the previous prediction results when comes to testing, where no ground-truth words are available. This could easily lead to error accumulation along with the sentence, which is not the result we want. Secondly, most seq2seq models are optimized by minimizing the sum of the cross-entropy loss at each time step. But when it comes to evaluation, the metrics take the whole sentence into account with discrete and non-differentiable metrics, such as BLEU <ref type="bibr" target="#b18">[19]</ref>, ROUGE <ref type="bibr" target="#b16">[17]</ref>, METEOR <ref type="bibr" target="#b4">[5]</ref>, CIDEr <ref type="bibr" target="#b24">[25]</ref>, WER (word error rate), CER (character error rate), SER (sentence error rate), and so on. The inconsistency between the loss and the evaluation metric is a serious problem, which could easily lead to the case that even we get a very small cross-entropy loss during training, we may still not be able to get a good performance when it comes to testing.</p><p>In this paper, we propose a novel pseudo-convolutional policy gradient (PCPG) based seq2seq models for the lipreading task, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In our model, the evaluation metric is introduced directly as a form of reward to optimize the model together with the original discriminative target. At the same time, a pseudo-convolutional operation is performed on the reward and loss dimension to take more context around each time step into account to generate a robust reward and loss for the whole optimization. The pseudo-convolutional operation is to mimic the local perception and weight sharing property of convolutional operation, contributing to enhancing the contextual relevance among different time steps for robust lip-reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED WORK</head><p>In this section, we present the proposed PCPG (Pseudo-Convolutional Policy Gradient) based seq2seq model in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-RNN Encoder</head><p>Encoder outputs Encoder hidden state</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent</head><p>Attention bin blue at f two now take an action Ground Truth:</p><p>bin blue at f tw New State:</p><p>bin blue at f t In this model, the GRU decoder is regarded as an agent and the ground truth is regarded as the environment. In the learning process, the agent observes the old state generated from the previous time steps and then takes an action to output a new character/word to obtain a new state. The new state, old state, and the environment would contribute to the reward together for the action. Finally, the reward is fed to the PCPG module to generate the final loss when passed to the agent.</p><p>detail. Firstly, we describe the overall model architecture in the first subsection. Then we introduce the PCPG based learning process. Finally, we state the advantages of our method compared with the traditional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Overall Model architecture</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our model can be divided into two main parts: the video encoder (shown with green color) and the GRU based decoder (shown with yellow color). The video encoder is responsible for encoding the spatiotemporal patterns in the image sequence to obtain a preliminary representation of the sequence. After encoding, the GRU decoder tries to generate predictions at each time step in the principle of making the reward at each time step maximized. The CNN and RNN based Encoder: As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the encoder consists of two main modules: the CNN based front-end to encode the short-term spatial-temporal patterns, and the RNN based back-end to model the long-term global spatial-temporal patterns. The input image sequence would firstly go through a 3D Convolutional layer to capture the initial short-term patterns in the sequence. Then a ResNet-18 <ref type="bibr" target="#b12">[13]</ref> module is followed to capture the specific movement patterns at each time step. Finally, a 2-layer Bi-GRU is adopted to produce a global representation of the whole sequence. We use the GRU's output O e = (o e 1 , o e 2 , . . . , o e T ) and hidden-state vector h e to record the patterns of the input video</p><formula xml:id="formula_0">X v = (x v 1 , x v 2 , . . . , x v T ), which is defined as: h e , O e = Encoder CNN RNN(X v ),<label>(1)</label></formula><p>where e, v, T denote the encoder, the original input video and the temporal length of input video rerspectively.</p><p>The PCPG based RNN Decoder: Our PCPG based RNN decoder is showed as <ref type="figure">Fig. 4</ref>. Given the representation of each input sequence, a 2-layer RNN is followed to decode each character at each output's time step u (u = 1, 2, ...,U), where U denote the maximum length of the output text sequence. The PCPG based loss would then guide the learning process of the model. In this paper, we use GRU as the basic RNN unit.</p><p>To learn the dependency of each output character with each input time step t (t = 1, 2, ..., T ), attention mechanism is introduced into the decoding process, which takes advantage of the context information around each time step t to aid the decoding of each output's time step u, where t and u are used to denote the time steps in the input image sequence and the output text sequence respectively. With the decoder GRU's hidden state h d u−1 (u = 1, 2, ...,U) where d denotes decoder, we can compute the attention weight α u on the current time step u with respect to each input time step t and the corresponding output y u as</p><formula xml:id="formula_1">e u,t = attention h d u−1 , o v t , a u,t = e u,t ∑ t e u,u , α u = ∑ t a ut o v t , y u = GRU h d u−1 , y d u , a u .<label>(2)</label></formula><p>where t = 1, 2, ..., T and u = 1, 2, ...,U. Finally, (y 1 , y 2 , ..., y U ) would be used as the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The PCPG based learning process</head><p>With the model given above, a popular way to learn the model is to minimize the cross-entropy loss L CE at each time </p><formula xml:id="formula_2">GRU (t 12 =2) GRU (t 13 =3) GRU (t 1U =U) GRU (ti1=1) GRU (ti3=3) GRU (t iU =U) GRU (ti2=2) y2 yU GRU (tM1=1) GRU (tM2=2) GRU (tM3=3) GRU (tMU=U)</formula><p>y <ref type="bibr" target="#b0">(1)</ref> M Prediction Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T h e F ir s t S a m p le</head><formula xml:id="formula_3">T h e M - t h S a m p l e The i-th Sample … … … … Ground Truth：</formula><p>bin blue at f two now <ref type="figure">Fig. 4</ref>: The Optimization of PCPG based learning process. Each output at the previous time step will feedback to the agent (GRU decoder) as the input. And the environment will feedback on an immediate reward when taking an action (choosing a character at each time step). To calculate the gradient for model updating, we utilize Monte Carlo sampling to sample M transcription sequences, leading to M output character sequences which would be taken to compute the PCPG loss.</p><formula xml:id="formula_4">y (i) y (M)</formula><p>step as follows:</p><formula xml:id="formula_5">L CE = − log p (c 1 , c 2 , . . . , c U ) = − U ∑ u=1 log p (c u |c 1 , c 2 , . . . , c u−1 ) .<label>(3)</label></formula><p>where c u = 1, 2, ...,C is the predicted class label index at time step u, C is the number of categories to be predicted at each time step. In this paper, there are C = 40 categories at each time step u, including 26 alphabets, 10 numbers, the space, the begining, padding and ending symbol. At each time step u, the model would generate a prediction result according to the predictions at previous time steps 1, 2, ..., u − 1. In other words, the prediction at each time step would be decided by the predictions at the previous time step.</p><p>Besides the above optimization target, we also view the seq2seq model as an 'agent' in this paper that interacts with an external 'environment' which is corresponding to video frames, words or sentences here, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> with the parameters of the model denoted as θ , the model can be viewed as a policy p θ leading to an 'action' of choosing a character to output. At each time step u, the agent will get a new internal 'state', which is decided by the attention weight a u , the previous hidden state h u−1 and the predicted character y u . With this state, there would be an immediate reward r u to evaluate the reward and cost of predicting the character y u at the time step u. Then the training goal is to maximize the expected reward E y [R|p θ ] where R refers to the cumulative reward. In the following, we would describe the reward function and the principle to update the parameters in this paper in detail.</p><p>Reward function: In lip-reading tasks, the performance of the model is finally evaluated by CER or WER, both of which are usually obtained by the edit-distance or Levenshtein distance between the predicted word/sentence and the ground truth word/sentence. Here, we choose the negative CER of the whole sentence as the immediate reward r u to evalute the effect of the prediction at each time step u, which is defined as follows:</p><formula xml:id="formula_6">r u = − (ED (y 1:u , S) − ED (y 1:u−1 , S)) if u &gt; 1 − (ED (y 1:u , S) − |S|) if u = 1.<label>(4)</label></formula><p>where S refers to the ground truth text sequence and |S| is the length of S, ED(a, b) refers to the CER between characeter sequences a and b, which is computed by the edit-distance. An example of the process is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, where the ground-truth S is the sequence of 'bin blue at f two now', the old state y 1:u−1 (i.e. the previous decoding sequence) is 'bin blue at f t'. The model observes the old state and then takes an action of choosing a character 'w' to generate a new state y 1:u , corresponding to 'bin blue at f tw'. We would compute the reward r u at the time step u as Eq. (4) to evaluate the reward of predict 'w'.</p><p>Optimization: We use r u and R u to denote the immediate reward at time step u and the future expected reward at time step u respectively. Given the reward r u at each time step u, the future expected reward R u at current time step u would be computed as R u = ∑ U i=u γ U−i r i , where γ is the discount factor and U is the max length of the character sequence. The final reward for the whole sequence R is R = ∑ U u=1 R u , which is used to denote the cumulative reward of the whole prediction result from the begining to the end U (y 1:U ). Inspired by the properties of local perception and weight sharing of convolutional operation, we compute the PCPG based loss L PCPG as</p><formula xml:id="formula_7">L u = L original = −R u · logP(y u |·; θ ).<label>(5)</label></formula><formula xml:id="formula_8">L u = L mapping = L u−|k/2|:u+|k/2| · w = u+|k/2| ∑ i=u−|k/2| w i · L u . (6) L PCPG = U ∑ u=1 L u = u+|k/2| ∑ i=u−|k/2| U ∑ u=1 w i · L i .<label>(7)</label></formula><p>where w, k, θ , p θ denotes the kernel weights, the kernel size, the parameters' distribution of the model and the parameters.  The computation process is shown in <ref type="figure" target="#fig_5">Fig. 5</ref> (where k = 5, s = 1, w = [1/5, 1/5, 1/5, 1/5, 1/5]). To ensure the value of the gradient in PCPG at the same quantitative level as the traditional PG, we set ∑ k i=1 w i = 1 in our paper where k is the kernel size and w i is the kernel weight.</p><p>Finally, when a pair of prediction result is generated, and denoted as</p><formula xml:id="formula_9">X v = (x v 1 , x v 2 , . . . , x v T ), y = (y 1 , y 2 , .</formula><p>. . , y U ), we would use L combine as our final loss for our PCPG based seq2seq model which L combine can be defined as:</p><formula xml:id="formula_10">L combine = (1 − λ ) · L CE + λ · L PCPG ,<label>(8)</label></formula><p>where the λ is a scalar weight to balance the two loss functions.</p><p>In practice, it is always much difficult to integrate all possible transcriptions (y) to compute the above gradient of the expected reward. So we introduce Monte Carlo sampling here to sample M transcription sequences y <ref type="bibr" target="#b0">(1)</ref> , y <ref type="bibr" target="#b1">(2)</ref> , ..., y (M) to estimate the true gradient, which is shown in <ref type="figure">Fig. 4</ref>. So the gradient can be computed finally as:</p><formula xml:id="formula_11">∇ CE−θ = − 1 M M ∑ m=1 U (m) ∑ u=1 ∇ θ log p y (m) u |y (m) 1 , y (m) 2 , . . . , y (m) u−1 ; θ .<label>(9)</label></formula><formula xml:id="formula_12">∇ PCPG−θ ≈ − 1 M M ∑ m=1 U (m) ∑ u=1 u+|k/2| ∑ i=u−|k/2| w i ·R (m) u ∇ θ log P y (m) u |y (m) &lt;u , X v ; θ .</formula><p>(10) Therefore, the parameters could be updated as follows:</p><formula xml:id="formula_13">∂ L combine (θ ) ∂ θ ≈ (1 − λ ) · ∇ CE−θ + λ · ∇ PCPG−θ θ = ∂ L combine (θ ) ∂ θ · lr + θ<label>(11)</label></formula><p>where lr denotes the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compared with traditional Policy Gradient</head><p>Compared with the traditional policy gradient (PG), the PCPG has two more important operations as the convolutional operation: local perception and weights sharing shown as <ref type="figure" target="#fig_5">Fig. 5</ref>. In traditional PG, there is no concept of receptive field and the loss function could not make use of the context information. However, the lip-reading task can be considered as a sequence-to-sequence task and, the context is very important for accurate decoding results. With the existence of local perception and weights sharing, the prposed PCPG can make the proposed model to establish stronger semantic relationships among different time steps in the optimization process.</p><p>On the other hand, it is well-known that it is usually unstable for the models to train with RL <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b14">[15]</ref>. There will be a big gradient change due to the randomness in the process of deciding an action with a traditional PG algorithm. While in our work, we can find that the immediate loss L u at each time-step will be used to generate an average value based on multiple time-steps in PCPG, as shown in Eq. <ref type="bibr" target="#b6">(7)</ref>. At the same time, the existence of the overlapping parts has further made the local gradient value not change dramatically. So the model can obtain more favorable contextual loss constraints to make the convergence more stable and faster with our proposed PCPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate our method on three large-scale benchmarks, including both the word-level and sentencelevel lip-reading benchmarks. At the same time, we also discuss the effects of the pseudo-convolutional kernel's hyperparameters k, w, s on the performance of lip-reading through a detailed ablation study. By comparing with several other related methods, the advantages of our proposed PCPG based seq2seq model are clearly shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate our method on three datasets in total, including the sentence-level benchmark, GRID, and the large-scale word-level datasets, LRW and LRW-1000.</p><p>GRID <ref type="bibr" target="#b10">[11]</ref>, released in 2006, is a widely used sentencelevel benchmark for the lip-reading task <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b25">[26]</ref>. There are 34 speakers and each one speaks out 1000 sentences, leading to about 34,000 sentence-level videos in total. All the videos are recorded with a fixed clean single-colored background and the speakers are required to face the camera with the frontal view in the speaking process.</p><p>LRW <ref type="bibr" target="#b7">[8]</ref>, released in 2016, is the first large scale wordlevel lip-reading datasets. The videos are all collected from BBC TV broadcasts including several different TV shows, leading to various types of speaking conditions in the wild. Most current methods perform word-level tasks using classification-based methods. In our experiments, we try to explore the potential of seq2seq models for the word-level tasks but we also perform classification based experiments to evaluate the representation learned by our PCPG based model.</p><p>LRW-1000 <ref type="bibr" target="#b28">[29]</ref>, released in 2018, is a naturallydistributed large-scale benchmark for Mandarin word-level lip-reading. There are 1000 Mandarin words and more than 700 thousand samples in total. Besides owning a diversified range of speakers' pose, age, make-up, gender and so on, this dataset has no length or frequency constraints in the words, forcing the corresponding model to be robust and adaptive enough to the practical case where some words are indeed more or less longer or frequent than others. These properties make LRW-1000 very challenging for most lipreading methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>In our experiments, all the images are normalized with the overall mean and variance of the whole dataset. When fed into models, each frame is randomly cropped, but all the frames in a sequence would be cropped in the same random position for training. All frames are centrally cropped for validation and test.</p><p>Our implementation is based on PyTorch and the model is trained on servers with four NVIDIA Titan X GPUs, with 12GB memory of each one. We use Adam optimizer with an initial learning rate of 0.001. Dropout with probability 0.5 is applied to the RNN and the final FC layer of the model. For PCPG, we consider the following three situations: (1) k=1, s=1 which is degenerated to the usual REINFORCEMENT algorithm, i.e. the traditional PG algorithm in this case.</p><p>(2) k = 5, s = 5, which is sample PCPG version without overlapping parts. (3) k = 5, s = 1, which has 4-time steps as the overlapped part between twice adjacent computation. Here, we use CER and WER as our evaluation metrics. The ↑ denotes that larger is better while the ↓ denotes that lower is better. And the kernel's weight w is set to [1/5, 1/5, 1/5, 1/5, 1/5] by default.</p><p>In this paper, we would use the common cross-entropy loss L CE (shown as Eq. (3)) based seq2seq model as our baseline model. To evaluate the representation generated by our PCPG based method, we also perform classification based experiments on the LRW and LRW-1000 with a fully connected (FC) classifier based back-end. Specifically, we firstly trained the PCPG based seq2seq model in LRW and LRW-1000 with the loss L combine and obtained a video encoder and a GRU based decoder. Then we just fixed the encoder part to fix the representation learned by the PCPG based seq2seq model and then trained the FC based classifier with this representation, where the loss used for the FC classifier L Classi f y is defined as:</p><formula xml:id="formula_14">L Classi f y = − ∑ k P k log P k .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>By performing like the convolutional operation, the proposed PCPG has gained the propertis of receptive field (RF) and overlapping parts (OP). When there is no overlapping parts and the receptive field is equal to 1, the proposed PCPG based seq2seq just equals to the traditional PG based seq2seq. We summarize the detailed comparison of different choices of RF and OP in TABLE I, where '+RF' and '−RF' means RF &gt; 1(k &gt; 1) and RF = 1(k = 1), and '+OP' and '−OP' means OP &gt; 1(k &gt; s) and OP = 1(k = s) respectively. k and s is set to 5 by default when they are not 1.</p><p>From TABLE I, we can see that the model with traditional RL when there is no extra RF and OP (k = 1, s = 1) performs better than the traditional cross-entropy based seq2seq baseline. This shows that RL is an effective strategy to improve the seq2seq model for lip-reading. When both RF and OP are effective (k = 5, s = 1), the best performance is given, which proves the effectiveness of the proposed PCPG for robust lip reading. Besides the comparison with the baseline, we also present the loss curves in different settings during the training process on different benchmarks in <ref type="figure" target="#fig_7">Fig. 6 (a)</ref>, (b), (c). We can see that the PCPG can make the model learned more stably and also take less time to converge than the other two baselines.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect of the kernel size k in PCPG</head><p>Different kernel size k corresponds to the different receptive fields when computing the reward at each time step. In theory, choices of different size of the receptive fields should bring different effects on the final lip-reading performance. To explore the impact of different kernel size k, we perform several different experiments on the sentencelevel benchmark GRID, because the samples in sentencelevel are long enough to test the effects of different k. In this part, we keep s at 1 to make the model have more overlapping parts. To make the pseudo-convolutional kernel put the same attention to the reward at each time step, the k-dimensional</p><formula xml:id="formula_15">kernel weight w is set to [ 1 k , 1 k ,. . . , 1 k ].</formula><p>The results are shown in TABLE IIa. As is shown, we get the best result when k = 3. When k is too small (such as k = 2), the context considered to compute the reward at each time step is not much enough and so there is an indeed improvement but not too much. When k is too big (such as k = 7), the context considered at each time step is so much that it may cover up and so weaken the contribution of the current time step. But no matter which value k is, the performance is always better than the baseline k = 1 when k &gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effect of the kernel weight w in PCPG</head><p>Different choices of the kernel weight means the different weight values would be put on the contextual time steps when computing the reward at each targeted time step. In this experiments, we fix k to the above-optimized value 3 and also keep s to 1 to evaluate and compare the effect of different kernel weights w on the sentence-level benchmark GRID, as shown in TABLE IIb. From this table, we can easily see that the performance with different kernel weights is almost kept at the same level where the gap between the best and the worst is no more than 1%, which shows the robustness of the PCPG based seq2seq model with respect to the value of the weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Evaluation of the learned representation</head><p>To evaluate the representation learned by the PCPG based seq2seq model, we fixed the video encoder with the same parameters as the learned PCPG based seq2seq model, and then train an FC based classifier to perform sequence-level classification back-end on the word-level lip-reading dataset LRW and LRW-1000. The results are shown in TABLE III. From this table, we can see that when we use the representation learned by the PCPG based seq2seq model, there is a clear improvement. And when training the representation and the FC based classifier together, the improvement is getting more obvious. We also compare with other sequence-level classification based methods in <ref type="table" target="#tab_0">Table IV</ref>, which also clearly show the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison with state-of-the-art methods</head><p>Besides the above thorough comparison and analysis of our proposed PCPG based seq2seq in different settings, we also perform a comparison with other related state-ofthe-art methods, including both sentence-level and wordlevel methods. Please note that we have not counted in the methods using large-scale extra data except the published dataset itself for fair comparison here. As shown in TABLE V, we can see that our proposed methods achieve state-ofthe-art performance in the decoding tasks, no matter with or without beam search (BM). As shown in TABLE IV, in the classifying tasks, our method has also achieved a significant improvement, especially on the LRW-1000 where the improvement is about 0.5 percents which is always hard to obtain for the difficulty of this dataset. These results clearly prove the effectiveness of the proposed PCPG module and the PCPG based seq2seq model for lip-reading. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Method Accuracy ↑ LRW <ref type="bibr" target="#b19">[20]</ref> 82.0% <ref type="bibr" target="#b23">[24]</ref> 83.0% <ref type="bibr" target="#b22">[23]</ref> 82.9% <ref type="bibr" target="#b26">[27]</ref> 83.34% ours 83.5%</p><p>LRW-1000 <ref type="bibr" target="#b26">[27]</ref> 36.91% <ref type="bibr" target="#b28">[29]</ref> 38.19% ours 38.70% In this work, we proposed a pseudo-convolutional policy gradient (PCPG) based seq2seq model for the lip-reading task. Inspired by the principle of convolutional operation, we consider to extend the policy gradient's receptive field and overlapping parts in the training process. We perform a thorough evaluation of both the word-level and the sentence-level dataset. Compared with the state-of-the-art results, the PCPG outperforms or equals to the state-of-the-art performance, which verifies the advantages of the PCPG. Moreover, the PCPG can also be applied to other seq2seq tasks, such as machine translation, automatic speech recognition, image caption, video caption and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Examples for lip-reading. The examples in A-C are randomly sampled from the GRID, LRW and the LRW-1000 dataset, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of our proposed PCPG based seq2seq model for lip-reading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The CNN and RNN based video encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Computing the PCPG loss for Optimization: bin bluc at f tto now : bin blue at f two now : biz alue ai f tlo nao m=i m=1 m=M r i2 r i1 y i1 , P(y i1 |·) y i2 , P(y i2 |·)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>The computation process of the PCPG based loss function, where the kernel size k is 5, the stride s is 1, and the kernel weights w is [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a). Loss v.s. iteration on GRID.(b). Loss v.s. iteration on LRW.(c). Loss v.s. iteration on LRW-1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>loss v.s. iteration on GRID, LRW, LRW-1000. The three vertical lines (blue, green, red) in (a), (b) and (c) refer to the state when the model converges under different conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The results of the seq2seq model with different settings on three lip-reading datasets. (RF: receptive field, OP: overlapping parts)</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Case</cell><cell>CER ↓</cell><cell>WER ↓</cell></row><row><cell></cell><cell>L CE (baseline)</cell><cell>/</cell><cell>8.4%</cell><cell>18.3%</cell></row><row><cell>GRID</cell><cell>− RF and − OP + RF and − OP</cell><cell>k=1, s=1 k=5, s=5</cell><cell>7.6% 6.9%</cell><cell>16.6% 15.3%</cell></row><row><cell></cell><cell>+ RF and + OP</cell><cell>k=5, s=1</cell><cell>5.9%</cell><cell>12.3%</cell></row><row><cell></cell><cell>L CE (baseline)</cell><cell>/</cell><cell>17.5%</cell><cell>28.3%</cell></row><row><cell>LRW</cell><cell>−RF and− OP + RF and − OP</cell><cell>k=1, s=1 k=5, s=5</cell><cell>15.2% 15.0%</cell><cell>24.8% 26.5%</cell></row><row><cell></cell><cell>+ RF and + OP</cell><cell>k=5, s=1</cell><cell>14.1%</cell><cell>22.7%</cell></row><row><cell></cell><cell>L CE (baseline)</cell><cell>/</cell><cell>52.1%</cell><cell>68.2%</cell></row><row><cell>LRW-1000</cell><cell>− RF and − OP + RF and − OP</cell><cell>k=1, s=1 k=5, s=5</cell><cell>51.4% 51.6%</cell><cell>67.7% 67.2%</cell></row><row><cell></cell><cell>+ RF and + OP</cell><cell>k=5, s=1</cell><cell>51.3%</cell><cell>66.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="2">: Evaluation of parameters k and w. (a) results</cell></row><row><cell cols="2">with different k (s = 1); (b) results with different choices of</cell></row><row><cell>w (k=3, s=1).</cell><cell></cell></row><row><cell>Kernel size k=1 k=2 k=3 k=5 k=7 (a) Evaluation on k WER ↓ 16.6% 16.0% 12.1% 12.3% 14.8%</cell><cell>Kernel weight w=[1/3,1/3,1/3] w=[1/4,1/2,1/4] w=[1/3,1/2,1/6] w=[1/6,1/2,1/3] (b) Evaluation on w WER ↓ 12.1% 11.9% 12.7% 12.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Evaluation of the learned representation with classification based back-end on LRW and LRW-1000. (baseline: without any extra pretraining, FE: fix encoder, TE: train encoder, TC: train classifier.)</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Accuracy ↑</cell></row><row><cell></cell><cell>baseline</cell><cell>82.1%</cell></row><row><cell>LRW</cell><cell>FE and TC</cell><cell>82.4%</cell></row><row><cell></cell><cell>TE and TC</cell><cell>83.5%</cell></row><row><cell></cell><cell>baseline</cell><cell>37.8%</cell></row><row><cell>LRW-1000</cell><cell>FE and TC</cell><cell>38.5%</cell></row><row><cell></cell><cell>TE and TC</cell><cell>38.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison with other classification based on methods on LRW and LRW-1000.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Comparison with other decoding based methods on GRID, LRW, and LRW-1000. For LRW and LRW-1000, Accuracy equals to 1−WER. (The work in<ref type="bibr" target="#b1">[2]</ref> is published in a poster from the University of Oxford.)</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell></cell><cell>WER ↓</cell></row><row><cell></cell><cell>[26]</cell><cell></cell><cell>20.4%</cell></row><row><cell></cell><cell>[12]</cell><cell></cell><cell>13.6%</cell></row><row><cell>GRID</cell><cell cols="2">[4] (No BM) ours (No BM)</cell><cell>13.6% 11.9%</cell></row><row><cell></cell><cell cols="2">[4] (With BM)</cell><cell>11.4%</cell></row><row><cell></cell><cell cols="2">ours (With BM)</cell><cell>11.2%</cell></row><row><cell cols="4">(a) Decoding results on GRID</cell></row><row><cell>Dataset</cell><cell cols="3">Method Accuracy ↑</cell></row><row><cell>LRW</cell><cell>[2] ours</cell><cell cols="2">76.2% 78.5%</cell></row><row><cell>LRW-1000</cell><cell>ours</cell><cell cols="2">33.1%</cell></row><row><cell cols="4">(b) Decoding results on LRW and LRW-1000</cell></row><row><cell cols="3">V. CONCLUSION</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning for lip reading. Poster Presentation in the University of Oxford</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep lip reading: a comparison of models and an online application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1806.06053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lipnet: End-to-end sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<editor>IEEvaluation@ACL</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Safe model-based reinforcement learning with stability guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Schoellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading in profile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to lip read words by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="76" to="85" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">D</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="page" from="2421" to="2425" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic stream weighting for turbo-decoding-based audiovisual asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal multimodal learning in audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3574" to="3582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Control-theoretic analysis of smoothness for stability-certified reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lavaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Decision and Control (CDC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6840" to="6847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparing visual features for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-J</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving stability in deep reinforcement learning with weight averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nikishin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shvechikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1511.06732</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selfcritical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1179" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of audiovisual word recognition using residual networks and lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="page" from="22" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining residual networks with lstms for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno>abs/1703.04105</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lipreading with long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6115" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-grained spatio-temporal modeling for lip-reading. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beamsearch optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lrw-1000: A naturally-distributed large-scale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
