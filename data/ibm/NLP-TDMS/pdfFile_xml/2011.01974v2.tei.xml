<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi Projection Fusion for Real-time Semantic Segmentation of 3D LiDAR Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yara</forename><forename type="middle">Ali</forename><surname>Alnaggar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Informatics Science</orgName>
								<orgName type="institution">Nile University Giza</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Afifi</surname></persName>
							<email>moh.afifi@nu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Informatics Science</orgName>
								<orgName type="institution">Nile University Giza</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Amer</surname></persName>
							<email>k.amer@nu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Informatics Science</orgName>
								<orgName type="institution">Nile University Giza</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhelw</surname></persName>
							<email>melhelw@nu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Informatics Science</orgName>
								<orgName type="institution">Nile University Giza</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi Projection Fusion for Real-time Semantic Segmentation of 3D LiDAR Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation of 3D point cloud data is essential for enhanced high-level perception in autonomous platforms. Furthermore, given the increasing deployment of LiDAR sensors onboard of cars and drones, a special emphasis is also placed on non-computationally intensive algorithms that operate on mobile GPUs. Previous efficient state-of-the-art methods relied on 2D spherical projection of point clouds as input for 2D fully convolutional neural networks to balance the accuracy-speed trade-off. This paper introduces a novel approach for 3D point cloud semantic segmentation that exploits multiple projections of the point cloud to mitigate the loss of information inherent in single projection methods. Our Multi-Projection Fusion (MPF) framework analyzes spherical and bird's-eye view projections using two separate highly-efficient 2D fully convolutional models then combines the segmentation results of both views. The proposed framework is validated on the SemanticKITTI dataset where it achieved a mIoU of 55.5 which is higher than state-of-the-art projection-based methods RangeNet++ [23] and PolarNet [44] while being 1.6x faster than the former and 3.1x faster than the latter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Currently, Light Detection and Ranging (LiDAR) sensors are widely used in autonomous navigation systems where captured 3D point cloud data provides a rich source of information on the surrounding scene. Analyzing such data with deep learning models has gained a lot of attention in the research community especially for extracting semantic information to improve navigation accuracy and safety. In this case, semantic segmentation algorithms assign a label for each point in the 3D point cloud representing different classes of objects in the scene.</p><p>Convolutional Neural Networks (CNNs) have achieved * equal contribution <ref type="figure" target="#fig_1">Figure 1</ref>: Computed scans per second vs. mIoU score on the test set of SemanticKITTI dataset using state-of-theart projection-based methods. Our framework achieves the highest mIoU score; in addition, it is 3.1 and 1.6 times faster than PolarNet and RangeNet53++, respectively.</p><p>state-of-the-art results in semantic segmentation tasks with fully convolutional architectures trained on huge amounts of labelled RGB data and making clever use of transfer learning. However, the same success has not yet been achieved in semantic segmentation of point cloud data due to lack of large annotated datasets. Furthermore, since point cloud semantic segmentation models are typically deployed on devices with limited computational capabilities onboard of mobile platforms (e.g. cars or drones), there is a need for high throughput while sustaining high accuracy to ensure the platform has enough time to make correct decisions.</p><p>There are two main approaches in the literature to tackle the task of semantic segmentation of 3D point clouds. The first applies 3D CNN models either on the raw cloud data points <ref type="bibr" target="#b26">[27]</ref> or after transforming the points into 3D volumetric grid representations <ref type="bibr" target="#b37">[38]</ref>. This incurs high computational costs <ref type="bibr" target="#b1">[2]</ref> and hence not suitable for real-time systems. The second approach applies 2D CNN models to 2D projections of the 3D point cloud based on either bird's-eye view <ref type="bibr" target="#b28">[29]</ref> or spherical view <ref type="bibr" target="#b39">[40]</ref>. Currently, state-of-the-art methods such as RangeNet++ <ref type="bibr" target="#b22">[23]</ref> applies a Fully Convolutional Neural Network (FCNN) on a spherical projection of the point cloud. However, there is an inevitable loss of information due to the projection operation which can limit model performance especially for distant points. In this paper, we introduce a novel framework for enhanced online semantic segmentation of 3D point clouds by incorporating multi-view projections of the same point cloud which results in an improved performance compared to singleprojection models while attaining real-time performance. The contributions of this work can be summarized as follows. First, a novel MPF framework that utilizes multi-view projections and fusion of input point cloud to make up for the loss of information inherent in single projection methods. Second, the MPF framework processes spherical and bird's-eye projections using two independent models that can be selected to achieve optimum performance for a given platform (road vehicle, aerial drone, etc.) and/or deployed on separate GPUs. Third, the framework is scalable and, despite using only two projections in the current work, it can be directly extended to exploit multiple projections.</p><p>Incorporating information from multiple projections of 3D data has been used before in other domains to improve performance. Mortazi et al. <ref type="bibr" target="#b24">[25]</ref> used a single 2D encoderdecoder CNN for CT-scan segmentation to parse all 2D slices in X, Y and Z directions. Chen et al. <ref type="bibr" target="#b3">[4]</ref> used multiple 2D encoder CNN on spherical and bird's-eye views for the task of 3D object detection in point cloud data. However, to the best of our knowledge this setup has not been used before in semantic segmentation of 3D point clouds. This is primarily due to the added computational overhead of having the same complex network architecture for multiple views and back projection of results to the original point cloud space to compute point-level predictions (unlike <ref type="bibr" target="#b3">[4]</ref> who only outputs 3D bounding boxes). The paper is organized as follows: Section 2 overviews related work, Section 3 describes the proposed framework and Section 4 presents obtained experimental results as well as an ablation study of our framework. Section 5 concludes the paper and points out future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Image Segmentation</head><p>Semantic image segmentation has attracted a lot of attention in recent years following the success of deep CNN models, such as AlexNet <ref type="bibr" target="#b15">[16]</ref>, VGGNet <ref type="bibr" target="#b32">[33]</ref>, and ResNet <ref type="bibr" target="#b9">[10]</ref> in image classification. The task aims at predicting pixel-level classification labels in order to have more precise information on objects in input images. One of the earliest work in this area is based on using Fully Convolutional Neural Networks (FCNNs) <ref type="bibr" target="#b20">[21]</ref> where the model can assign a label for each pixel in the image in a single forward pass by extracting features using multi-layer encoder (in this case it was VGG model <ref type="bibr" target="#b32">[33]</ref>) and apply up-sampling on these features combined with 1x1 convolution layer to classify each pixel. The idea was extended by Noh et al. <ref type="bibr" target="#b25">[26]</ref> and Badrinarayanan et al. <ref type="bibr" target="#b0">[1]</ref> by using a multi-layer decoder to transform extracted features into image space with the needed pixel labels. Ronneberger et al. <ref type="bibr" target="#b29">[30]</ref> introduced the UNet architecture where skip connections between encoder and decoder layers further improve segmentation results. In addition to advances in model architecture design, having large annotated datasets for semantic segmentation tasks, such as Microsoft's COCO <ref type="bibr" target="#b19">[20]</ref>, and utilizing transfer learning from image classification models pre-trained on large datasets such as ImageNet <ref type="bibr" target="#b6">[7]</ref> can significantly improve semantic segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic Segmentation of 3D Point Cloud</head><p>Most 3D point cloud semantic segmentation algorithms employ a Fully Convolutional Neural Network (FCNN) in a way similar to 2D semantic image segmentation but with the difference of how the FCNN is applied to 3D structures. These algorithms can essentially be grouped into two categories. The first category includes models that use 3D convolutions as in SegCloud <ref type="bibr" target="#b37">[38]</ref> where a 3D FCNN is applied to point cloud after voxelization and transformation into homogeneous 3D grid. However, 3D convolutions are computationally intensive and 3D volumes of point clouds are typically sparse. Other models have special convolution layers for 3d points in order to process a point cloud in its raw format with examples including PointNet++ <ref type="bibr" target="#b27">[28]</ref>, Tangent-Conv <ref type="bibr" target="#b36">[37]</ref> and KPConv <ref type="bibr" target="#b38">[39]</ref>. Recent work in RandLA <ref type="bibr" target="#b11">[12]</ref> improves run-time while maintaining a high segmentation accuracy compared to previously mentioned methods.</p><p>The second category of algorithms apply 2D FCNN models after projecting 3D point clouds onto 2D space. In SqueezeSeg <ref type="bibr" target="#b39">[40]</ref> and SqueezeSegV2 <ref type="bibr" target="#b40">[41]</ref>, spherical projection is performed on point cloud then 2D encoder-decoder architecture is applied. RangeNet++ <ref type="bibr" target="#b22">[23]</ref> improves segmentation results by using a deeper FCNN model and employing post-processing using K-Nearest Neighbour (KNN). Algorithms in this category not only improve inference time but also enhance segmentation accuracy by capitalizing on the success of CNNs in 2D image segmentation. Different projections can also be used as in VolMap <ref type="bibr" target="#b28">[29]</ref> which uses Cartesian bird's-eye projection and PolarNet <ref type="bibr" target="#b43">[44]</ref> which uses polar bird's-eye projection combined with ring convolutions. The proposed framework advances current state-of- the-art of projection based methods in point cloud segmentation by making use of both spherical and Cartesian bird'seye projections to reduce the loss of information stemming from using a single projection. Finally, it is worth mentioning that one of the key challenges in 3D point cloud semantic segmentation is the lack of large labeled point cloud datasets. The current benchmark dataset is SemanticKITTI <ref type="bibr" target="#b1">[2]</ref> which has around 43K frames collected using 360 Velodyne LiDAR sensor <ref type="bibr" target="#b17">[18]</ref>. Other datasets are either generated synthetically using simulation environments such as Virtual Kitti <ref type="bibr" target="#b7">[8]</ref> or have small number of samples such as Paris-Lille-3D [31].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Efficient Deep Learning Architectures</head><p>With the impressive results achieved by deep learning models in detection and classification tasks, there is a growing need for efficient models to be deployed on embedded devices and mobile platforms. A number of network architectures that achieve high classification accuracy while having real-time inference have been recently proposed. For instance, MobileNetV1 <ref type="bibr" target="#b10">[11]</ref> uses depth-wise separable convolutions while ShuffleNet <ref type="bibr" target="#b42">[43]</ref> utilizes group convolutions and channel shuffling to reduce computations. Mo-bileNetV2 <ref type="bibr" target="#b31">[32]</ref> achieves improved accuracy while maintaining fast inference by using inverted residual blocks. Although these models are designed for classification tasks, they can be used in the context of semantic segmentation as encoders in FCNN models in order to benefit from their efficient architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Segmentation Loss</head><p>Segmentation models, regardless of their input, are initially trained using classification losses such as Cross Entropy loss or Focal loss <ref type="bibr" target="#b18">[19]</ref> because the end goal is to assign a label for each pixel (or point). However, such losses lack the global information of predicted and target object masks. Therefore, research work has been conducted to develop a loss function that penalizes the difference between predicted and ground-truth masks as a whole. Milletari et al. <ref type="bibr" target="#b23">[24]</ref> developed a soft version of Dice coefficient with continuous probabilities instead of discrete 0 or 1 values while Berman et al. <ref type="bibr" target="#b2">[3]</ref> introduced Lovasz Softmax loss which is a function surrogate approximation of the Jaccard coefficient <ref type="bibr" target="#b13">[14]</ref>. Currently, Lovasz Softmax loss is the stateof-the-art segmentation loss and is usually combined with classification loss to have a penalty on both local and global information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We developed a Multi-Projection Fusion (MPF) framework for semantic segmentation of 3D point clouds that relies on using two 2D projections of the point cloud where each projection is processed by an independent FCNN model. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the proposed pipeline starts by feeding the input point cloud to two branches, one responsible for spherical view projection and the other for n is the repetition number for a sequence of layers in block, t is block expansion factor, c is the number of output channels, s is block stride.</p><p>bird's-eye view projection. Each branch applies semantic segmentation on the projected point cloud. Subsequently, predictions from the two branches are fused to produce the final prediction. It is assumed that the input point cloud is collected by a LiDAR sensor that returns point coordinates x, y, z values and remission of returned signals rem, e.g Velodyne HDL-64E <ref type="bibr" target="#b17">[18]</ref>. In the following sub-sections, we present details of each block in the two branches of the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spherical View Projection</head><p>This section explains the process of transforming a 360 • point cloud into a 2D spherical image that is fed into subsequent Spherical View Model block, as proposed by <ref type="bibr" target="#b39">[40]</ref>. At the start, the 3D point cloud is mapped from Euclidean space (x, y, z) to Spherical space (θ, φ, r) by applying Equation <ref type="bibr" target="#b0">1</ref>.</p><formula xml:id="formula_0">     θ φ r      =      arcsin( z √ x 2 +y 2 +z 2 ) arctan(y, x) x 2 + y 2 + z 2      (1)</formula><p>Subsequently, the points are embedded into a 2D spherical image with dimensions (H, W ) by discretizing points' θ and φ angles using Equation 2:</p><formula xml:id="formula_1">  u v   =   1 2 [1 − φπ −1 ]w [1 − (θ + f up )f −1 ]h   ,<label>(2)</label></formula><p>where u and v represent point indices in the spherical image and f = f up + f down is the sensor's vertical field-of-view.  <ref type="table">Table 2</ref>: Bird's-Eye View Model Architecture. c is the number of output channels and s is layer stride.</p><p>The mapping and discretization steps may result in some 3D points sharing the same u and v values. To mitigate this condition, 3D points that are closer to LiDAR are given priority to be represented in the 2D image by ordering the points descendingly based on their range value. The ordered list of points will be embedded into the 2D spherical image using its corresponding u and v coordinates. By the end of this process, the resulting 2D spherical image will have five channels corresponding to distinct point features: x, y, z, r and remission rem which is analogous to RGB images that have three channels, one for each color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spherical-View Model</head><p>The Spherical-View Model is a deep learning segmentation model based on FCNN architecture with encoder and decoder parts. The network encoder utilises MobileNetV2 <ref type="bibr" target="#b31">[32]</ref> as lightweight backbone that provides real-time performance on mobile devices. The backbone is composed of a sequence of basic building blocks called inverted residual blocks that form bottlenecks with residuals. The first and last bottleneck layer expands and compresses input and output tensors, respectively. The intermediate layers are highcapacity layers responsible for extracting high-level information from the expanded tensors.</p><p>For network decoder, we apply two learnable upsampling layers known as transposed convolution layers. The first layer upsamples the input tensor 8 times and the second layer 4 times. At the end, we add convolution layer and softmax logits to output semantically segmented image. Furthermore, dropout layers are added as regularization. <ref type="table">Table 1</ref> provides details of the Spherical-View Model layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bird's-Eye View Projection</head><p>The second projection in our framework is the 2D bird'seye view projection. It uses the x and y coordinates of each point and collapses the 3D cloud along the z dimension. The 3D point cloud is thus projected on the x − y plane that is discretized using a rectangular grid with a defined width and height. For each cell in the grid, we keep at most one projected point corresponding to the point that has the maximum z value among all points projected onto that cell. Points that get projected outside the boundaries of the grid are discarded. Finally, the grid is converted into a 4-channel image where each pixel in the image represents a cell in the grid. Four cell attributes are extracted to form the 4 channels of the image, namely x, y, z, and remission rem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Bird's-Eye View Model</head><p>Although MobileNetV2 <ref type="bibr" target="#b31">[32]</ref> is highly efficient, using it twice for both views will decrease the overall throughput. Since the MPF framework allows using independent network in each processing branch, we decided to use a network with fewer parameters compared to MobileNetV2 <ref type="bibr" target="#b31">[32]</ref>. Specifically, a light weight modified version of the UNet <ref type="bibr" target="#b29">[30]</ref> encoder-decoder architecture is used for segmentation of bird's-eye view images. As shown in table 2, the encoder consists of 2 downsampling convolutional blocks and the decoder consists of 2 upsampling convolutional blocks with skip connections between corresponding encoder and decoder blocks. In our experiments it is shown that it is sufficient to use only two blocks in both encoder and decoder which significantly improves network inference efficiency. Each block consists of two 2D convolution layers with kernel size 3 followed by max pooling for encoder and preceded by bi-linear upsampling for decoder. We use 2D batch normalization <ref type="bibr" target="#b12">[13]</ref> followed by ELU <ref type="bibr" target="#b4">[5]</ref> non-linearity between successive convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Post-Processing</head><p>The goal of the Post-Processing step is to get semantic labels for all points in the input 3D point cloud based on the semantically-segmented images produced by the Spherical and Bird's-Eye View Models. The segmentation results for each pixel are softmax probability scores for each of the 20 possible classes. The segmented 3D point cloud is computed as follows: each 3D point is projected onto the 2D segmented image and a 2D square window centered around the projection location is calculated. Then, a weighted vote over all classes is performed by computing a weighted sum of the softmax probabilities of all pixels inside the window, where the weights are inversely proportional to the distance between the 3D point under consideration and the 3D points represented by pixels in the window. In particular, we use a Gaussian function with zero mean and fixed standard deviation to compute the weight corresponding to each distance. The output of this step is a vector of scores for each point in the 3D point cloud. Finally, the score vector is normalized by dividing by the number of points that contributed in the voting. This step is necessary because both views have sparse pixels and the number of pixels inside a window can vary considerably from one view to another. The details of the algorithm are shown in Algorithm 1. During </p><formula xml:id="formula_2">u ∈ [u − k/2 : u + k/2 ] do foreach v ∈ [v − k/2 : v + k/2 ] do if Ixyz[u , v ] is not sparse then d = get distance(Pxyz[i], Ixyz[u , v ]) weight = exp(−d 2 /2σ 2 ) Scores[i]+ = weight * I sof tmax [u , v ] M + = 1 end end end Scores[i] = Scores[i]/M end return Scores</formula><p>implementation, we eliminated the use of all loops and used fully-vectorized code which run on GPU for fast processing. Our proposed post-processing is similar to KNN postprocessing in <ref type="bibr" target="#b22">[23]</ref> however it uses soft voting with softmax probabilities instead of hard voting and takes the vote of non-sparse pixels only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Fusion</head><p>After post-processing the outputs of the spherical and bird's-eye networks, we get two vectors of scores for each point, one vector for each view. These vector are simply added to get the final score vector for each 3D point. The class that has highest score is selected as the predicted label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We trained both Spherical View and Bird's-Eye View networks on the SemanticKITTI dataset <ref type="bibr" target="#b1">[2]</ref>   <ref type="table">Table 3</ref>: mIoU scores on SemanticKITTI test set 1 . Our proposed MPF utilizes smaller number of parameters compared to projection-based methods Rangenet53++ <ref type="bibr" target="#b22">[23]</ref> and PolarNet <ref type="bibr" target="#b43">[44]</ref> while maintaining higher segmentation results.</p><p>point-wise semantic label annotations for all scans in the KITTI odometry dataset <ref type="bibr" target="#b8">[9]</ref>. The dataset consists of over 43,000 360 • LiDAR scans, divided into 11 training sequences for which ground-truth annotations are provided and 11 test sequences. We used sequence 08 as our validation set and trained our networks on the other 10 sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Spherical View Model</head><p>The Spherical View Model was trained from scratch using a combined objective function of Focal <ref type="bibr" target="#b18">[19]</ref> and Lovász-Softmax <ref type="bibr" target="#b2">[3]</ref> losses:</p><formula xml:id="formula_3">L spherical view model = L f ocal + L lovasz L f ocal = − n i (1 − p n,i ) γ log(p n,i )</formula><p>where p n,i is the probability of the ground-truth class at image n and pixel i and γ is the focusing factor. For optimization, SGD with 0.9 momentum, 0.0001 weight decay and mini-batch of 8 was used. We also used Cosine Annealing scheduler with warm restart <ref type="bibr" target="#b21">[22]</ref> for 5 cycles with learning rate that starts at 0.05 and decreases till 0, and cycle length of 30 epochs. Following similar works, the model was trained with image sizes of 64x512, 64x 1024 and 64x2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Bird's-Eye View Model</head><p>To train the Bird's-Eye View Model, we used SGD with one-cycle <ref type="bibr" target="#b33">[34]</ref> learning and momentum annealing strategy.</p><p>Learning rate was cycled between 0.001 and 0.1, while momentum was cycled inversely to learning rate between 0.85 and 0.95. The model was trained for 30 epochs using cross entropy loss and Lovász-Softmax loss: where p n,i is the probability of the ground-truth class at image n and pixel i. The model was trained only using images of size 256x256.</p><formula xml:id="formula_4">L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Post-Processing</head><p>For post-processing, we used square kernels of size 3 for both views to extract local windows centered at projected pixels. Network predictions for sparse pixels (pixels that have no corresponding 3D points) are ignored. Then to compute the weight for each pixel, a Gaussian function is used as described in Section 3.5 with σ = 1 to compute the weights corresponding to different distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Augmentation</head><p>Data augmentation is considered an effective tool to improve model generalization to unseen data. We therefore apply Spherical-View augmentation and Bird's-Eye View augmentation. In the former, the 3D point cloud is processed by pipeline of four randomly executed (with 0.5 probability) Affine transformations: translation parallel to y-axis, rotation about z-axis, scaling around the origin and flipping around y-axis. Augmentation is also applied after projecting the 3D point cloud onto 2D space. A CoarseDropout function by <ref type="bibr" target="#b14">[15]</ref> is used to drop pixels randomly by 0.005 probability and the image is cropped to half on the horizontal axis starting from a randomly sampled coordinate. In Bird's-Eye View augmentation and prior to projecting the point cloud on the x − y plane, the cloud is transformed by applying random rotation around the z axis, scaling by a uniformly sampled factor, translating in the x and y directions and finally a random noise sampled from a normal distribution with 0 mean and 0.2 standard deviation is applied to the z channel. Each of these operations is applied with probability of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>This section describes the performance of the proposed Multi-Projection Fusion (MPF) framework. <ref type="table">Table 3</ref> presents the quantitative results obtained by the proposed approach versus state-of-the-art point cloud semantic segmentation methods on the SemanticKITTI test set over 19 classes. SemanticKITTI uses Intersection-over-Union (IoU) metric to report per-class score:</p><formula xml:id="formula_5">IoU = |P ∩ G| |P ∪ G|<label>(3)</label></formula><p>where P and G are class points predictions and ground truth, respectively. The mean IoU (mIoU) over all classes is also reported. The scans per second rate are reported by measuring combined projection and inference time (unlike PolarNet <ref type="bibr" target="#b43">[44]</ref> which reports inference time only) on a single NVIDIA GeForce GTX 1080 Ti GPU card. The results demonstrate that the proposed MPF framework achieves the highest mIoU score across all baseline projection-based methods while also having higher scans per second rate and less parameters compared to RangeNet++ <ref type="bibr" target="#b22">[23]</ref> and Polar-Net <ref type="bibr" target="#b43">[44]</ref>. Although 3D-methods achieves the highest mIOU scores, it lags significantly in running time as shown in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b11">[12]</ref>. <ref type="figure" target="#fig_1">Figure 1</ref> show qualitative examples from the Se-manticKITTI validation sequence where our proposed approach outperforms RangeNet53++ <ref type="bibr" target="#b22">[23]</ref> in segmenting objects located far from LiDAR position. This is attributed to using two independent complementary projections and intelligently fusing segmentation results of each projection. It is worth mentioning that data augmentation described in Section 4.3 improved validation mIoU by 2.7% for spherical view model, 7.7% for bird's-eye view model and an overall improvement by 7.2% as shown in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Post-Processing</head><p>We studied the effect of different standard deviation values on the performance of the Gaussian function used presented in Algorithm 1. A grid search was used to jointly compute the best values for the standard deviation used in both spherical view and bird's-eye view post-processing. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the results show that setting all standard devia-   tion values to 1 when using Manhattan distance consistently yields the best results. We also tried larger sliding window sizes but it did not improve the score and reduced the overall FPS of our framework. The best configurations from this study were used for test submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Multi-Projection Fusion</head><p>We conducted several ablation studies to demonstrate the efficacy of employing multiple projections as opposed to single projection. In the first experiment, we investigate the mIoU score of two established spherical projection models, SqueezeSeg <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> and RangeNet <ref type="bibr" target="#b25">[26]</ref>, with and without the incorporation of the bird's-eye projection in table 5. The results demonstrate that fusion of information from more than one projection significantly enhances the obtained segmentation results despite using simple network model and projected low-resolution images for the bird's-eye view. It can thus be concluded that using multiple projections of the same point cloud does improve overall segmentation results by providing additional information for model adaptation.</p><p>The second experiment shows that fusion helps to improve mIoU score for both near and far points as seen in <ref type="figure" target="#fig_5">Figure 5a</ref>. Since the majority of LiDAR points are typically at distances &lt; 20 meters, as shown in <ref type="figure" target="#fig_5">Figure 5b</ref>, a slight improvement of framework performance for near distance points can have a significant impact on the overall IoU results. It also observed that the performance of point cloud   <ref type="table">Table 5</ref>: Results of adding the bird's-eye projection to single spherical projection models on SemanticKITTI validation set.</p><p>segmentation using spherical view projections degrades for points farther away from the LiDAR position. Unlike spherical view images, the bird's-eye view images use Cartesian coordinates which means the pixels of the image correspond to uniform 3D elements whose spatial resolution does not change as we go farther from the LiDAR position which makes segmentation results independent of point distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>This paper presented a novel multi-projection fusion framework for point cloud semantic segmentation by using spherical and bird's-eye view projections and fusion of results using soft voting mechanism. The proposed framework achieves improved segmentation results over single projection methods while having higher throughput. Future work directions include combining both projections into a single multi-view unified model and investigating using more than two projections within the framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Proposed MPF framework overview. The framework takes as input a 3D point cloud that undergoes a series of operations in the Spherical View Branch and the Bird'e-Eye View branch. Each branch is composed of three main processing blocks where the first block transforms the 3D point point cloud into its respective 2D projection. The second block, Spherical or Bird's-Eye View Model, predicts segmentation of the projected 2D image with a FCNN model where each view has its own model. The third block, Post Processing, further processes the semantically-segmented projected view and assigns to each point in the input cloud its corresponding softmax probabilities. Finally, information from the two branches are fused by the Fusion block to produce the final semantic label of each point in the 3D point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Post-Processing Algorithm Post-Processing Parameters Number of classes: C Number of points: N Size of the projection image: HxW Size of the sliding window : KxK Standard deviation for the gaussian function: σ Data Point cloud coordinates Pxyz. Size = N x3 Projection image Ixyz. Size = HxW x3 Output of the segmentation network I sof tmax . Size = HxW xC Output Scores. Class scores for each point in the original cloud. Size = N xC Algorithm foreach i ∈ [1 : N ] do // Get the pixel to which this point is projected u, v = get projection indices(Pxyz[i]) // Initialize all class scores for the i'th points to zeros Scores[i] = zeros(C) // Initialize number of non-sparse pixels to zero M = 0 // Loop over pixels currently inside the sliding window foreach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>bird s−eye view model = L cross entropy + L lovasz L cross entropy = − n i log(p n,i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative segmentation results on SemanticKITTI validation sequence 8 that compare our MPF framework against RangeNet53++. Top: Our framework correctly segmented the 'person' labeled in red. Middle: Our framework segmented the 'bicyclist' object, a class rarely representated in the dataset, much better than RangeNet53++. Bottom: the MPF framework correctly segmented the 'other-vehicle' object (in upper middle location), which was completely missed by RangeNet53++.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Post-processing σ values of spherical and bird'seye views against validation mIoU. Top: Euclidean distance. Bottom: Manhattan distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>(a) The mIoU score vs. distance for near and far points. Spherical view image of size 512 used in this experiment. (b) Number of points in the validation set vs. distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>64 × 256 × 256 DownBlock 128 2 128 × 128 × 128 DownBlock 256 2 256 × 64 × 64</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell>c</cell><cell>s</cell></row><row><cell>4 × 256 × 256</cell><cell>ConvBlock</cell><cell cols="2">64 1</cell></row><row><cell></cell><cell>UpBlock</cell><cell cols="2">128 2</cell></row><row><cell>128 × 128 × 128</cell><cell>UpBlock</cell><cell cols="2">64 2</cell></row><row><cell>64 × 256 × 256</cell><cell>conv2d</cell><cell cols="2">20 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the effect of using augmentation on validation mIoU score. The image size used for spherical view model is 64x512.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We included only peer-reviewed works in the study however there are other interesting approaches that can be easily incorporated in our framework such as SalsaNext<ref type="bibr" target="#b5">[6]</ref> and SqueezeSegV3<ref type="bibr" target="#b41">[42]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE/CVF International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: a tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11236</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Étude comparative de la distribution florale dans une portion des alpes et des jura</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull Soc Vaudoise Sci Nat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="579" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Crall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Graving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Vecsei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirka</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Vallentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semen</forename><surname>Zhydenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Pfeiffer</surname></persName>
		</author>
		<editor>Weng, Abner Ayala-Acevedo, Raphael Meudec, Matias Laporte</editor>
		<imprint>
			<date type="published" when="2020-02" />
			<pubPlace>Ben Cook, Ismael Fernández, François-Michel De Rainville</pubPlace>
		</imprint>
	</monogr>
	<note>et al. imgaug. https:// github.com/aleju/imgaug, 2020. Online; accessed 01</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Velodyne lidar hdl-64e</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Velodyne</forename><surname>Lidar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cardiacnet: segmentation of left atrium and proximal pulmonary veins from mri using multiview cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliasghar</forename><surname>Mortazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashed</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawal</forename><surname>Rhode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulas</forename><surname>Bagci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="377" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Volmap: A real-time model for semantic segmentation of a lidar surrounding view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hager</forename><surname>Radi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11873</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Paris-lille-3d: A large and high-quality groundtruth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of residual networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Topin</surname></persName>
		</author>
		<idno>abs/1708.07120</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Haotian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Zhijian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tangent convolutions for dense prediction in 3D. CVPR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time roadobject segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Squeeze-segv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Squeeze-segv3: Spatially-adaptive convolution for efficient pointcloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01803</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.14032</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
