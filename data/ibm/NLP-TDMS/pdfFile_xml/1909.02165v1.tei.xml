<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POLY-GAN: MULTI-CONDITIONED GAN FOR FASHION SYNTHESIS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-06">September 6, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Pandey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Rochester Institute of Technology Rochester</orgName>
								<address>
									<postCode>14623</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Rochester Institute of Technology Rochester</orgName>
								<address>
									<postCode>14623</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">POLY-GAN: MULTI-CONDITIONED GAN FOR FASHION SYNTHESIS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-06">September 6, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Poly-GAN, a novel conditional GAN architecture that is motivated by Fashion Synthesis, an application where garments are automatically placed on images of human models at an arbitrary pose. Poly-GAN allows conditioning on multiple inputs and is suitable for many tasks, including image alignment, image stitching and inpainting. Existing methods have a similar pipeline where three different networks are used to first align garments with the human pose, then perform stitching of the aligned garment and finally refine the results. Poly-GAN is the first instance where a common architecture is used to perform all three tasks. Our novel architecture enforces the conditions at all layers of the encoder and utilizes skip connections from the coarse layers of the encoder to the respective layers of the decoder. Poly-GAN is able to perform a spatial transformation of the garment based on the RGB skeleton of the model at an arbitrary pose. Additionally, Poly-GAN can perform image stitching, regardless of the garment orientation, and inpainting on the garment mask when it contains irregular holes. Our system achieves state-of-the-art quantitative results on Structural Similarity Index metric and Inception Score metric using the DeepFashion dataset. * Use footnote for providing further information about author (webpage, alternative address)-not for acknowledging funding agencies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Networks (GANs) have been one of the most exciting developments in recent years, as they have demonstrated impressive results in various applications including Fashion Synthesis ZhuS et al. <ref type="bibr" target="#b33">[33]</ref>, Karras et al. <ref type="bibr" target="#b14">[14]</ref>, Han et al. <ref type="bibr" target="#b9">[9]</ref>.</p><p>Fashion Synthesis is a challenging task that requires placing a reference garment on a source model who is at an arbitrary pose and wears a different garment Han et al. <ref type="bibr" target="#b9">[9]</ref> WangB et al. <ref type="bibr" target="#b29">[29]</ref> Dong et al. <ref type="bibr" target="#b4">[5]</ref> Lassner et al. <ref type="bibr" target="#b15">[15]</ref> Cui et al. <ref type="bibr" target="#b3">[4]</ref>. The arbitrary human pose requirement creates challenges, such as handling self occlusion or limited availability of training data, as the training dataset may or not have the model's desired pose. Some of the challenges in Fashion Synthesis are encountered in other applications, such as person re-identification Dong et al. <ref type="bibr" target="#b5">[6]</ref>, person modeling LiuJ et al. <ref type="bibr" target="#b18">[18]</ref>, and Image2Image translation Jetchev et al. <ref type="bibr" target="#b11">[11]</ref>. Existing methods for Fashion Synthesis follow a pipeline consisting of three stages, each requiring different tasks, that are performed by different networks. These tasks include performing an affine transformation to align the reference garment with the source model Rocco et al. <ref type="bibr" target="#b25">[25]</ref>, stitching the garment on the source model, and refining or post-processing to reduce artifacts after stitching. The problem encountered with this pipeline is that stitching the warped garment often results in artifacts due to self occlusion, spill of color, and blurriness in the generation of missing body regions.</p><p>In this paper, we take a more universal approach by proposing a single architecture for all three tasks in the Fashion Synthesis pipeline. Instead of using an affine transformation to warp the garments to the body shape, we generate garments with our GAN conditioned on an arbitrary human pose. Generating transformed garments overcomes the problem of self occlusion and generates occluding arms and other body parts very effectively. The same architecture is then trained to perform stitching and inpainting. We demonstrate that our proposed GAN architecture not only achieves state of the art results for Fashion Synthesis, but it is also suitable for many tasks. Thus, we name our architecture Poly-GAN. <ref type="figure" target="#fig_0">Figure 1</ref> shows representative examples of the performance achieved with Poly-GAN.</p><p>Our Fashion Synthesis approach consists of the following stages, illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Stage 1 performs image generation conditioned on an arbitrary human pose, which changes the shape of the reference garment so it can precisely fit on the human body. Stage 2 performs image stitching of the newly generated garment (from Stage 1) with the model after the original garment is segmented out. Stage 3 performs refinement by inpainting the output of Stage 2 to fill any missing regions or spots. Stage 4 is a post-processing step that combines the results from Stages 2 and 3, and adds the model head for the final result. Our approach achieves state of the art quantitative results compared to the popular Virtual Try On (VTON) method WangB et al. <ref type="bibr" target="#b29">[29]</ref>.</p><p>The main contributions of this paper can be summarized as follows:</p><p>1. We propose a new conditional GAN architecture, which can operate on multiple conditions that manipulate the generated image. 2. In our Poly-GAN architecture, the conditions are fed to all layers of the encoder to strengthen their effects throughout the encoding process. Additionally, skip connections are introduced from the coarse layers of the encoder to the respective layers of the decoder. 3. We demonstrate that our architecture can perform many tasks, including shape manipulation conditioned on human pose for affine transformations, image stitching of a garment on the model, and image inpainting. 4. Poly-GAN is the first GAN to perform an affine transformation of the reference garment based on the RGB skeleton of the model at an arbitrary pose. 5. Our method is able to preserve the desired pose of human arms and hands without color spill, even in cases of self occlusion, while performing Fashion Synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Building on the successes of generative adversarial nets Goodfellow et al. <ref type="bibr" target="#b7">[8]</ref>, conditional GANs Mirza et al. <ref type="bibr" target="#b20">[20]</ref>,Chen et al. <ref type="bibr" target="#b2">[3]</ref> incorporate a specific conditional restriction in their generator network, so that it learns to generate fake samples under that condition. Conditional GAN incorporate a binary mask or label as conditional input by concatenating it with the input image or with a latent noise vector.</p><p>In the progressive GAN architecture Karras et al. <ref type="bibr" target="#b13">[13]</ref>, layers are added to both the generator and the discriminator during training. The addition of new layers increases the fine detail as training progresses. A random noise vector is given as an input to the generator. It is possible to have an embedding layer Mirza et al. <ref type="bibr" target="#b20">[20]</ref> at the input of the Progressive GAN which provides class conditional information to the network. However, even with the class information, the generated image will be random in nature with no control over the generated structure or texture of the image.</p><p>The family of methods that try to swap the source garment with the target garment are referred to as Virtual Try On Networks (VTON) WangB et al. <ref type="bibr" target="#b29">[29]</ref> Dong et al. <ref type="bibr" target="#b4">[5]</ref> Han et al. <ref type="bibr" target="#b9">[9]</ref>. Methods that fall under VTON usually have a similar pipeline consisting of three main components performed by different networks: a) Pose Alignment Network b) Stitch/Swap Network c) Refinement Network. The pose alignment network aligns the target garment with the source image by learning an affine transformation Rocco et al. <ref type="bibr" target="#b25">[25]</ref>. The stitch/swap network is a GAN with some approaches Dong et al. <ref type="bibr" target="#b4">[5]</ref>, but there are cases where it simply performs stitching WangB et al. <ref type="bibr" target="#b29">[29]</ref>. The refinement process generally differs among methods, as every approach has different shortcomings to refine.</p><p>The GAN based VTON methods Jetchev et al. <ref type="bibr" target="#b11">[11]</ref>, Cui et al. <ref type="bibr" target="#b3">[4]</ref>, Lassner et al. <ref type="bibr" target="#b15">[15]</ref> are becoming popular due to the ability of their GAN to generate images conditioned on image data such as mask, garment or pose. One of the first methods to use GANs for VTON Jetchev et al. <ref type="bibr" target="#b11">[11]</ref> uses the cycle consistency approach to generate humans in different garments. The conditional GAN in Jetchev et al. <ref type="bibr" target="#b11">[11]</ref> can accept the reference garment, model image, and model garment as inputs to the network, which gives the ability to control the generation of results conditioned on the reference garment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Poly-GAN</head><p>Poly-GAN, a new conditional GAN for fashion synthesis, is the first instance where a common architecture is used to perform many tasks previously performed by different networks. Poly-GAN is flexible and can accept multiple conditions as inputs for various tasks. We begin with an overview of the pipeline used for Fashion Synthesis, shown in <ref type="figure" target="#fig_1">Figure 2</ref>, and then we present the details of the Poly-GAN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pipeline for Fashion Synthesis</head><p>Two images are inputs to the pipeline, namely the reference garment image and the model image, which is the source person on whom we wish to place the reference garment. A pre-trained pose estimator is used to extract the pose skeleton of the model, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The model image is passed to the segmentation network to extract the segmented mask of the garment, which is used to replace the old garment on the model.</p><p>The entire flow can be divided into 4 stages illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. In Stage 1, the RGB pose skeleton is concatenated with the reference garment and passed to the Poly-GAN. The RGB skeleton acts as a condition for generating a garment that is reshaped according to an arbitrary human pose. The Stage 1 output is a newly generated garment which matches the shape and alignment of the RGB skeleton on which Poly-GAN is conditioned. The transformed garment from Stage 1 along with the segmented human body (without garment and without head) and the RGB skeleton are passed to the Poly-GAN in Stage 2. Stage 2 serves the purpose of stitching the generated garment from Stage 1 to the segmented human body which has no garment. In Stage 2, Poly-GAN assumes that the incoming garment may be positioned at any angle and does not require garment alignment with the human body. This assumption makes Poly-GAN more robust to potential misalignment of the generated garment during the transformation in Stage 1. Due to differences in size between the reference garment and the segmented garment on the body of the model, there may be blank areas due to missing regions. To deal with missing regions at the output of Stage 2, we pass the resulting image to Stage 3 along with the difference mask indicating missing regions. In Stage 3, Poly-GAN learns to perform inpainting on irregular holes and refines the final result. In Stage 4, we perform post processing by combining the results of Stage 2 and Stage 3, and stitching the head back on the body for the final result.</p><p>We used established architectures for segmentation and pose estimation. For the garment segmentation task, we trained U-Net++ ZhouZ et al. <ref type="bibr" target="#b32">[32]</ref> Ronneberger et al. <ref type="bibr" target="#b26">[26]</ref> from scratch on the DeepFashion dataset Liu et al. <ref type="bibr" target="#b17">[17]</ref>. It is important for the segmentation module to precisely separate the source garment from the body, so that the reference garment can be placed at the correct location. Otherwise the segmentation process could lead to artifacts in the form of holes or a visible outline around the garment. These artifacts were sometimes present in the U-Net++ segmentation results.</p><p>To extract the RGB skeleton, we used the pretrained LCR-net++ pose estimation method Rogez et al. <ref type="bibr" target="#b24">[24]</ref>. The network was trained on MPII Human Pose dataset Andriluka et al. <ref type="bibr" target="#b0">[1]</ref> and performed reliably. The pose estimator generated a missing pose even in cases with partial occlusion.</p><p>We also created human parsing data using a pre-trained method Nie et al. <ref type="bibr" target="#b21">[21]</ref> Nie et al. <ref type="bibr" target="#b22">[22]</ref>. The human parsing data is used for segmenting the head from the body of the model, as well as for creating data to train U-Net++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Poly-GAN Architecture</head><p>The Poly-GAN architecture, shown in <ref type="figure" target="#fig_2">Figure 3</ref>, follows the encoder-decoder style of generator. The discriminator architecture is shown in <ref type="figure">Figure 4</ref>. The network architecture is inspired by recent developments in generative adversarial networks, and includes some unique features. We explain the details of our architecture next. <ref type="figure">Figure 4</ref>: Architecture of the Discriminator. The example shown is for garment transformation but the architecture is the same for image stitching (Stage 2) and inpainting (Stage 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Encoder</head><p>The encoder in our architecture, shown in <ref type="figure" target="#fig_2">Figure 3</ref>, can be divided in three main components, a) Conv Module, b) ResNet Module, and c) Conv-Norm Module. We discuss the use of two new modules that are Conv Module and Conv-Norm Module. The ResNet module follows the standard architecture of residual networks He, K. et al. <ref type="bibr" target="#b10">[10]</ref>.</p><p>Conv Module. Conditional GANs Mirza et al. <ref type="bibr" target="#b20">[20]</ref>, Chen et al. <ref type="bibr" target="#b2">[3]</ref> have conditional inputs at the first layer. e.g., the source image or latent noise. An important benefit of our architecture is that it includes conditional inputs at every layer, instead of only having conditional inputs at the first layer. We observed that if the conditional input is limited to the first layer, then its effect diminishes as the features propagate deeper in the network. As a result, there is hardly any conditional information left in deeper layers. We decided to feed the conditional inputs at every layer through a Conv module shown in the encoder module of <ref type="figure" target="#fig_2">Figure 3</ref>. The Conv Module consists of 3 convolution layers, each followed by a ReLU activation function which outputs the number of features required in the layer under consideration.</p><p>Conv-Norm Module. The ResNet module outputs newly learned features, which are concatenated with the features learned from the Conv Module. There is significant difference between the features from ResNet and features from the Conv Module. Thus, we pass the concatenated features to the Conv-Norm module to take advantage of the variation in features from the two modules. The Conv-Norm module has 2 convolution layers, with each convolution layer followed by an instance normalization layer and activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Decoder</head><p>The decoder part of the network learns to generate the desired image based on the features encoded by the encoder. The decoder part of Poly-GAN, shown in the decoder block of <ref type="figure" target="#fig_2">Figure 3</ref>, is similar to the decoder used in other GANs, e.g., Cycle GAN ZhuJ et al. <ref type="bibr" target="#b34">[34]</ref>. The decoder consists of a series of ResNet modules followed by transposed convolution to up-sample the input in each block. However, there are fewer layers in the decoder which receive information from the encoder through skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Skip Connections</head><p>One of the important design decisions of Poly-GAN is the placement of skip connections. From previous works Karras et al. <ref type="bibr" target="#b14">[14]</ref>, it is understood that coarser spatial resolution results in higher level spatial change, such as pose and shape. We use skip connections to pass the information from the encoded coarse layers (4x4, 8x8, 16x16) to the respective decoder layers (4x4, 8x8, 16x16) and concatenate to augment the feature representation. We observed that if we use skip connections between encoder and decoder at spatial resolution above (16x16), then the generated image is not deformable enough to be close to the ground truth. If we don't use skip connections above spatial resolution of (16x16), then the problem of missing minute details arises in the generated image. This is illustrated in the results of <ref type="figure" target="#fig_3">Figure  5</ref>. Using skip connection to connect all layers of the encoder to their respective decoder layer would help in passing minute details from the encoder side, but would also hinder learning and generating new images effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discriminator</head><p>The discriminator used in our experiments is the same discriminator used in the Super Resolution GAN Ledig et al. <ref type="bibr" target="#b16">[16]</ref>. The architecture of the discriminator is shown in <ref type="figure">Figure 4</ref>. The reason behind the selection of the discriminator is to penalize for blurry images, which are often generated by GANs using the L 2 loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>Our loss function consists of three components: adversarial loss L adv , GAN loss L gan and identity loss L id . The total loss and its components are presented below, where D is the discriminator, G is the generator, x i , i = 1, ..., N represent N input samples from different distributions p i (x i ), i = 1, ...N , t is the target, F are fake labels and R are real labels.</p><formula xml:id="formula_0">L loss = L adv + L gan + l id (1) min D L adv = λ 1 E t∼p d (t) D(t) − R 2 2 + λ 2 E x1∼p1(x1),..,x N ∼p N (x N ) D(G(x 1 , .., x N ) − F 2 2 (2) min G L gan = λ 3 E x1∼p1(x1),..,x N ∼p N (x N ) D(G(x 1 , .., x N ) − R 2 2 (3) L id = λ 4 E t∼p d (t),x1∼p1(x1),..,x N ∼p N (x N ) G(x 1 , .., x N ) − t 1<label>(4)</label></formula><p>In the above equations, λ 1 to λ 4 are hyperparamters that are tuned during training. We use the L 2 function as the adversarial loss for our GAN, similar to Mao et al. <ref type="bibr" target="#b19">[19]</ref>. We also use the L 1 function for the identity loss, which helps reduce texture and color shift between the generated image and the ground truth. We considered adding other loss functions, such as the perceptual loss Johnson et al. <ref type="bibr" target="#b12">[12]</ref> and SSIM loss WangZ et al. <ref type="bibr" target="#b31">[31]</ref>, but they did not improve our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Methodology</head><p>The work on StyleGAN Karras et al. <ref type="bibr" target="#b14">[14]</ref> revealed certain input conditions that contributed to the generation of super realistic images, improving upon the blurry or distorted images generated by previous works. These conditions are a) images in the dataset should be of similar zoom ratio; b) the input to the GAN should be simple; c) diversity of the data should be balanced. In our initial experiments with DCGAN Radford et al. <ref type="bibr" target="#b23">[23]</ref>, ProgressiveGAN Karras et al. <ref type="bibr" target="#b13">[13]</ref>, CycleGAN ZhuJ et al. <ref type="bibr" target="#b34">[34]</ref>, and StyleGAN, we observed that if we pass the whole body with the head attached, then the results are blurry and the GAN fails to generate properly. In our experiments we made sure that the dataset is clean enough to have similar zoom ratio on the subject, images are diverse, and we removed any complex distractions, such as the head, before passing to the Poly-GAN inputs.</p><p>We chose to use the RGB skeleton over a binary image of the pose. Our decision was based on the observation that color coded feature presentation are more effective. The other important observation we made while experimenting with Poly-GAN is that the results are affected by the order in which the conditions are concatenated and we adjusted the inputs accordingly. We used hyperparameters similar to CycleGAN ZhuJ et al. <ref type="bibr" target="#b34">[34]</ref> for Poly-GAN training. We used a learning rate of 0.0002, an Adam optimizer with β 1 = 0.5 and β 1 = 0.999, batch size of 1 in all our experiments, and image size of 128x128 to feed images in the network. We also used an image buffer during training similar to the one suggested in Shrivastava et al. <ref type="bibr" target="#b28">[28]</ref>. The image buffer helps in stabilizing the discriminator by keeping a history of a fixed number of generated images which are randomly passed to the discriminator. All the experiments are performed on a workstation with 16GB of RAM using an Nvidia 1080ti GPU and Ubuntu 16.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset</head><p>We follow the process used in VITON Han et al. <ref type="bibr" target="#b9">[9]</ref> while creating our training and testing datasets from the publicly available DeepFashion dataset Liu et al. <ref type="bibr" target="#b17">[17]</ref>. We use LCR-net++ Rogez et al. <ref type="bibr" target="#b24">[24]</ref> for 2D human pose estimation, and Pytorch-MULA Nie et al. <ref type="bibr" target="#b21">[21]</ref> Nie et al. <ref type="bibr" target="#b22">[22]</ref> to get the parsing results for the models. The human parser Nie et al. <ref type="bibr" target="#b21">[21]</ref> is used to create data to train U-Net++ ZhouZ et al. <ref type="bibr" target="#b32">[32]</ref>, to segment the garments from the models, and to remove the head from the models. We have 14,221 training samples, as in the VITON dataset, and 900 paired samples for testing our method. We note that the authors in VITON Han et al. <ref type="bibr" target="#b9">[9]</ref> have used Gong et al. <ref type="bibr" target="#b6">[7]</ref> for human parsing and Cao et al. <ref type="bibr" target="#b1">[2]</ref> for 2D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Evaluation</head><p>We use the Structural Similarity Index metric (SSIM) WangZ et al. <ref type="bibr" target="#b31">[31]</ref> and Inception Score metric (IS) Salimans et al. <ref type="bibr" target="#b27">[27]</ref> which are widely accepted metrics for evaluation of images generated by GANs. SSIM predicts the similarity between two images, where the higher the score the better the network is in generating realistic images. The Inception Score compares the quality of an image to human level grading, and is sensitive to blurring in an image.</p><p>We compare our results to the state of art method CP-VTON WangB et al. <ref type="bibr" target="#b29">[29]</ref>. Since the code for CP-VTON is publicly released WangB et al. <ref type="bibr" target="#b30">[30]</ref>, we are able to obtain results on our dataset for comparison. The state of the art method MG-VTON Dong et al. <ref type="bibr" target="#b4">[5]</ref> tries to solve the problem in a significantly different way than ours, but unfortunately their  code is not available. Therefore, our comparison was limited to CP-VTON. We perform our evaluation on test data that consist of 900 paired images. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. SSIM and IS scores are included for the final stage of Poly-GAN, as well as for Stages 2 and 3 (after stitching the head of the model). The results illustrate that Poly-GAN outperforms CP-VTON in all cases. By comparing the scores of different stages, we see that the output of Stage 2 is the sharpest and has the highest IS score, but it suffers from holes due to blank regions. The output of Stage 3 has the highest SSIM score, i.e. is the most similar to the desired output, but it suffers from blurriness artifacts. The combination on the two gives a balanced score without scoring the highest for either metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Evaluation</head><p>We provide a visual comparison of our results with CP-VTON in <ref type="figure" target="#fig_4">Figure 6</ref>. Our Poly-GAN method is able to keep body parts intact. especially in cases where the arms occlude the body. For example, in <ref type="figure" target="#fig_4">Figure 6</ref> the second image from top shows that CP-VTON fails in the case of self occlusion which our method is able to handle in almost all cases. Poly-GAN is able to precicely map the generated garment from Stage 1 (see <ref type="figure" target="#fig_1">Figure 2</ref>) to the missing garment region on the body in Stage 2, thus avoiding the spill of color, which is evident in CP-VTON.</p><p>While the placement of the garment on the model is very good, both methods suffer from slight color shift in some samples, which is a common problem with GANs that are asked to generate colors from various distributions. One limitation of our method is texture preservation when generating letters, graphics or patterns that are present in the reference garment. This artifact is due to the use of a GAN to reshape the garment, instead of using image warping methods. A less pronounced artifact in some images is a visible boundary outline that is caused by errors in the segmentation by U-Net++. This limitation can be overcome by using a larger and more diverse dataset for training.  For qualitative evaluation, we also present images that were randomly generated with StyleGAN in <ref type="figure" target="#fig_5">Figure 7</ref>. It is evident that although StyleGAN is not conditioned on pose, the generated images suffer from color spill due to occlusion and artifacts in preserving textures and graphics on the garments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We offer a novel approach to the Fashion Synthesis problem by introducing Poly-GAN, a new multi-conditioned GAN architecture that is suitable for many tasks. Qualitative and quantitative results on DeepFashion demonstrate the benefits of our approach by achieving state of the art results. Future research will focus on further exploring the Poly-GAN architecture for a variety of applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of Fashion Synthesis results generated with Poly-GAN. Shown in the columns from left to right: model image, reference garment, Poly-GAN result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Poly-GAN pipeline. Stage 1: Garment transformation with Poly-GAN conditioned on the RGB skeleton of the model and the reference garment. Stage 2: Garment stitching with Poly-GAN conditioned on the segmented model, the RGB skeleton and the transformed garment. Stage 3: Refinement for hole filling with Poly-GAN conditioned on the stitched image and difference mask indicating missing regions. Stage 4: Postprocessing for combining the outputs of Stages 2 and 3 with the model head for the final result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Poly-GAN architecture. The left side shows the encoder in green and decoder in blue. The example shown is for garment transformation (Stage 1) but the architecture is the same for image stitching and inpainting. The conditions of reference garment and body pose are fed at all of the encoder layers. Skip connections from coarse layers of the encoder are fed to the corresponding layers of the decoder. The top right block shows the decoder module and the bottom right block shows the encoder module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Poly-GAN results shown from left to right column: Model image; Pose Skeleton; Reference Garment; Stage 1: transformed garment; Stage 2: garment stitched on segmented model; Stage 3: refinement of outputs from stage 2: Stage 4: post-process result from stage 2 and stage 3 with head in original model image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison examples of Poly-GAN with CP-VTON. Shown in columns from left to right: model image, reference garment, CP-VTON result, Poly-GAN result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Results of models randomly generated using StyleGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Fashion Synthesis Quantitative Results. Bold numbers indicate best performance.</figDesc><table><row><cell></cell><cell>Metric</cell><cell></cell></row><row><cell>Method</cell><cell>SSIM</cell><cell>IS</cell></row><row><cell>CP-VTON</cell><cell cols="2">0.6889 2.6049</cell></row><row><cell cols="3">Poly-GAN Stage 2 0.7174 2.8193</cell></row><row><cell cols="3">Poly-GAN Stage 3 0.7369 2.6549</cell></row><row><cell cols="3">Poly-GAN Stage 4 0.7251 2.7904</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of The Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FashionGAN: Display your fashion design using Conditional Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11026</idno>
		<title level="m">Towards Multi-pose Guided Virtual Try-on Network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="474" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look into Person: Self-supervised Structure-sensitive Learning and a New Benchmark for Human Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="932" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VITON: An Image-based Virtual Try-on Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Conditional Analogy GAN: Swapping Fashion Articles on People Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jetchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bergmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2287" to="2292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<title level="m">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Generative Model of People in Clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="853" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Photo-Realistic Single Image Super-Resolution using a Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pose Transferrable Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4099" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Least Squares Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mutual Learning to Adapt for Joint Human Parsing and Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="502" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mutual Learning to Adapt for Joint Human Parsing and Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<ptr target="https://github.com/NieXC/pytorch-mula" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LCR-net++: Multi-Person 2d and 3d Pose Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network Architecture for Geometric Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6148" to="6157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from Simulated and Unsupervised Images through Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Toward Characteristic-Preserving Imagebased Virtual Try-On Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="589" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">CP-VTON -Reimplemented code for &quot;Toward Characteristic-Preserving Image-based Virtual Try-On Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/sergeywong/cp-vton" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: From Error Visibility to Structural Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">UNet++: A Nested U-Net Architecture for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Be Your Own Prada: Fashion Synthesis with Structural Coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
