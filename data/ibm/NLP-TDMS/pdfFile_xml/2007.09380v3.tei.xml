<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CATCH: Context-based Meta Reinforcement Learning for Transferrable Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawen</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CATCH: Context-based Meta Reinforcement Learning for Transferrable Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Neural Architecture Search, Meta Reinforcement Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) achieved many breakthroughs in recent years. In spite of its remarkable progress, many algorithms are restricted to particular search spaces. They also lack efficient mechanisms to reuse knowledge when confronting multiple tasks. These challenges preclude their applicability, and motivate our proposal of CATCH, a novel Context-bAsed meTa reinforcement learning (RL) algorithm for transferrable arChitecture searcH. The combination of metalearning and RL allows CATCH to efficiently adapt to new tasks while being agnostic to search spaces. CATCH utilizes a probabilistic encoder to encode task properties into latent context variables, which then guide CATCH's controller to quickly "catch" top-performing networks. The contexts also assist a network evaluator in filtering inferior candidates and speed up learning. Extensive experiments demonstrate CATCH's universality and search efficiency over many other widely-recognized algorithms. It is also capable of handling cross-domain architecture search as competitive networks on ImageNet, COCO, and Cityscapes are identified. This is the first work to our knowledge that proposes an efficient transferrable NAS solution while maintaining robustness across various settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The emergence of many high-performance neural networks has been one of the pivotal forces pushing forward the progress of deep learning research and production. Recently, many neural networks discovered by Neural Architecture Search (NAS) methods have surpassed manually designed ones on a variety of domains including image classification <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b60">61]</ref>, object detection <ref type="bibr" target="#b60">[61]</ref>, semantic segmentation <ref type="bibr">[5]</ref>, and recommendation systems <ref type="bibr" target="#b30">[31]</ref>. Many potential applications of <ref type="figure" target="#fig_6">Fig. 1</ref>: Upper: drawbacks of current NAS schemes. Lower: the overall framework of CATCH. Our search agent, CATCHer, consists of three core components: context encoder, RL controller and network evaluator. CATCHer first goes through the meta-training phase to learn an initial search policy, then it adapts to target tasks efficiently.</p><p>practical interests are calling for solutions that can (1) efficiently handle a myriad of tasks, (2) be widely applicable to different search spaces, and (3) maintain their levels of competency across various settings. We believe these are important yet somewhat neglected aspects in the past research, and a transformative NAS algorithm should be able to respond to these needs to make a real influence.</p><p>Many algorithms <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> have been proposed to improve the efficiency of NAS. However, they lack mechanisms to seek and preserve information that can be meaningfully reused. Hence, these algorithms can only repeatedly and inefficiently search from scratch when encountering new tasks. To tackle this problem, a rising direction of NAS attempts to create efficient transferrable algorithms. Several works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref> try to search for architectures that perform well across tasks, but the solutions may not be optimal on the target tasks, especially when the target task distributions are distant from the training task distributions. Some recent works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15]</ref> use meta-learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref> for one-shot NAS instead. With recent critiques <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b25">26]</ref> pointing out some one-shot solutions' dependence on particular search spaces and sensitivity to hyperparameters, many concerns arise on the practicality of these meta NAS works based on one-shot methods. To avoid ambiguity, throughout this paper, tasks are defined as problems that share the same action space, but differ in reward functions. In NAS, the change of either the dataset or domain (e.g. from classification to detection) alters the underlying reward function, and thus can be treated as different tasks.</p><p>Striking a balance between universality and efficiency is hard. Solving the universality problem needs a policy to disentangle from specifics of search spaces, which uproots an important foundation of many efficient algorithms. The aim to improve efficiency on multiple tasks naturally links us to a transfer/meta learning paradigm. Meta Reinforcement Learning (RL) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25]</ref> offers a solution to achieving both efficiency and universality, which largely inspired our proposal of CATCH, a novel context-guided meta reinforcement learning framework that is both search space-agnostic and swiftly adaptive to new tasks.</p><p>The search agent in our framework, namely CATCHer, acts as the decisionmaker to quickly "catch" top-performing networks on a task. As is shown in <ref type="figure" target="#fig_6">Figure 1</ref>, it is first trained on a set of meta-training tasks then deployed to target tasks for fast adaptation. CATCHer leverages three core components: context encoder, RL controller, and network evaluator. The context encoder adopts an amortized variational inference approach <ref type="bibr">[1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24]</ref> to encode task properties into latent context variables that guide the controller and evaluator. The RL controller makes sequential decisions to generate candidate networks in a stochastic manner. The network evaluator predicts the performance of the candidate networks and decides which nets are valuable for training. All three components are optimized in an end-to-end manner.</p><p>We test the method's universality and adaptation efficiency on two fundamentally different search spaces: cell-based search space <ref type="bibr">[13]</ref> and Residual blockbased <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b56">57]</ref> search space. The former focuses on cell structure design, while the latter targets macro skeleton search. With NAS-Bench-201 <ref type="bibr">[13]</ref>, we can compare CATCH fairly with other algorithms by eliminating performance fluctuations rising from different search spaces and training settings. Our experiments demonstrate CATCH's superiority over various other works, including R-EA <ref type="bibr" target="#b39">[40]</ref> and DARTS <ref type="bibr" target="#b32">[33]</ref>. On Residual block-based search space, we use image classification tasks on sub-datasets of ImageNet <ref type="bibr">[10]</ref> as meta-training tasks, and then adapt the CATCHer to target tasks, such as image classification on full ImageNet, object detection on COCO <ref type="bibr" target="#b29">[30]</ref>, and semantic segmentation on Cityscapes <ref type="bibr">[9]</ref>. CATCH discovered networks on these tasks with competitive performance and inference latency. Our results demonstrated CATCH's robustness across various settings, easing previously raised concerns of NAS algorithms' sensitivity to search space, random seeds, and tendencies to overfit to only one or two reported tasks.</p><p>Our key contribution is the first attempt to design an efficient and universal transferrable NAS framework. It swiftly handles various tasks through fast adaptation, and robustly maintains competitive performance across different settings. Our work brings along new perspectives on solving NAS problems, including using amortized variational inference to generate task characteristics that inform network designs. It also demonstrates the possibility of creating efficient sample-based NAS solutions that are comparable with widely-recognized one-shot methods. With competitive networks identified across classification, detection, and segmentation domains, it further opens the investigation on the feasibility of cross-domain architecture search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>NAS is an algorithmic approach to design neural networks through searching over candidate architectures. Many harness the power of Reinforcement Learning (RL) <ref type="bibr" target="#b59">[60]</ref>, Bayesian Optimization <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>, Evolutionary Algorithm <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref>, and Monte Carlo Tree Search <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b51">52]</ref>. The field gradually gains its tractions with the emergence of highly-efficient algorithms <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> and architectures <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b46">47]</ref> with remarkable performance.</p><p>Our method is inspired by PEARL <ref type="bibr" target="#b37">[38]</ref>, a recent work in context-based meta reinforcement learning, which captures knowledge about a task with probabilistic latent contexts. The knowledge is then leveraged for informed policy training. There are a few key challenges in efficiently applying it to NAS: (1) PEARL models the latent context embeddings of RL tasks as distributions over Markov Decision Processes (MDP), but it is less clear how a task in NAS can be meaningfully encoded. (2) RL is notoriously famous for its sample inefficiency, but it is extremely expensive to obtain reward signals on NAS. We address these challenges by <ref type="bibr">(1)</ref> proposing the use of network-reward pairs to represent a task, <ref type="bibr">(2)</ref> introducing meta-training tasks that can be cheaply evaluated to obtain more data for learning, and including a network evaluator that acts like Q-learning agents to speed up learning.</p><p>Previous works also explored the possibility of using meta-learning for NAS. Some <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref> aimed to identify a single architecture that simultaneously works well on all considered tasks. These solutions may not be scalable when confronting a large pool of target tasks. An early work <ref type="bibr" target="#b52">[53]</ref> aimed to learn a general policy across tasks. However, it generates task embeddings from images, which may fail at datasets with the same images, and is unable to differentiate among classification, detection, and segmentation tasks on the same dataset. A few recent papers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15]</ref> combined gradient-based meta-learning with DARTS, but the algorithms are only applicable to search spaces compatible with DARTS. Additionally, none of the above proposals reported their performance on largescale tasks like ImageNet full dataset. This leaves questions on these proposals' generalizability and adaptation efficiency on more challenging datasets, where scientists expect meta-NAS algorithms should have an edge over typical NAS methods. CATCH is the first NAS algorithm to our knowledge that deploys meta-learning while maintaining universality, robustness across different search spaces, and capability to handle large-scale tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CATCH Framework</head><p>In NAS, the change of dataset (e.g. CIFAR-10 vs. ImageNet) or domain (e.g. image classification vs. object detection) essentially indicates the shift of underlying reward distribution. The goal of a cross-task transfer algorithm is hence to quickly identify the best actions under the changed reward dynamics. To handle this challenge, the CATCH framework consists of two phases: a meta-training phase and an adaptation phase, as is presented in Algorithm 1. In the metatraining phase, we train the CATCHer on a pool of meta-training tasks that can be cheaply evaluated. A key goal of this phase is to present the context encoder with sufficiently diversified tasks, and encourage it to consistently encode meaningful information for different tasks. Meanwhile, both the controller and the evaluator may gain a good initialization for adaptation. In the adaptation   The search procedure of CATCH on a given task. The procedure starts from initializing the search history by storing a randomly selected network m and its reward r. The encoder applies amortized variational inference approach to generate latent context encoding z by encoding network-reward pairs from the search history. The controller then generates candidate networks for the evaluator to choose the most promising ones to train and evaluate. Newly selected networks and their rewards will be stored in the search history. The loop continues after the three components are optimized.</p><p>phase, the meta-trained CATCHer then learns to find networks on the target task efficiently through the guidance of the latent context encoding. We show the search procedure on any single task in <ref type="figure" target="#fig_7">Figure 2</ref>, which corresponds to line 3-13 of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Encoding</head><p>The use of latent context encoding is a crucial part of CATCH. The question is what information about the task is reliable to construct such latent contexts. Directly extracting feature maps of images of the dataset is an intuitive solution. However, for the same dataset, the best network configurations to perform different tasks like object detection and semantic segmentation may differ a lot. Hence, simply extracting information directly from images may not be a viable approach.</p><p>We instead believe that the task-specific contextual knowledge can be mined from the search history (i.e. sets of network-reward pairs). If the same group of networks have similar relative strengths on two tasks, it might mean these tasks are "close" to each other. It is also helpful to break the barriers for cross-task architecture search, since the network-reward pair of information is universal across tasks. Before searching on a task, we randomly form a few networks m and evaluate their performance r to initialize the search history. The retrieved network-reward pairs are stored in the search history for its initialization. To start the search, we sample a number of network-reward pairs {(m, r) i } N 1 (denoted by c 1:N for simplicity) from the search history, which will be fed into the encoder to generate a latent context vector z representing the salient knowledge about the task.</p><p>We model the latent context encoding process in a probabilistic manner, because it allows the context encoder to model a distribution over tasks and conduct exploration via posterior sampling. Following the amortized variational inference approach used in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b23">24]</ref>, we aim to estimate the posterior p(z|c 1:N ) with the encoder q φ (z|c 1:N ), parametrized by φ. We assume the prior p(z) is a unit multivariate Gaussian distribution with diagonal covariance matrix N (0, diag(1)), and hence, the posterior p(z|c) conditioning on c is Gaussian. Since the network-reward pairs c 1:N are independent on a task, we could factor q φ (z|c 1:N ) into the product of Gaussian factors conditioning on each piece of contexts c i ,</p><formula xml:id="formula_0">q φ (z|c 1:N ) ∝ N i=1 N (fμ φ (c i ), diag(fσ φ (c i )),<label>(1)</label></formula><p>where f φ is an inference network parametrized by φ, which predicts the meañ µ i and the standard deviationσ i of q φ (z|c i ) as a function of c i to approximate Gaussian p(z|c i ). During the forward pass, the encoder network f φ outputsμ i ,σ i of the Gaussian posterior q φ (z|c i ) conditioning on each context, then we take their product q φ (z|c 1:N ). Each context c i is (m, r) i , where r is normalized among {r} 1:N to reflect the relative advantage of each network. All the network-reward pairs in the search history are utilized. We then sample z from q φ (z|c 1:N ). Further implementation details can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Sampling</head><p>The generation of a network can be treated as a decision-making problem, where each of the RL controller's actions determines one attribute of the resulting architecture. The attribute can be an operation type to form a certain edge in a cell-based search (e.g. skip-connect, convolution operations, etc.), or the shape of a network in a macro-skeleton search (e.g. width, depth, etc.). Both ways are explored in our work.</p><p>A network, denoted by m, is represented as a list of actions [a 1 , a 2 , ..., a L ] taken by the controller in a sequential manner. At each time step l, the controller makes a decision a l according to its policy π θc , parametrized by θ c . The controller policy takes z and the previous actions [a 1 ...a l−1 , 0, ..., 0] as inputs, and outputs the probability distribution of choosing a certain action π θc (a l |[a 1 ...a l−1 , 0, ..., 0], z), where the actions will be sampled accordingly. z is the latent context vector generated by the encoder, and [a 1 ...a l−1 , 0, ..., 0] is a collection of one-hot vectors indicating all the actions taken so far at l-th timestep, leaving untaken actions [a l , ..., a L ] as zero vectors. The reward for each action is the normalized performance score of the network. The controller samples M networks stochastically as candidates for the network evaluator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Context-based Meta Architecture Search (CATCH)</head><p>Inputs: {Tmeta} (meta-training task pool), {Ttarget} (target task pool), Nmeta (# of meta epochs), N search (# of search epochs), C (# of contexts to sample), M (# of models to sample) Meta-training Phase: 1: for Nmeta meta epochs do 2:</p><p>Select meta-training task T from {Tmeta} 3:</p><p>Initialize SearchHistory 4:</p><p>for n = 1 to N search do 5:</p><formula xml:id="formula_1">{(m, r)i} C 1 = SearchHistory.sample contexts(C) 6: z = Encoder.encode({(m, r)i} C 1 ) 7: {m} M 1 ← Controller.sample networks(z, M ) 8: m ← Evaluator.choose best({mj} M 1 , z) 9:</formula><p>r ← train and evaluate(m , T ) 10:</p><p>SearchHistory.add((m , z, r)) 11:</p><p>Encoder, Controller, Evaluator optimization 12:</p><p>end for 13: end for Adaptation Phase: 14: Select target task T from {Ttarget} 15: Repeat Line 3-13 16: BestModel ← SearchHistory.best model() 17: return BestModel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Scoring and Evaluation</head><p>Since the candidate networks are sampled stochastically by the controller, it is almost inevitable that some inferior models will be generated. We set up a filtering mechanism, namely network evaluator, which acts like a Q-learning agent that predicts the actual performance of each network, and selects the top one for training. The predicted value is not necessarily an accurate prediction of the training performance, but should be able to provide a ranking among candidate models roughly similar to their true performance.</p><p>The evaluator f θe (m, z) is parameterized by θ e . It takes M tuples of networkcontext pairs (m, z) as inputs, and outputs the predicted performance of input architectures. The network with the highest predicted performance score will be trained to obtain the true reward r. The network-context-reward tuple (m, z, r) is then stored in the evaluator's local memory for future gradient updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization of CATCHer</head><p>To optimize the controller policy, we maximize the expected reward for the task it is performed on. The controller is trained using Proximal Policy Optimization (PPO) <ref type="bibr" target="#b42">[43]</ref> with a clipped surrogate objective L c .</p><p>To optimize the evaluator, we deploy Prioritized Experience Replay (PER) <ref type="bibr" target="#b41">[42]</ref>, a Deep Q-learning <ref type="bibr" target="#b33">[34]</ref>   and thus improves sample efficiency. The loss of the evaluator L e is the Huber loss <ref type="bibr" target="#b21">[22]</ref> between the evaluator's predictionr and the normalized true performance score. Further details of L c and L e can be found in the Appendix.</p><p>To optimize the encoder, we take L c and L e as part of the objective. The resulting variational lower bound for each task T is</p><formula xml:id="formula_2">L = E z∼q φ (z|c T ) [L c + L e + βD KL (q φ (z|c T )||p(z))],<label>(2)</label></formula><p>where D KL serves as an approximation to a variational information bottleneck that constrains the mutual information between z and c, as is shown in <ref type="bibr">[1,</ref><ref type="bibr" target="#b37">38]</ref>. This information bottleneck acts as a regularizer to avoid overfitting to training tasks. β is the weight of D KL in the objective, and p(z) is a unit Gaussian prior. Since (1) the latent context z serves as input to both controller and evaluator, and (2) q φ (z|c) and p(z) are Gaussian, with D KL computed using their mean and variance, gradient of Eq. 2 can be back-propagated end-to-end to the encoder with the reparameterization trick.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We use Multi-layer Perceptrons (MLP) as the controller policy network to generate the probability of choosing a certain action. The parameters θ c of the controller is trained on-policy via the PPO algorithm. We mask invalid actions by zeroing out their probabilities in the controller's outputs, then softmax the remaining probabilities and sample actions accordingly.</p><p>The evaluator is an MLP to generate the predicted score of a network. In the meta-training phase, we reset in the -greedy exploration strategy each time when the agent initializes a new task. We sample 80% of the entries as a batch from the replay buffer using PER. ImageNet16-120</p><formula xml:id="formula_3">Global Max CATCH ENAS GDAS RNAS SETN DARTS-V1 DARTS-V2</formula><p>(c) ImageNet16-120 <ref type="figure">Fig. 4</ref>: Learning curves of one-shot algorithms and CATCH. Each curve is an average of three runs. We plot the first 100 search epochs for algorithms except for DARTS, which is trained only for 50 search epochs.</p><p>The encoder MLP outputs a 10-dim latent context vector z, and the weight of the KL-Divergence β in the combined loss is set to be 0.1. More details of the components' hyperparameters can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark on NAS-Bench-201</head><p>As recent work <ref type="bibr" target="#b55">[56]</ref> indicated, NAS algorithms are usually compared unfairly under different settings. To mitigate such problems, we first tested CATCH on NAS-Bench-201. It is a benchmark dataset that enables fair comparisons among NAS methods under the same configurations. It supports searching over cellbased architectures, where a directed acyclic graph represents each cell with 4 nodes and 5 possible connection operations on each edge. It provides the validation and test accuracies of 15,625 architectures on CIFAR-10, CIFAR-100, and ImageNet16-120 datasets. ImageNet16-120 is a subdataset for ImageNet, which downsampled all its images to 16 × 16, and contains only the first 120 classes of ImageNet.</p><p>Experiment Settings. In the meta-training phase, each task is formed as a classification task on an X-class sub-dataset of ImageNet16 (ImageNet downsampled to 16 × 16) to maintain consistency with the configurations in NAS-Bench-201. The number of classes X ∈ <ref type="bibr">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. In each meta-epoch, the agent searches 20 networks whose validation accuracies after 12 training epochs are used as the reward signals. The hyperparameters used for training the networks in both phases are identical to those in NAS-Bench-201. In the following experiments, CATCH-meta is meta-trained with 25 meta epochs for 10.5 GPU hours on Tesla V100. We apply the same configurations as those in NAS-Bench-201.</p><p>Comparison with Sample-based Algorithms. We display the search results of the meta-trained version (CATCH-meta) and the search-from-scratch  <ref type="bibr">[2]</ref>, Regularized Evolution Algorithm (R-EA) <ref type="bibr" target="#b39">[40]</ref>, and REINFORCE <ref type="bibr" target="#b50">[51]</ref>. The results of other methods are reproduced by running the code and configurations originally provided by NAS-bench-201. Each experiment is repeated for 500 trials with different seeds. The algorithms are trained for 50 search epochs in each trial. <ref type="figure" target="#fig_3">Figure 3</ref> presents the search results on CIFAR-10, CIFAR-100, ImageNet16-120, with the highest validation accuracy on each task. The reproduced results are consistent with the experiments performed in NAS-Bench-201. The performance of CATCH-sfs is similar to the other four methods, but CATCH-meta dominates all other algorithms in the searched network accuracies. On CIFAR-10, CATCH-meta finds the best model in 280/500 trials. On CIFAR-100, over half of them find top-3 performance networks within 50 samples, while other algorithms barely touch the roof. On ImageNet16-120, CATCH reaches the best network for more than 22% trials. We can see tremendous benefits for using the meta-trained CATCH to reduce time and cost.</p><p>Comparison with One-shot Algorithms. One of the central controversies around meta-NAS algorithms is: given the high searching efficiency of one-shot methods, can sample-based algorithms outperform them? We therefore compare the performance of CATCH with many state-of-the-art one-shot NAS solutions. For fair comparisons, instead of querying the NAS-Bench-201 network database, we train each child network for 12 epochs and obtain their early-stop validation accuracies as training feedbacks. The early-stop training setup is the same as the one in the meta-training phase. The one-shot algorithms involved are first-order DARTS (DARTS-V1) <ref type="bibr" target="#b32">[33]</ref>, second-order DARTS (DARTS-V2), GDAS <ref type="bibr">[12]</ref>, Random NAS (R-NAS) <ref type="bibr" target="#b25">[26]</ref>, ENAS <ref type="bibr" target="#b36">[37]</ref>, and SETN <ref type="bibr">[11]</ref>. We run the algorithms with the original code and configurations released from NAS-Bench-201. DARTS-V1 and DARTS-V2 are run for 50 search epochs, and other algorithms are trained for 250 search epochs. <ref type="figure">Figure 4</ref> presents the learning curves of each algorithm in the first 100 search epochs. For CATCH, at each search epoch, we identify networks with the best partially trained accuracy found so far, and report their fully trained accuracies. Both DARTS and ENAS have a relatively strong performance at the beginning, but the curves drop significantly afterward. SETN resembles Random NAS a lot. GDAS is among the best one-shot algorithms, but it seems to plateau at local maximums after a few search epochs. CATCH has the best performance among all, as it quickly adapts and identifies promising architectures that are beyond other algorithms' search capacity.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we report the best fully trained accuracy of networks that each algorithm identifies over their complete training process. We set the time budget for CATCH to search on CIFAR-10, CIFAR-100, and ImageNet16-120 as 3, 4, and 5 hours. It is roughly equivalent to cutting the search on these tasks at 70, 50, and 40 search epochs, respectively. Although DARTS-V1, R-NAS, and ENAS spend less time in total, they are highly unstable and the performance of DARTS and ENAS tends to deteriorate over time. CATCH spends 22.5 (10.5 meta + 12 adaptation) hours on all three tasks, and its searched networks surpass all other algorithms. The presented results have proved that CATCH is swiftly adaptive, and it is able to identify networks beyond many one-shot algorithms' reach within a reasonable time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Residual Block-based Search Space</head><p>Having proved that CATCH can adapt to new tasks efficiently with metatraining, we further inquire whether CATCH has the ability to transfer across different domains including image classification, objection detection, and semantic segmentation. In this section, we consider a more challenging setting where <ref type="table" target="#tab_7">Table 3</ref>: Results on COCO compared to manually designed and NAS searched backbones. Latency results of networks except CATCH are referred from <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Input size Latency (ms) mAP RetinaNet <ref type="bibr" target="#b28">[29]</ref> ResNet101-FPN 1333x800 91.7 (V100) 39.1 FSAF <ref type="bibr" target="#b58">[59]</ref> ResNet101-FPN 1333x800 92.5 (V100) 40.9 GA-Faster RCNN <ref type="bibr" target="#b47">[48]</ref> ResNet50-FPN 1333x800 104.2 (V100) <ref type="bibr">39.8</ref> Faster-RCNN <ref type="bibr" target="#b40">[41]</ref> ResNet101-FPN 1333x800 84.0 (V100) 39.4 Mask-RCNN <ref type="bibr" target="#b17">[18]</ref> ResNet101-FPN 1333x800 105.0 (V100) 40.2 DetNAS <ref type="bibr">[8]</ref> Searched Backbone 1333x800 -42.0 SM-NAS: E3</p><p>Searched Backbone 800x600 50.7(V100) 42.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM-NAS: E5</head><p>Searched Backbone 1333x800 108.1(V100) 45.9 Auto-FPN <ref type="bibr" target="#b54">[55]</ref> Searched Backbone 1333x800 -40.5 CATCH CATCH-Net-C 1333x800 123.5 (V100) 43.2 the meta-training phase contain only image classification tasks while tasks in all the three domains are targeted in the adaptation phase. The architectures are very different among these domains, so we search for their common component -the feature extractor (backbone). ResNet is one popular backbone for these tasks, thus we design the search space following <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b56">57]</ref>. Constructing a model in the Residual block-based search space requires the controller to make several decisions: (1) select the network's base channel from <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr">64</ref>, 72], (2) decide the network's depth within <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>, (3) choose the number of stages s, which is either 4 or 5, (4) schedule the number of blocks contained in each stage, and (5) arrange the distribution of blocks holding different channels. Details of the Residual block-based search space can be found in the Appendix. Experiment Settings. We use the same meta-training settings as the ones we used in NAS-Bench-201. For each meta epoch, an ImageNet sub-dataset is created. To form such sub-datasets, we sample X classes from all classes of ImageNet, where X ∈ <ref type="bibr">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. Then the images are resize to <ref type="bibr" target="#b15">16</ref>  possible sub-datasets. To achieve the balance between inference latency and network performance, we adopt the multi-objectve reward function</p><formula xml:id="formula_4">R = P (m) × [ LAT (m)</formula><p>Ttarget ] w in <ref type="bibr" target="#b45">[46]</ref>, where P (m) denotes the model 's performance (e.g. validation accuracy for classification, mAP for object detection or mIoU for semantic segmentation), LAT (m) measures the model's inference latency, and T target is the target latency. w serves as a hyperparameter adjusting the performance-latency tradeoff. In our experiments, we set w = −0.05. With this reward, we hope to find models that excel not only in performance but also in inference speed. We meta train CATCHer for 5 GPU days, and adapt on each target task to search for 10 architectures. We target ImageNet dataset for image classification, COCO dataset for object detection and Cityscapes dataset for semantic segmentation. The detailed settings can be found in the Appendix.  <ref type="table" target="#tab_7">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>. Our network again shows faster inference time and competitive performance. We also transfer CATCH-Net-B found during the search on ImageNet to COCO and Cityscapes, which yield 42% mAP with 136ms inference time and 80.87% mIoU (MS) with 52ms latency, respectively. Our results again show that directly transferring top architectures from one task to another cannot guarantee optimality. It also reveals CATCH's potentials to transfer across tasks even when they are distant from the meta-training ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>The context encoder is the spotlight component of our algorithm. We are especially curious about: (1) Is the encoder actually helpful for adaptation (compared with simply plugging in the meta-learned controller and evaluator priors)? <ref type="bibr">(2)</ref> If so, does the improvement come from good estimates of the posterior, or is it from the stochastic generation of z that encourages exploration and benefits generalization?</p><p>To answer these questions, we designed two extra sets of experiments: (1) CATCH-zero: We set z = 0, and thereby completely eliminate the encoder's effect on both the controller and the evaluator; (2) CATCH-random: We sample each z from a unit Gaussian prior N (0, diag(1)) during the search as random inputs. The results are presented in <ref type="figure" target="#fig_5">Figure 5</ref> (a)-(c). In both settings, the agents are still meta-trained for 10.5 hours before they are plugged in for adaptation.</p><p>The gaps among the lines in <ref type="figure" target="#fig_5">Figure 5</ref> answered our questions. The encoder not only helps with adaptation (through comparing CATCH-meta and CATCHzero), but also provides assistance in a much more meaningful way than using random inputs for exploration, as CATCH-meta outperforms CATCH-random on both CIFAR-10 and CIFAR-100. Interestingly, we observe less significant improvement on ImageNet16-120. One hypothesis is since we perform the metatraining phase on sub-datasets of ImageNet16, the meta-trained controller and evaluator are already tuned towards policies that fit the search on ImageNet16. Hence, the transferred policies require less adaptation assistance from the encoder. More ablation studies can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>In this work, we propose CATCH, a transferrable NAS approach, by designing an efficient learning framework that leverages the benefits of context-based meta reinforcement learning. The key contribution of CATCH is to boost NAS efficiency by extracting and utilizing task-specific latent contexts, while maintaining universality and robustness in various settings. Experiments and ablation studies show its dominant position in search efficiency and performance over non-transferrable schemes on NAS-Bench-201. Extensive experiments on residual block-based search space also demonstrate its capability in handling cross-task architecture search. As a task-agnostic transferrable NAS framework, CATCH possesses great potentials in scaling NAS to large datasets and various domains efficiently. During our research into transferrable NAS frameworks, we identified many potentially valuable questions to be explored. Efficient adaptation among domains is challenging, and we demonstrated a first attempt to simplify it by searching for backbones with a shared search space. A possible future investigation would be to generalize cross-task architecture search to flexibly include more decisions, such as searching for detection and segmentation heads. Meanwhile, our meta-training tasks involve only classification tasks, but it is also possible to diversify the pool and explore whether it leads to further performance boosts. , CIFAR-100 <ref type="bibr">[6]</ref>, and ImageNet16-120 <ref type="bibr">[3]</ref>.</p><p>We compare the learning curve of CATCH with other sample-based algorithms in <ref type="figure" target="#fig_6">Figure 1</ref>. We plot each curve with the highest fully-train validation accuracy the agent has seen at each search epoch. Each curve is plotted with an average of 500 trials. The shaded area shows the mean ± standard deviation among all trials at each search epoch. CATCH stands out among others with higher performance and lower variation on all three datasets (CIFAR-10, CIFAR-100, and ImageNet16-120). It is also on average a magnitude faster than other algorithms to find their best architectures after 500 searching epochs. On ImageNet16-120, none of the algorithms except CATCH could even identify the best architecture within 500 searching epochs across all 500 trials. CATCH is also more stable, as is indicated by its much lower variation compared with other algorithms. Its</p><p>Equal contribution. Correspondence to: tongzhang@tongzhang-ml.org </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Encoder's Adaptation Result</head><p>Throughout the adaptation process, we hypothesize that the encoder can provide dataset-specific guidance to the controller and the evaluator. To test this hypothesis, we visualize the encoded latent context vector z of each dataset through Principle Component Analysis, with the results presented in <ref type="figure" target="#fig_7">Figure 2</ref>. Each point is generated by randomly selecting and encoding 80% network-reward pairs from the search history. We freeze the weights of the meta-trained controller and evaluator policy, and only allow gradient updates for the encoder. This operation eliminates influence from the changing controller and evaluator policies, and thus enables us to closely observe just the behaviors of the encoder. When the encoder is first adapted to CIFAR-10, CIFAR-100, and ImageNet16-120, the generated context vectors are not distinguishable across the three datasets. However, after just 10 search epochs of adaptation, we can already identify a cluster of ImageNet16-120 context vectors. The clusters then quickly evolve as the encoder sees more architectures. By the 50-th search epoch, we can see three distinctive clusters as a result of the encoder's fast adaptation towards the three datasets. This observation is consistent with the results of NAS-Bench-201 <ref type="bibr">[3]</ref>. In the original paper, the network-performance pairs have higher correlation between CIFAR-10 and CIFAR-100 (0.968) than that between CIFAR-10 and ImageNet16-120 (0.827). This correlation is also higher than the correlation between CIFAR-100 and ImageNet16-120 (0.91). This attributes to the reason why the encoder takes more search epochs to distinguish CIFAR-10 from CIFAR-100. The results are in support of our hypothesis, and show the encoder's capability to learn and express dataset-specific information effectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ablation Study on the Evaluator</head><p>We also explored the effects of the evaluator by eliminating it from both the meta-training and adaptation phase, and its performance is presented in <ref type="figure" target="#fig_3">Figure  3</ref> (a)-(c). As the figure shows, the evaluator lifts the performance by a large margin, making it a crucial component in the search algorithm. <ref type="table" target="#tab_1">Table 1</ref> provides further information on the evaluator when comparing it with CATCH using ground truth as the evaluator (CATCH-GT). CATCH-GT is a hard-to-defeat baseline, but CATCH-meta managed to get very close to it and the global max accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Controller Settings and Hyperparameters</head><p>The controller is trained with Proximal Policy Optimization (PPO) <ref type="bibr">[10]</ref> algorithm, and its loss L c is defined following the original PPO loss:</p><formula xml:id="formula_5">L c =Ê t min r t (θ c )Â t , clip (r t (θ c ) , 1 − , 1 + )Â t</formula><p>is the PPO clipping parameter, r t (θ c ) = π θc (a l |st) π θ old (a l |st) is the probability ratio, andÂ t is the General Advantage Estimate (GAE) <ref type="bibr">[9]</ref> estimate:</p><formula xml:id="formula_6">A t = t l=0 (γλ) l δ V l where δ V l = r t + γV (s l+1 ) − V (s l )</formula><p>is the Bellman residual term. The definition of s l can be found in <ref type="table" target="#tab_7">Table 3</ref>. We show the training hyperparameters and our settings on translating architecture search elements as Markov Decision Processes (MDP) in the following tables.  Latent context and the current network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encoder and Evaluator Settings</head><p>The encoder generates the latent conext through the network-reward information (m, r). This is done by taking the encoder output as the means and variances of a D-dimensional Gaussian distribution, from which we sample z. We provide pseudocode for this process in Algorithm 1. The evaluator uses the Huber loss <ref type="bibr">[5]</ref> to close the gap between its predicted network performancer and the actual performance r. L e = 1 n i loss(r i ,r i ), where loss(r,r) = 0.5(r i −r i ) 2 if | r i −r i |&lt; 1, | r i −r i | −0.5 otherwise.</p><p>(1)    <ref type="bibr">[7]</ref>, and Cityscapes <ref type="bibr">[1]</ref> . On COCO, Faster R-CNN with the ResNet backbone and Cascade FPN is used as our baseline. It is extremely costly to perform ImageNet pretrain for search, but training detection networks without ImageNet pretrain was made possible by <ref type="bibr">[4]</ref>. For COCO and Cityscapes, we use Group Normalization with halved-base-channel groups instead of Batch Normalization. Conv2D with weight standardization (ConvWS2D) is also applied.   We show an example model in our Residual Block search space in <ref type="figure" target="#fig_3">Figure 3</ref>. It consists of 5 stages, with depth=15, stage distribution= <ref type="bibr">[3,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5]</ref>, and channel distribution= <ref type="bibr">[2,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">7]</ref>. We use the same notation format to show the searched models in <ref type="table" target="#tab_15">Table 9</ref>.  <ref type="figure">Fig. 4</ref>: An example model in the Residual Block search space following <ref type="bibr">[12,</ref><ref type="bibr">11]</ref>. C-N-R stands for a combination of Convolution layer, Normalization layer, and a ReLU operation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2: The search procedure of CATCH on a given task. The procedure starts from initializing the search history by storing a randomly selected network m and its reward r. The encoder applies amortized variational inference approach to generate latent context encoding z by encoding network-reward pairs from the search history. The controller then generates candidate networks for the evaluator to choose the most promising ones to train and evaluate. Newly selected networks and their rewards will be stored in the search history. The loop continues after the three components are optimized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>(a)-(c) show the results of 500 trials for CATCH-meta, CATCH-sfs(search from scratch) and other sample-based algorithms. Each individual trial is sorted by the final validation accuracy of the searched network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>(a)-(c) compare results of 500 trials for CATCH-meta, CATCH-sfs(search from scratch), CATCH-zero, CATCH-random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1Fig. 1 :</head><label>1</label><figDesc>The University of Hong Kong 2 Huawei Noah's Ark Lab 3 Sun Yat-sen University 4 The Hong Kong University of Science and Technology 1 Learning Curve Comparison with Sample-based Algorithms Comparison of CATCH with other sample-based algorithms on CIFAR-10 [6]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2 :</head><label>2</label><figDesc>The encoder's adaptation process. It learns to distinguish different datasets throughout the learning process, and thus provide informed input to the controller and the evaluator. variance tends to shrink over time, while R-EA and REINFORCE policies are almost as unstable as random search. Through this comparison, we further prove the adaptation speed and stability of CATCH, along with its competency across various datasets and random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of CATCH-meta, CATCH-sfs with CATCH-withoutevaluator. Including the evaluator significantly raises the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode of Latent Context Encoding Procedure in a PyTorchlike style. def encode_z(B, D, Contexts, Encoder): # Contexts: a batch of contexts {(m, r)} use for encoding # B: len(Contexts), batch # D: the dimension of latent context variable z # Encoder: 3-layer MLP mapping (m, r) to (mean, var) of z_i # encode each (m, r) to (mean, var) of z context_batch.rewards = normalize(context_batch.rewards) params = Encoder.forward(context_batch) # shape: [B, 2 * D] # get mean and var; t(): matrix transpose means = params[..., :D].t() # shape: [D, B] vars = F.softplus(params[..., D:].t()) # shape: [D, B] # get mean &amp; var of each z_i; ds: torch.distributions posteriors = [] for ms, vs in zip(unbind(means), unbind(vars)): z_i_mean, z_i_var = _product_of_gaussian(ms, vs) # form a Gaussian Posterior from z_i_mean, sqrt(z_i_var) z_i_posterior = ds.Gaussian(z_i_mean, sqrt(z_i_var)) posteriors.append(z_i_posterior) # sample z from q(z|Contexts); rsample(): random sample z = [d.rsample() for d in posteriors] return torch.stack(z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>optimization technique. During the update, it prompts the evaluator to prioritize sampling entries that it makes the most mistakes on,</figDesc><table><row><cell></cell><cell>92.0</cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell>74</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell>48</cell><cell></cell><cell></cell><cell cols="2">ImageNet16-120</cell><cell></cell></row><row><cell>validation accuracy</cell><cell>89.0 89.5 90.0 90.5 91.0 91.5</cell><cell>0</cell><cell>100</cell><cell>200 trial number 300</cell><cell>400 Global Max RS REINFORCE R-EA CATCH-sfs CATCH-meta</cell><cell>500</cell><cell>validation accuracy</cell><cell>68 69 70 71 72 73</cell><cell>0</cell><cell>100</cell><cell>200 trial number 300</cell><cell>400 Global Max RS REINFORCE R-EA CATCH-sfs CATCH-meta</cell><cell>500</cell><cell>validation accuracy</cell><cell>41 42 43 44 45 46 47</cell><cell>0</cell><cell>100</cell><cell>200 trial number 300</cell><cell>400 Global Max RS REINFORCE R-EA CATCH-sfs CATCH-meta</cell><cell>500</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(c) ImageNet16-120</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of CATCH with one-shot algorithms. The top accuracies of identified models, standard deviations, search time (hour), total search time (hour), and the highest validation accuracies among all the networks in NAS-Bench-201 are reported. The same three random seeds are used to run through each algorithm. The time budget for search on CIFAR-10, CIFAR-100, and ImageNet16-120 are 3, 4, and 5 hours respectively.</figDesc><table><row><cell cols="6">Algorithm DARTS-V1 [33] 88.08±1.89 2.46 68.99±1.93 2.44 23.66±0 4.55 CIFAR-10 CIFAR-100 ImageNet16-120 Total Time Acc ±std Time Acc±std Time Acc±std Time 9.45</cell></row><row><cell cols="2">DARTS-V2 [33] 87.16±0.39</cell><cell>9</cell><cell cols="2">65.06±2.95 7.91 26.29±0 22.14</cell><cell>39.05</cell></row><row><cell>GDAS [12]</cell><cell>90.32±0.08</cell><cell>6</cell><cell cols="2">70.33±0.85 6.23 44.81±0.97 17</cell><cell>29.23</cell></row><row><cell>R-NAS [26]</cell><cell cols="4">90.45±0.43 2.19 70.39±1.36 2.26 44.12±1.04 5.94</cell><cell>10.39</cell></row><row><cell>ENAS [37]</cell><cell cols="4">90.2±0.63 4.22 69.99±1.03 4.26 44.92±0.51 5.18</cell><cell>13.66</cell></row><row><cell>SETN [11]</cell><cell cols="4">90.26±0.75 7.62 68.01±0.21 7.74 41.04±1.64 20.33</cell><cell>35.69</cell></row><row><cell cols="5">CATCH-meta 91.33±0.07 3 72.57±0.81 4 46.07±0.6 5</cell><cell>22.5</cell></row><row><cell>Max Acc.</cell><cell>91.719</cell><cell></cell><cell>73.45</cell><cell>47.19</cell><cell>-</cell></row></table><note>version (CATCH-sfs where the meta-training phase is skipped) of our method, and compare them with other sample-based algorithms: Random Search (RS)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on ImageNet compared to manually designed and NAS searched architectures. Latency is measured on one Tesla V100 with one image with shape (3, 720, 1080).</figDesc><table><row><cell>Network</cell><cell cols="3">Top-1 Acc (%) Top-5 Acc (%) Latency (ms)</cell></row><row><cell>ResNet50 [19]</cell><cell>77.15</cell><cell>93.29</cell><cell>16.4</cell></row><row><cell>DenseNet201 [20]</cell><cell>77.42</cell><cell>93.66</cell><cell>31.6</cell></row><row><cell>ResNext101 [54]</cell><cell>79.31</cell><cell>94.5</cell><cell>76.7</cell></row><row><cell>Inception-V3 [45]</cell><cell>78.8</cell><cell>94.4</cell><cell>16.4</cell></row><row><cell>EfficientNet-B1 [47]</cell><cell>77.3</cell><cell>93.5</cell><cell>29.5</cell></row><row><cell>EfficientNet-B2</cell><cell>79.2</cell><cell>94.5</cell><cell>47.6</cell></row><row><cell>NASNet-A [61]</cell><cell>78.6</cell><cell>94.2</cell><cell>-</cell></row><row><cell>BASE [44]</cell><cell>74.3</cell><cell>91.9</cell><cell>-</cell></row><row><cell>CATCH-Net-A</cell><cell>79.04</cell><cell>94.43</cell><cell>16.9</cell></row><row><cell>CATCH-Net-B</cell><cell>79.46</cell><cell>94.7</cell><cell>33.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>× 16, 32 × 32, or 224 × 224. Thus there are 3 × 1000 10</figDesc><table><row><cell>+ 1000 20</cell><cell>+ 1000 30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on Cityscapes compared to manually designed and NAS searched backbones. Latency is measured on Tesla V100 with one image with shape(3, 1024, 1024). SS and MS denote for single scale and multiple scale testing respectively.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Latency (ms) mIoU (SS) mIoU (MS)</cell></row><row><cell>BiSeNet [58]</cell><cell>ResNet101</cell><cell>41</cell><cell>-</cell><cell>80.3</cell></row><row><cell>DeepLabv3+ [7]</cell><cell>Xception-65</cell><cell>85</cell><cell>77.82</cell><cell>79.3</cell></row><row><cell>CCNet [21]</cell><cell>ResNet50</cell><cell>175</cell><cell>-</cell><cell>78.5</cell></row><row><cell>DUC [50]</cell><cell>ResNet152</cell><cell>-</cell><cell>76.7</cell><cell>-</cell></row><row><cell>DANet [17]</cell><cell>ResNet50</cell><cell>-</cell><cell>76.34</cell><cell>-</cell></row><row><cell cols="2">Auto-DeepLab [32] Searched Backbone</cell><cell>-</cell><cell>79.94</cell><cell>-</cell></row><row><cell>DPC [6]</cell><cell>Xception-71</cell><cell>-</cell><cell>80.1</cell><cell>-</cell></row><row><cell>CATCH</cell><cell>CATCH-Net-D</cell><cell>27</cell><cell>79.52</cell><cell>81.12</cell></row></table><note>Search Results. Table 2 compares the searched architectures with other widely- recognized networks on ImageNet. CATCH-Net-A outperforms many listed net- works. Its accuracy is comparable with EfficientNet-B1 and ResNext-101, yet it is 2.82X and 4.54X faster. CATCH-Net-B outperforms ResNext-101 while short- ens the latency by 2.28X. The network comparison on COCO and Cityscapes is presented in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Comparison of CATCH when using ground truth as the evaluator (CATCH-GT), CATCH without evaluator (CATCH-w/o-evaluator), and CATCH-meta. The results are taken from 100 trials where each trail contains 50 search epochs. We report the mean ± std for each setting in the table.</figDesc><table><row><cell></cell><cell cols="3">CIFAR-10 CIFAR-100 ImageNet16-120</cell></row><row><cell>CATCH-GT</cell><cell cols="2">91.64±0.09 73.31±0.16</cell><cell>47.18±0.09</cell></row><row><cell cols="3">CATCH-w/o-evaluator 91.17±0.25 72.08±0.68</cell><cell>45.86±0.54</cell></row><row><cell>CATCH-meta</cell><cell cols="2">91.63±0.11 73.29±0.31</cell><cell>46.37±0.53</cell></row><row><cell>Max Acc.</cell><cell>91.719</cell><cell>73.45</cell><cell>47.19</cell></row><row><cell cols="2">4 CATCHer Training Details</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Controller hyperparameters</figDesc><table><row><cell>Hyperparameter</cell><cell>Value (meta-train)</cell><cell>NAS-Bench-201 [3] (adaptation)</cell><cell>Residual Block Search Space (adaptation)</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0001</cell></row><row><cell>Adam scheduler step size</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell>Adam scheduler gamma</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell>Update frequency</cell><cell>1 epoch</cell><cell>1 epoch</cell><cell>1 epoch</cell></row><row><cell>Clipping parameter</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Memory size</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>Discount γ</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell>GAE parameter λ</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell></row><row><cell>Value Function coeff.</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Entropy coeff.</cell><cell>0.01</cell><cell>0.03</cell><cell>0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>A mapping of Neural Architecture Search elements to MDP factors for controller training. l denotes the current timestep. Invalid actions are masked by zeroing out their probabilities in the outputs, then softmax the remaining probabilities and sample accordingly. (z, [a1...a l−1 ]) Latent context and the current network design. Next state s l+1 (z, [a1...a l ])</figDesc><table><row><cell>MDP Factor</cell><cell>Value</cell><cell>Explanation</cell></row><row><cell>Current state s l Current action a</cell><cell>a l</cell><cell>A one-hot vector of the current design choice.</cell></row><row><cell>Reward r</cell><cell>R</cell><cell>A function of the evaluated network's performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Encoder hyperparameters</figDesc><table><row><cell cols="2">Hyperparameter Value</cell></row><row><cell cols="2">Learning rate 0.01</cell></row><row><cell cols="2">Dimension of z 10</cell></row><row><cell>KL weight β</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Evaluator hyperparameters</figDesc><table><row><cell>Hyperparameter</cell><cell cols="2">Value (meta-train) (adaptation) Value</cell></row><row><cell>Learning rate</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>Exploration factor initial value</cell><cell>1.0</cell><cell>0.5</cell></row><row><cell>Exploration factor decay rate</cell><cell>0.025</cell><cell>0.025</cell></row><row><cell>Exploration factor decay step</cell><cell>20</cell><cell>20</cell></row><row><cell>Number of networks evaluated per epoch</cell><cell>25</cell><cell>25</cell></row><row><cell>PER [8] prioritization factor α</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>PER bias correction factor β</cell><cell>0.575</cell><cell>0.575</cell></row><row><cell>PER β annealing step size</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell cols="3">5 ImageNet, COCO, and Cityscapes Training Settings</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 -</head><label>6</label><figDesc></figDesc><table /><note>8 shows our training configurations on ImageNet [2] , COCO</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>ImageNet training hyperparameters with 8 GPUs.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value (partial-train)</cell><cell>Value (fully-train)</cell></row><row><cell>Learning rate</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Learning rate momentum</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell cols="3">Weight decay Learning rate warmup linear for 3 epochs linear for 3 epochs 1 × 10 −3 4 × 10 −5</cell></row><row><cell>Learning rate decay policy</cell><cell>cosine</cell><cell>cosine</cell></row><row><cell>Total epoch</cell><cell>40</cell><cell>240</cell></row><row><cell>Batch size</cell><cell>1024</cell><cell>512</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>COCO training hyperparameters with 8 GPUs.</figDesc><table><row><cell>Hyperparameters</cell><cell>Value (partial-train)</cell><cell>Value (fully-train)</cell></row><row><cell>Normalization</cell><cell cols="2">Group Normalization Batch Normalization</cell></row><row><cell>Batch size</cell><cell>16</cell><cell>16</cell></row><row><cell>Learning rate</cell><cell>0.18</cell><cell>0.02</cell></row><row><cell>Learning rate momentum</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>Weight decay</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>Learning rate decay policy</cell><cell>cosine</cell><cell>step</cell></row><row><cell>Total epoch</cell><cell>9</cell><cell>24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Cityscapes training hyperparameters with 8 GPUs.</figDesc><table><row><cell>Hyperparameters</cell><cell>Value (partial-train)</cell><cell>Value (fully-train)</cell></row><row><cell>Baseline model</cell><cell>BiSeNet [13]</cell><cell>BiSeNet</cell></row><row><cell>Convolution</cell><cell>ConvWS2D</cell><cell>Conv2D</cell></row><row><cell>Normalization</cell><cell cols="2">Group Normalization Synchronized BN</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>16</cell></row><row><cell>Learning rate</cell><cell>0.02</cell><cell>0.025</cell></row><row><cell>Learning rate momentum</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>Weight decay Learning rate warmup</cell><cell cols="2">5 × 10 −4 linear for 5 epochs linear for 5 epochs 1 × 10 −4</cell></row><row><cell>Learning rate decay policy</cell><cell>cosine</cell><cell>polynomial</cell></row><row><cell>Total epoch</cell><cell>40</cell><cell>100</cell></row><row><cell cols="3">6 Searched Models of Residual Block Search Space</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Searched models in Residual Block search space.</figDesc><table><row><cell>Searched Model</cell><cell cols="2">Input Depth Channel</cell><cell cols="3">Stage Distribution Distribution Channel FLOPS(G) Params(MB)</cell></row><row><cell>CATCH-Net-A</cell><cell>64</cell><cell>20</cell><cell>[2, 7, 8, 3] [5, 4, 8, 3]</cell><cell>4.45</cell><cell>25.96</cell></row><row><cell>CATCH-Net-B</cell><cell>64</cell><cell>25</cell><cell>[8, 5, 8, 4] [3, 10, 8, 4]</cell><cell>9.84</cell><cell>32.16</cell></row><row><cell>CATCH-Net-C</cell><cell>64</cell><cell>20</cell><cell>[5, 4, 5, 6] [1, 8, 5, 6]</cell><cell>8.08</cell><cell>37.03</cell></row><row><cell>CATCH-Net-D</cell><cell>64</cell><cell>20</cell><cell>[1, 8, 5, 6] [2, 7, 7, 4]</cell><cell>4.46</cell><cell>30.98</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Xin Chen 1 , Yawen Duan 1 , Zewei Chen 2 , Hang Xu 2 , Zihao Chen 2 , Xiaodan Liang 3 , Tong Zhang 4 , Zhenguo Li 2</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix for CATCH: Context-based Meta</head><p>Reinforcement Learning for Transferrable Architecture Search</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Searching for efficient multiscale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8699" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8699" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10979</idno>
		<title level="m">Neural architecture search on object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient multiobjective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09081</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Staffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11090</idno>
		<title level="m">Meta-learning of neural architectures for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="492" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-meta: Automated gradient based meta learner search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongseok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Yeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06927</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinghui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06527</idno>
		<title level="m">Meta reinforcement learning with task embedding and shared policy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards fast adaptation of neural architectures with meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yintao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxiong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jincai</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11235</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<title level="m">Deeparchitect: Automatically designing and training deep architectures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Continual and multi-task architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05226</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Efficient off-policy meta-reinforcement learning via probabilistic context variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aging evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Prioritized experience replay. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11225" to="11235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">NAS-FCOS: fast neural architecture search for object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07420</idno>
		<title level="m">Finding competitive network architectures within a day using uct</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transfer learning with neural automl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8356" to="8365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Auto-fpn: Automatic network architecture adaptation for object detection beyond classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6649" to="6658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Nas evaluation is frustratingly hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperanã</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Sm-nas: Structural-to-modular neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09929</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno>abs/1811.08883</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="492" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Prioritized experience replay. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">NAS-FCOS: fast neural architecture search for object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Sm-nas: Structural-to-modular neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09929</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
