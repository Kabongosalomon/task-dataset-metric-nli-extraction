<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
							<email>xiang43@purdue.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
							<email>yapengtian@rochester.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
							<email>yulun100@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<email>yunfu@ece.neu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
							<email>allebach@ecn.purdue.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
							<email>chenliang.xu@rochester.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Overlayed LR inputs HR intermediate frame Overlayed LR inputs DAIN+Bicubic DAIN+EDVR Ours Figure 1: Example of space-time video super-resolution. We propose a one-stage space-time video super-resolution (STVSR) network to directly predict high frame rate (HFR) and high-resolution (HR) frames from the corresponding lowresolution (LR) and low frame rate (LFR) frames without explicitly interpolating intermediate LR frames. A HR intermediate frame t and its neighboring low-resolution frames: t − 1 and t + 1 as an overlayed image are shown. Compare to a state-ofthe-art two-stage method: DAIN [1]+EDVR [37] on the HR intermediate frame t, our method is more capable of handling visual motions and therefore restores more accurate image structures and sharper edges.</p><p>In addition, our network is more than 3 times faster on inference speed with a 4 times smaller model size than the DAIN+EDVR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we explore the space-time video superresolution task, which aims to generate a high-resolution (HR) slow-motion video from a low frame rate (LFR), lowresolution (LR) video. A simple solution is to split it into two sub-tasks: video frame interpolation (VFI) and video superresolution (VSR). However, temporal interpolation and spatial super-resolution are intra-related in this task. Twostage methods cannot fully take advantage of the natural property. In addition, state-of-the-art VFI or VSR networks require a large frame-synthesis or reconstruction module for predicting high-quality video frames, which makes the two-stage methods have large model sizes and thus be timeconsuming. To overcome the problems, we propose a onestage space-time video super-resolution framework, which directly synthesizes an HR slow-motion video from an LFR, LR video. Rather than synthesizing missing LR video frames * Equal contribution; † Equal advising.</p><p>as VFI networks do, we firstly temporally interpolate LR frame features in missing LR video frames capturing local temporal contexts by the proposed feature temporal interpolation network. Then, we propose a deformable ConvL-STM to align and aggregate temporal information simultaneously for better leveraging global temporal contexts. Finally, a deep reconstruction network is adopted to predict HR slow-motion video frames. Extensive experiments on benchmark datasets demonstrate that the proposed method not only achieves better quantitative and qualitative performance but also is more than three times faster than recent two-stage state-of-the-art methods, e.g., DAIN+EDVR and DAIN+RBPN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Space-Time Video Super-Resolution (STVSR) <ref type="bibr" target="#b29">[30]</ref> aims to automatically generate a photo-realistic video sequence with a high space-time resolution from a low-resolution and low frame rate input video. Since HR slow-motion videos are more visually appealing containing fine image details and clear motion dynamics, they are desired in rich applications, such as film making and high-definition television.</p><p>To tackle the problem, most existing works in previous literatures <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref> usually adopt hand-crafted regularization and make strong assumptions. For example, space-time directional smoothness prior is adopted in <ref type="bibr" target="#b29">[30]</ref>, and <ref type="bibr" target="#b21">[22]</ref> assumes that there is no significant change in illumination for the static pixels. However, these strong constraints make the methods have limited capacity in modeling various and diverse space-time visual patterns. Besides, the optimization for these methods is usually computationally expensive (e.g., ∼ 1 hour for 60 frames in <ref type="bibr" target="#b21">[22]</ref>).</p><p>In recent years, deep convolutional neural networks have shown promising efficiency and effectiveness in various video restoration tasks, such as video frame interpolation (VFI) <ref type="bibr" target="#b23">[24]</ref>, video super-resolution (VSR) <ref type="bibr" target="#b3">[4]</ref>, and video deblurring <ref type="bibr" target="#b31">[32]</ref>. To design an STVSR network, one straightforward way is by directly combining a video frame interpolation method (e.g., SepConv <ref type="bibr" target="#b24">[25]</ref>, ToFlow <ref type="bibr" target="#b39">[40]</ref>, DAIN <ref type="bibr" target="#b0">[1]</ref> etc.) and a video super-resolution method (e.g., DUF <ref type="bibr" target="#b10">[11]</ref>, RBPN <ref type="bibr" target="#b7">[8]</ref>, EDVR <ref type="bibr" target="#b36">[37]</ref> etc.) in a two-stage manner. It firstly interpolates missing intermediate LR video frames with VFI and then reconstructs all HR frames with VSR. However, temporal interpolation and spatial super-resolution in STVSR are intra-related. The two-stage methods splitting them into two individual procedures cannot make full use of this natural property. In addition, to predict high-quality video frames, both state-of-the-art VFI and VSR networks require a big frame reconstruction network. Therefore, the composed two-stage STVSR model will contain a large number of parameters and is computationally expensive.</p><p>To alleviate the above issues, we propose a unified onestage STVSR framework to learn temporal interpolation and spatial super-resolution simultaneously. We propose to adaptively learn a deformable feature interpolation function for temporally interpolating intermediate LR frame features rather than synthesizing pixel-wise LR frames as in twostage methods. The learnable offsets in the interpolation function can aggregate useful local temporal contexts and help the temporal interpolation handle complex visual motions. In addition, we introduce a new deformable ConvL-STM model to effectively leverage global contexts with simultaneous temporal alignment and aggregation. HR video frames can be reconstructed from the aggregated LR features with a deep SR reconstruction network. To this end, the one-stage network can learn end-to-end to map an LR, LFR video sequence to its HR, HFR space in a sequence-tosequence manner. Experimental results show that the proposed one-stage STVSR framework outperforms state-ofthe-art two-stage methods even with much fewer parameters. An example is illustrated in <ref type="figure">Figure 1</ref>.</p><p>The contributions of this paper are three-fold: (1) We propose a one-stage space-time super-resolution network that can address temporal interpolation and spatial SR simultaneously in a unified framework. Our one-stage method is more effective than two-stage methods taking advantage of the intra-relatedness between the two subproblems. It is also computationally more efficient since only one frame reconstruction network is required rather than two large networks as in state-of-the-art two-stage approaches. <ref type="bibr" target="#b1">(2)</ref> We propose a frame feature temporal interpolation network leveraging local temporal contexts based on deformable sampling for intermediate LR frames. We devise a novel deformable ConvLSTM to explicitly enhance temporal alignment capacity and exploit global temporal contexts for handling large motions in videos. (3) Our onestage method achieves state-of-the-art STVSR performance on both Vid4 <ref type="bibr" target="#b16">[17]</ref> and Vimeo <ref type="bibr" target="#b39">[40]</ref>. It is 3 times faster than the two-stage network: DAIN [1] + EDVR <ref type="bibr" target="#b36">[37]</ref> while having a nearly 4× reduction in model size. The source code is released in https://github.com/Mukosame/Zooming-SlowMo-CVPR-2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we discuss works on three related topics: video frame interpolation (VFI), video super-resolution (VSR), and space-time video super-resolution (STVSR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Frame Interpolation</head><p>The target of video frame interpolation is to synthesize non-existent intermediate frames in between the original frames. Meyer et al. <ref type="bibr" target="#b20">[21]</ref> introduced a phase-based frame interpolation method, which generates intermediate frames through per-pixel phase modification. Long et al. <ref type="bibr" target="#b18">[19]</ref> predicted intermediate frames directly with an encoder-decoder CNN. Niklaus et al. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> regarded the frame interpolation as a local convolution over the two input frames and used a CNN to learn a spatiallyadaptive convolution kernel for each pixel for high-quality frame synthesis. To explicitly handle motions, there are also many flow-based video interpolation approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref>. These methods usually have inherent issues with inaccuracies and missing information from optical flow results. In our one-stage STVSR framework, rather than synthesizing the intermediate LR frames as current VFI methods do, we interpolate features from two neighboring LR frames to directly synthesize LR feature maps for missing frames without requiring explicit supervision.</p><p>Video Super-Resolution Video super-resolution aims to reconstruct an HR video frame from the corresponding LR frame (reference frame) and its neighboring LR frames (supporting frames). One key problem for VSR is how to temporally align the LR supporting frames with the reference frame. Several VSR methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref> use optical flow for explicit temporal alignment, which first estimates motions between the reference frame and each supporting frame with optical flow and then warps the supporting frame using the predicted motion map. Recently, RBPN proposes to incorporate the single image and multi-frame SR for VSR in which flow maps are directly concatenated with LR video frames. However, it is difficult to obtain accurate flow; and flow warping also introduces artifacts into the aligned frames. To avoid this problem, DUF <ref type="bibr" target="#b10">[11]</ref> with dynamic filters and TDAN <ref type="bibr" target="#b34">[35]</ref> with deformable alignment were proposed for implicit temporal alignment without motion estimation. EDVR <ref type="bibr" target="#b36">[37]</ref> extends the deformable alignment in TDAN by exploring multiscale information. However, most of the above methods are many-to-one architectures, and they need to process a batch of LR frames to predict only one HR frame, which makes the methods computationally inefficient. Recurrent neural networks, such as convolutional LSTMs <ref type="bibr" target="#b38">[39]</ref> (ConvLSTM), can ease sequence-to-sequence (S2S) learning; and they are adopted in VSR methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9]</ref> for leveraging temporal information. However, without explicit temporal alignment, the RNN-based VSR networks have limited capability in handling large and complex motions within videos. To achieve efficient yet effective modeling, unlike existing methods, we propose a novel ConvLSTM structure embedded with an explicit state updating cell for space-time video superresolution.</p><p>Rather than simply combining a VFI network and a VSR network to solve STVSR, we propose a more efficient and effective one-stage framework that simultaneously learns temporal feature interpolation and spatial SR without accessing to LR intermediate frames as supervision.</p><p>Space-Time Video Super-Resolution Shechtman et al. <ref type="bibr" target="#b28">[29]</ref> firstly proposed to extend SR to the space-time domain. Since pixels are missing in LR frames and even several entire LR frames are unavailable, STVSR is a highly ill-posed inverse problem. To increase video resolution both in time and space, <ref type="bibr" target="#b28">[29]</ref> combines information from multiple video sequences of dynamic scenes obtained at sub-pixel and sub-frame misalignments with a directional space-time smoothness regularization to constrain the ill-posed problem. Mudenagudi <ref type="bibr" target="#b21">[22]</ref> posed STVSR as a reconstruction problem using the Maximum a posteriori-Markov Random Field <ref type="bibr" target="#b6">[7]</ref> with graph-cuts <ref type="bibr" target="#b2">[3]</ref> as the solver. Takeda et al. <ref type="bibr" target="#b32">[33]</ref> exploited local orientation and local motion to steer spatio-temporal regression kernels. Shahar et al. <ref type="bibr" target="#b27">[28]</ref> proposed to exploit a space-time patch recurrence prior within natural videos for STVSR. However, these methods have limited capacity to model rich and complex space-time visual patterns, and the optimization for these methods is usually computationally expensive. To address these issues, we propose a one-stage network to directly learn the mapping between partial LR observations and HR video frames and to achieve fast and accurate STVSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Space-Time Video Super-Resolution</head><p>Given an LR, LFR video sequence:</p><formula xml:id="formula_0">I L = {I L 2t−1 } n+1 t=1</formula><p>, our goal is to generate the corresponding high-resolution slow-motion video sequence:</p><formula xml:id="formula_1">I H = {I H t } 2n+1 t=1 . To inter- mediate HR frames {I H 2t } n t=1</formula><p>, there are no corresponding LR counterparts in the input sequence. To fast and accurately increase resolution in both space and time domains, we propose a one-stage space-time super-resolution framework: Zooming Slow-Mo as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. The framework mainly consists of four parts: feature extractor, frame feature temporal interpolation module, deformable ConvLSTM, and HR frame reconstructor.</p><p>We first use a feature extractor with a convolutional layer and k 1 residual blocks to extract feature maps:</p><formula xml:id="formula_2">{F L 2t−1 } n+1 t=1</formula><p>from input video frames. Taking the feature maps as input,</p><formula xml:id="formula_3">Concat. " Conv $ % " % offset field offsets approximated 0 % Φ " " ( " % , Φ " ) $ ( $ % , Φ $ ) blending offsets Concat. $ Conv Φ $</formula><p>Deformable convolution offset field <ref type="figure">Figure 3</ref>: Frame feature temporal interpolation based on deformable sampling. Since approximated F L 2 will be used to predict the corresponding HR frame, it will implicitly enforce the learnable offsets to capture accurate local temporal contexts and be motion-aware.</p><p>we then synthesize the LR feature maps: {F L 2t } n t=1 of intermediate frames with the proposed frame feature interpolation module. Furthermore, to better leverage temporal information, we use a deformable ConvLSTM to process the consecutive feature maps:</p><formula xml:id="formula_4">{F L t } 2n+1 t=1</formula><p>. Unlike vanilla Con-vLSTM, the proposed deformable ConvLSTM can simultaneously perform temporal alignment and aggregation. Finally, we reconstruct the HR slow-mo video sequence from the aggregated feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Frame Feature Temporal Interpolation</head><p>Given extracted feature maps: F L 1 and F L 3 from input LR video frames: I L 1 and I L 3 , we want to synthesize the feature map F L 2 corresponding to the missing intermediate LR frame I L 2 . Traditional video frame interpolation networks usually perform temporal interpolation on pixel-wise video frames, which will lead to a two-stage STVSR design. Unlike previous methods, we propose to learn a feature temporal interpolation function f (·) to directly synthesize the intermediate feature map F L 2 (see <ref type="figure">Fig. 3</ref>). A general form of the interpolation function can be formulated as:</p><formula xml:id="formula_5">F L 2 = f (F L 1 , F L 3 ) = H(T 1 (F L 1 , Φ 1 ), T 3 (F L 3 , Φ 3 )) ,<label>(1)</label></formula><p>where T 1 (·) and T 3 (·) are two sampling functions and Φ 1 and Φ 3 are the corresponding sampling parameters; H(·) is a blending function to aggregate sampled features. For generating accurate F L 2 , the T 1 (·) should capture forward motion information between F L 1 and F L 2 , and the T 3 (·) should capture backward motion information between F L 3 and F L 2 . However, the F L 2 is not available for computing forward and backward motion information in this task.</p><p>To alleviate this problem, we use motion information between F L 1 and F L 3 to approximate forward and backward motion information. Inspired by recent deformable alignment in <ref type="bibr" target="#b34">[35]</ref> for VSR, we propose to use deformable sampling functions to implicitly capture motion information for frame feature temporal interpolation. With exploring rich local temporal contexts by deformable convolutions in sampling functions, our feature temporal interpolation can even handle very large motions in videos. The two sampling functions share the same network design but have different weights. For simplicity, we use the T 1 (·) as an example. It takes LR frame feature maps F L 1 and F L 3 as input to predict an offset for sampling the F L 1 :</p><formula xml:id="formula_6">∆p 1 = g 1 ([F L 1 , F L 3 ]) ,<label>(2)</label></formula><p>where ∆p 1 is a learnable offset and also refers to the sampling parameter: Φ 1 ; g 1 denotes a general function of several convolution layers; [, ] denotes the channel-wise concatenation. With the learned offset, the sampling function can be performed with a deformable convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>:</p><formula xml:id="formula_7">T 1 (F L 1 , Φ 1 ) = DConv(F L 1 , ∆p 1 ) .<label>(3)</label></formula><p>Similarly, we can learn an offset</p><formula xml:id="formula_8">∆p 3 = g 3 ([F L 3 , F L 1 ])</formula><p>as the sampling parameter: Φ 3 and then obtain sampled fea-</p><formula xml:id="formula_9">tures T 3 (F L 3 , Φ 3 )</formula><p>with a deformable convolution. To blend the two sampled features, we use a simple linear blending function H(·):</p><formula xml:id="formula_10">F L 2 = α * T 1 (F L 1 , Φ 1 ) + β * T 3 (F L 3 , Φ 3 ) ,<label>(4)</label></formula><p>where α and β are two learnable 1 × 1 convolution kernels and * is a convolution operator. Since the synthesized LR feature map F L 2 will be used to predict the intermediate HR frame I H 2 , it will enforce the synthesized LR feature map to be close to the real intermediate LR feature map. Therefore, the two offsets ∆p 1 and ∆p 3 will implicitly learn to capture the forward and backward motion information, respectively.</p><p>Applying the designed deformable temporal interpolation function to {F L 2t−1 } n+1 t=1 , we can obtain intermediate frame feature maps {F L 2t } n t=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deformable ConvLSTM</head><p>Now we have consecutive frame feature maps:</p><formula xml:id="formula_11">{F L t } 2n+1 t=1</formula><p>for generating the corresponding HR video frames, which will be a sequence-to-sequence mapping. It has been proved in previous video restoration tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37</ref>] that temporal information is vital. Therefore, rather than reconstructing HR frames from the corresponding individual feature maps, we aggregate temporal contexts from neighboring frames. ConvLSTM <ref type="bibr" target="#b38">[39]</ref> is a popular 2D sequence data modeling method and we can adopt it to perform temporal aggregation. At the time step t, the ConvL-STM updates hidden state h t and cell state c t with: From its state updating mechanism <ref type="bibr" target="#b38">[39]</ref>, we can learn that the ConvLSTM can only implicitly capture motions between previous states: h t−1 and c t−1 and the current input feature map with small convolution receptive fields. Therefore, ConvLSTM has limited ability to handle large motions in natural videos. If a video has large motions, there will be a severe temporal mismatch between previous states and F L t . Then, h t−1 and c t−1 will propagate mismatched "noisy" content rather than useful global temporal contexts into h t . Consequently, the reconstructed HR frame I H t from h t will suffer from annoying artifacts.</p><formula xml:id="formula_12">h t , c t = ConvLST M (h t−1 , c t−1 , F L t ) .<label>(5)</label></formula><p>To tackle the large motion problem and effectively exploit global temporal contexts, we explicitly embed a stateupdating cell with deformable alignment into ConvLSTM (see <ref type="figure" target="#fig_1">Fig. 4</ref>):</p><formula xml:id="formula_13">∆p h t = g h ([h t−1 , F L t ]) , ∆p c t = g c ([c t−1 , F L t ]) , h a t−1 = DConv(h t−1 , ∆p h t ) , c a t−1 = DConv(c t−1 , ∆p c t ) , h t , c t = ConvLST M (h a t−1 , c a t−1 , F L t ) ,<label>(6)</label></formula><p>where g h and g c are general functions of several convolution layers, ∆p h t and ∆p c t are predicted offsets, and h a t−1 and c a t−1 are aligned hidden and cell states, respectively. Compared with vanilla ConvLSTM, we explicitly enforce the hidden state h t−1 and cell state c t−1 to align with the current input feature map F L t in our deformable ConvL-STM, which makes it more capable of handling motions in videos. Besides, to fully explore temporal information, we use the Deformable ConvLSTM in a bidirectional manner <ref type="bibr" target="#b26">[27]</ref>. We feed temporally reversed feature maps into the same Deformable ConvLSTM and concatenate hidden states from forward pass and backward pass as the final hidden state h t 2 for HR frame reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Frame Reconstruction</head><p>To reconstruct HR video frames, we use a temporally shared synthesis network, which takes individual hidden state h t as input and outputs the corresponding HR frame. It has k 2 stacked residual blocks <ref type="bibr" target="#b15">[16]</ref> for learning deep features and utilizes a sub-pixel upscaling module with Pix-elShuffle as in <ref type="bibr" target="#b30">[31]</ref> to reconstruct HR frames {I H t } 2n+1 t=1 . To optimize our network, we use a reconstruction loss function:</p><formula xml:id="formula_14">l rec = ||I GT t − I H t || 2 + 2 ,<label>(7)</label></formula><p>where I GT t refers to the t-th ground-truth HR video frame, Charbonnier penalty function <ref type="bibr" target="#b12">[13]</ref> is used as the loss term, and is empirically set to 1 × 10 −3 . Since the space and time SR problems are intra-related in STVSR, our model is end-to-end trainable and can simultaneously learn this spatio-temporal interpolation with only supervision from HR video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>In our implementation, k 1 = 5 and k 2 = 40 residual blocks are used in feature extraction and HR frame reconstruction modules, respectively. We randomly crop a sequence of down-sampled image patches with the size of 32 × 32 and take out the odd-indexed 4 frames as LFR and LR inputs, and the corresponding consecutive 7-frame sequence of 4× 3 size as supervision. Besides, we perform data augmentation by randomly rotating 90 • , 180 • and 270 • , and horizontal-flipping. We adopt a Pyramid, Cascading and Deformable (PCD) structure in <ref type="bibr" target="#b36">[37]</ref> to employ deformable alignment and apply Adam <ref type="bibr" target="#b11">[12]</ref> optimizer, where we decay the learning rate with a cosine annealing for each batch <ref type="bibr" target="#b19">[20]</ref> from 4e − 4 to 1e − 7. The batch size is set to be 24 and trained on 2 Nvidia Titan XP GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets We use Vimeo-90K as the training set <ref type="bibr" target="#b39">[40]</ref>, including more than 60,000 7-frame training video sequences. The dataset is widely used in previous VFI and VSR works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37]</ref>. Vid4 <ref type="bibr" target="#b16">[17]</ref> and Vimeo testset <ref type="bibr" target="#b39">[40]</ref> are used as the evaluation datasets. To measure the performance of different methods under different motion conditions, we split the Vimeo testset into fast motion, medium motion, and slow motion sets as in <ref type="bibr" target="#b7">[8]</ref>, which include 1225, 4977 and 1613 video clips, respectively. We remove 5 video clips from the original medium motion set and 3 clips from the slow motion set, which have consecutively all-black background frames that will lead to infinite values on PSNR. We generate LR frames by bicubic with a downsampling factor 4 and use odd-indexed LR frames as input to predict the corresponding consecutive HR and HFR frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to State-of-the-art Methods</head><p>We compare the performance of our one-stage Zooming SlowMo network to two-stage methods composed of stateof-the-art (SOTA) VFI and VSR networks. Three recent SOTA VFI approaches, SepConv <ref type="bibr" target="#b24">[25]</ref>, Super-SloMo 4 <ref type="bibr" target="#b9">[10]</ref>, and DAIN <ref type="bibr" target="#b0">[1]</ref>, are compared. To achieve STVSR, three SOTA SR models, including single-image SR model, RCAN <ref type="bibr" target="#b40">[41]</ref>, and two recent VSR models, RBPN <ref type="bibr" target="#b7">[8]</ref> and EDVR <ref type="bibr" target="#b36">[37]</ref>, are used to generate HR frames from both original LR and interpolated LR frames.</p><p>Quantitative results are shown in <ref type="table">Table 1</ref>. From the table, we can learn the following facts: (1) DAIN+EDVR is <ref type="table">Table 1</ref>: Quantitative comparison of our results and two-stage VFI and VSR methods on testsets. The best two results are highlighted in red and blue colors, respectively. The total runtime is measured on the entire Vid4 dataset <ref type="bibr" target="#b16">[17]</ref>. Note that we omit the baseline models with Bicubic when comparing in terms of runtime. (3) VSR also matters. For example, with the same VFI network: DAIN, EDVR consistently achieves better STVSR performance than other VSR methods. In addition, we can see that our network outperforms the DAIN+EDVR by 0.19dB on Vid4, 0.25dB on Vimeo-Slow, 0.75dB on Vimeo-Medium, and 1dB on Vimeo-Fast in terms of PSNR. The significant improvements obtained on videos with fast motions demonstrate that our one-stage network with simultaneously leveraging local and global temporal contexts is more capable of handling diverse spatio-temporal patterns, including challenging large motions in videos than two-stage methods.</p><p>Moreover, we also investigate model sizes and runtime of different networks in <ref type="table">Table 1</ref>. For synthesizing highquality frames, SOTA VFI and VSR networks usually have very large frame reconstruction modules. Thus, the composed two-stage SOTA STVSR networks will contain a huge number of parameters. With only one frame reconstruction module, our one-stage model has much fewer parameters than the SOTA two-stage networks. From <ref type="table">Table 1</ref>, we can see that it is more than 4× and 3× smaller than the DAIN+EDVR and DAIN+RBPN, respectively. The small model size makes our network more than 3× faster than the DAIN+EDVR and 8× faster than DAIN+RBPN. Compared to two-stage methods with a fast VFI network: Su-perSlowMo, our method is still more than 2× faster.</p><p>Visual results of different methods are illustrated in <ref type="figure">Figure 5</ref>. We see that our method achieves noticeably visual improvements over other two-stage methods. Clearly, the proposed network can synthesize visually appealing HR video frames with more fine details, more accurate  <ref type="figure">Figure 6</ref>: Ablation study on feature interpolation. The naive feature interpolation model without deformable sampling will obtain overly smooth results for videos with fast motions. With the proposed deformable feature interpolation (DFI), our model can well exploit local contexts in adjacent frames, thus is more effective in handling large motions.</p><p>structures, and fewer blurring artifacts even for challenging fast motion video sequences. We also observe that current SOTA VFI methods: SepConv and DAIN fail to handle large motions. Consequently, two-stage networks tend to generate HR frames with severe motion blurs. In our one-stage framework, we simultaneously learn temporal and spatial SR with exploring the natural intra-relatedness. Even with a much smaller model, our network can well address the large motion issue in temporal SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We have already shown the superiority of our one-stage framework over two-stage networks. To further demonstrate the effectiveness of different modules in our network, we make a comprehensive ablation study.  <ref type="figure">Figure 7</ref>: Ablation study on Deformable ConvLSTM (DConvLSTM). ConvLSTM will fail when meeting videos with fast motions. Embedded with state updating cells, the proposed DConvLSTM is more capable of leveraging global temporal contexts for reconstructing more accurate visual content even for fast motion videos.  <ref type="figure">Figure 8</ref>: Ablation study on the bidirectional mechanism in DConvLSTM. Adding the bidirectional mechanism into DConvLSTM, the model can leverage both previous and future contexts, and therefore reconstructs more visually appealing frames with finer image details, especially for video frames at the first time step, which can not access any temporal information from other frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Deformable Feature Interpolation</head><p>(DFI) module, we introduce two baselines: (a) and (b), where the model (a) only uses convolutions to blend LR features without deformable sampling functions as in model (b). In addition, neither (a) or (b) has ConvLSTM or DCon-vLSTM. From <ref type="table" target="#tab_3">Table 2</ref>, we find that (b) outperforms (a) by 0.16dB on Vid4 with slow motions and 0.73dB on Vimeo-Fast with fast motions in terms of PSNR. <ref type="figure">Figure 6</ref> shows a visual comparison. We can see that (a) produces a face with severe motion blur, while the proposed deformable feature interpolation with exploiting local temporal contexts can effectively address the large motion issue and help the model (b) generate a frame with more clear face structures and details. The superiority of the proposed DFI module demonstrates that the learned offsets in the deformable sampling functions can effectively exploit local temporal contexts and successfully capture forward and backward motions even without any explicit supervision. From <ref type="table" target="#tab_3">Table 2</ref>, we can see that (c) outperforms (b) on Vid4 with slow motion videos while it is worse than (b) on Vimeo-Fast with fast motion sequences. The results validate that vanilla ConvLSTM can leverage useful global temporal contexts for slow motion videos, but cannot handle large motions in videos. Moreover, we observe that (d) is significantly better than both (b) and (c), which demonstrates that our DConvLSTM can successfully learn the temporal alignment between previous states and the current feature map. Therefore, it can better exploit global contexts for reconstructing visually pleasing frames with more details. Visual results in <ref type="figure">Figure 7</ref> further support our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Deformable ConvLSTM</head><p>In addition, we compare (e) and (d) in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="figure">Figure 8</ref> to verify the bidirectional mechanism in DConvL-STM. From <ref type="table" target="#tab_3">Table 2</ref>, we can see that (e) can further improve STVSR performance over (d) on both slow motion and fast motion testing sets. The visual results in <ref type="figure">Figure 8</ref> further shows that our full model with a bidirectional mechanism can restore more visual details by making full use of global temporal information for all input video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a one-stage framework for space-time video super-resolution to directly reconstruct high-resolution and high frame rate videos without synthesizing intermediate low-resolution frames. To achieve this, we introduce a deformable feature interpolation network for feature-level temporal interpolation. Furthermore, we propose a deformable ConvLSTM for aggregating temporal information and handling motions. With such a one-stage design, our network can well explore intra-relatedness between temporal interpolation and spatial super-resolution in the task. It enforces our model to adaptively learn to leverage useful local and global temporal contexts for alleviating large motion issues. Extensive experiments show that our one-stage framework is more effective yet efficient than existing two-stage networks, and the proposed feature temporal interpolation network and deformable ConvLSTM are capable of handling very challenging fast motion videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices Network Architecture</head><p>We further illustrate the feature temporal interpolation network in <ref type="figure">Figure 9</ref> and the proposed STVSR framework in <ref type="figure">Figure 10</ref> to help readers better understand the overall structure of our proposed network.</p><p>To make our paper be concise and easy to follow, we use a simple version of deformable sampling to introduce the proposed feature temporal interpolation and deformable ConvLSTM. However, in our implementation, as stated in Section 3.4 of the paper, we adopt a Pyramid, Cascading and Deformable (PCD) structure 5 as in <ref type="bibr" target="#b36">[37]</ref> to implement the deformable sampling, which can exploit multi-scale contexts with a feature pyramid.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our one-stage STVSR framework. It directly reconstructs consecutive HR video frames without synthesizing LR intermediate frames I L t . Feature temporal interpolation and bidirectional deformable ConvLSTM are utilized to leverage local and global temporal contexts for better exploiting temporal information and handling large motions. Note that we only show two input LR frames from a long sequence in this figure for a better illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Deformable ConvLSTM for better exploiting global temporal contexts and handling fast motion videos. At time step t, we introduce state updating cells to learn deformable sampling to adaptively align hidden state h t−1 and cell state c t−1 with current input feature map: F L t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>To validate the effect of the proposed Deformable ConvLSTM (DConvL-STM), we compare four different models: (b), (c), (d), and (e), where (c) adds a vanilla ConvLSTM structure into (b), (d) utilizes the proposed DConvLSTM, and (e) adopts a DConvLSTM in a bidirectional manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Feature te 1 OutputFigure 10 :</head><label>110</label><figDesc>Flowchart of the proposed one-stage STVSR framework. The feature extraction and HR frame reconstruction networks are temporally shared for all frames, in which different frames are processed independently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Visual comparisons of different methods on video frames from Vid4 and Vimeo datasets. Our one-stage Zooming SlowMo model can reconstruct more visually appealing HR video frames with more accurate image structures and fewer blurring artifacts.</figDesc><table><row><cell></cell><cell>HR</cell><cell>Overlayed LR</cell><cell>SepConv+RCAN</cell><cell>SepConv+RBPN</cell><cell>SepConv+EDVR</cell></row><row><cell>Overlayed LR frames</cell><cell>DAIN+Bicubic</cell><cell>DAIN+RCAN</cell><cell>DAIN+RPBN</cell><cell>DAIN+EDVR</cell><cell>Ours</cell></row><row><cell></cell><cell>HR</cell><cell>Overlayed LR</cell><cell>SepConv+RCAN</cell><cell>SepConv+RBPN</cell><cell>SepConv+EDVR</cell></row><row><cell>Overlayed LR frames</cell><cell>DAIN+Bicubic</cell><cell>DAIN+RCAN</cell><cell>DAIN+RPBN</cell><cell>DAIN+EDVR</cell><cell>Ours</cell></row><row><cell></cell><cell>HR</cell><cell>Overlayed LR</cell><cell>SepConv+RCAN</cell><cell>SepConv+RBPN</cell><cell>SepConv+EDVR</cell></row><row><cell>Overlayed LR frames</cell><cell>DAIN+Bicubic</cell><cell>DAIN+RCAN</cell><cell>DAIN+RPBN</cell><cell>DAIN+EDVR</cell><cell>Ours</cell></row><row><cell></cell><cell>HR</cell><cell>Overlayed LR</cell><cell>SepConv+RCAN</cell><cell>SepConv+RBPN</cell><cell>SepConv+EDVR</cell></row><row><cell>Overlayed LR frames</cell><cell>DAIN+Bicubic</cell><cell>DAIN+RCAN</cell><cell>DAIN+RPBN</cell><cell>DAIN+EDVR</cell><cell>Ours</cell></row><row><cell cols="3">Figure 5: Evaluation Peak Signal-to-Noise Ratio (PSNR) and Struc-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tural Similarity Index (SSIM) [38] are adopted to evaluate</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">STVSR performance of different methods. To measure the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">efficiency of different networks, we also compare the model</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">sizes and inference time of the entire Vid4 [17] dataset mea-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>sured on one Nvidia Titan XP GPU.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="7">: Ablation study on the proposed modules. Proposed</cell></row><row><cell cols="7">deformable feature interpolation network and deformable</cell></row><row><cell cols="7">ConvLSTM can effectively handle motions and improve</cell></row><row><cell cols="7">STVSR performance, while the vanilla ConvLSTM per-</cell></row><row><cell cols="6">forms worse when meeting large motions in videos.</cell></row><row><cell cols="2">Method Naive feature interpolation Deformable feature interpolation (DFI) ConvLSTM Deformable ConvLSTM (DConvLSTM) Bidirectional DConvLSTM</cell><cell>(a) √</cell><cell>(b) √</cell><cell>(c) √ √</cell><cell>(d) √ √</cell><cell>(e) √ √</cell></row><row><cell>Vid4 (slow motion)</cell><cell></cell><cell cols="5">25.18 25.34 25.68 26.18 26.31</cell></row><row><cell>Vimeo-Fast (fast motion)</cell><cell></cell><cell cols="5">34.93 35.66 35.39 36.56 36.81</cell></row><row><cell>HR</cell><cell cols="2">w/o bidirectional</cell><cell></cell><cell cols="3">w/ bidirectional</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Feature temporal interpolation for intermediate LR frames. It will predict an intermediate LR frame feature map F L 2t from two neighboring feature maps: F L 2t−1 and F L 2t+1 , where t = 1, 2, ..., n. Note that the deformable sampling module on the left samples features from F L 2t−1 with generated sampling parameters from both F L 2t−1 and F L 2t+1 ; on the contrary, the deformable sampling module on the right samples features from F L 2t+1 .</figDesc><table><row><cell cols="2">"#$% &amp; deformable sampling 1x1 conv, 64, 64 Feature Size: deformable "#'% &amp; sampling 64  *   *  1x1 conv, 64, 64 + "# &amp; Feature Size: 64  *   *  3x3 conv, 3, 64, stride=1 Residual Block conv, Relu, conv 3x3, 64, 64, stride=1 x 5 + 1 LR Frames: % &amp; , ' &amp; , … , *+,% &amp; Input Size: + 1  *  3  *   *  Output Size: + 1  *  64  *   *  Feature Temporal Interpolation Output Size: 2 + 1  *  64  *   *  Interpolate intermediate Feature extraction: temporally shared for all frames LR features Bidirectional Deformable LSTM Output Size: 2 + 1  *  64  *   *  Residual Block 3x3, 64, 64, stride=1 Figure 9: Input conv, Relu, conv x 40</cell></row><row><cell></cell><cell>3x3conv 64, 256,</cell></row><row><cell></cell><cell>Stride=1</cell></row><row><cell></cell><cell>Pixelshuffle, scale=2</cell></row><row><cell>HR frame reconstruction :</cell><cell>x 2</cell></row><row><cell>temporally shared for all frames</cell><cell>LeakyRelu</cell></row><row><cell></cell><cell>3x3 conv, 64, 64, stride=1</cell></row><row><cell></cell><cell>LeakyRelu</cell></row><row><cell></cell><cell>3x3 conv, 64, 3, stride=1</cell></row><row><cell></cell><cell>2 + 1 HR Frames: % 4 , * 4 , … , *+,% 4</cell></row><row><cell></cell><cell>Output Size: 2 + 1  *  3  *  4  *  4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use ht to denote final hidden state, but it will refer to a concatenated hidden state in the Bidirectional Deformable ConvLSTM.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Considering recent state-of-the-art methods (e.g., EDVR<ref type="bibr" target="#b36">[37]</ref> and RBPN<ref type="bibr" target="#b7">[8]</ref>) use only 4 as the upscaling factor, we adopt the same practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Since there is no official source code released, we used an unofficial PyTorch implementation from https://github.com/avinashpaliwal/Super-SloMo.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The official PyTorch implementation of the PCD can be found in https://github.com/xinntao/EDVR.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work was partly supported by NSF 1741472, 1813709, and 1909912. This article solely reflects the opinions and conclusions of its authors but not the funding agents.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3703" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Space-time super-resolution from multiple-videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esmaeil</forename><surname>Faramarzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc P</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 11th International Conference on Information Science, Signal Processing and their Applications (ISSPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video superresolution via bidirectional recurrent convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1015" to="1028" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Space-time super-resolution with patch group cuts prior. Signal Processing: Image Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhi</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="147" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep recurrent resnet for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1452" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gucan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="434" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Space-time super-resolution using graph-cut optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uma</forename><surname>Mudenagudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashis</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Kumar Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="995" to="1008" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6626" to="6634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Space-time super-resolution from a single video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Increasing space-time resolution in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="753" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Space-time super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1279" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatiotemporal video upscaling using motion-assisted steering kernel (mask) regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Van Beek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Quality Visual Experience</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="245" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4472" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Tdan: Temporally deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02898</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning for video super-resolution through hr optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="514" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video enhancement with taskoriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
