<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
							<email>tianshuichen@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxi</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefang</forename><surname>Gao</surname></persName>
							<email>gaoyuefang@scau.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Dong</surname></persName>
							<email>ledong@uestc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Agricultural University</orgName>
								<address>
									<country>South China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Science and Technology of China</orgName>
								<orgName type="institution">University of Electronic</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Guilin University of Electronic Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3240508.3240523</idno>
					<note>978-1-4503-5665-7/18/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Embedding</term>
					<term>Fine-Grained Image Recognition</term>
					<term>Category Hierarchy *</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object categories inherently form a hierarchy with di erent levels of concept abstraction, especially for ne-grained categories. For example, birds (Aves) can be categorized according to a four-level hierarchy of order, family, genus, and species. This hierarchy encodes rich correlations among various categories across di erent levels, which can e ectively regularize the semantic space and thus make prediction less ambiguous. However, previous studies of negrained image recognition primarily focus on categories of one certain level and usually overlook this correlation information. In this work, we investigate simultaneously predicting categories of di erent levels in the hierarchy and integrating this structured correlation information into the deep neural network by developing a novel Hierarchical Semantic Embedding (HSE) framework. Specically, the HSE framework sequentially predicts the category score vector of each level in the hierarchy, from highest to lowest. At each level, it incorporates the predicted score vector of the higher level as prior knowledge to learn ner-grained feature representation. During training, the predicted score vector of the higher level is also employed to regularize label prediction by using it as soft targets of corresponding sub-categories. To evaluate the proposed framework, we organize the 200 bird species of the Caltech-UCSD birds dataset with the four-level category hierarchy and construct a large-scale butter y dataset that also covers four level categories. Extensive experiments on these two and the newly-released VegFru datasets demonstrate the superiority of our HSE framework over the baseline methods and existing competitors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Object categories inherently form a hierarchy with di erent levels of concept abstraction, in which nodes closer to the root of the hierarchy refer to more abstract concepts while nodes closer to the leaves refer to ner-grained concepts. This hierarchy organization is especially important and obvious for ne-grained categories. For example, the ne-grained categories of birds (Aves) can be organized with a four-level hierarchy of order, family, genus and species, where an order consists of several families while a family consists of several genera, and so on. This category hierarchy provides very rich semantic correlations among categories across di erent levels, which can e ectively regularize semantic space and provide extra guidance to attend more subtle regions for better recognition. For example, to recognize the ne-grained category of a given object (e.g., the species of a bird), we might rst recognize its superclass (e.g., genus). Then, we prefer to concentrate on the ne-grained categories that are subject to this superclass and xate on object parts that are more distinguishable among these ne-grained categories.</p><p>Existing methods on ne-grained image recognition (FGIR) primarily focus on classifying categories of one particular level, e.g., categorizing 200 species of birds <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b49">50]</ref> or 431 models of cars <ref type="bibr" target="#b14">[15]</ref>, and usually overlook this correlation information. In this work, we simultaneously predict categories of all levels in the hierarchy, and integrate this structured correlation information into the deep neural network to progressively regularize label prediction and guide representation learning. To this, we formulate a novel Hierarchical Semantic Embedding (HSE) framework that orderly predicts the score vector of each level, from highest to lowest. At each level, it incorporates the predicted score vector of the higher level as prior knowledge to learn ner-grained feature representation. This is implemented by a semantic guided attentional mechanism that learns to xate on more discriminative regions for better distinguishing. During training, we also utilize the predicted score vector of the higher level as soft targets to regularize the label prediction, thus that the predicted result at this level nely accords with that predicted at the higher level.</p><p>Caltech-UCSD birds dataset <ref type="bibr" target="#b38">[39]</ref> is the most widely used benchmark for evaluating the FGIR task. To evaluate our proposed HSE framework on this benchmark, we organize the 200 bird categories with a four-level hierarchy of 13 orders, 37 families, 122 genera, and 200 species according to the ornithological systematics <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. In addition, we also create a new large-scale butter y (namely Butter y-200) dataset that also covers four-level categories for multi-granularity image recognition. Currently, this dataset consists of 200 prevalent species of butter ies, which are grouped into 116 genera, 23 sub-families, and 5 families according to the insect taxonomy <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref>. It contains 25,279 images in total and at least 30 images per species. It's worth noting that these category hierarchies can be obtained from the literature of taxonomy <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref> or directly retrieved from Wikipedia conveniently, thus the methods of embedding this structured information can be easily adapted to various domains.</p><p>The major contributions of this work are concluded to three folds: 1) We formulate a novel Hierarchical Semantic Embedding (HSE) framework that integrates semantically structured information of category hierarchy into the deep neural network for FGIR. To our knowledge, this is the rst work that explicitly incorporates this structured information to aid FGIR. 2) We introduce a fourlevel category hierarchy for the Caltech-UCSD birds dataset <ref type="bibr" target="#b38">[39]</ref> and construct a new large-scale butter y dataset that also covers four-level categories for evaluation. To our knowledge, these two datasets are the rst that involves in four-level categories in FGIR and they may bene t research on multi-granularity image recognition. 3) We conduct experiments on the two and the VegFru <ref type="bibr" target="#b13">[14]</ref> datasets, and demonstrate the e ectiveness of our proposed HSE framework over the baseline and existing state-of-the-art methods. Moreover, we also conduct ablative studies to carefully evaluate and analyze the contribution of each component of the proposed framework. The code, trained models, and dataset are available online: https:// github.com/ HCPLab-SYSU/ HSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Fine-grained image recognition</head><p>Recent progress on image classi cation mainly bene ted from the advancement of deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref> that learned powerful feature representation via stacking multiple nonlinear transformations. To adapt the deep CNNs for handling the FGIR task, a bilinear model <ref type="bibr" target="#b24">[25]</ref> was proposed to compute high-order image representation that captured local pairwise interactions between features generated by two independent subnetworks, but the bilinear feature is extremely high-dimensional, making it impractical for subsequent analysis. To reduce the feature dimension while keeping comparable performance on FGIR task, Gao et al. <ref type="bibr" target="#b8">[9]</ref> developed a compact model that approximates bilinear feature with the polynomial kernels. Kong et al. <ref type="bibr" target="#b18">[19]</ref> proposed classi er co-decomposition to further compress the bilinear model.</p><p>To better capture subtle visual di erence among sub-ordinate categories, a series works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> were also proposed to leverage extra supervision of bounding boxes and parts to locate discriminative regions. However, the heavy involvement of manual annotations prevents these methods from application to large-scale real-world problems. Recently, visual attention models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref> were intensively proposed to automatically search the informative regions and various works successfully applied this technique to FGIR <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">50]</ref>. Liu et al. <ref type="bibr" target="#b27">[28]</ref> formulated a reinforcement learning framework to adaptively glimpse local regions regarding discriminative object parts and trained the framework using a greedy reward strategy with image-level labels. Zheng et al. <ref type="bibr" target="#b49">[50]</ref> introduced a multi-attention convolutional neural network that learned channel grouping for parts localization, and aggregated features from the located regions as well as the global object for classi cation. These works learned to locate informative regions merely based on image content by the self-attention mechanism. In contrast, some works also introduced extra guidance to learn more meaningful and semantic-related regions to aid FGIR. For example, Liu et al., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> introduce part-based attribute to guide learning more discriminative features for ne-grained bird recognition. Similarly, He et al. <ref type="bibr" target="#b11">[12]</ref> further utilized more detailed language descriptions to help mine discriminative parts or characteristics.</p><p>Our framework is also related to some existing works that exploit category hierarchy. For example, Srivastava et al. <ref type="bibr" target="#b36">[37]</ref> exploited class hierarchy prior to transfer knowledge among similar lowerlevel classes for transfer learning. Jia et al. <ref type="bibr" target="#b5">[6]</ref> proposed a probabilistic classi cation model based on a hierarchy and exclusion graph to capture label relations of mutual exclusion, overlap, and subsumption for object classi cation. Works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41]</ref> utilized an RNN to model label co-occurrence dependencies for multi-label recognition. In contrast to these methods that merely model dependencies on label space, our HSE framework introduces the hierarchical information to progressively regularize label prediction and simultaneously guide learning ner-grained feature representation. Besides, using predicted results of the higher level as soft targets for label regularization can distill knowledge learned from the high level to lower level, which is also original compared with these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-grained image datasets</head><p>In the past decade, datasets of FGIR have intensively emerged across various domains ranging from man-made objects to natural plants or animals, including FGVC-Aircraft <ref type="bibr" target="#b28">[29]</ref>, Stanford Cars <ref type="bibr" target="#b20">[21]</ref>, Caltech-UCSD birds <ref type="bibr" target="#b38">[39]</ref>, Stanford Dogs <ref type="bibr" target="#b17">[18]</ref>, Oxford Flowers <ref type="bibr" target="#b30">[31]</ref>, to name a few. As a representative dataset that was widely used in previous FGIR works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>, Caltech-UCSD birds dataset contained 11,788 images and covered 200 species of birds. These datasets signi cantly evolved the research of FGIR, but they primarily focus on categories of one certain level, e.g., Caltech-UCSD birds with 200 species of birds and Stanford Dogs with 120 breeds of dogs. More recently, there also released some datasets that involved categories of multiple levels, like CompCars <ref type="bibr" target="#b44">[45]</ref>, Boxcars <ref type="bibr" target="#b35">[36]</ref>, Cars-333 <ref type="bibr" target="#b43">[44]</ref> with three-level car categories of make, model, and year, and VegFru <ref type="bibr" target="#b13">[14]</ref> with 25 upper-level categories and 292 subordinate classes of vegetables and fruits. These datasets mainly include man-made vehicles <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> and domestic food materials <ref type="bibr" target="#b13">[14]</ref>. To better evaluate our proposed frameworks and increase the diversity of dataset with categories of multiple levels, we further organize the 200 bird species with four-level category hierarchy and construct a new butter y dataset that also covers four-level categories. Besides the research on FGIR with categories of multiple levels, these two datasets have potential to bene t practical applications of wildlife recognition, protection, and discovery.  <ref type="figure">Figure 1</ref>: An overall pipeline of our proposed hierarchical semantic embedding framework. It employs a trunk network to extract image features and subsequently utilizes a branch network to predict the categories of each level. At each level, it incorporates the predicted score vector to guide learning ner-grained feature and simultaneously regularizes label prediction during training.</p><p>respectively. Then, it orderly utilizes a small branch network to predict the score vectors of all levels, from highest to lowest. At each level, the branch network incorporates the predicted score vector of higher level as prior guidance to learn ner-grained representation via a soft attention mechanism and aggregates this representation with features learned without guidance to predict the score vector of this level. During training, we further use the predicted score vector of higher level as soft targets to regularize the label prediction, such that the predicted result at this level tends to accord with that predicted at the higher level. Since there is no guidance at the rst level, we merely use the representation learned without guidance to make prediction and no label regularization is involved either. <ref type="figure">Fig. 1</ref> gives an overall illustration of the HSE framework.</p><p>Before delving deep into the formulation, we rst present some notations associated with our task that will be used throughout this article. Without loss of generality, we consider the FGIR task with a category hierarchy of L levels. We utilize l 1 , l 2 , . . . , l L to denote each level and s 1 , s 2 , . . . , s L to denote the predicted score vectors correspondingly. n 1 , n 2 , . . . , n L are used to represent the category number for each level, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic embedding representation learning</head><p>As we orderly predict the score vector of each level, s i−1 is given when making prediction at level l i . Generally, s i−1 encodes the category that the object of the given image belongs to with a high probability at level l i−1 , and make prediction at level l i may tend to distinguish the sub-ordinate categories of this category. As discussed above, some certain parts play key roles to distinguish the sub-ordinate categories of a superclass. In this work, we take full advantage of this information by incorporating s i−1 to guide learning ner-grained feature representation at level l i . Naturally, this can be implemented by a soft mechanism that learns to xate on the discriminative regions under the guidance of s i−1 . At level l i , we rst map the image feature maps f I to higher-level</p><formula xml:id="formula_0">featuresf i ∈ R W ×H ×C viaf i = ϕ i (f I ),<label>(1)</label></formula><p>where ϕ i (·) is a transformation that is implemented by a small network. Then, at each location (w, h), we introduce a shared attentional mechanism a i (·) to compute the attention coe cient vector under the guidance of s i−1 bŷ</p><formula xml:id="formula_1">e iwh = a i ([f iwh , φ i (s i−1 )]),<label>(2)</label></formula><formula xml:id="formula_2">whereê iwh = {ê iwh1 ,ê iwh2 , . . . ,ê iwhC } denote the importance of each neuron of feature vector f iwh . In the equation, φ i (·)</formula><p>is a linear transformation that transforms s i−1 to a semantic feature vector. To make the coe cients easily comparable across di erent channels, we normalize the coe cients across all the locations of each channels c using a softmax function</p><formula xml:id="formula_3">e iwhc = exp(ê iwhc ) w ,h exp(ê iw h c ) .<label>(3)</label></formula><p>In this way, we can obtain e iwh = {e iwh1 , e iwh2 , . . . , e iwhC } denoting the normalized weight of each neuron of feature vector f iwh . Finally, we perform weighted average across all locations of each <ref type="figure">Figure 2</ref>: An illustration of the semantic guided label regularization. Top: correlations among categories of level l i−1 and l i . Bottom: s i−1 is rst extended to s i−1 according to the structured correlations and s i is pulled close to s i−1 for regularization.</p><formula xml:id="formula_4">s i-1 s i s i-1 s i-1 s i '</formula><p>channel to produce the nal ner-grained features</p><formula xml:id="formula_5">f i = w,h e iwh f iwh ,<label>(4)</label></formula><p>where denotes the element-wise multiplication operation.</p><p>As the feature vector f i pays much attention to the local discriminative regions that may tend to capture subtle di erence for distinguishing sub-ordinate categories of a superclass. It may ignore the overall description of the object and some background information that may provide contextual cues. Thus, we further extract a feature vector directly from the image feature maps f I without guidance for complementary. Similarly, we also adopt a simple transformation ψ i (·) on f I bŷ</p><formula xml:id="formula_6">f i = ψ i (f I ),<label>(5)</label></formula><p>wheref i ∈ R W ×H ×C . Similar to <ref type="bibr" target="#b10">[11]</ref>, we simply perform average pooling to obtain the feature vector</p><formula xml:id="formula_7">f i = 1 W H w,hf iwh .<label>(6)</label></formula><p>The obtained feature vectors f i , f i and the concatenation of them [f i , f i ] are fed to three classi ers to predict the score vectors independently, which are then averaged to produce the nal score vector s i . Network details. Similar to recent FGIR works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, we implement our framework based on the widely used ResNet-50 <ref type="bibr" target="#b10">[11]</ref>. Speci cally, we implement the trunk network with the preceding 41 convolutional layers of the ResNet-50, and the transformations of ϕ i (·), ψ i (·) with the following 9 layers of the ResNet-50. We make the trunk network be shared across di erent levels to better balance prediction accuracy and computational e ciency. φ i (·) is simply implemented by a single fully connected layer that map the c-dim score vector to a 1,024-dimemsion features and the attention mechanism a i (·) is implemented by two stacked fully connected layers, in which the rst one is c+1,024 to 1,024 followed by the tanh non-linear function and the second one is 1,024 to c. As we use the identical architecture with ResNet-50, c is 2,048 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic guided label regularization</head><p>The hierarchy encodes rich semantic correlations among categories across di erent levels. For example, the ground truth category at level l i is the child sub-category of the ground truth category at level l i−1 . This correlation information can e ectively regularize semantic space and thus make prediction less ambiguous. These correlations should also be maintained among predicted categories of di erent levels. To this, we incorporate s i−1 as soft targets to regularize label prediction at level l i .</p><p>Given the predicted score vector s i−1 = {s i−1,1 , s i−1,2 , . . . , s i−1,n i −1 }, a high value s i−1,c denotes high con dence that the object in given image belongs to category c at level l i−1 , and the predicted scores for the corresponding child sub-categories at level l i should also be assigned with high values. To this, we rst extend s i−1 to s i−1 according to the structured correlations thus that s i−1 has the same dimension as s i and pull s i close to s i−1 , as shown in <ref type="figure">Fig. 2</ref>. Concretely, if category c at level l i−1 has k child sub-categories at level l i , we duplicate the score s i−1,c by k times. Then we orderly get these duplicated scores together and re-arrange their subscripts to obtain the extended score vector s i−1 = {s i−1,1 , s i−1,2 , . . . , s i−1,n i }.</p><p>To make these two vectors easily comparable, we normalize them into probability distribution using the softmax function with temperature T</p><formula xml:id="formula_8">p T i−1,c = exp( s i −1,c T ) c exp( s i −1,c T ) , p T i,c = exp( s i,c T ) c exp( s i,c T ) ,<label>(7)</label></formula><p>where T is normally set to 1, and we use a high temperature to produce softer probability distribution over classes in our experiment. In this way, we can obtain two normalized probability distributions, i.e., p T i−1 = {p T i−1,1 , p T i−1,2 , . . . , p T i−1,n i } and p T i = {p T i,1 , p T i,2 , . . . , p T i,n i }, and de ne a regularization term as the Kullback-</p><formula xml:id="formula_9">Leibler divergence from p T i to p T i−1 r i = D K L (p T i−1 ||p T i ) = − c p T i−1,c log p T i,c p T i−1,c .<label>(8)</label></formula><p>As r i is de ned on a single sample, we simply sum up r i over the training set to de ne the regularization loss term L r i . As suggested in <ref type="bibr" target="#b12">[13]</ref>, when using soft targets that have high entropy, more information can be provided than hard target per training sample, and the gradient between training samples enjoy less variance. Thus, it can be trained more steadily and using much less training samples. In our experiments, T is set as 4 to produce a su ciently soft target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>Besides the regularization term, we also employ the cross-entropy loss with the correct labels as the objective function. We rst normalize the predicted score vector using exactly the same logits in softmax function but at a normal temperature of 1, expressed as</p><formula xml:id="formula_10">p i,c = exp(s i,c ) c exp(s i,c ) .<label>(9)</label></formula><p>Then suppose the ground truth label at level l i is c i , its loss can be de ned as</p><formula xml:id="formula_11">c i = − c 1(c = c i ) log p i,c ,<label>(10)</label></formula><p>where 1(·) is the indication function that is assigned as 1 if the expression is true, and assigned as 0 otherwise. We have de ne the same loss for the score vectors predicted by the three classi er, respectively. Thus, each sample has four losses, and we sum up the four losses over the training set to de ne the classi cation loss L c i . The proposed framework consists of a trunk network and L branch network, and it is trained using a weighted combination of the classi cation and regularization losses. The training process is empirically divided into two stages, i.e. level-wise training followed by joint ne tuning. Stage 1: Level-wise training. When training the branch network of level l i , it needs the predicted score vector of level l i−1 to de ne the regularization loss. Thus, we rst train the branch networks in a level-wise manner, from level l 1 to l L . As our framework is implemented based on the ResNet-50 <ref type="bibr" target="#b10">[11]</ref>, we initialize the parameters with those of the corresponding layers of ResNet-50 pre-trained on the ImageNet dataset <ref type="bibr" target="#b6">[7]</ref>. Concretely, the parameters of the trunk network are initialized by those of the corresponding 41 convolutional layers and the parameters of the transformation ϕ i (·) and ψ i (·) are initialized with those of the 9 corresponding layers. The parameters of other modules, including the attentional mechanism a i (·), semantic mapper φ i (·) and the three classi ers, are automatically initialized with the Xavier algorithm <ref type="bibr" target="#b9">[10]</ref>. As the trunk network is shared by all branch networks, its parameters are kept xed at this stage. We train the branch network of level l i with a weighted combination of the classi cation and regularization losses</p><formula xml:id="formula_12">L i = L c i + γ L r i ,<label>(11)</label></formula><p>where γ is a balance parameter. As discussed in <ref type="bibr" target="#b12">[13]</ref>, the magnitudes of the gradients produced by L r i are scaled by 1 T 2 , thus it is important to multiply them by a scale of T 2 . Thus, we set γ as T 2 , i.e., 16 in our experiments. Note that we merely use the classi cation loss L c 1 to train the branch network of level l 1 , as there is no guidance to de ne the regularization loss term at this level. Similar to previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref> on FGIR task, we resize the input images to 512 × 512 and perform randomly cropping with a size of 448 × 448 and their horizontal re ections for data augmentation. Then, we train the branch network using the stochastic gradient descent (SGD) algorithm with a batch size of 8, a momentum of 0.9 and a weight decay of 0.00005. The initial learning rate is set as 0.001, and it is divided by 10 when the error plateaus. Stage 2: Joint ne tuning. After all branch networks are trained, we jointly ne tune the entire framework by combining the loss terms over all granularities</p><formula xml:id="formula_13">L = L c 1 + L i=2 L i .<label>(12)</label></formula><p>We adopt the same strategies for data augmentation and hyperparameter setting as Stage 1 except using a smaller initial learning rate 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATASETS</head><p>We construct a new large-scale butter y (Butter y-200) dataset with four-level categories and organize the 200 bird species of the Caltech-UCSD Birds (CUB) dataset also with four-level categories. We evaluate our proposed framework, the baseline methods and  the existing competitors on these two and the VegFru <ref type="bibr" target="#b13">[14]</ref> datasets. In this section, we rst introduce these three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Butter y-200 dataset construction</head><p>We select 200 common species of butter ies and build the hierarchical structure with 116 genera, 23 subfamilies, and 5 families according to the insect taxonomy. The butter y images are collected from two scenarios, natural images with the butter y in their natural living environment and standard images with the butter y in the form of specimens, as both are widely used in the real-world applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Caltech-UCSD birds dataset extenstion</head><p>The CUB dataset <ref type="bibr" target="#b38">[39]</ref> is the most widely used benchmark for FGIR task. It covers 200 species of birds and contains 11,788 bird images that are divided into a training set of 5,994 images and a test set of 5,794 images. In this work, we build a bird taxonomy hierarchy according to the ornithological systematics, which groups the 200 species into 122 genera, 37 families, and 13 orders. We follow the standard train/test split as <ref type="bibr" target="#b38">[39]</ref> for evaluation. <ref type="figure" target="#fig_1">Figure 3</ref> also shows some samples from the order of "Passeriformes" and their corresponding hierarchical labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VegFru dataset introduction</head><p>VegFru <ref type="bibr" target="#b13">[14]</ref> is a newly released large-scale dataset for ne-grained vegetables and fruits recognition. It covers two-level categories of 25 upper-level categories and 292 subordinate classes. The dataset contains 160,731 images in total, including a training set of 29,200 images, a validation set of 14,600 images, and a test set of 116,931 images. Similarly, we follow this standard train/val/test splits as <ref type="bibr" target="#b13">[14]</ref> to evaluate our HSE framework and the existing methods for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT 5.1 Signi cance of semantic embedding</head><p>We rst implement two baseline methods that use network architecture similar to ours but do not consider the structured correlations to demonstrate the e ectiveness of the proposed HSE framework. Baseline. Similar to our framework, we utilize a trunk network to extract image features and then utilize four small networks to predict the category of all levels, separately. For fair comparison, we also implement the trunk network with the preceding 41 convolutional layers of the ResNet-50 and the small network with the following 9 layers. Baseline+backtrack. We utilize the baseline methods to predict the category of the nest level, and backtrack through the hierarchy to obtain the categories of the other levels.</p><p>We compare the HSE with these two baseline methods on the CUB and Butter y-200 datasets in <ref type="table" target="#tab_1">Table 1</ref>. Here, we present the accuracies of all levels for comprehensive comparisons. At level l 1 , we nd the HSE achieves comparable accuracies with those of the two baseline methods, as there is no semantic guidance at this level. However, at level l 2 to l 4 , the HSE performs consistently better than the baseline methods on both datasets. For example on the CUB dataset, the HSE achieves accuracies of 95.7%, 92.7%, and 88.1%, outperforming the baseline methods by 0.6%, 1.2%, and 2.9%, respectively. It is noteworthy that the improvement is more obvious for predicting categories of ner levels, e.g., 1.2% accuracy improvement at level l 3 while 2.9% at level l 4 on the CUB dataset. This phenomenon suggests that incorporating semantic correction information bene ts more to challenging tasks. To delve deep into the e ect of semantic embedding on network learning, we further present the curve of loss v.s. training epoch on the training set and the curve of accuracy v.s. training epoch on the test set in <ref type="figure" target="#fig_2">Fig 4.</ref> These experiments are conducted on recognizing the category of l 4 on the CUB dataset. Compared with the baseline, the HSE can be trained more stably and converged faster.</p><p>The foregoing comparisons with the baseline methods demonstrate the e ectiveness of the HSE as a whole. Actually, the HSE incorporates the semantic correlation information from two aspects, i.e., semantic embedding representation learning (SERL) and semantic guided label regularization (SGLR). Here, we further conduct ablative studies to assess the actual contributions of these two components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of semantic guided label regularization (SGLR).</head><p>We rst evaluate the contribution of SGLR by comparing the performance with and without regularization loss. Speci cally, we simply remove the regularization loss terms of each level with others keep xed and re-train the model in an identical way. As shown in <ref type="table" target="#tab_1">Table  1</ref>, removing this term leads to an obvious drop in performance over all levels on both datasets. We further analyze how SGLR improves the performance. When the category of an image is wrongly predicted, we denote it as an inter-superclass error if the wrongly predicted category and ground truth category do not belong to the same superclass, and denote it as an intra-superclass error if they belong to the same superclass. As discussed before, SGLR regularizes label prediction thus that the predicted category at level l i tends to be the child sub-category of the predicted category at level l i−1 . Thus, this tends to help correct the inter-superclass error. To validate this, we present the sample number of inter-superclass and the intra-superclass errors at level l 4 of our HSE with and without SGLR on both datasets. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, introducing SGLR mainly reduces the sample number of inter-superclass error (17.5% relative reduction on the CUB dataset and 13.5% on the Butter y-200 dataset), nely in accordance with our motivation. Contribution of semantic embedding representation learning (SERL). Here, we evaluate the bene t of SERL. To this, we remove the feature embedding module (i.e., ϕ i and a i ) and simply use the feature without guidance for recognition. To ensure fair comparisons, we also re-train the model with both of the classi cation and regularization losses. Similarly, the performance at each level su ers from an evident drop on both datasets.</p><p>As discussed before, SERL helps to attend regions that help to distinguish sub-ordinate categories of the predicted superclass of the higher level. Here, we visualize the attentional regions learned by our HSE framework in <ref type="figure" target="#fig_4">Fig. 6</ref>. At each row, we present some samples of a speci c species, and the rst two species belong to the same genus while the last two belong to another genus. For the samples from di erent species of the same genus, our framework actually attends discriminative regions to better distinguish these species. For example, to di erentiate the species of "Bohemian Waxwing" and "Cedar Waxwing" that belong to the genus of "Phoebastria", the HSE pay much attention to the throat and wing tail regions that provide most discriminative information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with state-of-the-art methods</head><p>In this subsection, we compare the HSE framework with existing state-of-the-art methods on the CUB <ref type="bibr" target="#b38">[39]</ref> and VegFru <ref type="bibr" target="#b13">[14]</ref> datasets. Here, we evaluate on recognizing the categories of the nest level (200 species on CUB and 292 subcategories on VegFru) as existing methods primarily report their results of this level.</p><p>Comparison on Caltech-UCSD birds dataset. CUB dataset is the most widely used benchmark for FGIR task, and most works have reported their results on this dataset. We compare our HSE framework with 17 state-of-the-art methods, including Deep Localization, Alignment and Classi cation (DeepLAC) <ref type="bibr" target="#b23">[24]</ref>, Semantic Part Detection and Abstraction (SPDA-CNN) <ref type="bibr" target="#b45">[46]</ref>, Part-RCNN <ref type="bibr" target="#b46">[47]</ref>, Part Alignment-based (PA-CNN) <ref type="bibr" target="#b19">[20]</ref>, Pose Normalized CNN (PN-CNN) <ref type="bibr" target="#b0">[1]</ref>, Picking Deep Filter Responses (PDFR) <ref type="bibr" target="#b47">[48]</ref>, Multiple Granularity (MG-CNN) <ref type="bibr" target="#b39">[40]</ref>, Spatial Transformer (ST-CNN) <ref type="bibr" target="#b16">[17]</ref>, Bilinear-CNN (B-CNN) <ref type="bibr" target="#b24">[25]</ref>, Compact Bilinear CNN (CB-CNN) <ref type="bibr" target="#b8">[9]</ref>, Two-Level Attention Network (TLAN) <ref type="bibr" target="#b42">[43]</ref>, Diverse Attention Network (DAN) <ref type="bibr" target="#b48">[49]</ref>, Fully Convolutional Attentional Network (FCAN) <ref type="bibr" target="#b27">[28]</ref>, Recurrent Attention (RA-CNN) <ref type="bibr" target="#b7">[8]</ref>, Combine Vision and Language (CVL) <ref type="bibr" target="#b11">[12]</ref>, Attribute-Guided Attention Localization (AGAL) <ref type="bibr" target="#b26">[27]</ref>, Multi-Attentional CNN (MA-CNN) <ref type="bibr" target="#b49">[50]</ref>. Among these methods, some use merely image-level labels (i.e., image-level setting), and some also use bounding box/parts annotations (i.e., box-level setting); thus we also present these information for fair and direct comparisons.</p><p>Under the box-level setting, the previous well-performing methods include PN-CNN and B-CNN that achieve accuracies of 85.4% and 85.1%. However, PN-CNN requires strong supervision of both human-de ned bounding box and ground truth parts while B-CNN relies on a very high-dimension feature representation (250k dimensions). Under the image-level setting, most works resort to attentional model that automatically search the discriminative regions and aggregate deep features of these regions for classi cation. For example, MA-CNN learns to attend multiple discriminative regions, and adopt a CNN to extract the global feature from the whole and multiple part-CNNs to extract the local feature from each attentional regions. It achieves an accuracy of 86.5%, which is the best among existing methods. Di erent from these methods, our HSE framework requires no bounding box and part annotations and does not use multiple CNN to extract local and global features.  <ref type="bibr" target="#b19">[20]</ref> √ 82.8 CB-CNN w/ bbox <ref type="bibr" target="#b8">[9]</ref> √ 84.6 FCAN w/ bbox <ref type="bibr" target="#b27">[28]</ref> √ 84.7 B-CNN w/ bbox <ref type="bibr" target="#b24">[25]</ref> √ 85.1 AGAL w/ bbox <ref type="bibr" target="#b26">[27]</ref> √ 85.5 TLAN <ref type="bibr" target="#b42">[43]</ref> 77.9 DVAN <ref type="bibr" target="#b48">[49]</ref> 79.0 MG-CNN <ref type="bibr" target="#b39">[40]</ref> 81.7 B-CNN w/o bbox <ref type="bibr" target="#b24">[25]</ref> 84.1 ST-CNN <ref type="bibr" target="#b16">[17]</ref> 84.1 FCAN w/o bbox <ref type="bibr" target="#b27">[28]</ref> 84.3 PDFR <ref type="bibr" target="#b47">[48]</ref> 84.5 CB-CNN w/o bbox <ref type="bibr" target="#b8">[9]</ref> 85.0 RA-CNN <ref type="bibr" target="#b7">[8]</ref> 85.3 AGAL w/o bbox <ref type="bibr" target="#b26">[27]</ref> 85.4 CVL <ref type="bibr" target="#b11">[12]</ref> 85.6 MA-CNN <ref type="bibr" target="#b49">[50]</ref> 86.5 Ours 88.1 <ref type="table">Table 2</ref>: Comparisons of our HSE framework with existing state of the arts on recognizing categories of level l 4 on the CUB dataset. BA and PA denote bounding box annotations and part annotations, respectively.</p><p>√ indicates corresponding annotations are used during training or test.</p><p>Instead, it embeds structure information of category hierarchy to learn ne-grained feature representation and regularize label prediction, leading to obvious performance improvement, i.e., 88.1% in accuracy.</p><p>Note that our HSE introduces extra guidance of the category hierarchy. However, this hierarchy can be easily obtained from the literature of taxonomy or retrieved from the Wikipedia. Besides, we also compare with existing methods that also rely on extra supervisions, like AGAL requiring attribute annotations and CVL depending on sentence description. Our HSE achieves an accuracy of 88.1%, much better than theirs, i.e., 85.5% and 85.6%, respectively. Comparison on VegFru dataset. VegFru is a newly released largescale dataset for ne-grained vegetables and fruits recognition, and some works also report their results on this dataset. Here, we also present comparisons with the baseline and existing methods on this dataset in <ref type="table">Table 3</ref>. As shown, the HSE also signi cantly outperforms all these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Fine-grained categories naturally form a hierarchy with di erent levels of concept abstraction, and this hierarchy encodes rich correlations among categories across di erent levels. In this work, we investigate simultaneously predicting categories of all levels in the hierarchy and integrating this structured correlation information into the deep neural network by developing a novel Hierarchical</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc. (%) Baseline 87.1 CB-CNN <ref type="bibr" target="#b8">[9]</ref> 82.2 HybridNet <ref type="bibr" target="#b13">[14]</ref> 83.5 Ours (full) 89.4 <ref type="table">Table 3</ref>: Comparison of accuracy of our HSE framework, existing state-of-the-art methods, and the baseline methods on the VegFru dataset.</p><p>Semantic Embedding (HSE) framework. Speci cally, the HSE orderly predicts the score vector for each level, and at each level, it incorporates the predicted score vector of the higher level to guide learning ner-grained feature representation and simultaneously regularize label prediction during training. To evaluate the HSE framework, we extend the Caltech-UCSD birds with four-level categories and construct a butter y dataset also with four-level categories. Extensive experiments and thorough analysis on these two and the VegFru datasets demonstrate the superiority of the proposed HSE framework over the baseline methods and existing competitors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Some samples and their corresponding hierarchical labels from the family of "Pieridae" in the Butter y-200 dataset (the rst two rows) and from the order of "Passeriformes" in the CUB dataset (the last two rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of the e ect of semantic embedding on network learning. These experiments are conducted on categories at level l 4 on the CUB dataset. (a) and (b) are the curves of loss v.s. training epoch on the training set and accuracy v.s. training epoch on the test set, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Sample number of inter-superclass and intrasuperclass errors of our framework with and without SGLR on the (a) CUB and (b) Butter y-200 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the attentional regions learned by the HSE framework. At each row, we present some samples of a speci c species, and the rst two species belong to the same genus while the last two belong to another genus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The natural images are collected by searching the keywords of butter y species names on the internet including Google, Flicker, Bing, Baidu, etc. The standard images are collected by capturing the samples in Lab. In this way, a large number of candidate images for each species are collected. To ensure the dataset highly reliable, the candidate images are carefully identi ed by four experts on butter ies. Currently, we have collected 25,279 butter y images of the 200 species, with each species containing 30 images at least, which are divided into training, validation, and test set for evaluation. For each species, we randomly select 20% for training, : order l 2 : family l 3 : genus l 4 : species l 1 : family l 2 : sub-family l 3 : genus l 4 : species Comparison of the accuracy (in %) of all levels of our HSE framework, two baseline methods, and two variants of our framework that removes semantic embedding representation learning (Ours w/o SERL) and that removes semantic guided label regularization (Ours w/o SGLR) on the CUB and Butter y-200 test sets, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Butter y-200</cell><cell></cell></row><row><cell>Methods l 1 Baseline</cell><cell>98.8</cell><cell>95.0</cell><cell>91.5</cell><cell>85.2</cell><cell>98.9</cell><cell>97.6</cell><cell>94.8</cell><cell>85.1</cell></row><row><cell>Baseline+backtrack</cell><cell>98.6</cell><cell>95.1</cell><cell>90.9</cell><cell>85.2</cell><cell>98.7</cell><cell>97.2</cell><cell>94.1</cell><cell>85.1</cell></row><row><cell>Ours w/o SERL</cell><cell>98.8</cell><cell>95.1</cell><cell>91.9</cell><cell>86.6</cell><cell>98.9</cell><cell>97.4</cell><cell>95.3</cell><cell>85.8</cell></row><row><cell>Ours w/o SGLR</cell><cell>98.8</cell><cell>95.6</cell><cell>92.2</cell><cell>86.7</cell><cell>98.9</cell><cell>97.6</cell><cell>95.1</cell><cell>85.5</cell></row><row><cell>Ours (full)</cell><cell>98.8</cell><cell>95.7</cell><cell>92.7</cell><cell>88.1</cell><cell>98.9</cell><cell>97.7</cell><cell>95.4</cell><cell>86.1</cell></row></table><note>20% for validation and the rest 60% for test, resulting in a training of 5,135 images, a validation set of 5,135 images, and a test set of 15,009 images, respectively. Figure 3 shows some samples from the family of "Pieridae" and their corresponding hierarchical labels.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">HSE FRAMEWORKIn this section, we describe the proposed HSE framework in detail. Given an image, the framework rst utilizes a trunk network to extract image feature maps f I ∈ R W ×H ×C , where W , H and C denote the width, height and channel number of the feature maps,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank Prof. Min   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge-Embedded Representation Learning for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Joint Conference on Arti cial Intelligence</title>
		<meeting>of International Joint Conference on Arti cial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DISC: Deep Image Saliency Computing via Progressive Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1135" to="1149" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Conference on Arti cial Intelligence</title>
		<meeting>of AAAI Conference on Arti cial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6722" to="6729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent Attentional Reinforcement Learning for Multi-label Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Conference on Arti cial Intelligence</title>
		<meeting>of AAAI Conference on Arti cial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6730" to="6737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale object classication using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Look closer to see better: recurrent attention convolutional neural network for ne-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the di culty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on arti cial intelligence and statistics</title>
		<meeting>the thirteenth international conference on arti cial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-graind Image Classi cation via Combining Vision and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitions</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognitions</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">VegFru: A Domain-Speci c Dataset for Fine-grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="541" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep CNNs With Spatially Weighted Pooling for Fine-Grained Car Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qichang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huibing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3147" to="3156" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Part-stacked CNN for ne-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Novel dataset for ne-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Low-rank Bilinear Pooling for Fine-Grained Classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05109</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d object representations for ne-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classi cation with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo Rey E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ha Ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classi cation for ne-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for ne-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crowd Counting using Deep Recurrent Spatial-Aware Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Joint Conference on Arti cial Intelligence</title>
		<meeting>of International Joint Conference on Arti cial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Localizing by Describing: Attribute-Guided Attention Localization for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4190" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks: E cient attention localization for ne-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fine-grained visual classi cation of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automated ower classication over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
	<note>ICVGIP&apos;08. Sixth Indian Conference on</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A revised classi cation of the Icteridae (Aves) based on DNA sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><forename type="middle">Fla</forename><surname>Jv Remsen</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Schodde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott M Lanyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zootaxa</title>
		<imprint>
			<biblScope unit="volume">4093</biblScope>
			<biblScope unit="page" from="285" to="292" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Taxonomy of the European Pied Flycatcher Ficedula hypoleuca (Aves: Muscicapidae)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henk</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeugd</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">M</forename><surname>Tomotani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zootaxa</title>
		<imprint>
			<biblScope unit="volume">4291</biblScope>
			<biblScope unit="page" from="171" to="182" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Butteries (Lepidoptera) of Guyana: A compilation of records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemchandranauth</forename><surname>Sambhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alliea</forename><surname>Nankishore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zootaxa</title>
		<imprint>
			<biblScope unit="volume">4371</biblScope>
			<biblScope unit="page" from="1" to="187" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Boxcars: 3d boxes as cnn input for improved ne-grained vehicle recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Havel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3006" to="3015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2094" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Annotated checklist of Albanian butteries (Lepidoptera, Papilionoidea and Hesperioidea)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudi</forename><surname>Verovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZooKeys</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for ne-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2399" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A uni ed framework for multi-label image classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multilabel Image Recognition by Recurrently Discovering Attentional Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for ne-grained image classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hyper-class augmented and regularized deep learning for ne-grained image classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for ne-grained categorization and veri cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spda-cnn: Unifying semantic part detection and abstraction for ne-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1143" to="1152" />
		</imprint>
	</monogr>
	<note>Shaoting Zhang, Ahmed Elgammal, and Dimitris Metaxas</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for ne-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Picking deep lter responses for ne-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diversi ed Visual Attention Networks for Fine-Grained Object Classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning multiattention convolutional neural network for ne-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
