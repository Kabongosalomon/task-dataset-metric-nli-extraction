<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
							<email>gongjiezhang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Luo</surname></persName>
							<email>zhipeng001@e.ntu.edu.sgkaiwen001@e.ntu.edu.sgshijian.lu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot object detection aims at detecting novel objects with only a few annotated examples. Prior works have proved meta-learning a promising solution, and most of them essentially address detection by meta-learning over regions for their classification and location fine-tuning. However, these methods substantially rely on initially welllocated region proposals, which are usually hard to obtain under the few-shot settings. This paper presents a novel meta-detector framework, namely Meta-DETR, which eliminates region-wise prediction and instead meta-learns object localization and classification at image level in a unified and complementary manner. Specifically, it first encodes both support and query images into category-specific features and then feeds them into a category-agnostic decoder to directly generate predictions for specific categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM), which aligns high-level and low-level feature semantics to improve the generalization of meta-learned representations. Experiments over multiple few-shot object detection benchmarks show that Meta-DETR outperforms state-of-the-art methods by large margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computer vision has witnessed significant progress in recent years. However, there still exists a huge gap between current computer vision techniques and human visual systems in learning new concepts from very few examples: most existing methods require large amounts of annotated samples, while humans can effortlessly recognize a new concept even with very little instruction <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b56">55]</ref>. Such a human-like capability of generalizing from limited examples is highly desirable for machine vision systems, especially when sufficient training samples are not available or their annotations are hard to obtain <ref type="bibr" target="#b58">[57,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b20">19]</ref>. â€  denotes equal contribution.</p><p>* denotes corresponding author. <ref type="figure">Figure 1</ref>. Upper: Most existing meta-detectors essentially perform region-wise predictions, which heavily rely on the quality of initial region proposals that cannot be guaranteed under the fewshot settings. Lower: The proposed Meta-DETR meta-learns object localization and classification at image level in a unified and complementary manner (without region-wise prediction), leading to superior few-shot object detection performance.</p><p>In this work, we explore the challenging few-shot object detection task, which requires both recognition and localization of novel objects within an image. Prior works <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b75">74,</ref><ref type="bibr" target="#b82">81,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b81">80]</ref> have proved meta-learning a promising solution. As illustrated in the upper part of <ref type="figure">Fig. 1</ref>, they essentially address object detection by performing metalearning over regions, including region proposals <ref type="bibr" target="#b82">[81,</ref><ref type="bibr" target="#b81">80]</ref>, anchors <ref type="bibr" target="#b23">[22]</ref>, and window centers <ref type="bibr" target="#b47">[46]</ref>, for their classification and location fine-tuning. However, as identified in <ref type="bibr" target="#b10">[10]</ref> and <ref type="bibr" target="#b92">[91]</ref>, these methods rely heavily on the quality of initial region proposals, which cannot be guaranteed in the fewshot setups with scarce training samples, thus producing inaccurate or missed detection. Though FSOD <ref type="bibr" target="#b10">[10]</ref> proposes to meta-learn the generation of region proposals, this issue remains as the framework is still inherently region-based. Based on the analysis above, a key limitation rooted in existing meta-detectors is the region-wise prediction approach. Besides, under the challenging settings of few-shot object detection where supervision from annotated examples is minimal, the complementary effect between classification and localization (as demonstrated in <ref type="bibr" target="#b94">[93,</ref><ref type="bibr" target="#b79">78,</ref><ref type="bibr" target="#b54">53]</ref>) should be maximally exploited. Therefore, an ideal metadetector should discard such region-based prediction and effectively leverage the synergistic relationship between classification and localization by meta-learning both sub-tasks in a fully end-to-end manner. However, such a framework is still absent to the best of our knowledge.</p><p>Recently, the emergence of fully end-to-end detection frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b97">96]</ref> clears the way to such a framework. This paper presents Meta-DETR, a novel region-free framework for few-shot object detection that meta-learns imagelevel localization and classification in a unified and complementary manner. Concretely, it incorporates meta-learning into the DETR frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b97">96]</ref> by first encoding support and query images into category-specific features and then feeding them into a category-agnostic decoder to directly generate detection results for the target categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM) that aligns high-level and lower-level feature semantics and prevents reliance on category-specific representations with low generalization capability.</p><p>The contributions of this work are threefold. First, we propose Meta-DETR, a novel few-shot object detection framework that unifies image-level meta-learning of object localization and classification into a single module without requiring region-wise prediction. Such a design can effectively leverage the synergistic relationship between the two sub-tasks and avoid constraints caused by region-wise prediction. Second, we design a simple but effective Semantic Alignment Mechanism (SAM) that enhances the general-ization capacity of meta-learning by aligning high-level and low-level semantics to avoid reliance on category-specific representations. Third, extensive experiments show that our method achieves state-of-the-art performance on multiple benchmarks for few-shot object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection. Generic object detection <ref type="bibr" target="#b39">[38]</ref> is a joint task on object localization and classification. Modern object detectors can be broadly classified into two categories including two-stage detectors and single-stage detectors. The dominant two-stage detectors are Faster R-CNN <ref type="bibr" target="#b53">[52]</ref> and its variants <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b60">59,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b90">89,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b83">82,</ref><ref type="bibr" target="#b50">49]</ref>, which first adopt a Region Proposal Network (RPN) to generate region proposals as coarse localization and then perform perregion classification and location fine-tuning. Differently, single-stage detectors <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b96">95,</ref><ref type="bibr" target="#b71">70,</ref><ref type="bibr" target="#b91">90,</ref><ref type="bibr" target="#b41">40]</ref> employ densely placed anchors as region proposals and directly make predictions on them. These aforementioned methods still rely on many heuristics like anchor generation. Recently, DETR <ref type="bibr" target="#b1">[2]</ref> and its variants <ref type="bibr" target="#b97">[96,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b93">92]</ref> have received vast attention thanks to their merits of no heuristic design, fully end-to-end pipeline, and comparable or even better performance. However, these detectors still heavily rely on human supervision in the form of large amounts of annotated training samples, thus will suffer from huge performance drop in the context of few-shot learning.</p><p>Few-Shot Learning. Few-shot learning aims at bridging the gap between existing models and human intelligence in learning novel concepts from very few samples. One promising solution is meta-learning <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b67">66]</ref>, which aims to extract meta-level knowledge that can generalize across various tasks via 'learning to learn'. Extensive researches <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr">61,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b74">73,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b80">79,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b40">39]</ref>  Decoding Branch (DB) <ref type="figure">Figure 3</ref>. The architecture of our proposed Meta-DETR. It consists of a Query Encoding Branch (QEB), a Support Encoding Branch (SEB), and a Decoding Branch (DB). QEB receives a query image and generates its query features through a feature extractor and a transformer encoder. SEB, which shares all learnable parameters with QEB, extracts support category codes from the support images. Given the query features with a support category code, DB first aggregates them into category-specific features and then applies a categoryagnostic transformer decoder to predict the detection results over the corresponding support category.</p><p>of the meta-learning paradigm for the few-shot classification task. However, other more complex few-shot learning tasks <ref type="bibr" target="#b66">[65,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr" target="#b72">71]</ref> are still relatively underexplored.</p><p>Few-Shot Object Detection. Prior works on few-shot object detection can be formulated in two paradigms: transferlearning-based and meta-learning-based. Methods using transfer-learning include LSTD <ref type="bibr" target="#b2">[3]</ref>, PNPDet <ref type="bibr" target="#b89">[88]</ref>, TFA <ref type="bibr" target="#b73">[72]</ref>, and MPSR <ref type="bibr" target="#b77">[76]</ref>, where novel concepts are learned via finetuning. Differently, methods using meta-learning extract meta-level knowledge that can efficiently adapt to novel categories by constructing and learning on various auxiliary tasks, in which target categories are dynamically conditioned on support images. Of them, Meta-YOLO <ref type="bibr" target="#b23">[22]</ref> and ONCE <ref type="bibr" target="#b47">[46]</ref> are based on single-stage detectors, and Meta R-CNN <ref type="bibr" target="#b82">[81]</ref> and its variants <ref type="bibr" target="#b78">[77,</ref><ref type="bibr" target="#b75">74,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr">30,</ref><ref type="bibr" target="#b81">80,</ref><ref type="bibr" target="#b38">37]</ref> are built upon Faster R-CNN <ref type="bibr" target="#b53">[52]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, existing metadetectors essentially perform region-wise meta-learning, thus requiring initially well-located regions. However, such well-located regions for novel objects are usually hard to obtain with non-learnable shape priors and fine-tuned RPN when training samples are scarce. FSOD <ref type="bibr" target="#b10">[10]</ref> attempts to mitigate this issue by meta-learning an Attention-RPN, but the issue remains as this framework and Attention-RPN are still innately region-based. Our Meta-DETR follows the track of meta-learning. Unlike previous works, it discards region-wise prediction and instead unifies the meta-learning of localization and classification at image level with a category-agnostic decoder, thus leveraging global contexts and the synergistic relationship of the two sub-tasks to achieve superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Given two sets of categories C base and C novel , where C base âˆ© C novel = âˆ…, a few-shot object detector aims at detecting objects of C base âˆª C novel by learning from a base dataset D base with abundant annotated instances of C base and a novel dataset D novel with very few annotated instances of C novel . In the task of K-shot object detection, there are exactly K annotated object instances for each novel category in D novel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Meta-DETR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Revisiting DETR Frameworks</head><p>Modern detectors like Faster R-CNN <ref type="bibr" target="#b53">[52]</ref> address object detection by performing the surrogate task of classification and location fine-tuning on a number of regions. Such detectors require many heuristics and are not fully end-to-end. Recently, DETR <ref type="bibr" target="#b1">[2]</ref> eliminates the need for such heuristic designs and achieves the first fully end-to-end detection framework. It is built upon the Transformer encoderdecoder architecture <ref type="bibr" target="#b68">[67]</ref>, combined with a set-based Hungarian loss that forces unique predictions for each object via bipartite matching. Besides, Deformable DETR <ref type="bibr" target="#b97">[96]</ref> further extends DETR by mitigating its high complexity and slow convergence issue.</p><p>Meta-DETR extends the DETR frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b97">96]</ref> by incorporating meta-learning into such fully end-to-end detection frameworks. Its innovative designs can help evade various issues such as the constraint of region-wise prediction under the context of few-shot object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Network Description</head><p>Aiming at performing unified meta-learning for localization and classification at image level, our Meta-DETR is conceptually simple. As shown in <ref type="figure">Fig. 3</ref>, it consists of a Query Encoding Branch (QEB), a Support Encoding Branch (SEB), and a Decoding Branch (DB). Given a Query Image and several Support Images with instance annotations, QEB and SEB first encode them into Query Features and Category Codes, respectively. DB then takes the query features and category codes as input and predicts Detection Results over the corresponding support categories. As target categories to detect are dynamically conditioned on the provided support images, Meta-DETR is able to extract category-agnostic meta-level knowledge that can easily adapt to novel categories.</p><p>Query Encoding Branch (QEB). The design of QEB follows Deformable DETR <ref type="bibr" target="#b97">[96]</ref> except for a residual connection that will be introduced later. As illustrated in <ref type="figure">Fig. 3</ref>, it mainly consists of a feature extractor and a transformer encoder. Given a query image, the feature extractor (a CNN backbone such as ResNet <ref type="bibr" target="#b16">[16]</ref>) generates its feature maps and then adopts 1Ã—1 convolution to make the feature maps' channel dimension compatible with the downstream modules. Since the transformer encoder expects a sequence as input, we first inject positional encoding into the feature maps, collapse the feature maps' spatial dimensions into one dimension, and then feed them into the transformer encoder to produce the query features. Support Encoding Branch (SEB). SEB shares all learnable parameters with QEB following the philosophy of Siamese Networks <ref type="bibr" target="#b26">[25]</ref>. Unlike QEB that preserves imagelevel information within the query features, SEB aims at extracting category codes that mostly relate to certain object instances within the support images. We, therefore, introduce a Category Code Extractor (CCE) to filter out irrelevant information within the support images. CCE has no learnable parameters. It derives support category codes via three sequential operations: 1) restoring the features' spatial dimension from the transformer encoder, 2) locating support object instances with RoIAlign <ref type="bibr" target="#b15">[15]</ref>, and 3) global average pooling followed by a sigmoid function. When there are multiple support images for a category, it averages all category codes as the final category code.</p><p>Decoding Branch (DB). DB receives the outputs of QEB and SEB and produces object detection results, and its target categories are dynamically determined by the category codes. Concretely, it aggregates the query features and category codes into a set of category-specific features. The design of aggregator follows previous work <ref type="bibr" target="#b81">[80]</ref>. A transformer decoder with a feed-forward network (FFN, omitted in <ref type="figure">Fig. 3</ref> for simplicity) then takes the category-specific features and a small fixed number of object queries as input  and produces detection results over the corresponding categories. Similar to the decoder in DETR frameworks, DB eliminates region-wise prediction and addresses object detection at image level. However, DB is category-agnostic with no intention to detect objects of specific categories. Such unique design enables joint meta-learning of object localization and classification at image level, which can avoid potential issues with region-wise prediction and achieve superior few-shot detection performance.</p><p>Semantic Alignment Mechanism (SAM). Meta-learning has been proved promising for few-shot learning. Its major motivation is to obtain meta-level knowledge that can generalize to various categories instead of focusing on specific categories. However, most works <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b87">86]</ref> perform meta-learning on relatively shallow networks, such as ResNet-12 and ResNet-18. There is also evidence <ref type="bibr" target="#b95">[94,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">48]</ref> that meta-learning a deeper network from scratch performs comparable or even worse than without metalearning. One possible reason is that, even with metalearning, deeper networks still tend to learn and rely on category-specific semantics with poor generalization undesirably. To mitigate this issue, we propose to incorporate a simple but effective Semantic Alignment Mechanism (SAM), which is essentially a residual connection as illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>, into the proposed Meta-DETR. The motivation behind SAM is simple and straightforward. As observed in the feature visualization literature <ref type="bibr" target="#b86">[85,</ref><ref type="bibr" target="#b85">84]</ref>, features from bottom layers relate to low-level cues such as colors and shapes that have better generalization; while features from top layers relate to more complex and specific concepts such as categories. To avoid reliance on such high-level category-specific features, SAM incorporates a shortcut connection to bypass the transformer encoder, which works as self-regularization to guide the fea-ture semantics from the transformer encoder to align with its input feature semantics with better generalization.</p><p>It is worth mentioning that the motivation behind SAM is very different from the residual connections that have been widely used in various neural network architectures. The residual connections in ResNet <ref type="bibr" target="#b16">[16]</ref> only bypass several convolutional layers and aim at improving the gradient flow and solving the gradient vanishing issue when training very deep neural networks. Meta-DETR does not suffer from gradient vanishing as its transformer <ref type="bibr" target="#b68">[67]</ref> building blocks already incorporate such residual connections. In contrast, the residual connection used in SAM bypasses the entire transformer encoder, aiming to align its outputs' feature semantics with its inputs', thus acting as self-regularization to prevent reliance on category-specific semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Training Objective</head><p>Detection Target Generation. Assume the fixed number of object queries is N , which means Meta-DETR infers N predictions over each category in a single pass through the decoder. Let us denote by x query the query image, and</p><formula xml:id="formula_0">y = {y i } N i=1 = {(c i , b i )} N i=1</formula><p>the ground truth set of objects within the query image, which is a set of size N . When y i indicates an object, y i = (c i , b i ), where c i denotes the target category label and b i denotes the bounding box of the object. When y i indicates no object, y i = (âˆ…, âˆ…).</p><p>Meta-DETR dynamically conditions its detection targets on support images. Given a support image x supp along with its object annotation (c supp , b supp ), the detection targets are defined as:</p><formula xml:id="formula_1">y = {y i } N i=1 = {(c i , b i )} N i=1 = {Ïˆ(y i , c supp )} N i=1 (1)</formula><p>where Ïˆ(y i , c supp ) acts to filter irrelevant object annotations, which can be formulated as:</p><formula xml:id="formula_2">Ïˆ(y i , c supp ) = ï£± ï£´ ï£² ï£´ ï£³ (âˆ…, âˆ…), if y i = (âˆ…, âˆ…) (âˆ…, âˆ…), if c i = c supp . (1, b i ), if c i = c supp<label>(2)</label></formula><p>Note that y can completely consist of (âˆ…, âˆ…). In this case we call c supp a negative target category.</p><p>Loss Function. Assume the N predictions for target cate-</p><formula xml:id="formula_3">gory made by Meta-DETR areÅ· = {Å· i } N i=1 = (Ä‰ i ,b i ) N i=1</formula><p>. We adopt a pair-wise matching loss L match (y i ,Å· Ïƒ(i) ) to search for a bipartite matching betweenÅ· and y with the lowest cost:</p><formula xml:id="formula_4">Ïƒ = arg min Ïƒ N i=1 L match (y i ,Å· Ïƒ(i) )<label>(3)</label></formula><p>where Ïƒ denotes a permutation of N elements, andÏƒ denotes the optimal assignment between predictions and targets. Since the matching should consider both classification and localization, the matching loss is defined as:</p><formula xml:id="formula_5">L match (y i ,Å· Ïƒ(i) ) =1 {c i =âˆ…} L cls (c i ,Ä‰ Ïƒ(i) ) + 1 {c i =âˆ…} L box (b i ,b Ïƒ(i) ) .<label>(4)</label></formula><p>With the optimal assignmentÏƒ obtained with Eq. 3 and Eq. 4, we optimize the network using the following loss function:</p><formula xml:id="formula_6">L(y ,Å·) = N i=1 L cls (c i ,Ä‰Ïƒ (i) ) + 1 {c i =âˆ…} L box (b i ,bÏƒ (i) )<label>(5)</label></formula><p>where we adopt sigmoid focal loss <ref type="bibr" target="#b34">[33]</ref> for L cls and a linear combination of 1 loss and GIoU loss <ref type="bibr" target="#b55">[54]</ref> for L box . Similar to <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b97">[96]</ref>, L(y ,Å·) is applied to each layer of the transformer decoder.</p><p>Following <ref type="bibr" target="#b82">[81]</ref>, we also adopt a conventional crossentropy loss, denoted as L SEB , to classify the category codes produced by SEB. This encourages category codes that belong to different categories to be distinguished from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training and Inference Scheme</head><p>The training procedure consists of two stages. The first stage is base training stage. During this stage, the model is trained on the base dataset D base with abundant training samples for each base category. The second stage is fewshot fine-tuning stage. In this stage, we train the model on both base and novel categories with limited training samples. Only K object instances are available for each novel category in K-shot object detection. Following <ref type="bibr" target="#b73">[72,</ref><ref type="bibr" target="#b82">81,</ref><ref type="bibr" target="#b81">80]</ref>, we also include several object instances for each base category to prevent performance drop for base categories. In both stages, we optimize the network in an end-to-end manner using the loss functions described in Section 3.2.3.</p><p>In both training stages, multiple auxiliary tasks, also known as episodes, are formed to train the proposed Meta-DETR. Specifically, each episode contains one query image and 10 support images representing different target categories to detect. Target categories include both positive categories and negative categories. Support images are randomly sampled from the training dataset.</p><p>Before inference, we first use SEB to obtain the category codes for all categories once and for all. For each category, if there are multiple support images, we average all corresponding category codes as the final category code. After acquiring the category codes, SEB can be detached. During inference, Meta-DETR does not need to repeatedly compute category codes as in the training stage, which promises the efficient inference of Meta-DETR. Category Split 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category Split 2</head><p>Category <ref type="table" target="#tab_2">Split 3   Method  multi-scale  1  2  3  5  10  1  2  3  5  10  1  2  3  5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We follow the data setups of prior works for few-shot object detection <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b75">74,</ref><ref type="bibr" target="#b82">81,</ref><ref type="bibr" target="#b73">72,</ref><ref type="bibr" target="#b81">80,</ref><ref type="bibr" target="#b77">76]</ref>. Concretely, two widely used few-shot object detection benchmarks are evaluated in our experiments.</p><p>Pascal VOC <ref type="bibr" target="#b9">[9]</ref> consists of images with object annotations of 20 categories. We use trainval 07+12 for training and perform evaluations on test 07. Following <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b82">81,</ref><ref type="bibr" target="#b73">72,</ref><ref type="bibr" target="#b81">80]</ref>, we use 3 novel / base category splits, i.e., ("bird", "bus", "cow", "motorbike", "sofa" / others); ("aeroplane", "bottle","cow","horse","sofa" / others) and ("boat", "cat", "motorbike","sheep", "sofa" / others). The number of shots is set to 1, 2, 3, 5 and 10. Mean average precision (mAP) at IoU threshold 0.5 is used as the evaluation metric. Results are averaged over 10 randomly sampled support datasets.</p><p>MS COCO <ref type="bibr" target="#b35">[34]</ref> is a more challenging object detection dataset, which contains 80 categories including those 20 categories in Pascal VOC. We adopt the 20 shared categories as novel categories, and adopt the remaining 60 categories in MS COCO dataset as base categories. The number of shots is 10 and 30. We use train 2017 for training, and perform evaluations on val 2017. Standard evaluation metrics for MS COCO are adopted. Results are averaged over 5 randomly sampled support datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We adopt commonly used ResNet-101 <ref type="bibr" target="#b16">[16]</ref> as the feature extractor in both QEB and SEB. The network architectures and hyper-parameters of transformer encoder and decoder remain the same as Deformable DETR <ref type="bibr" target="#b97">[96]</ref>. The feedforward network (FFN) after the transformer decoder is a 3-layer MLP for box prediction and a 1-layer MLP for ob- shot fine-tuning stage, the same settings (excluding the total number of epochs and the learning rate decay epochs) are applied to train the model until full convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art Methods</head><p>Pascal VOC. <ref type="table" target="#tab_2">Table 1</ref> shows the few-shot detection performance for novel categories of Pascal VOC. It can be seen that Meta-DETR outperforms existing methods for most cases except when training samples are extremely scarce. We conjecture that the unsatisfactory performance for extremely low-shot settings is largely attributed to the large search space that comes with Meta-DETR's image-level prediction, which may lead to overfitting when training samples are extremely insufficient. However, when there are slightly more training samples for novel categories, e.g., 3-shot, 5-shot, and 10-shot, Meta-DETR performs significantly better across all category splits. Such experimental results demonstrate the superior robustness and generalization capability of our method. <ref type="table">Table 2</ref> shows experimental results while taking base categories into consideration. While achieving good performance for novel categories with limited training samples, Meta-DETR can still detect objects of base categories with competitive performance. TFA <ref type="bibr" target="#b73">[72]</ref> produces outstanding performance for base categories since it works more like conventional detectors with fine-tuning, thus having constrained capacity in generalizing on novel categories.</p><p>MS COCO. <ref type="table">Table 3</ref> shows experimental results on MS COCO. It can be seen that, although MS COCO is more challenging with higher complexity like occlusions and large scale variations, Meta-DETR still outperforms all existing methods for all setups by even larger margins. Specifically, on the primary metric AP 0.5:0.95 , Meta-DETR outperforms state-of-the-art methods by 5.3% for 10-shot and 8.2% for 30-shot. On the strict metric AP 0.75 , Meta-DETR almost doubles the state-of-the-art method's performance from 9.8% to 18.5% for 10-shot and from 12.2% to 23.8% for 30-shot. This demonstrates Meta-DETR's precise localization, which is largely attributed to the unified image-level meta-learning that exploits the synergistic effects of localization and classification. Besides, Meta-DETR achieves the best performance for objects of all scales, especially for large objects, largely because Meta-DETR exploits global contexts via image-level predictions effectively.</p><p>Except for Average Precision (AP) that directly measures the performance of a detector, Average Recall (AR) is also an important metric. Higher AR indicates less missed detection. As shown in <ref type="table">Table 3</ref>, Meta-DETR also outperforms the state-of-the-art by large margins regarding AR 100 (+8.4% for 10-shot and +10.0% for 30-shot). It is noteworthy that FSOD <ref type="bibr" target="#b10">[10]</ref> achieves the highest AR 100 among the region-based counterparts, thanks to its meta-learningbased AttentionRPN that generates more accurate region proposals. However, FSOD still suffers from inaccurate or missed detection as it is fundamentally region-based, rely-  ing on high-quality region proposals that are hard to obtain under the few-shot scenarios. In contrast, Meta-DETR fully eliminates region-wise prediction and makes predictions at image level, thus avoiding this constraint and achieving superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We design extensive ablation experiments to study how our designed technical components contribute to the overall few-shot object detection performance. Effect of Category Code Extractor (CCE). We introduce CCE into SEB to extract object-level instead of image-level information for generating category codes, thus solving the task mismatch issue between the two encoding branches. Another strategy adopted by prior works <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b82">81,</ref><ref type="bibr" target="#b81">80]</ref> is to directly use support images with an extra channel representing objects' locations as input. As shown in <ref type="table" target="#tab_4">Table 4</ref>, CCE achieves better performance, which shows CCE can effectively filter out redundant information and generate more accurate category codes compared with previous strategy. Effect of Semantic Alignment Mechanism (SAM). Metalearning does not aim to learn specific categories. However, with a limited number of base categories, it still inevitably learns category-specific features that only perform well on certain categories and fail to generalize to novel categories. As shown in <ref type="table" target="#tab_4">Table 4</ref>, SAM consistently boosts few-shot detection performance for novel categories, which demonstrates its effectiveness in preventing reliance on category-specific features. In <ref type="figure" target="#fig_2">Fig. 5, we</ref>  the attention maps of correlations between query features and category codes. Without SAM, our method produces strong responses for the base category (cat) with the learned category-specific features, while failing to produce clear responses for the novel category (bird). With SAM included, clear responses are produced for both base and novel categories, which implies that more generalizable representations are learned effectively. Effect of L SEB . We introduce L SEB , which is essentially a conventional cross-entropy loss, to classify the category codes of different categories for better discrimination. As shown in <ref type="table" target="#tab_4">Table 4</ref>, L SEB slightly but consistently boosts the performance. When there are relatively more training samples for novel categories (10-shot), the performance gain brought by L SEB is marginal, which means Meta-DETR already can discriminate novel categories even without L SEB . Effect of Unified Meta-Learning. We also study the effect of unified meta-learning in <ref type="table" target="#tab_5">Table 5</ref>. Specifically, we make modifications to Meta-DETR to perform separated learning for localization and classification, the two sub-tasks of object detection. Detailed architectures for this study are presented in appendices. As shown in <ref type="table" target="#tab_5">Table 5</ref>, unified meta-learning significantly outperforms other design choices, which proves the synergistic effect of the two subtasks. Interestingly, separate meta-learning for both subtasks performs slightly worse than meta-learning classification alone. This can be attributed to the intrinsic difficulty in meta-learning image-level localization with no support from the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents Meta-DETR, a novel few-shot object detection framework that unifies the meta-learning of object localization and classification at image level. By eliminating the region-wise prediction that is problematic in the few-shot scenarios and effectively leveraging the synergistic relationship between localization and classification, it overcomes the common weaknesses rooted in existing methods. Extensive experiments validate that Meta-DETR establishes new state-of-the-art and outperforms prior works by large margins without bells and whistles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>This section provides more details of our proposed method and experimental setups, which are omitted in the main paper due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Detailed Architecture of Meta-DETR</head><p>The transformer encoder and decoder in the proposed Meta-DETR have similar setups as Deformable DETR <ref type="bibr" target="#b97">[96]</ref>. Concretely, both transformer encoder and decoder have 6 layers and adopt the multi-scale deformable attention module <ref type="bibr" target="#b97">[96]</ref> as their attention mechanism. The channel dimension is 256, and the intermediate dimension of fullyconnected layers (FC) inside the transformer is 1024. The dropout probability, number of attention heads, and number of object queries are set at 0.1, 8, and 300, respectively. <ref type="figure" target="#fig_3">Fig. 6</ref> shows the architecture of the Aggregator inside the Decoding Branch (DB). The architecture has similar design as FsDetView <ref type="bibr" target="#b81">[80]</ref>, except that the query features represent whole-image rather than region-level information. Aggregation is conducted between category codes and each position of query features. <ref type="figure" target="#fig_4">Fig. 7</ref> illustrates the feed-forward network (FFN) in Decoding Branch (DB) that produces final predictions (omitted for simplicity in <ref type="figure">Fig. 3</ref> in the manuscript). It consists of a 1-layer MLP for confidence prediction and a 3-layer MLP for box prediction. FFN is shared for all the embeddings that are generated from the transformer decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Modified Meta-DETR for Ablation Study</head><p>In Section 4.4, we modified the proposed Meta-DETR to study the effect of unified meta-learning. In <ref type="table" target="#tab_5">Table 5</ref>, transfer-learning means that the specific sub-tasks (classification or localization, or both) are learned via naive fine-tuning strategy. Separated meta-learning means that the specific sub-tasks are learned via a standalone metalearning-based component. To achieve this, we move the Aggregator after the transformer decoder and perform feature aggregation between category codes and the embeddings generated from the transformer decoder. Therefore, FFN becomes meta-learning-based components for specific sub-tasks, which manages to disentangle the meta-learning for the two sub-tasks. This design enables us to explore the effect of unified meta-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Detailed Training and Inference Setups</head><p>Base Training Stage. All essential setups are provided in Section 4.2. For further details, please refer to our codes.</p><p>Few-Shot Fine-Tuning Stage. The few-shot fine-tuning stage shares the same setups as the base training stage, except for the total number of epochs and the learning rate decay epochs. Such differences are due to the significantly  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setups</head><p>Pascal VOC MS COCO 1 2 3 5 10 10 30</p><p>Total Epochs 700 600 600 500 500 500 500 Decay Epochs 600 500 500 425 425 425 425 <ref type="table">Table 6</ref>. Setups of total number of epochs and learning rate decay epochs for the few-shot fine-tuning stage. smaller number of training samples under the few-shot scenarios, so that more training epochs are required to reach full convergence. Detailed setups are presented in <ref type="table">Table 6</ref>. These numbers are empirically set solely based on the training loss trajectory, so we expect further performance gain if comprehensive hyper-parameter search is conducted.</p><p>Inference. Given a query image, Meta-DETR produces 300 predictions for each category when performing inference. However, both Pascal VOC and MS COCO accept only 100 predictions per image. We choose the top-scored 100 predictions across all categories as the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Evaluation Metrics</head><p>Pascal VOC. For Pascal VOC, mean average precision (mAP) at IoU threshold 0.5 is used as the evaluation metric. In the context of few-shot object detection, mAP is averaged over all novel categories. Evaluation with Multiple Repeated Runs. More and more researchers have realized that few-shot object detection performance often comes with a large variance. The lower the number of shots, the more unstable the results are. This is because few-shot detection performance relies heavily on the quality of the training samples for novel categories. Therefore, with results from a single run, it is not easy to draw convincing conclusions. To address this issue, following <ref type="bibr" target="#b73">[72]</ref> and <ref type="bibr" target="#b81">[80]</ref>, our results, as reported in <ref type="table" target="#tab_2">Table 1</ref>-5, are averaged over multiple repeated runs with different randomly sampled support datasets. Specifically, as we observe large performance variances in Pascal VOC, especially for 1-shot, 2-shot, and 3-shot, all our results on Pascal VOC are averaged over 10 randomly sampled support datasets. For MS COCO, we observe smaller variances with repeated runs, which can be attributed to the larger number of categories and shots. Therefore, we average our results on MS COCO over 5 randomly sampled support datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Inference Speed of Meta-DETR</head><p>During inference, the category codes for all base and novel categories can be computed once and for all. This enables efficient inference of Meta-DETR. <ref type="table" target="#tab_6">Table 7</ref> presents the inference speed of Meta-DETR and Deformable DETR <ref type="bibr" target="#b97">[96]</ref>. We can see that Meta-DETR only introduces moderate extra computational costs as compared with the naive fine-tuning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Qualitative Results</head><p>We provide multiple qualitative visualizations of Meta-DETR's few-shot detection results in <ref type="figure" target="#fig_2">Figs. 8-15</ref>, which give a straightforward illustration of the performance of our method. Note that only detection results of novel categories are presented, as the major focus is to detect objects of novel categories. In addition, we only show results with confidence scores higher than 0.3. White boxes indicate correct detections, red solid boxes indicate false positives, and red dashed boxes indicate false negatives. It can be observed that the proposed Meta-DETR is able to detect novel objects even with scarce training samples. In addition, Meta-DETR performs exceptionally well on large objects and we will investigate how to handle small objects and cluttered objects in our future research.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of existing few-shot detectors with our Meta-DETR. Dashed blue boxes indicate meta-learning components. âŠ— indicates feature aggregation. Unlike prior works that rely on region-wise predictions, Meta-DETR unifies the meta-learning of object localization and classification at image level with a single meta-learning module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Semantic Alignment Mechanism. A simple residual connection acts as self-regularization to prevent the transformer encoder from relying on undesirable category-specific features by aligning the feature semantics of its input and output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of correlations between query features and category codes. With semantic alignment mechanism (SAM) introduced, clear responses for both base category (cat) and novel category (bird) are observed, demonstrating SAM's effectiveness in enhancing generalization of meta-learned representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of the detailed architecture of Aggregator in Decoding Branch (DB). Aggregation is performed between category codes and each position of query features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>, y1, x2, y2) Illustration of the feed-forward network (FFN) in Decoding Branch (DB) to produce final predictions. FFN is shared for all the embeddings generated from the transformer decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 1. Novel categories include bird, bus, cow, motorcycle, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives. 280 Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 1. Novel categories include bird, bus, cow, motorcycle, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 2. Novel categories include airplane, bottle, cow, horse, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 2. Novel categories include airplane, bottle, cow, horse, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 3. Novel categories include boat, cat, motorcycle, sheep, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Visualization of multi-scale Meta-DETR's 10-shot object detection results on Pascal VOC category split 3. Novel categories include boat, cat, motorcycle, sheep, and sofa. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 .</head><label>15</label><figDesc>Visualization of multi-scale Meta-DETR's 30-shot object detection results on MS COCO. Novel categories include person, bicycle, car, motorcycle, airplane, bus, train, boat, bird, cat, dog, horse, sheep, cow, bottle, chair, couch, potted plant, dining table, and tv. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>have proved the effectiveness</figDesc><table><row><cell></cell><cell cols="2">Support Encoding Branch (SEB)</cell><cell></cell></row><row><cell></cell><cell>Feature Extractor</cell><cell>Transformer Encoder</cell><cell>Aggregator</cell><cell>Transformer Decoder</cell></row><row><cell>Support Images</cell><cell></cell><cell></cell><cell>Category Codes</cell><cell>Category-Specific Features</cell><cell>Detection Results</cell></row><row><cell></cell><cell>Feature Extractor</cell><cell>Transformer Encoder</cell><cell></cell><cell>Positional Encoding Element-Wise Addition</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Category Code Extractor</cell></row><row><cell>Query Image</cell><cell cols="2">Query Encoding Branch (QEB)</cell><cell>Query Features</cell><cell>Weight Sharing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>10FRCN-ft-full<ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b73">72]</ref> 15.2 20.<ref type="bibr" target="#b2">3</ref> 29.0 40.1 45.5 13.4 20.6 28.6 32.4 38.8 19.6 20.8 28.7 42.2 42.1 D-DETR-ft-full [96] â€  5.6 13.3 21.7 34.2 45.0 10.9 13.0 18.4 27.3 39.4 7.3 16.6 20.8 32.2 41.8 32.9 34.4 38.6 41.3 17.2 22.1 23.4 28.3 35.8 27.5 31.1 31.5 34.4 37.2 TFA w/ fc [72] â€  22.9 34.5 40.4 46.7 52.0 16.9 26.4 30.5 34.6 39.7 15.7 27.2 34.7 40.8 44.6 TFA w/ cos [72] â€  25.3 36.4 42.1 47.9 52.8 18.3 27.5 30.9 34.1 39.5 17.9 27.2 34.3 40.8 45.6 MPSR [76] â€  34.7 42.6 46.1 49.4 56.7 22.6 30.5 31.0 36.7 43.3 27.5 32.5 38.2 44.6 50.0 Meta-YOLO [22] 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 40.5 21.3 25.6 28.4 42.8 45.9 Meta Det [74] 18.9 20.6 30.2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1 Meta R-CNN [81] 19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1 FsDetView [80] 24.2 35.3 42.2 49.1 57.4 21.6 24.6 31.9 37.0 45.7 21.2 30.0 37.2 43.8 49.6 Meta-DETR (Ours) 17.5 36.0 45.1 51.2 57.1 18.5 27.5 34.7 41.1 49.8 15.4 32.6 39.4 49.0 54.3 Meta-DETR (Ours) 20.4 35.0 46.3 52.2 57.8 20.2 30.9 38.2 44.0 52.6 22.8 34.9 43.0 50.2 54.9 Few-shot detection performance (mAP@0.5) on Pascal VOC test 07 set for novel categories. Results are averaged over multiple repeated runs with different randomly sampled support datasets. â€  indicates results are re-evaluated using official codes for multiple runs since original results are evaluated with a single run.</figDesc><table><row><cell>LSTD [3]</cell><cell>8.2</cell><cell>1.0 12.4 29.1 38.5 11.4 3.8</cell><cell>5.0 15.7 31.0 12.6 8.5 15.0 27.3 36.3</cell></row><row><cell>RepMet [56]</cell><cell>26.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 Table 3</head><label>23</label><figDesc>For ablation study, we only adopt the single-scale setting for Meta-DETR. We train our model using the AdamW [24, 42] optimizer with an initial learning rate of 2 Ã— 10 âˆ’4 and a weight decay of 1 Ã— 10 âˆ’4 . We adopt a batch size of 32 and each query image is associated with 10 support images to form an episode. Conventional data augmentation as used in<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b97">96]</ref> is adopted during training. In the base training stage, we train the model for 100 epochs for Pascal VOC and 50 epochs for MS COCO. Learning rate is decayed at the 85 th and 40 th epoch by a factor of 0.1, respectively. In the few-AP 0.5:0.95 AP 0.5 AP 0.75 AP S AP M AP L AR 1 AR 10 AR 100 AR S AR M AR L</figDesc><table><row><cell>Shot</cell><cell>Method</cell><cell cols="2">multi-scale Base Novel</cell></row><row><cell></cell><cell>LSTD [3]</cell><cell>66.3</cell><cell>12.4</cell></row><row><cell></cell><cell>TFA w/ cos [72]  â€ </cell><cell>77.3</cell><cell>42.1</cell></row><row><cell></cell><cell>MPSR [76]  â€ </cell><cell>65.9</cell><cell>46.1</cell></row><row><cell>3</cell><cell>Meta-YOLO [22]</cell><cell>64.8</cell><cell>26.7</cell></row><row><cell></cell><cell>Meta R-CNN [81]</cell><cell>64.8</cell><cell>35.0</cell></row><row><cell></cell><cell>Meta-DETR (Ours)</cell><cell>65.2</cell><cell>45.1</cell></row><row><cell></cell><cell>Meta-DETR (Ours)</cell><cell>66.5</cell><cell>46.3</cell></row><row><cell></cell><cell>LSTD [3]</cell><cell>66.3</cell><cell>38.5</cell></row><row><cell></cell><cell>TFA w/ cos [72]  â€ </cell><cell>77.5</cell><cell>52.8</cell></row><row><cell></cell><cell>MPSR [76]  â€ </cell><cell>69.8</cell><cell>56.7</cell></row><row><cell>10</cell><cell>Meta-YOLO [22]</cell><cell>63.6</cell><cell>47.2</cell></row><row><cell></cell><cell>Meta R-CNN [81]</cell><cell>67.9</cell><cell>51.5</cell></row><row><cell></cell><cell>Meta-DETR (Ours)</cell><cell>67.1</cell><cell>57.1</cell></row><row><cell></cell><cell>Meta-DETR (Ours)</cell><cell>67.4</cell><cell>57.8</cell></row></table><note>. Few-shot detection performance (mAP@0.5) for base and novel categories on category split 1 of Pascal VOC. Results are averaged over multiple runs. â€  indicates re-evaluated results.ject confidence prediction. Thanks to the multi-scale atten- tion module introduced in Deformable DETR [96], Meta- DETR supports multi-scale features as input by nature with- out any modification. For a comprehensive comparison, we present results of Meta-DETR with both single-scale and multi-scale features in benchmarking.. Few-shot detection performance on MS COCO val 2017 set for novel categories. Results are averaged over multiple repeated runs with different randomly sampled support datasets. â€  indicates results are re-evaluated using official codes for multiple runs since original results are evaluated with a single run.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies over several design choices of Meta-DETR. Results for novel categories are averaged over multiple runs on the category split 1 of Pascal VOC.</figDesc><table><row><cell>Design Choice</cell><cell></cell><cell></cell><cell>Shot</cell></row><row><cell cols="2">CCE SAM L SEB</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell cols="3">11.6 37.6 54.2</cell></row><row><cell></cell><cell></cell><cell cols="3">15.1 39.6 53.2</cell></row><row><cell></cell><cell></cell><cell cols="3">15.8 40.4 53.4</cell></row><row><cell></cell><cell></cell><cell cols="3">17.2 43.0 56.7</cell></row><row><cell></cell><cell></cell><cell cols="3">17.5 45.1 57.1</cell></row><row><cell>Query Image</cell><cell cols="3">Bird (Novel)</cell><cell>Cat (Base)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/o</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SAM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SAM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies over the effect of unified meta-learning. Results for novel categories are averaged over multiple runs on the category split 1 of Pascal VOC.</figDesc><table><row><cell>further visualize</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Inference speed comparison. Results are obtained using NVIDIA GeForce RTX 2080Ti GPU with single batch size on Pascal VOC.MS COCO. MS COCO's standard metrics are used for evaluation. Specifically, AP 0.5:0.95 is the primary metric that directly measures detectors' performance, which adopts 10 different IoU thresholds to reward detectors with better localization. Standard metrics also include AP 0.5 and AP 0.75 , which correspond to the Pascal VOC metric and a more strict metric, respectively. In addition to average precision (AP), average recall (AR) also serves as an important evaluation metric, which measures the percentage of detected objects among all ground truth objects. Higher AR indicates less missed detection. Concretely, AR 1 , AR 10 , and AR 100 correspond to AR given 1 detection per image, 10 detections per image, and 100 detections per image, respectively. The MS COCO metrics also evaluate the performance for objects of different sizes (small, medium, and large), including AP S , AP M , AP L , AR S , AR M , and AR L . Similar to Pascal VOC, all these metrics are averaged over all novel categories in our experiments.</figDesc><table><row><cell>Method</cell><cell cols="2">single-scale multi-scale</cell></row><row><cell>Deformable DETR [96]</cell><cell>17.8 FPS</cell><cell>11.3 FPS</cell></row><row><cell>Meta-DETR (Ours)</cell><cell>11.0 FPS</cell><cell>5.3 FPS</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LSTD: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A new meta-baseline for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arxiv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UP-DETR: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning RoI transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fewshot object detection with attention-RPN and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Few-shot human motion prediction via metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JosÃ©</forename><forename type="middle">M F</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr DollÃ¡r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Meta-learning in neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arxiv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-view regularization for domain adaptive panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoran</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contextual-relation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fewshot object detection via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geonuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggyu</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SMC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RON: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosting few-shot learning with adaptive margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for few-shot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">F</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MM-FSOD: Meta and metric integrated fewshot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuewen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenquan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mask TextSpotter: An end-toend trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="532" to="548" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DETR for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr DollÃ¡r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Few-shot pill recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suiyi</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junle</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Few-shot open-set recognition using metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gang Hua, and Nuno Vasconcelos</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">AFD-Net: Adaptive fully-dual network for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>PietikÃ¤inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Incremental few-shot meta-learning via indirect discriminant alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orchid</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Incremental few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fewshot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">YOLO 9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Instance-aware, context-focused, and memoryefficient weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">They call it like they see it: Spontaneous naming and attention to shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="198" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rep-Met: Representative-based metric learning for classification and one-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharathchandra</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Il</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="248" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection -SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">SNIPER: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Object name learning provides on-the-job training for attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Gershkoff-Stowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="13" to="19" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Rethinking transformer-based set prediction for object detection. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fewshot viewpoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Meta-learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<idno>1810.03548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">and Illia Polosukhin. Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tracking by instance detection: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning rich features at high-speed for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fewshot adaptive Faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">TAFE-Net: Task-aware feature embeddings for low shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Metalearning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Few-shot learning with localization in realistic settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multi-scale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Meta-RCNN: Meta learning for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Rethinking classification and localization for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">PARN: Position-aware relation networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta R-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Fewshot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">DeepEMD: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Defect-GAN: High-fidelity defect synthesis for automated defect inspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Yi</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">PNPDet: Efficient few-shot detection without forgetting via plug-and-play sub-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongliang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<idno>WACV, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">CAD-Net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10015" to="10024" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Cooperating RPN&apos;s improve few-shot object detection. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ã€gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Deep metalearning: Learning to learn in the concept space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno>1802.03596</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Scale-transferrable object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Visualization of multi-scale Meta-DETR&apos;s 30-shot object detection results on MS COCO. Novel categories include person, bicycle, car, motorcycle, airplane, bus, train, boat, bird, cat, dog, horse, sheep, cow, bottle, chair, couch, potted plant, dining table, and tv. For simplicity, only results of novel categories are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives</title>
		<imprint/>
	</monogr>
	<note>Figure 14. Red dashed boxes indicate false negatives</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
