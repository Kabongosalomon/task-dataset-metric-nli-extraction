<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepLight: Deep Lightweight Feature Interactions for Accel-erating CTR Predictions in Ad Serving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>March 8-12, 2021. Israel 2021. March 8-12, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Deng</surname></persName>
							<email>weideng056@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhou</surname></persName>
							<email>tian.zhou@verizonmedia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deguang</forename><surname>Kong</surname></persName>
							<email>doogkong@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Flores</surname></persName>
							<email>aaron.flores@verizonmedia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Lin</surname></persName>
							<email>guanglin@purdue.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deguang</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Flores</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yahoo Research Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Yahoo Research Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Yahoo Research Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Yahoo Research Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepLight: Deep Lightweight Feature Interactions for Accel-erating CTR Predictions in Ad Serving</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the Fourteenth ACM International Conference on Web Search and Data Mining (WSDM &apos;21)</title>
						<meeting>the Fourteenth ACM International Conference on Web Search and Data Mining (WSDM &apos;21)						</meeting>
						<imprint>
							<date type="published">March 8-12, 2021. Israel 2021. March 8-12, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3437963.3441727</idno>
					<note>† The work was done while Wei Deng was working as an intern at Yahoo Research; Deguang Kong is now affiliated with Google, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Virtual Event, Israel. ACM, New York, NY, USA, 9 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Display advertising</term>
					<term>Recommender sys- tems</term>
					<term>• Computing methodologies → Ranking KEYWORDS Deep acceleration</term>
					<term>ad serving</term>
					<term>structural pruning</term>
					<term>preconditioner</term>
					<term>lightweight models</term>
					<term>fast inference</term>
					<term>low memory * Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate (CTR) prediction is a crucial task in recommender system and online advertising. The embedding-based neural networks have been proposed to learn both explicit feature interactions through a shallow component and deep feature interactions by a deep neural network (DNN) component. These sophisticated models, however, slow down the prediction inference by at least hundreds of times. To address the issue of significantly increased serving latency and high memory usage for real-time serving in production, this paper presents DeepLight: a framework to accelerate the CTR predictions in three aspects: 1) accelerate the model inference via explicitly searching informative feature interactions in the shallow component; 2) prune redundant parameters at the inter-layer level in the DNN component; 3) prune the dense embedding vectors to make them sparse in the embedding matrix. By combining the above efforts, the proposed approach accelerates the model inference by 46X on Criteo dataset and 27X on Avazu dataset without any loss on the prediction accuracy. This paves the way for successfully deploying complicated embedding-based neural networks in real-world serving systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Online advertising has grown into a hundred-billion-dollar business since 2020, and the revenue has been increasing by more than 15% per year for nine consecutive years <ref type="bibr" target="#b1">[2]</ref>. CTR prediction is critical in the online advertising industry, and the main goal is to deliver the right ads to the right users in the right context. Therefore, how to predict CTR accurately and efficiently has drawn the attention of both academic and industry communities.</p><p>Generalized linear models and factorization machine (FM) <ref type="bibr" target="#b30">[30]</ref> have achieved great successes. However, they are limited in their prediction power due to the lack of mechanisms to learn deeper feature interactions. To tackle this issue, the embedding-based neural networks seek inspirations from computer vision and natural language processing and propose to include a shallow component to learn the informative low-order feature interactions and a DNN component for powerful high-order interaction modeling. In particular, Wide &amp; Deep <ref type="bibr" target="#b3">[4]</ref> proposed to train a joint network that combines a linear model and a DNN model, which, however, still requires feature engineering and is not end-to-end. DeepFM <ref type="bibr" target="#b10">[10]</ref> solved that problem by learning low-order feature interactions through the FM component instead of the linear model. Since then, various embedding-based neural networks have been proposed to improve the performance: Neural Factorization Machines (NFM) <ref type="bibr" target="#b13">[13]</ref> uses a bilinear interaction pooling to connect the embedding vectors with the DNN component; Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b34">[34]</ref> models cross features of bounded degrees in terms of layer depth; eXtreme Deep Factorization Machine (xDeepFM) <ref type="bibr" target="#b22">[22]</ref> incorporates a Compressed Interaction Network (CIN) <ref type="bibr" target="#b34">[34]</ref> and a DNN to automatically learn high-order feature interactions in both explicit and implicit manners; AutoInt <ref type="bibr" target="#b33">[33]</ref> proposes to model highorder feature interactions using self-attentive neural networks with residual connections; AutoFIS <ref type="bibr" target="#b23">[23]</ref> proposes to identify important high-order feature interactions in the factorization machines for a more effective representation. Other extensions of embeddingbased neural networks include <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Despite the advances of these novel models applied in click prediction tasks in online advertising, the prediction is slowed down by hundreds of times compared to simple models such as logistic regression or factorization machine. This leads to unrealistic latency for the real-time ad serving system. One question that naturally follows is: are we able to serve the high quality deep models with satisfactory model latency and resource consumption for real-time response in ad serving?</p><p>Towards this goal, a practical solution needs to address the following challenges (C1-C3).</p><p>(C1) High quality: the served "slim" model is expected to be as accurate as the the original "fat" model.</p><p>(C2) Low latency: the serving latency should be at very low level to maintain high QPS (Query per second) with few timeout.</p><p>(C3) Low consumption: memory costs should be low for pulling model checkpoint and storing them in memory in online ad serving.</p><p>However, all the existing embedding-based neural networks, such as DeepFM, NFM, xDeepFM, and AutoInt, still focus on increasing the model complexity to achieve (C1) while sacrificing the performance in (C2) and (C3). Whereas a few approaches, such as AutoCross <ref type="bibr" target="#b24">[24]</ref>, are proposed to improve the model efficiency, they didn't adopt the DNN framework and fail in achieving the state-of-the-art. To address these challenges all together, we propose an efficient model, so-called field-weighted embedding-based neural network (DeepFwFM) by improving the FM module via a field pair importance matrix, which is empirically as powerful as xDeepFM but becomes much more efficient. As shown in <ref type="figure" target="#fig_2">Figure  2</ref>, each component of DeepFwFM has an approximately sparse structure, which implies an advantage in structural pruning and potentially leads to an even more compact structure. By pruning the DNN component of DeepFwFM and further compressing the shallow component, the resulting deep lightweight structure, DeepLight, greatly reduces the inference time and still maintains the model performance. By contrast, the other structures may fail in either deep model accelerations <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b34">34]</ref> or accurate predictions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b33">33]</ref>.</p><p>To the best of our knowledge, this is the first paper that studies pruning embedding based DNN models for accelerating CTR predictions in ad serving. To summarize, the proposed field-weighted embedding-based neural network (DeepFwFM) has a great potential in fast and accurate inference. Compared to the existing embeddingbased neural networks, the model has the following advantages:</p><p>• To address the challenge of (C1) high quality, the proposed model exploits the field pair importance idea in FwFM <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> to improve the understanding of low-order feature interactions instead of exploring high-order feature interactions <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b34">34]</ref>. Most notably, such an improvement achieves the state-of-the-art with a very low complexity compared to the state-of-the-art xDeepFM model and still shows a great potential in deep model accelerations.</p><p>• To address the challenge of (C2) low latency, the model can be pruned for further acceleration: 1) prune redundant parameters in the deep component to obtain the most accelerations; 2) remove the weak field pairs in FwFM to obtain additional significant accelerations. The resulting lightweight structure ends up with almost a hundred times of accelerations.</p><p>• To address the challenge of (C3) low consumption, we can promote sparsity on the embedding vectors and preserve the most discriminant signals, which yields a substantial compression w.r.t. number of parameters.</p><p>By overcoming these challenges (C1-C3), the resulting sparse DeepFwFM, so-called DeepLight, eventually achieves remarkable performance not only in predictions, but also in deep model accelerations. It achieves 46X speed-ups on Criteo dataset and 27X speed-ups on Avazu dataset without loss on AUC. To help reproduce the results and benefit the community, we have made code available at https://github.com/WayneDW/sDeepFwFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>CTR prediction is typically achieved by feeding a machine learning model with well-designed features <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b26">26]</ref>. There have been extensive efforts on building features, such as textual features <ref type="bibr" target="#b32">[32]</ref>, click feedback features <ref type="bibr" target="#b9">[9]</ref>, contextual features <ref type="bibr" target="#b26">[26]</ref>, and psychology features <ref type="bibr" target="#b35">[35]</ref>. To avoid the costly feature engineering, we need to build an end-to-end model by modeling feature interactions automatically. Given a dataset D = {( , )}, where is the label and is a -dimensional sparse feature vector, we can consider a degree-2 polynomial Poly2</p><formula xml:id="formula_0">min 2 ∥ ∥ 2 2 + | D | ∑︁ =1 log(1 + exp(− Poly2 ( , ))),<label>(1)</label></formula><p>where is the model parameter, is the 2 penalty, |D | is the number of data points, and</p><formula xml:id="formula_1">Poly2 ( , ) = 0 + ∑︁ =1 + ∑︁ =1 ∑︁ = +1 , .<label>(2)</label></formula><p>Estimating the feature interaction matrix in (2) given insufficient data is not easy, FM <ref type="bibr" target="#b30">[30]</ref> proposed a matrix decomposition method to learn the -dimensional embedding vectors { } =1 through the inner product ⟨ , ⟩:</p><formula xml:id="formula_2">FM ( , ) = 0 + ∑︁ =1 + ∑︁ =1 ∑︁ = +1 ⟨ , ⟩.<label>(3)</label></formula><p>Notably, the embedding vectors { } =1 reduces the number of parameters from ( 2 ) to ( ). Although the above idea can be naturally extended to higher-order FMs, there is no efficient algorithm for training these models <ref type="bibr" target="#b0">[1]</ref>. As such, Deep &amp; Cross <ref type="bibr" target="#b34">[34]</ref> tackles that problem by a series of cross operations:</p><formula xml:id="formula_3">= 0 −1 + + −1 ,<label>(4)</label></formula><p>where , , denote the weights, bias and output of the -th layer of the DeepCross network, respectively. Such an improvement alleviates the computation problems in solving high-order FM models and becomes quite powerful in modeling low-order feature interactions. By further combining a DNN component, we can obtain xDeepFM <ref type="bibr" target="#b22">[22]</ref>, which is the state-of-the-art model in CTR predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEPFWFM: AN EFFICIENT MODEL</head><p>Considering the increasing latency concern of deploying complicated embedding-based neural networks in the real-world serving system, our goal is not only to build a model as accurate as xDeepFM <ref type="bibr" target="#b22">[22]</ref> but also to show the potential for significant deep model accelerations. Despite the powerful performance in modeling feature interactions, xDeepFM <ref type="bibr" target="#b22">[22]</ref> is known to be much more costly than DeepFM <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24]</ref> and raises the risks in deep model accelerations. Such a problem motivates us to step back and rethink the framework of DeepFM. However, it is known that an ill-conditioned matrix suffers from a serious stability issue in matrix decomposition <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b25">25]</ref> and affects the performance of DeepFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model architecture</head><p>To solve the stability issue, we seek inspirations from the preconditioner for robust matrix decompositions and consider the FwFM model <ref type="bibr" target="#b28">[28]</ref>, which includes the field pair importance to further improve the training of FM. In what follows, we propose the Deep Field-weighted Factorization Machine (DeepFwFM) by replacing Poly2 in (1) with DeepFwFM :</p><formula xml:id="formula_4">DeepFwFM ( , , , R) = Deep ( , ) + FwFM ( , , R),<label>(5)</label></formula><p>where is the DNN parameter that contains the weights and bias, ∈ R × , denotes the set of embedding vectors, R ∈ R × is a matrix to model field pair interaction strength, and FwFM is an efficient model <ref type="bibr" target="#b28">[28]</ref> in studying second-order feature interactions on multi-field categorical data such that</p><formula xml:id="formula_5">FwFM ( , , R) = 0 + ∑︁ =1 ⟨ , ( ) ⟩ + ∑︁ =1 ∑︁ = +1 ⟨ , ⟩ ( ), ( ) ,<label>(6)</label></formula><p>where ( ) denotes the field of feature , ( ) denotes the linear embedding vector for field ( ), ( ), ( ) denotes the field pair interaction weight between field pair ( ( ), ( )) and Deep is a non-linear transformation of the embeddings through a Multilayer Perceptron to learn high-order feature interactions.</p><p>To summarize, our model has the following innovations: 1) DeepFwFM is much faster than xDeepFM in that we don't attempt to model high-order (3rd or more) feature interactions via the shallow component. Whereas xDeepFM contains a powerful compressed interaction network (CIN) to approximate any fixedorder of polynomials, the major drawback is that CIN has an even higher time complexity than the DNN component as discussed in <ref type="bibr" target="#b22">[22]</ref>, resulting in expensive computations in large-scale ad systems;</p><p>2) DeepFwFM is more accurate than DeepFM because it overcomes the stability issue in matrix decomposition and leads to more accurate predictions <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b28">28]</ref> by considering the field pair importance. DeepFM models low-order feature interactions through the weight-1 connections between each hidden node in the FM component and the output node as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, which, however, fails to adapt to the local geometry and sacrifices the robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Benefits of computation efficiency</head><p>To demonstrate the benefits of our model, we perform quantitative analysis for computational complexity of DeepFwFM, compared to DeepFM and xDeepFM.</p><p>Computational complexity. Given the embedding size , the number of layers , and the number of nodes in each layer ℎ, the embedding layer only has lookups and leads to little computational cost. The number of floating point operations (FLOPs) of the DNN component and the FwFM component is O ( ℎ 2 + ℎ) and O ( 2 ), respectively. Similarly, we can derive the computational complexity of DeepFM <ref type="bibr" target="#b10">[10]</ref> and xDeepFM <ref type="bibr" target="#b22">[22]</ref>. We summarize the results in <ref type="table">Table.</ref>1 and observe that a -layer CIN in xDeepFM takes O ( ℎ 2 ) operations <ref type="bibr" target="#b22">[22]</ref>, which is much more than the DNN component.   </p><formula xml:id="formula_6">DeepFM O ( ) O ( ℎ 2 + ℎ) xDeepFM O ( ℎ 2 ) O ( ℎ 2 + ℎ) DeepFwM O ( 2 ) O ( ℎ 2 + ℎ)</formula><p>than the DNN component. For example, considering standard parameter settings in <ref type="table">Table.</ref>1, we see that theoretically DeepFwFM is as efficient as DeepFM and is 18X faster than xDeepFM. Despite the initial progresses on accelerating CTR predictions, DeepFwFM still fails to reduce the latency to a good level (e.g. 10 ms for each bid request). In fact, DeepFwFM can be hundreds-oftimes slower due to the inclusion of the DNN component, which significantly slows down the online CTR predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEPLIGHT: A LIGHTWEIGHT MODEL VIA STRUCTURAL PRUNING</head><p>The DNN component is undoubtedly the main reason that causes the high latency issue and fails to meet the online requirement. Therefore, a proper acceleration method is on great demand to speed up the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Why structural pruning?</head><p>Deep model acceleration has achieved great popularity in computer vision and it consists of three main methods: structural pruning <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b21">21]</ref>, knowledge distillation <ref type="bibr" target="#b15">[15]</ref>, and quantization <ref type="bibr" target="#b5">[5]</ref>, among which structural pruning methods have received wide attentions <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref> due to their remarkable performance. Moreover, each component of DeepFwFM, such as the DNN component, the field pair interaction matrix and the embedding vectors, possesses the highly-sparse structure as shown in <ref type="figure">Figure.</ref>2. This motivates us to consider structural pruning to accelerate both the shallow component and the DNN component. For the other choices, quantization <ref type="bibr" target="#b11">[11]</ref> adopts the effective fixed point precision in inference time. However, it doesn't fully utilize the sparse structure of each component of DeepFwFM and even damages the precision of large coefficients. While knowledge distillation technique <ref type="bibr" target="#b15">[15]</ref> trains small networks (i.e. student model) to mimic larger ones (i.e., teacher model), it suffers from the following issues: a) the student model may have a limited capacity to learn from the teacher; b) where there is a performance deterioration issue, it's hard to figure out whether it's due to the teacher model or due to the teaching procedure.</p><p>That's the reason why we adopt structure pruning for CTR prediciton models in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">How to do structural pruning?</head><p>Design In this section, we show how to do structural pruning for DeepFwFM. Instead of simply applying pruning technique in ad prediction task with a uniform sparse rate, we propose to prune the following three components (in the context of DeepFwFM model) that are particularly designed to adapt for ad prediction task (given the existing feature embedding and field relevance in shallow and deep components):</p><p>• Prune the weights (excluding bias) of the DNN component to remove the neural connections;</p><p>• Prune the field pair interaction matrix to remove redundant field interactions;</p><p>• Prune the elements in the embedding vectors, leading to sparse embedding vectors.</p><p>Combining the above efforts, we obtain the desired DeepLight model, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. DeepLight is a sparse DeepFwFM that provides a holistic approach to accelerate inference by modifying the trained architecture at inference time. Benefit As is evident in empirical study, DeepLight enjoys the following properties</p><p>• The sparse DNN component has much less computation complexity, compared with the original one. It leads to the most accelerations in computation;</p><p>• The sparse field pair interaction matrix further achieves significant accelerations in the FwFM component. In addition, pruning is actually doing feature selection, or more accurately, field pair selection. Once a field pair interaction weight , ′ is pruned, all feature pairs from field pair ( , ′ ) are dropped. AutoFIS <ref type="bibr" target="#b23">[23]</ref> also achieves a similar kind of field pair selection;</p><p>• The sparse embedding greatly reduces the memory usage, since the feature embeddings dominate the number of parameters in deep learning models for click prediction.</p><p>Implementation Selecting a good sparse network from an overparameterized model is NP-hard, and no optimal algorithms are guaranteed to solve it. There are a lot of empirical studies existed in the area of structural pruning <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref>, including weight pruning, neuron pruning, or pruning of other units. Considering the fact that our model only contains a standard FwFM component and a vanilla DNN component, we conduct weight pruning and seek to achieve high sparsity and accelerations without calling specifically dedicated libraries.</p><p>Regarding the implementations, we adopt a standard algorithm used in <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8]</ref> to study the structural pruning of the mainstream CTR-prediction models and compare them with our proposed sparse DeepFwFM, namely DeepLight. As in most of the pruning works, we put the community wisdom in our algorithm by adopting the 2 penalty <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. Now, we present the main algorithm in Alg.1. We first train the model a few epochs to provide a good initialization and then conduct pruning to remove the redundant weights with the lowest magnitude. After each pruning, we retrain the model to finetune the network so that mistakenly pruned weights has a potential to become active again. We keep repeating this pruning procedure and set adaptive sparse rates such that the rate increases faster in the early phase when the network is stable and slower in the late phase when the network becomes sensitive. The algorithm behaves similar to the greedy algorithms based on the weak sub-modular optimization <ref type="bibr" target="#b6">[6]</ref>, but it also injects some additional uncertainty during the pruning process, which potentially avoids damaging good weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reduction of computational complexity</head><p>The DNN component is the bottleneck that causes the high inference time. After the pruning of the DNN component, the FwFM component becomes the limitations, which requires further pruning on the field matrix . The pruning of the embedding layer has no significant speed-ups on the computations. With a medium dnn % sparsity on the weights in the DNN component (excluding the bias), the corresponding speed-ups can be close to the ideal 1/(1 − dnn %) times. However, when the sparsity dnn % is higher than 95%, we may not achieve the ideal rate because of the computations in the biases and the overhead of sparse structures, such as the compressed row storage (CRS). As to the pruning of the field matrix , the speed-ups becomes more significant as we increase the sparsity dnn % in the DNN component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Reduction of space complexity</head><p>Pruning in the embedding layer also dramatically reduces the number of parameters in DeepFwFM and therefore saves lots of memory. In the embedding layer, a emb % sparsity reduces the number of parameters from to (1 − emb %) . While in the DNN component, the number of weights (excluding the bias) can be reduced   <ref type="bibr" target="#b18">[18]</ref>. It contains 45 million samples and each sample has 13 numerical features (counting) and 26 categorical features. We adopt the log transformation of log( ) 2 if &gt; 2 proposed by the winner of Criteo Competition 1 to normalize the numerical features. We count the frequency of categorical features and treat all the features with a frequency less than 8 as unknown features. We randomly split the datasets into two parts: 90% is used for training and the rest is left for testing. 2. Avazu Dataset: We use the 10 days of click-through log on users' mobile behaviors and randomly split 80% of the samples for training and leave the rest for testing. We treat the features with frequency less than 5 as unknown and replace them by a field-wise default feature. A description of the two datasets is shown in <ref type="table">Table.</ref>3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation metrics.</head><p>To evaluate the prediction performance on Criteo dataset and Avazu dataset, we use LogLoss and AUC where LogLoss is the cross-entropy loss to evaluate the performance of a classification model and AUC is the area under the ROC curve to measure the probability that a random positive sample is ranked higher than a random negative sample.  <ref type="bibr" target="#b34">[34]</ref>, AutoInt <ref type="bibr" target="#b33">[33]</ref>, DeepFM <ref type="bibr" target="#b10">[10]</ref>, NFM <ref type="bibr" target="#b13">[13]</ref> and xDeepFM <ref type="bibr" target="#b22">[22]</ref>, we choose the last three because they have similar architectures to DeepFwFM and they are also the state-of-the-art models for CTR prediction 2 . As a result, the 6 baseline models to evaluate DeepFwFM are LR (Logistic regression), FM <ref type="bibr" target="#b30">[30]</ref>, FwFM <ref type="bibr" target="#b28">[28]</ref>, DeepFM <ref type="bibr" target="#b10">[10]</ref>, NFM <ref type="bibr" target="#b13">[13]</ref>, xDeepFM <ref type="bibr" target="#b22">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Implementation details.</head><p>We train our model using PyTorch.</p><p>To make a fair comparison on Criteo dataset, we follow the parameter settings in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b22">22]</ref> and set the learning rate to 0.001. The embedding size is set to 10. The default settings for the DNN components of DeepFM, NFM, and xDeepFM are: (1) the network architecture is 400 × 400 × 400; (2) the dropout rate is 0.5. Specifically for xDeepFM, the cross layer in the CIN architecture is 100 × 100 × 50. We finetuned the 2 penalty and set it to 3e-7. We use the Adam optimizer <ref type="bibr" target="#b17">[17]</ref> for all of the experiments and the minibatch is chosen as 2048.</p><p>Regarding Avazu dataset, we keep the same settings except that the embedding size is 20, the 2 penalty is 6e-7, and the DNN network structure is 300 × 300 × 300. Regarding the training time (not inference time) in practice, all the models don't differ each other too much. FwFM and DeepFwFM are slightly faster than DeepFM and xDeepFM due to the innovations in the linear terms, owing to the innovations in the inner products of FwFM and DeepFwFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DeepFwFM v.s. other dense models</head><p>The evaluations of dense models without pruning show the maximum potential that the over-parameterized models perform. From <ref type="table" target="#tab_2">Table 2</ref>, we observe that LR underperforms the other methods by at least 0.7% on Criteo dataset and 1.7% on Avazu dataset in terms of AUC, which shows that feature interactions are critical to improving the CTR prediction. Most of the embedding-based neural networks outperform the low-order methods such as LR and FM, implying the importance of modeling high-order feature interactions. However, the low-order FwFM still wins over NFM and DeepFM, showing the strength of field matrix to learn second-order feature interactions to adapt to the local geometry.</p><p>NFM utilizes a black-box DNN to implicitly learn the low-order and high-order feature interactions, which may potentially over-fit the datasets due to the lack of mechanism to identify the loworder feature interactions explicitly. Among all the embeddingbased neural network models, xDeepFM and DeepFwFM achieves the best result on Criteo dataset and Avazu dataset and outperform the other models by roughly 0.7% on Criteo dataset and 0.4% on Avazu dataset in terms of AUC. However, the inference time of xDeepFM is almost ten times longer than DeepFwFM on Criteo dataset, showing the inefficiency in real-time predictions for large-scale ad serving systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The sparse model: DeepLight</head><p>Following Alg. 1 with damping ratios D = 0.99 and Ω = 100, we first train the network by 2 epochs for warm-ups, and then run 8 epochs for the pruning experiments. We prune the network every 10 iterations to reduce the computational cost. The biases of DNN, the field matrix and the parameters in the embedding layer is treated as usual. We try different pruning rates to study the prediction performance and deep model accelerations.</p><p>To show the superiority of network pruning on a large network over training from a smaller one, we compare the networks with different sparse rates to the networks of smaller structures.</p><p>As shown in <ref type="table">Table.</ref>4 (top), we see the DeepFwFMs with sparse DNN components outperforms the dense DeepFwFMs even when the sparse rate is as high as 95% on Criteo dataset. This phenomenon remains the same until we increase the sparsity to 99% on Criteo dataset. By contrast, the corresponding small networks with similar number of parameters such as N-25 3 and N-15 obtain much worse results than the original N-400, showing the power of pruning an over-parametrized network over training from a smaller one. On Avazu dataset, we obtain the same conclusions. The sparse model obtains the best prediction performance with 90% sparsity and only goes worse when the sparsity is larger than 99.5%.</p><p>Dense 80% 90% 95% 98% 99% Different sparse rates on the DNN component  Regarding the deep model acceleration, we see from <ref type="figure" target="#fig_6">Fig.4</ref> that a larger sparsity always brings a lower latency and when the sparsity is 98% on Criteo dataset and 99% on Avazu dataset, we realize the performance is still surprisingly better than the original dense network and we achieve as large as 16X speed-ups on both datasets. <ref type="bibr" target="#b2">3</ref> A model with 25 nodes in each DNN layer is referred to as N-25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.2</head><p>Pruning of the field matrix for accelerations. After applying a high sparsity on the DNN component, we already obtain significant speed-ups which is close to 20X. To further boost the accelerations, increasing the sparsity on the DNN component may risk in decreasing the performance and doesn't yield obvious accelerations due to the overhead in matrix computations. Recall from <ref type="figure" target="#fig_2">Fig.2</ref> that the field matrix possesses an approximately sparse structure. This motivates us to further prune the field matrix to obtain deep accelerations. Given a 99% sparsity for the DNN component on the Criteo dataset (98% sparsity on the Avazu dataset), we study the sparse model performance based on different sparse rates in the field matrix . From <ref type="figure" target="#fig_8">Fig.5</ref>, we observe that we can adopt up-to 95% sparsity on the field matrix without sacrificing the performance. Additionally, the predictions can be further accelerated by two to three times. As a result, we can eventually obtain 46X and 27X speed-ups without sacrificing the performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Pruning of embedding vectors for memory savings.</head><p>As to the pruning of embeddings <ref type="bibr" target="#b3">4</ref> , we find that setting a global threshold for the embeddings of all fields obtains a slightly better performance than setting individual thresholds for the embedding vector from each field. Therefore, we conduct the experiments based on a global threshold.  As shown in <ref type="figure" target="#fig_10">Fig.6</ref>, Criteo can adopt a high sparse rate, such 80%, to remain the same performance on Criteo dataset; by contrast, the model is sensitive on Avazu dataset and starts to decrease the performance when a 60% sparsity is applied. From <ref type="table">Table.</ref> 4 (bottom),  we see most of the models outperforms the baseline models (referred to as E-X) with a smaller embedding size, which sheds light on the use of large embedding sizes and pruning techniques to overparameterize the network to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.4</head><p>Structural pruning of DeepFwFM. From the above experiments, we see that the DNN component and the field matrix accept a much higher sparse rate to remain the same prediction performance, which inspires us to apply different pruning rates on the hybrid components.</p><p>As shown in <ref type="table">Table.</ref>5, for the performance-driven tasks, we can improve the state-of-the-art AUC from 0.8116 to 0.8223 on Criteo dataset via a sparse DeepFwFM where 90% of the parameters in both the DNN component and the field matrix and 40% of the parameters in the embedding vectors are pruned, and such model is denoted by D-90% &amp; R-90% &amp; F-40%. On Avazu dataset, a sparse DeepFwFM with structure D-90% &amp; R-90% &amp; F-20% further improves the state-of-the-art AUC from 0.7893 to 0.7897. For the memorydriven tasks, the memory savings are up to 10X and 2.5X on Criteo dataset and Avazu dataset, respectively. For the latency-driven tasks, we achieve 46X speed-ups on Criteo dataset using a DeepLight with the structure D-99% &amp; R-95% &amp; F-40% and 27X speed-ups on Avazu dataset using the structure D-98% &amp; R-90% &amp; F-0% without loss of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">DeepLight v.s. other sparse models</head><p>For the other models, we also try the corresponding best structure for accelerating the predictions without sacrificing the performance. With respect to the CIN component in xDeepFM, we denote the 99% sparsity on the CIN component by C-99%. We report the results in <ref type="table">Table.</ref>6 and observe that all the embedding based neural networks adopt high sparse rates to maintain the performance. Moreover, DeepLight is comparable to sparse DeepFM and sparse NFM in terms of prediction time but improves the AUC by at least 0.8% on Criteo dataset and 0.4% on Avazu dataset. DeepLight obtains a similar prediction performance as xDeepFM but is almost 10X faster. This shows the superiority of DeepLight over sparse DeepFM, sparse NFM, sparse xDeepFM in large-scale online ad serving systems for both fast and accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we propose the lightweight DeepLight model for efficient CTR predictions. A key advantage of this model is that <ref type="table">Table 6</ref>: Evaluation of sparse models on Criteo and Avazu datasets. For each individual model, we only report the most efficient structure that yields the best accelerations with almost no sacrifice on the prediction performance. the field matrix not only provides a robust matrix decomposition to improve the performance with little costs, but also possesses a highly-sparse structure with acceleration potentials after pruning.</p><p>To the best of our knowledge, this is the first work of network pruning applied to the area of CTR prediction in online advertising to solve the high-latency issues. We observe that network pruning is not only powerful to prune redundant parameters to alleviate overfitting but also achieves significant acceleration on the inference time and shows a pronounced reduction on the memory usage with little impact on the prediction performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) DeepFM (b) DeepFwFM (the proposed unpruned model) A model architecture comparison between DeepFM and proposed DeepFwFM. The inner products in the linear part of DeepFwFM are simplified. DNN component is generally built via the standard fully connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>By contrast, the FM component and FwFM component is much faster (a) Criteo: DNN component (b) Criteo: Field matrix (c) Criteo: Embedding vectors (d) Avazu: DNN component (e) Avazu: Field matrix (f) Avazu: Embedding vectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Weight demonstration of DeepFwFM model on Criteo and Avazu datasets. In particular for the DNN component and embedding vectors, we only choose a representative part to present due to their complexity. Moreover, we apply the magnitude-based max pooling operation to large matrices for illustration purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>DeepLight: A sparse architecture of DeepFwFM. The inner products in the linear part are simplified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 3 . 1</head><label>31</label><figDesc>DNN pruning for accelerations. When we prune the DNN component, only the weights of the DNN component are pruned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>DNN pruning for accelerations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Pruning of the field matrix for accelerations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Pruning of embedding vectors for memory savings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Computational complexity for three standard deep models. For example, a popular choice in Criteo dataset is to set = 3, ℎ = 400, = 39, = 10, ℎ = 100. As a result, the computations for DeepFM, xDeepFM and DeepFwFM are of order 0.64M, 12M and 0.65M, respectively.</figDesc><table><row><cell>Data</cell><cell>shallow component DNN component</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1</head><label>1</label><figDesc>Structural pruning for a target model, the target sparse rate % = 99% means 99% of the parameters are pruned.</figDesc><table><row><cell cols="2">Enumerate the candidate component in a model</cell></row><row><cell>Update the current sparse rate</cell><cell>← (1 − D /℧ ).</cell></row><row><cell cols="2">Prune the bottom-% lowest magnitude weights.</cell></row></table><note>1: Input Set the target sparse rate , damping ratios D and ℧.2: Warm up Initialize a neural network by training epochs.3: Pruning For = 1, 2, ... do Train the network for one iteration.4: Online Prediction Transform the sparse model to efficient structure.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model comparison on the Criteo and Avazu datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Criteo</cell><cell></cell><cell></cell><cell></cell><cell>Avazu</cell><cell></cell></row><row><cell>Models</cell><cell cols="2">Test LogLoss AUC</cell><cell cols="2"># Parameters Latency ( )</cell><cell cols="2">Test LogLoss AUC</cell><cell cols="2"># Parameters Latency ( )</cell></row><row><cell>LR</cell><cell>0.4615</cell><cell>0.7881</cell><cell>1,326,056</cell><cell>0.001</cell><cell>0.3904</cell><cell>0.7617</cell><cell>1,544,393</cell><cell>0.001</cell></row><row><cell>FM</cell><cell>0.4565</cell><cell>0.7949</cell><cell>14,586,606</cell><cell>0.005</cell><cell>0.3816</cell><cell>0.7782</cell><cell>32,432,233</cell><cell>0.009</cell></row><row><cell>FwFM</cell><cell>0.4466</cell><cell>0.8049</cell><cell>13,261,682</cell><cell>0.145</cell><cell>0.3764</cell><cell>0.7866</cell><cell>30,888,853</cell><cell>0.105</cell></row><row><cell>DeepFM</cell><cell>0.4495</cell><cell>0.8036</cell><cell>15,064,206</cell><cell>4.181</cell><cell>0.3780</cell><cell>0.7852</cell><cell>32,751,433</cell><cell>2.719</cell></row><row><cell>NFM</cell><cell>0.4497</cell><cell>0.8030</cell><cell>15,204,206</cell><cell>4.091</cell><cell>0.3777</cell><cell>0.7854</cell><cell>32,689,033</cell><cell>2.704</cell></row><row><cell>xDeepFM</cell><cell>0.4420</cell><cell>0.8102</cell><cell>15,508,958</cell><cell>40.85</cell><cell>0.3749</cell><cell>0.7894</cell><cell>32,927,058</cell><cell>7.129</cell></row><row><cell>DeepFwFM</cell><cell>0.4403</cell><cell>0.8116</cell><cell>13,739,321</cell><cell>4.271</cell><cell>0.3751</cell><cell>0.7893</cell><cell>31,208,053</cell><cell>2.824</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Statistics of datasets</figDesc><table><row><cell>Data</cell><cell cols="4">Training set # Fields # Numerical # Features</cell></row><row><cell>Criteo</cell><cell>41.3M</cell><cell>39</cell><cell>13</cell><cell>1.33M</cell></row><row><cell>Avazu</cell><cell>32.3M</cell><cell>23</cell><cell>0</cell><cell>1.54M</cell></row><row><cell cols="2">5 EXPERIMENTS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">5.1 Experimental setup</cell><cell></cell><cell></cell></row></table><note>from O ( ℎ + ℎ 2 ) to O ( ℎ + ℎ 2 )(1 − dnn %) by storing the sparse weight matrix through the CRS. Similarly, a R % sparsity on the field matrix reduces the parameters proportionally. Since the parameters in the embedding vectors dominate the total parameters in DeepFwFM, a emb % sparsity on the embedding vectors leads to the total memory reduction by roughly 1/(1 − emb %) times.5.1.1 Data sets. 1. Criteo Dataset: It is a well-known benchmark dataset for CTR prediction</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>DeepFwFMs with sparse DNN components/ embedding vectorss v.s. DeepFwFMs with smaller DNN components/ embedding vectors. The DeepFwFM model with X nodes in each DNN layer is referred to as N-X; The DeepFwFM model with embedding size X is referred to as E-X. The baselines are chosen to have a close number of parameters of the sparse network.</figDesc><table><row><cell>Criteo</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Structural pruning of DeepFwFM on Criteo dataset. D-90% &amp; R-90% &amp; F-40% is short for the sparse DeepFwFM which has 90% sparse rate on the DNN component and the field matrix and a 40% sparse rate on the embedding vectors.</figDesc><table><row><cell cols="2">Dataset Goal</cell><cell>Structural Pruning</cell><cell>Test Logloss</cell><cell>AUC</cell><cell cols="2"># Parameters Latency ( )</cell></row><row><cell></cell><cell>None</cell><cell>No Pruning</cell><cell>0.4403</cell><cell>0.8116</cell><cell>13,739,321</cell><cell>4.271</cell></row><row><cell>Criteo</cell><cell cols="4">High performance D-90% &amp; R-90% &amp; F-40% 0.4395 0.8123 Low memory D-90% &amp; R-90% &amp; F-90% 0.4404 0.8114</cell><cell>8,012,094 1,376,431</cell><cell>0.469 0.472</cell></row><row><cell></cell><cell>Low latency</cell><cell>D-99% &amp; R-95% &amp; F-40%</cell><cell>0.4405</cell><cell>0.8114</cell><cell>7,413,578</cell><cell>0.093</cell></row><row><cell></cell><cell>None</cell><cell>No Pruning</cell><cell>0.3751</cell><cell>0.7893</cell><cell>31,208,053</cell><cell>2.824</cell></row><row><cell>Avazu</cell><cell cols="4">High performance D-90% &amp; R-90% &amp; F-20% 0.3748 0.7897 Low memory D-90% &amp; R-90% &amp; F-60% 0.3753 0.7892</cell><cell>24,808,262 9,322,791</cell><cell>0.422 0.318</cell></row><row><cell></cell><cell>Low latency</cell><cell>D-98% &amp; R-90% &amp; F-0%</cell><cell>0.3753</cell><cell>0.7894</cell><cell>30,859,675</cell><cell>0.104</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We didn't compare DeepCross because it is a special case of xDeepFM.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To the best of our knowledge, this is the first structural pruning applied in embedding layers for memory savings and robust predictions by eliminating noisy estimates.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT Lin acknowledges the support from NSF (DMS-1555072, DMS-1736364), BNL Subcontract 382247, W911NF-15-1-0562, and DE-SC0021142.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-Order Factorization Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Interactive Advertising Bureau. 2020. IAB internet advertising revenue report. In Iab Pwc</title>
		<imprint>
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FLEN: Leveraging Field for Scalable CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlong</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20 DLP workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakaria</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemal</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<idno type="arXiv">arXiv:1606.07792</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Submodular meets Spectral: Greedy Algorithms for Subset Selection, Sparse Approximation and Dictionary Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kempe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Adaptive Empirical Bayesian Method for Sparse Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR&apos;19</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Web-scale Bayesian Click-through Rate Prediction for Sponsored Search Advertising in Microsoft&apos;s Bing Search Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><forename type="middle">Quiñonero</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical Lessons from Predicting Clicks on Ads at Facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADKDD&apos;14</title>
		<meeting><address><addrLine>Stuart Bowers, and Joaquin Quinonero Candela</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and RL Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Field-aware Factorization Machines for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In RecSys&apos;16</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Display Advertising Challenge</title>
		<editor>Kaggle</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Criteo Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deguang</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pruning Filters for Efficient ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. KDD&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AutoFIS: Automatic Feature Interaction Selection in Factorization Models for Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jincai</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AutoCross: Automatic Feature Crossing for Tabular Data in Real-World Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed Matrix Completion and Robust Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lester</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR&apos;</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="913" to="960" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharat</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Ad Click Prediction: a View from the Trenches</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Predicting different types of conversions with multi-task learning in online advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfonso</forename><forename type="middle">Lobos</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Flores</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfonso</forename><forename type="middle">Lobos</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Product-based Neural Networks for User Response Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Factorization Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep Crossing: Web-Scale Modeling Without Manually Crafted Combinatorial Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Data-driven Text Features for Sponsored Search Click Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benyah</forename><surname>Shaparenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozgur</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rukmini</forename><surname>Iyer</surname></persName>
		</author>
		<idno>ADKDD&apos;09</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. CIKM&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05123</idno>
		<title level="m">Deep &amp; Cross Network for Ad Click Predictions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Psychological Advertising: Exploring User Psychology for Click Prediction in Sponsored Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Structured Sparsity in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Greedy Optimization Provably Wins the Lottery: Logarithmic Number of Winning Tickets is Enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS&apos;20</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Interest Network for Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
