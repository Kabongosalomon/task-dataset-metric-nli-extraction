<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Scale Approximation for Object Detection in CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Laboratory at The</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Laboratory at The</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
							<email>weifangyin@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Laboratory at The</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Laboratory at The</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Scale Approximation for Object Detection in CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since convolutional neural network (CNN) lacks an inherent mechanism to handle large scale variations, we always need to compute feature maps multiple times for multiscale object detection, which has the bottleneck of computational cost in practice. To address this, we devise a recurrent scale approximation (RSA) to compute feature map once only, and only through this map can we approximate the rest maps on other levels. At the core of RSA is the recursive rolling out mechanism: given an initial map at a particular scale, it generates the prediction at a smaller scale that is half the size of input. To further increase efficiency and accuracy, we (a): design a scale-forecast network to globally predict potential scales in the image since there is no need to compute maps on all levels of the pyramid. (b): propose a landmark retracing network (LRN) to trace back locations of the regressed landmarks and generate a confidence score for each landmark; LRN can effectively alleviate false positives caused by the accumulated error in RSA. The whole system can be trained end-to-end in a unified CNN framework. Experiments demonstrate that our proposed algorithm is superior against state-of-the-art methods on face detection benchmarks and achieves comparable results for generic proposal generation. The source code of our system is available. 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is one of the most important tasks in computer vision. The convolutional neural network (CNN) based approaches have been widely applied in object detection and recognition with promising performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. To localize objects at arbitrary scales and locations in an image, we need to han- <ref type="bibr" target="#b0">1</ref> Our codes and annotations mentioned in Sec.4.1 can be accessed at github.com/sciencefans/RSA-for-object-detection large small middle I 1 I 1  dle the variations caused by appearance, location and scale. Most of the appearance variations can now be handled in CNN, benefiting from the invariance property of convolution and pooling operations. The location variations can be naturally solved via sliding windows, which can be efficiently incorporated into CNN in a fully convolutional manner. However, CNN itself does not have an inherent mechanism to handle the scale variations.</p><formula xml:id="formula_0">I 1/2 I 1/2 N small middle large F 1/2 F 1/2 N F 1 F 1 F 1/2 F 1/2 N large I 1 small middle</formula><p>The scale problem is often addressed via two ways, namely, multi-shot by single-scale detector and single-shot by multi-scale detector. The first way, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, handles objects of different scales independently by resizing the input into different scales and then forwarding the resized images multiple times for detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref>. Models in such a philosophy probably have the highest recall as long as the sampling of scales is dense enough, but they suffer from high computation cost and more false positives. The second way, as depicted in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, forwards the image only once and then directly regresses objects at multiple scales <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Such a scheme takes the scale variation as a black box. Although more parameters and complex structures would improve the performance, the spirit of direct regression still has limitations in real-time applications, for example in face detection, the size of faces can vary from 20 × 20 to 1920 × 1080.</p><p>To handle the scale variation in a CNN-based detection system in terms of both efficiency and accuracy, we are inspired by the fast feature pyramid work proposed by Dollár et al. <ref type="bibr" target="#b6">[7]</ref>, where a detection system using hand-crafted features is designed for pedestrian detection. It is found that image gradients across scales can be predicted based on natural image statistics. They showed that dense feature pyramids can be efficiently constructed on top of coarsely sampled feature pyramids. In this paper, we extend the spirit of fast feature pyramid to CNN and go a few steps further. Our solution to the feature pyramid in CNN descends from the observations of modern CNN-based detectors, including Faster-RCNN <ref type="bibr" target="#b26">[27]</ref>, R-FCN <ref type="bibr" target="#b3">[4]</ref>, SSD <ref type="bibr" target="#b20">[21]</ref>, YOLO <ref type="bibr" target="#b25">[26]</ref> and STN <ref type="bibr" target="#b1">[2]</ref>, where feature maps are first computed and the detection results are decoded from the maps afterwards. However, the computation cost of generating feature maps becomes a bottleneck for methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref> using multi-scale testing and it seems not to be a neat solution to the scale variation problem.</p><p>To this end, our philosophy of designing an elegant detection system is that we calculate the feature pyramid once only, and only through that pyramid can we approximate the rest feature pyramids at other scales. The intuition is illustrated in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>. In this work, we propose a recurrent scale approximation (RSA, see <ref type="figure" target="#fig_2">Fig. 3</ref>) unit to achieve the goal aforementioned. The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale. The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input. Such a scheme could feed the network with input at one scale only and approximate the rest features at smaller scales through a learnable RSA unit -a balance considering both efficiency and accuracy.</p><p>We propose two more schemes to further save the computational budget and improve the detection performance under the RSA framework. The first is a scale-forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction. There are only a few scales of objects appearing in the image and hence most of the feature pyramids correspond to the background, indicating a redundancy if maps on all levels are computed. The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set. The final score of identifying a face within an anchor is thereby revised by the LRN network. Such a design alleviates false positives caused by the accumulated error in the RSA unit.</p><p>The pipeline of our proposed algorithm is shown in <ref type="figure">Fig. 2</ref>. The three components can be incorporated into a unified CNN framework and trained end-to-end. Experiments show that our approach is superior to other state-ofthe-art methods in face detection and achieves reasonable results for object detection.</p><p>To sum up, our contributions in this work are as follows: 1) We prove that deep CNN features for an image can be approximated from different scales using a portable recurrent unit (RSA), which fully leverages efficiency and accuracy. 2) We propose a scale-forecast network to predict valid scales of the input, which further accelerates the detection pipeline. 3) We devise a landmark retracing network to enhance the accuracy in face detection by utilizing landmark information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Multi-shot by single-scale detector. A single-scale detector detects the target at a typical scale and cannot handle features at other scales. An image pyramid is thus formulated and each level in the pyramid is fed into the detector. Such a framework appeared in pre-deep-learning era <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> and usually involves hand-crafted features, e.g., HOG <ref type="bibr" target="#b4">[5]</ref> or SIFT <ref type="bibr" target="#b23">[24]</ref>, and some classifier like Adaboost <ref type="bibr" target="#b29">[30]</ref>, to verify whether the context at each scale contains a target object. Recently, some CNN-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> also employ such a spirit to predict the objectness and class within a sliding window at each scale. In this way, the detector only handles features in a certain range of scales and the variance is taken over by the image pyramid, which could reduce the fitting difficulty for detector but potentially increase the computational cost.</p><p>Single-shot by multi-scale detector. A multi-scale detector takes one shot for the image and generates detection results aross all scales. RPN <ref type="bibr" target="#b26">[27]</ref> and YOLO <ref type="bibr" target="#b25">[26]</ref> have fixed size of the input scale, and proposals for all scales are generated in the final layer by using multiple classifiers. However, it is not easy to detect objects in various scales based on the final feature map. Liu et al. <ref type="bibr" target="#b20">[21]</ref> resolved the problem via a multi-level combination of predictions from feature maps on different scales. And yet it still needs a large model for large receptive field for detection. Other works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref> proposed to merge deep and shallow features in a conv/deconv structure and to merge boxes for objects from different scales. These methods are usually faster than the single-scale detector since it only takes one shot for image, but the large-scale invariance has to be learned by an expensive feature classifier, which is unstable and heavy.</p><p>Face detection. Recent years have witnessed a performance boost in face detection, which takes advantage of the <ref type="figure">Figure 2</ref>. Pipeline of our proposed algorithm. (a) Given an image, we predict potential scales from the scale-forecast network and group the results in six main bins (m = 0, · · · , 5). (b) RSA unit. The input is resized based on the smallest scale (corresponding to the largest feature map) and the feature maps at other scales are predicted directly from the unit. (c) Given predicted maps, LRN performs landmark detection in an RPN manner. The landmarks can trace back locations via regression to generate individual confidence regarding the existence of the landmark. (d) Due to the retracing mechanism, the final score of detecting a face is revised by the confidence of landmarks, which can effectively dispose of false positives. development in fully convolutional network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>. Multi-task RPN is applied <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref> to generate face confidence and landmarks together. Both single-scale and multi-scale strategies are introduced in these methods. For example, Chen et. al <ref type="bibr" target="#b1">[2]</ref> propose a supervised spatial transform layer to utilize landmark information and thus enhance the quality of detector by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Algorithm</head><p>In this section, we depict each component of our pipeline <ref type="figure">(Fig. 2</ref>) in detail. We first devise a scale-forecast network to predict potential scales of the input; the RSA unit is proposed to learn and predict features at smaller scales based on the output of the scale-forecast network; the image is fed into the landmark retracing network to detect faces of various sizes, using the scale prediction in Section 3.1 and approximation in Section 3.2. The landmark retracing network stated in Section 3.3 can trace back features of regressed landmarks and generate individual confidence of each landmark to revise the final score of detecting a face. At last, we discuss the superiority of our algorithm's design over other alternatives in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scale-forecast Network</head><p>We propose a scale-forecast network (see <ref type="figure">Fig. 2</ref>(a)) to predict the possible scales of faces given an input image of fixed size. The network is a half-channel version of ResNet-18 with a global pooling at the end. The output of this network is a probability vector of B dimensions, where B = 60 is the predefined number of scales. Let B = {0, 1, · · · , B} denote the scale set, we define the mapping from a face size x, in the context of an image being resized to a higher dimension 2048, to the index b in B as:</p><formula xml:id="formula_1">b = 10(log 2 x − 5).<label>(1)</label></formula><p>For example, if the face has size of 64, its corresponding bin index b = 10 2 . Prior to being fed into the network, an image is first resized with the higher dimension equal to 224. During training, the loss of our scale-forecast network is a binary multi-class cross entropy loss:</p><formula xml:id="formula_2">L SF = − 1 B b p b logp b + (1 − p b ) log(1 −p b ),<label>(2)</label></formula><p>where p b ,p b are the ground truth label and prediction of the b-th scale, respectively. Note that the ground truth label for the neighbouring scales b i of an occurring scale b * (p b * = 1) is not zero and is defined as the Gaussian sampling score:</p><formula xml:id="formula_3">p bi = Gaussian(b i , µ, σ), b i ∈ N(b * )<label>(3)</label></formula><p>where µ, σ are hyperparameters in the Gaussian distribution and N(·) denotes the neighbour set. Here we use ±2 as the neighbour size and set µ, σ to b * , 1/ √ 2π, respectively. Such a practice could alleviate the difficulty of feature learning in the discrete distribution between occurring scales (1) and non-occurring scales (0).</p><p>For inference, we use the Gaussian mixture model to determine the local maximum and hence the potential occurring scales. Given observations x, the distribution, parameterized by θ, can be decomposed into K mixture components:</p><formula xml:id="formula_4">p(θ|x) = K i=1 φ i N (µ i , Σ i ),<label>(4)</label></formula><p>where the i-th component is characterized by Gaussian distributions with weights φ i , means µ i and covariance matrices Σ i . Here K = {1, ..., 6} denotes selected scale numbers of six main scales from 2 5 to 2 11 and the scale selection is determined by the threshold φ i of each component.</p><p>Finally the best fitting model with a specific K is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recurrent Scale Approximation (RSA) Unit</head><p>The recurrent scale approximation (RSA) unit is devised to predict feature maps at smaller scales given a map at the largest scale. <ref type="figure">Fig. 2</ref> depicts the RSA unit. The network architecture follows a build-up similar to the residual network <ref type="bibr" target="#b12">[13]</ref>, where we reduce the number of channels in each convolutional layer to half of the original version for time efficiency. The structure details are shown in Section 4.1. Given an input image I, I m denotes the downsampled result of the image with a ratio of 1/2 m , where m ∈ {0, · · · , M } is the downsample level and M = 5. Note that I 0 is the original image. Therefore, there are six scales in total, corresponding to the six main scale ranges defined in the scale-forecast network (see Section 3.1). Given an input image I m , we define the output feature map of layer res2b as:</p><formula xml:id="formula_5">f (I m ) = G m ,<label>(5)</label></formula><p>where f (·) stands for a set of convolutions with a total stride of 8 from the input image to the output map. The set of feature maps G m at different scales serves as the ground truth supervision of the recurrent unit. The RSA module RSA(·) takes as input the feature map of the largest scale G 0 at first, and repeatedly outputs a map with half the size of the input map:</p><formula xml:id="formula_6">h (0) = F 0 = G 0 , h (m) = RSA h (m−1) |w = F m .<label>(6)</label></formula><p>where F m is the resultant map after being rolled out m times and w represents the weights in the RSA unit. The RSA module has four convolutions with a total stride of 2 (1,2,1,1) and their kernal sizes are (1,3,3,1). The loss is therefore the l 2 norm between prediction F m and supervision G m across all scales:</p><formula xml:id="formula_7">L RSA = 1 2M M m=1 F m − G m 2 .<label>(7)</label></formula><p>The gradients in the RSA unit are computed as:</p><formula xml:id="formula_8">∂L RSA ∂w xy = m ∂L RSA ∂h (m) · ∂h (m) ∂w xy , = 1 M m F m − G m · F m−1 xy ,<label>(8)</label></formula><p>where x and y are spatial indeces in the feature map <ref type="bibr" target="#b2">3</ref> . The essence behind our RSA unit is to derive a mapping RSA(·) → f (·) to constantly predict smaller-scale features based on the current map instead of feeding the network with inputs of different scales for multiple times. In an informal mathematical expression, we have:</p><formula xml:id="formula_9">lim 0→m RSA h (m−1) = f (I m ) = G m ,</formula><p>to indicate the functionality of RSA: an approximation to f (·) from the input at the largest scale 0 to its desired level m. The computation cost of generating feature map F m using RSA is much lower than that of resizing the image and feeding into the network (i.e., f (I m ) through conv1 to res2b; see quantitative results in Section 4.4).</p><p>During inference, we first obtain the possible scales of the input from the scale-forecast network. The image is then resized accordingly so that the smallest scale (corresponding to the largest feature map) is resized to the range of <ref type="bibr">[64,</ref><ref type="bibr">128]</ref>. The feature maps at other scales are thereby predicted by the output of RSA unit via Eqn <ref type="bibr" target="#b5">(6)</ref>. <ref type="figure" target="#fig_2">Fig. 3</ref> depicts a rolled-out version of RSA to predict feature maps of smaller scales compared with the ground truth. We can observe from both the error rate and predicted feature maps in each level that RSA is capable of approximating the feature maps at smaller scales.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Landmark Retracing Network</head><p>In the task of face detection, as illustrated in <ref type="figure">Fig. 2</ref>, the landmark retracing network (LRN) is designed to adjust the confidence of identifying a face and to dispose of false positives by learning individual confidence of each regressed landmark. Instead of directly using the ground truth location of landmarks, we formulate such a feature learning of landmarks based on the regression output of landmarks in the final RPN layer.</p><p>Specifically, given the feature map F at a specific scale from RSA (m is dropped for brevity), we first feed it into the res3a layer. There are two branches at the output: one is the landmark feature set P to predict the individual score of each landmark in a spatial context. The number of channels in the set equals to the number of landmarks. Another branch is continuing the standard RPN <ref type="bibr" target="#b26">[27]</ref> pipeline (res3b-3c) which generates a set of anchors in the final RPN layer. Let p i = [p i0 , p i1 , · · · , p ik , · · · ] denote the classification probability in the final RPN layer, where k is the class index and i is the spatial location index on the map; t ij denotes the regression target (offset defined in <ref type="bibr" target="#b9">[10]</ref>) of the j-th landmark in the i-th anchor, where j = {1, · · · , 5} is the landmark index. Note that in face detection task, we only have one anchor so that p i contains one element. In the traditional detection-to-landmark formulation, the following loss, which consists of two heads (i.e., classification and regression), is optimized:</p><formula xml:id="formula_10">i − log p ik * + δ(k * )S(t i − t * i ),</formula><p>where δ(·) is the indicator function; k * denotes the correct label of anchor i and we have only two classes here (0 for background, 1 for positive); t * i is the ground truth regression target and S(·) is the smoothing l 1 loss defined in <ref type="bibr" target="#b9">[10]</ref>.</p><p>However, as illustrated in <ref type="figure">Fig. 2(c)</ref>, using the confidence of anchor p ik * alone results in false positives in some cases, which inspires us to take advantage of the landmark features based on the regression output. The revised classification output, p trace ik * (t ij ), now considers both the feature in the final RPN layer as well as those in the landmark feature set:</p><formula xml:id="formula_11">p trace ik * (t ij ) = p i0 , k * = 0, max pool(p i1 , p land ij ), k * = 1,<label>(9)</label></formula><p>where p land ij is the classification output of point j from the landmark feature set P and it is determined by the regression output:</p><formula xml:id="formula_12">p land ij = P r(t ij ) ,<label>(10)</label></formula><p>where r(·) stands for a mapping from the regression target to the spatial location on map P. To this end, we have the revised loss for our landmark retracing network:</p><formula xml:id="formula_13">L LRN = i − log p trace ik * (t ij ) + δ(k * ) j S(t ij − t * ij ) .</formula><p>(11) Apart from the detection-to-landmark design as previous work did, our retracing network also fully leverages the feature set of landmarks to help rectify the confidence of identifying a face. This is achieved by utilizing the regression output t ij to find the individual score of each landmark on the preceding feature map P. Such a scheme is in a landmark-to-detection spirit.</p><p>Note that the landmark retracing network is trained endto-end with the RSA unit stated previously. The anchor associated with each location i is a square box of fixed size 64 √ 2. The landmark retracing operation is performed only when the anchor is a positive sample. The base landmark location with respect to the anchor is determined by the average location of all faces in the training set. During test, LRN is fed with feature maps at various scales and it treats each scale individually. The final detection result is generated after performing NMS among results from multi-scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Comparison to RPN. The region proposal network <ref type="bibr" target="#b26">[27]</ref> takes a set of predefined anchors of different sizes as input and conducts a similar detection pipeline. Anchors in RPN vary in size to meet the multi-scale training constraint. During one iteration of update, it has to feed the whole image of different sizes (scales) from the start to the very end of the network. In our framework, we resize the image once to make sure at least one face falls into the size of [64, 128], thus enforcing the network to be trained within a certain range of scales. In this way, we are able to use only one anchor of fixed size. The multi-scale spirit is embedded in an RSA unit, which directly predicts the feature maps at smaller scales. Such a scheme saves parameters significantly and could be considered as a 'semi' multi-scale training and 'fully' multi-scale test.</p><p>Prediction-supervised or GT-supervised in landmark feature sets. Another comment on our framework is the supervision knowledge used in training the landmark features P. The features are learned using the prediction output of regression targets t ij instead of the ground truth targets t * ij . In our preliminary experiments, we find that if p land i ∼ t * i , the activation in the landmark features would be heavily suppressed due to the misleading regression output by t ij ; however, if we relax the learning restriction and accept activations within a certain range of misleading locations, i.e., p land i ∼ t i , the performance can be boosted further. Using the prediction of regression as supervision in the landmark feature learning makes sense since: (a) we care about the activation (classification probability) rather than the accurate location of each landmark; (b) t i and p land i share similar learning workflow and thus the location of t i could better match the activation p land i in P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we first conduct the ablation study to verify the effectiveness of each component in our method and compare exhaustively with the baseline RPN <ref type="bibr" target="#b26">[27]</ref>; then we compare our algorithm with state-of-the-art methods in face detection and object detection on four popular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup and Implementation Details</head><p>Annotated Faces in the Wild (AFW) <ref type="bibr" target="#b36">[37]</ref> contains 205 images for evaluating face detectors' performance. However, some faces are missing in the annotations and could trigger the issue of false positives, we relabel those missing faces and report the performance difference in both cases. Face Detection Data Set and Benchmark (FDDB) <ref type="bibr" target="#b13">[14]</ref> has 5,171 annotated faces in 2,845 images. It is larger and more challenging than AFW. Multi-Attribute Labelled Faces (MALF) <ref type="bibr" target="#b31">[32]</ref> includes 5,250 images with 11,931 annotated faces collected from the Internet. The annotation set is cleaner than that of AFW and it is the largest benchmark for face detection.</p><p>Our training set has 184K images, including 171K images collected from the Internet and 12.9K images from the training split of Wider Face Dataset <ref type="bibr" target="#b32">[33]</ref>. All faces are labelled with bounding boxes and five landmarks. The structure of our model is a shallow version of the ResNet <ref type="bibr" target="#b12">[13]</ref> where the first seven ResNet blocks are used, i.e., from conv1 to res3c. We use this model in scale-forecast network and LRN. All numbers of channels are set to half of the original ResNet model, for the consideration of time efficiency. We first train the scale-forecast network and then use the output of predicted scales to launch the RSA unit and LRN. Note that the whole system (RSA+LRN) is trained end-to-end and the model is trained from scratch without resorting to a pretrained model since the number of channels is halved. The ratio of the positive and the negative is 1 : 1 in all experiments. The batch size is 4; base learning rate is set to 0.001 with a decrease of 6% every 10,000 iterations. The maximum training iteration is 1,000,000. We use stochastic gradient descent as the optimizer. The scale-forecast network is of vital importance to the computational cost and accuracy in the networks afterwards. <ref type="figure" target="#fig_3">Fig. 4</ref> reports the overall recall with different numbers of predicted scales on three benchmarks. Since the number of faces and the number of potential scales in the image vary across datasets, we use the number of predicted scales per face (x, total predicted scales over total number of faces) and a global recall (y, correct predicted scales over all ground truth scales) as the evaluation metric. We can observe from the results that our trained scale network recalls almost 99% at x = 1, indicating that on average we only need to generate less than two predictions per image and that we can retrieve all face scales. Based on this prior  <ref type="figure">Figure 5</ref>. Investigation on the source layer to branch out the RSA unit. For each case, we report the error rate v.s. the level of down-sampling ratio in the unit. We can conclude that the deeper the RSA is branched out, the worse the feature approximation at smaller scales will be. knowledge, during inference, we set the threshold for predicting potential scales of the input so that it has approximately two predictions. <ref type="figure">Fig. 5</ref> investigates the effect of appending the RSA unit to different layers. For each case, the error rate between the ground truth and corresponding prediction is computed. We define the error rate (ER) on level m as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance of Scale-forecast Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablative Evaluation on RSA Unit</head><formula xml:id="formula_14">ER m = 1 N N i (F (m) − G (m) )./G (m) 2 ,<label>(12)</label></formula><p>where './' implies an element-wise division between maps; N is the total number of samples. We use a separate validation set to conduct this experiment. The image is first resized to higher dimension being 2048 and the RSA unit predicts six scales defined in Section 3.1 (1024, 512, 256, 128 and 64). Ground truth maps are generated accordingly as we iteratively resize the image (see <ref type="figure" target="#fig_2">Fig. 3</ref>). There are two remarks regarding the result: First, feature depth matters. Theoretically RSA can handle all scales of features in a deep CNN model and therefore can be branched out at any depth of the network. However, results from the figure indicate that as we plug RSA at deeper layers, its performance decades. Since features at deeper layers are more sparse and abstract, they barely contain information for RSA to approximate the features at smaller scales. For example, in case final feature which means RSA is plugged at the final convolution layer after res3c, the error rate is almost 100%, indicating RSA's incapability of handling the insufficient information in this layer. The error rate decreases in shallower cases.</p><p>However, the computation cost of RSA at shallow layers is much higher than that at deeper layers, since the stride is <ref type="table">Table 1</ref>. The proposed algorithm is more computationally efficient and accurate by design than baseline RPN. Theoretical operations of each component are provided, denoted as 'Opts. (VGA input)' below. The minimum operation in each component means only the scaleforecast network is used where no face appears in the image; and the maximum operation indicates the amount when faces appear at all scales. The actual runtime comparison between ours and baseline RPN is reported in smaller and the input map of RSA is thus larger. The path during one-time forward from image to the input map right before RSA is shorter; and the rolling out time increases accordingly. Therefore, the trade-off is that we want to plug RSA at shallow layers to ensure a low error rate and at the same time, to save the computational cost. In practice we choose case res2b to be the location where RSA is branched out. Most of the computation happens before layer res2b and it has an acceptable error rate of 3.44%. We use this setting throughout the following experiments. Second, butterfly effect exists. For a particular case, as the times of the recurrent operation increase, the error rate goes up due to the cumulative effect of rolling out the predictions. For example, in case res2b, the error rate is 3.44% at level m = 1 and drops to 5.9% after rolling out five times. Such an increase is within the tolerance of the system and still suffices the task of face detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Our Algorithm vs. Baseline RPN</head><p>We compare our model (denoted as RSA+LRN), a combination of the RSA unit and a landmark retracing network, with the region proposal network (RPN) <ref type="bibr" target="#b26">[27]</ref>. In the first setting, we use the original RPN with multiple anchors (denoted as RPN m) to detect faces of various scales. In the second setting, we modify the number of anchors to one (denoted as RPN s); the anchor can only detect faces in the range from 64 to 128 pixels. To capture all faces, it needs to take multiple shots in an image pyramid spirit. The network structurse of both baseline RPN and our LRN descend from ResNet-18 <ref type="bibr" target="#b12">[13]</ref>. Anchor sizes in the first setting RPN m are 32 √ 2, 64 √ 2, · · · , 1024 √ 2 and they are responsible for detecting faces in the range of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">64)</ref>, <ref type="bibr">[64,</ref><ref type="bibr">128)</ref>, · · · , [1024, 2048], respectively. In the second setting RPN s, we first resize the image length to 64, 256, · · · , 2048, then test each scale individually and merge all results through NMS <ref type="bibr" target="#b0">[1]</ref>. <ref type="table">Table 1</ref> shows the theoretical computation cost and test performance of our algorithm compared with baseline RPN. We can observe that RPN s needs six shots for the same image during inference and thus the computation cost is much larger than ours or RPN m; Moreover, RPN m performs worse than the rest two for two reasons: First, the receptive field is less than 500 and therefore it cannot see the context of faces larger than 500 pixels; second, it is hard for the network (its model capacity much less than the original ResNet <ref type="bibr" target="#b12">[13]</ref>) to learn the features of faces in a wide scale range from 32 to 2048. <ref type="table" target="#tab_1">Table 2</ref> depicts the runtime comparison during test. The third column LRN means without using the RSA unit. Our method runs fast enough compared with its counterparts for two reasons. First, there are often one or two valid scales in the image, and the scale-forecast network can automatically select some particular scales, and ignore all the other invalid ones in the multi-scale test stage; second, the input of LRN descends from the output of RSA to predict feature maps at smaller scales; it is not necessary to compute feature maps of multiple scales in a multi-shot manner as RPN m does. <ref type="figure" target="#fig_6">Fig. 7</ref> shows the comparison against other approaches on three benchmarks. On AFW, our algorithm achieves an AP of 99.17% using the original annotation and an AP of 99.96% using the revised annotation 7(c). On FDDB, RSA+LRN recalls 93.0% faces with 50 false positives 7(a). On MALF, our method recalls 82.4% faces with zero false positive 7(d). It should be noticed that the shape and scale definition of bounding box on each benchmark varies. For instance, the annotation on FDDB is ellipse while others are rectangle. To address this, we learn a transformer to fit each annotation from the landmarks. This strategy significantly enhances performance in the continuous setting on FDDB. <ref type="figure">Figure 6</ref>. Our proposed model can detect faces at various scales, including the green annotations provided in AFW <ref type="bibr" target="#b36">[37]</ref> as well as faces marked in red that are of small sizes and not labeled in the dataset.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Face Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">RSA on Generic Object Proposal</head><p>We now verify that the scale approximation learning by RSA unit also generalizes comparably well on the generic region proposal task. Region proposal detection is a basic stage for generic object detection task and is more difficult than face detection. ILSVRC DET <ref type="bibr" target="#b5">[6]</ref> is a challenging dataset for generic object detection. It contains more than 300K images for training and 20K images for validation. We use a subset (around 170k images) of the original training set for training, where each category has at most 1000 samples; for test we use the val2 split <ref type="bibr" target="#b10">[11]</ref> with 9917 images. We choose the single anchor RPN with ResNet-101 as the baseline. RSA unit is set after res3b3. The anchors are of size 128 √ 2 squared, 128×256 and 256×128. During training, we randomly select an object and resize the image so that the object is rescaled to <ref type="bibr">[128,</ref><ref type="bibr">256]</ref>. Scale-forecast network is also employed to predict the higher dimension of objects in the image.</p><p>Recalls with different number of proposals are shown in <ref type="table" target="#tab_4">Table 3</ref>. The original RPN setting has 18 anchors with 3 as-pect ratios and 6 scales. Without loss of recall, RPN+RSA reduces around 61.05% computation cost compared with the single-scale RPN, when the number of boxes is over 100. RPN+RSA is also more efficient and recalls more objects than original RPN. Our model and the single-anchor RPN both perform better than the original RPN. This observation is in accordance with the conclusion in face detection. Overall, our scheme of using RSA plus LRN competes comparably with the standard RPN method in terms of computation efficiency and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we prove that deep CNN features of an image can be approximated from a large scale to smaller scales by the proposed RSA unit, which significantly accelerates face detection while achieving comparable results in object detection. In order to make the detector faster and more accurate, we devise a scale-forecast network to predict the potential object scales. We further design a landmark retracing network to fuse global and local scale information to enhance the predictor. Experimental results show that our algorithm significantly outperforms state-of-the-art methods. Future work includes exploring RSA on generic object detection task. Representation approximation between video frames is also an interesting research avenue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F 1 (</head><label>1</label><figDesc>b) Single-shot by multi-scale detector (a) Multi-shot by single-scale detector (c) Single-shot by single-scale detector with recurrent scale approximation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Different detection pipelines. (a) Image pyramid is generated for multi-scale test. The detector only handles a specific range of scales. (b) Image is forwarded once at one scale and the detector generates all results. (c) Our proposed RSA framework. Image is forwarded once only and feature maps for different scales are approximated by a recurrent unit. Blue plates indicate images of different scales and orange plates with red boarder indicate CNN feature maps at different levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>RSA by rolling out the learned feature map at smaller scales. The number in the orange box indicates the average mean squared error between ground truth and RSA's prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Recall v.s. the number of predicted scales per face on three benchmarks. Our scale-forecast network recalls almost all scales when the number of predicted scale per face is 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Comparison to state-of-the-art approaches on face detection benchmarks. The proposed algorithm (Scale-forecast network with RSA+LRN, tagged by LRN+RSA) outperforms other methods by a large margin. 'revised gt.' and 'original gt.' in AFW stand for fully annotated faces by us and partially labeled annotations provided by the dataset, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Component</cell><cell>Scale-forecast</cell><cell>RSA</cell><cell>LRN</cell><cell>Total Pipeline</cell><cell cols="2">Baseline RPN</cell></row><row><cell>Structure</cell><cell>tiny ResNet-18</cell><cell>4-layer FCN</cell><cell>tiny ResNet-18</cell><cell>-</cell><cell cols="2">single anchor multi anchors</cell></row><row><cell>Opts. (VGA input)</cell><cell>95.67M</cell><cell>0 to 182.42M</cell><cell>0 to 1.3G</cell><cell>95.67M to 1.5G</cell><cell>1.72G</cell><cell>1.31G</cell></row><row><cell>AP@AFW</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.96%</cell><cell>99.90%</cell><cell>98.29%</cell></row><row><cell>Recall@FDDB1%fpi</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.92%</cell><cell>90.61%</cell><cell>86.89%</cell></row><row><cell>Recall@MALF1%fpi</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.09%</cell><cell>88.81%</cell><cell>84.65%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Test runtime (ms per image) of RSA compared with RPN on three benchmarks. We conduct experiments of each case five times and report the average result to avoid system disturbance.</figDesc><table><row><cell>Speed</cell><cell>LRN+RSA</cell><cell>LRN</cell><cell cols="2">RPN s RPN m</cell></row><row><cell>AFW</cell><cell>13.95</cell><cell>28.84</cell><cell>26.85</cell><cell>18.92</cell></row><row><cell>FDDB</cell><cell>11.24</cell><cell>27.10</cell><cell>25.01</cell><cell>18.34</cell></row><row><cell>MALF</cell><cell>16.38</cell><cell>29.73</cell><cell>27.37</cell><cell>19.37</cell></row><row><cell>Average</cell><cell>14.50</cell><cell>28.78</cell><cell>26.52</cell><cell>18.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Recall (%) vs. the number of proposals and Speed (ms per image) breakdown on ILSVRC DET val2.</figDesc><table><row><cell>Recall</cell><cell>100</cell><cell>300</cell><cell cols="2">1000 2000</cell><cell>Speed</cell></row><row><cell>Original RPN</cell><cell cols="2">88.7 93.5</cell><cell>97.3</cell><cell>97.7</cell><cell>158 ms</cell></row><row><cell cols="3">Single-scale RPN 89.6 94.4</cell><cell>97.2</cell><cell>98.0</cell><cell>249 ms</cell></row><row><cell>RSA+RPN</cell><cell cols="2">89.1 94.4</cell><cell>97.2</cell><cell>98.0</cell><cell>124 ms</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here we assume the minimum and maximum face sizes are 32 and 2048, respectively, if the higher dimension of an image is resized to 2048. In exponential expression, the face size is divided into six main bins from 2 5 = 32 to 2 11 = 2048, the denotation of which will be used later.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For brevity of discussion, we ignore the spatial weight sharing of convolution here. Note that the weight update in wxy also includes the loss from the landmark retracing network.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on PAMI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supervised transformer network for efficient face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors</editor>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="379" to="387" />
			<date type="published" when="2016" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view face detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 5th ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="643" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascade object detection with deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scale-aware face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM- CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<title level="m">3d object understanding with 3d convolutional neural networks. Information Sciences</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="188" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Zoom out-and-in network with recursive training for object proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05711</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Do we really need more training data for object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face detection with endto-end integration of a convnet and a 3d model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="420" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning deep features via congenerous cosine loss for person recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06890</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Face detection using deep learning: An improved faster rcnn approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08289</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fine-grained evaluation on face detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 11th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Poi: Multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Crafting gbd-net for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02579</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
