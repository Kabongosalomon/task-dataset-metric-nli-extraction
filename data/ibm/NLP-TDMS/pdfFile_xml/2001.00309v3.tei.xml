<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at https://git.io/AdelaiDet</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instance segmentation is one of the fundamental vision tasks. Recently, fully convolutional instance segmentation methods have drawn much attention as they are often simpler and more efficient than two-stage approaches like Mask R-CNN. To date, almost all such approaches fall behind the two-stage Mask R-CNN method in mask precision when models have similar computation complexity, leaving great room for improvement. In this work, we achieve improved mask prediction by effectively combining instancelevel information with semantic information with lowerlevel fine-granularity. Our main contribution is a blender module which draws inspiration from both top-down and bottom-up instance segmentation approaches. The proposed BlendMask can effectively predict dense per-pixel position-sensitive instance features with very few channels, and learn attention maps for each instance with merely one convolution layer, thus being fast in inference. BlendMask can be easily incorporated with the state-of-the-art onestage detection frameworks and outperforms Mask R-CNN under the same training schedule while being 20% faster. A light-weight version of BlendMask achieves 34.2% mAP at 25 FPS evaluated on a single 1080Ti GPU card. Because of its simplicity and efficacy, we hope that our BlendMask could serve as a simple yet strong baseline for a wide range of instance-wise prediction tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The top performing object detectors and segmenters often follow a two-stage paradigm. They consist of a fully convolutional network, region proposal network (RPN), to perform dense prediction of the most likely regions of interest (RoIs). A set of light-weight networks, a.k.a. heads, are applied to re-align the features of RoIs and generate predictions <ref type="bibr" target="#b25">[24]</ref>. The quality and speed for mask generation is strongly tied to the structure of the mask heads. In addition, , ,</p><p>Attns Masks <ref type="figure">Figure 1</ref> -Blending process. We illustrate an example of the learned bases and attentions. Four bases and attention maps are shown in different colors. The first row are the bases, and the second row are the attentions.</p><p>Here ⊗ represents element-wise product and ⊕ is element-wise sum. Each basis multiplies its attention and then is summed to output the final mask.</p><p>it is difficult for independent heads to share features with related tasks such as semantic segmentation which causes trouble for network architecture optimization. Recent advances in one-stage object detection prove that one-stage methods such as FCOS can outperform their twostage counterparts in accuracy <ref type="bibr" target="#b26">[25]</ref>. Enabling such onestage detection frameworks to perform dense instance segmentation is highly desirable as 1) models consisting of only conventional operations are simpler and easier for cross-platform deployment; 2) a unified framework provides convenience and flexibility for multi-task network architecture optimization.</p><p>Dense instance segmenters can date back to Deep-Mask <ref type="bibr" target="#b24">[23]</ref>, a top-down approach which generates dense instance masks with a sliding window. The representation of mask is encoded into a one-dimensional vector at each spatial location. Albeit being simple in structure, it has several obstacles in training that prevent it from achieving superior performance: 1) local-coherence between features and masks is lost; 2) the feature representation is redundant because a mask is repeatedly encoded at each foreground feature; 3) position information is degraded after downsampling with strided convolutions.</p><p>The first issue was studied by Dai et al. <ref type="bibr" target="#b8">[8]</ref>, who attempt to retain local-coherence by keeping multiple positionsensitive maps. This idea has been explored to its limits by Chen et al. <ref type="bibr" target="#b7">[7]</ref>, who proposes a dense aligned representation for each location of the target instance mask. However, this approach trades representation efficiency for alignment, making the second issue difficult to resolve. The third issue prevents heavily downsampled features to provide detailed instance information.</p><p>Recognizing these difficulties, a line of research takes a bottom-up strategy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b23">22]</ref>. These methods generate dense per-pixel embedding features and use some techniques to group them. Grouping strategies vary from simple clustering <ref type="bibr" target="#b3">[4]</ref> to graph-based algorithms <ref type="bibr" target="#b22">[21]</ref> depending on the embedding characteristics. By performing per-pixel predictions, the local-coherence and position information is well retained. The shortcomings for bottom-up approaches are: 1) heavy reliance on the dense prediction quality, leading to sub-par performance and fragmented/joint masks; 2) limited generalization ability to complex scenes with a large number of classes; 3) requirement for complex postprocessing techniques.</p><p>In this work, we consider hybridizing top-down and bottom-up approaches. We recognize two important predecessors, FCIS <ref type="bibr" target="#b18">[18]</ref> and YOLACT <ref type="bibr" target="#b2">[3]</ref>. They predict instance-level information such as bounding box locations and combine it with per-pixel predictions using cropping (FCIS) and weighted summation (YOLACT), respectively. We argue that these overly simplified assembling designs may not provide a good balance for the representation power of top-and bottom-level features.</p><p>Higher-level features correspond to larger receptive field and can better capture overall information about instances such as poses, while lower-level features preserve better location information and can provide finer details. One of the focuses of our work is to investigate ways to better merging these two in fully convolutional instance segmentation. More specifically, we generalize the operations for proposal-based mask combination by enriching the instance-level information and performing more finegrained position-sensitive mask prediction. We carry out extensive ablation studies to discover the optimal dimensions, resolutions, alignment methods, and feature locations. Concretely, we are able to achieve the followings:</p><p>• We devise a flexible method for proposal-based instance mask generation called blender, which incorporate rich instance-level information with accurate dense pixel features. In head-to-head comparison, our blender surpasses the merging techniques in YOLACT <ref type="bibr" target="#b2">[3]</ref> and FCIS [18] by 1.9 and 1.3 points in mAP on the COCO dataset respectively. • We propose a simple architecture, BlendMask, which is closely tied to the state of the art one-stage object detector, FCOS <ref type="bibr" target="#b26">[25]</ref>, by adding moldiest computation overhead to the already simple framework.</p><p>• One obvious advantage of BlendMask is that its inference time does not increase with the number of predictions as conventional two-stage methods do, which makes it more robust in real-time scenarios. • The performance of BlendMask achieves mAP of 37.0% with the ResNet-50 <ref type="bibr" target="#b15">[15]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Anchor-free object detection Recent advances in object detection unveil the possibilities of removing bounding box anchors <ref type="bibr" target="#b26">[25]</ref>, largely simplifying the detection pipeline. This much simpler design improves the box average precision (AP bb ) by 2.7% comparing to its anchor-based counterpart RetinaNet <ref type="bibr" target="#b19">[19]</ref>. One possible reason responsible for the improvement is that without the restrictions of predefined anchor shapes, targets are freely matched to prediction features according to their effective receptive field. The hints for us are twofold. First, it is important to map target sizes with proper pyramid levels to fit the effective receptive field for the features. Second, removing anchors enables us to assign heavier duties to the top-level instance prediction module without introducing overall computation overhead. For example, inferring shape and pose information alongside the bounding box detection would take about eight times more computation for anchor-based frameworks than ours. This makes it intractable for anchor based detectors to balance the top vs. bottom workload (i.e., learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom Module Bases</head><formula xml:id="formula_1">P7 Tower C5 C4 C3 FPN Backbone P2 P3 …</formula><p>Boxes Attns Blender P6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P5 P4</head><p>Detector module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BlendMask module</head><p>Detector feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BlendMask feature</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optional Connection</head><p>Tower <ref type="figure">Figure 2</ref> -BlendMask pipeline Our framework builds upon the state-of-the-art FCOS object detector <ref type="bibr" target="#b26">[25]</ref> with minimal modification. The bottom module uses either backbone or FPN features to predict a set of bases. A single convolution layer is added on top of the detection towers to produce attention masks along with each bounding box prediction. For each predicted instance, the blender crops the bases with its bounding box and linearly combine them according the learned attention maps. Note that the Bottom Module can take features either from 'C', or 'P' as the input.</p><p>instance-aware maps 1 vs. bases). We assume that this might be the reason why YOLACT can only learn one single scalar coefficient for each prototype/basis given an instance when computation complexity is taken into account. Only with the use of anchor-free bounding box detectors, this restriction is removed. Detect-then-segment instance segmentation The dominant instance segmentation paradigms take the two-stage methodology, first detecting the objects and then predicting the foreground masks on each of the proposals. The success of this framework partially is due to the alignment operation, RoIAlign <ref type="bibr" target="#b13">[13]</ref>, which provides local-coherence for the second-stage RoI heads missing in all one-stage top-down approaches. However, two issues exist in two-stage frameworks. For complicated scenarios with many instances, inference time for two-stage methods is proportional to the number of instances. Furthermore, the resolution for the RoI features and resulting mask is limited. We discuss the second issue in detail in Section 4.3.</p><p>These problems can be partly solved by replacing a RoI head with a simple crop-and-assemble module. In FCIS, Li et al. <ref type="bibr" target="#b18">[18]</ref> add a bottom module to a detection network, for predicting position-sensitive score maps shared by all instances. This technique was first used in R-FCN <ref type="bibr" target="#b9">[9]</ref> and later improved in MaskLab <ref type="bibr" target="#b4">[5]</ref>. Each channel of the k 2 score maps corresponds to one crop of k × k evenly partitioned grid tiles of the proposal. Each score map represents the likelihood of the pixel belongs to a object and is at a certain relative position. Naturally, a higher resolution for location crops leads to more accurate predictions, but the computation cost also increases quadratically. Moreover, there are special cases where FCIS representation is <ref type="bibr" target="#b0">1</ref> Attention maps for BlendMask and simple weight scalars for YOLACT. not sufficient. When two instances share center positions (or any other relative positions), the score map representation on that crop is ambiguous, it is impossible to tell which instance this crop is describing.</p><p>In YOLACT <ref type="bibr" target="#b2">[3]</ref>, an improved approach is used. Instead of using position-controlled tiles, a set of mask coefficients are learned alongside the box predictions. Then this set of coefficients guides the linear combination of cropped bottom mask bases to generate the final mask. Comparing to FCIS, the responsibility for predicting instance-level information is assigned to the top-level. We argue that using scalar coefficients to encode the instance information is suboptimal.</p><p>To break through these limitations, we propose a new proposal-based mask generation framework, termed Blend-Mask. The top-and bottom-level representation workloads are balanced by a blender module. Both levels are guaranteed to describe the instance information within their best capacities. As shown in our experiments in Section 4, our blender module improves the performance of bases combination methods comparing to YOLACT and FCIS by a large margin without increasing computation complexity.</p><p>Refining coarse masks with lower-level features BlendMask merges top-level coarse instance information with lower-level fine-granularity. This idea resembles MaskLab <ref type="bibr" target="#b4">[5]</ref> and Instance Mask Projection (IMP) <ref type="bibr" target="#b10">[10]</ref>, which concatenates mask predictions with lower layers of backbone features. The differences are clear. Our coarse mask acts like an attention map. The generation is extremely light-weight, without the need of using semantic or positional supervision, and is closely tied to the object generation. As shown in Section 3.4, our lower-level features have clear contextual meanings, even though not explicitly guided by bins or crops. Further, our blender does not require a subnet on top of the merged features as in MaskLab <ref type="bibr" target="#b4">[5]</ref> and IMP <ref type="bibr" target="#b10">[10]</ref>, which makes our method more efficient. In parallel to this work recent two single shot instance segmentation methods have shown good performance <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b28">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our BlendMask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall pipeline</head><p>BlendMask consists of a detector network and a mask branch. The mask branch has three parts, a bottom module to predict the score maps, a top layer to predict the instance attentions, and a blender module to merge the scores with attentions. The whole network is illustrated in <ref type="figure">Figure 2</ref>.</p><p>Bottom module Similar to other proposal-based fully convolutional methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">18]</ref>, we add a bottom module predicting score maps which we call bases, B. B has a shape of</p><formula xml:id="formula_2">N × K × H s × W s ,</formula><p>where N is the batch size, K is the number of bases, H × W is the input size and s is the score map output stride. We use the decoder of DeepLabV3+ in our experiments. Other dense prediction modules should also work without much difference. The input for the bottom module could be backbone features like conventional semantic segmentation networks <ref type="bibr" target="#b6">[6]</ref>, or the feature pyramids like YOLACT and Panoptic FPN <ref type="bibr" target="#b16">[16]</ref>.</p><p>Top layer We also append a single convolution layer on each of the detection towers to predict top-level attentions A. Unlike the mask coefficients in YOLACT, which for each pyramid with resolution W l × H l takes the shape of N × K × H l × W l , our A is a tensor at each location with shape N × (K · M · M ) × H l × W l , where M × M is the attention resolution. With its 3D structure, our attention map can encode instance-level information, e.g. the coarse shape and pose of the object. M is typically smaller 2 than the mask predictions in top-down methods since we only ask for a rough estimate. We predict it with a convolution with K · M · M output channels. Before sending them into the next module, we first apply FCOS <ref type="bibr" target="#b26">[25]</ref> post-process to select the top D box predictions</p><formula xml:id="formula_3">P = {p d ∈ R 4</formula><p>≥0 |d = 1 . . . D} and corresponding atten-</p><formula xml:id="formula_4">tions A = {a d ∈ R K×M ×M |d = 1 . . . D}.</formula><p>Blender module is the key part of our BlendMask. It combines position-sensitive bases according to the attentions to generate the final prediction. We discuss this module in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Blender module</head><p>The inputs of the blender module are bottom-level bases B, the selected top-level attentions A and bounding box proposals P . First we use RoIPooler in Mask R-CNN <ref type="bibr" target="#b13">[13]</ref> to crop bases with each proposal p d and then resize the re- <ref type="bibr" target="#b1">2</ref> The largest M we try is <ref type="bibr">14.</ref> gion to a fixed size R × R feature map r d .</p><formula xml:id="formula_5">r d = RoIPool R×R (B, p d ), ∀d ∈ {1 . . . D}. (1)</formula><p>More specifically, we use sampling ratio 1 for RoIAlign, i.e. one bin for each sampling point. The performance of using nearest and bilinear poolers are compared in <ref type="table">Table 6</ref>. During training, we simply use ground truth boxes as the proposals. During inference, we use FCOS prediction results.</p><p>Our attention size M is smaller than R. We interpolate</p><formula xml:id="formula_6">a d from M × M to R × R, into the shapes of R = {r d |d = 1 . . . D}. a d = interpolate M ×M →R×R (a d ), ∀d ∈ {1 . . . D}. (2)</formula><p>Then a d is normalize with softmax function along the K dimension to make it a set of score maps s d .</p><formula xml:id="formula_7">s d = softmax(a d ), ∀d ∈ {1 . . . D}.<label>(3)</label></formula><p>Then we apply element-wise product between each entity r d , s d of the regions R and scores S, and sum along the K dimension to get our mask logit m d :</p><formula xml:id="formula_8">m d = K k=1 s k d • r k d , ∀d ∈ {1 . . . D},<label>(4)</label></formula><p>where k is the index of the basis. We visualize the mask blending process with K = 4 in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Configurations and baselines</head><p>We consider the following configurable hyperparameters for BlendMask:   </p><formula xml:id="formula_9">• R,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semantics encoded in learned bases and attentions</head><p>By examining the generated bases and attentions on val2017, we observe this pattern. On its bases, BlendMask encodes two types of local information, 1) whether the pixel is on an object (semantic masks), 2) whether the pixel is on certain part of the object (position-sensitive features).</p><p>The complete bases and attentions projected onto the original image are illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The first two bases (red and blue) detects points on the upper-right and bottom-left parts of the objects. The third (yellow) base activates on points more likely to be on an object. The fourth (green) base only activates on the borders of objects. Position-sensitive features help us separate overlapping instances, which enables BlendMask to represent all instances more efficiently than YOLACT <ref type="bibr" target="#b2">[3]</ref>. The positive semantic mask makes our final prediction smoother than FCIS <ref type="bibr" target="#b18">[18]</ref> and the negative one can further suppress out-of-instance activations. We compare our blender with YOLACT and FCIS counterparts in <ref type="table">Table 1</ref>. BlendMask can learn more accurate features than YOLACT and FCIS with much fewer number of bases (4 vs. 32 vs. 49, see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our experiments are reported on the MSCOCO 2017 instance segmentation datatset <ref type="bibr" target="#b20">[20]</ref>. It contains 123K images with 80-class instance labels. Our models are trained on the train2017 split (115K images) and the ablation study is carried out on the val2017 split (5K images). Final results are on test-dev. The evaluation metrics are COCO mask average precision (AP), AP at IoU 0.5 (AP 50 ), 0.75 (AP 75 ) and AP for objects at different sizes AP S , AP M , and AP L .</p><p>Training details Unless specified, ImageNet pretrained ResNet-50 <ref type="bibr" target="#b14">[14]</ref> is used as our backbone network. DeepLabv3+ <ref type="bibr" target="#b6">[6]</ref> with channel width 128 is used as our bot- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation experiments</head><p>We investigate the effectiveness of our blender module by carrying out ablation experiments on the configurable hyperparameters in Section 3.3.</p><p>Merging methods: Blender vs. YOLACT vs. FCIS Similar to our method, YOLACT <ref type="bibr" target="#b2">[3]</ref> and FCIS <ref type="bibr" target="#b18">[18]</ref>  and FCIS is the case where we use fixed one-hot blending attentions and nearest neighbour top-level interpolation. Results of these variations are shown in <ref type="table">Table 1</ref>. Our blender surpasses the other alternatives by a large margin. We assume the reason is that other methods lack instanceaware guidance on the top. By contrast, our blender has a fine-grained top-level attention map, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Top and bottom resolutions: We measure the performances of our model with different top-and bottom-level resolutions, trying bottom pooler resolution R being 28 and 56, with R/M ratio from 14 to 4. As shown in <ref type="table" target="#tab_3">Table 2</ref>, by increasing the attention resolution, we can incorporate more detailed instance-level information while keeping the running time roughly the same. Notice that the gain slows down at higher resolutions revealing limit of detailed information on the top-level. So we don't include larger top settings with R/M ratio smaller than 4.</p><p>Different from two-stage approaches, increasing the bottom-level bases pooling resolution does not introduce much computation overhead. Increasing it from 28 to 56 only increases the inference time within 0.2ms while mask AP increases by 1 point. In further ablation experiment, we set R = 56 and M = 7 for our baseline model if not specified.</p><p>Number of bases: YOLACT <ref type="bibr" target="#b2">[3]</ref> uses 32 bases concerning the inference time. With our blender, the number of bases can be further reduced, to even just one. We report our models with number of bases varying from 1 to 8. Different from normal blender, the one-basis version uses sigmoid ac- tivation on both the base and the attention map. Results are shown in <ref type="table">Table 3</ref>. Since instance-level information is better represented with the top-level attentions, we only need 4 bases to get the optimal accuracy. K = 4 is adopted by all subsequent experiments.  the detailed positions are predicted. Bilinear top and bottom interpolation are adopted for our final models.</p><p>Other improvements: We experiment on other tricks to improve the performance. First we add auxiliary semantic segmentation supervision on P3 similar to YOLACT <ref type="bibr" target="#b2">[3]</ref>. Then we increase the width of our bottom module from 128 to 256. Finally, we reduce the bases output stride from 8 to 4, to produce higher-quality bases. We achieve this by using P2 and P5 as the bottom module input. <ref type="table">Table 7</ref> shows the results. By adding semantic loss, detection and segmentation results are both improved. This is an interesting effect since the instance segmentation task itself does not improve the box AP. Although all tricks contribute to the improvements, we decide to not use larger basis resolution because it slows down the model by 10ms per image.</p><p>We also implement the protonet module in YOLACT <ref type="bibr" target="#b2">[3]</ref> for comparison. We include a P3 version and an FPN version. The P3 version is identical to the one used in YOLACT. For the FPN version, we first change the channel width of P3, P4, and P5 to 128 with a 3 × 3 convolution. Then upsample all features to s/8 and sum them up. Following are the same as P2 version except that we reduce convolution layers by one. Auxiliary semantic loss is applied to both versions. As shown in <ref type="table">Table 7</ref>, changing the bottom module from DeepLabv3+ to protonet does not modify the speed and performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main result</head><p>Quantitative results We compare BlendMask with Mask R-CNN <ref type="bibr" target="#b13">[13]</ref> and TensorMask <ref type="bibr" target="#b7">[7]</ref> on the COCO testdev dataset 3 . We use 56 4 14 with bilinear top interpolation, the DeepLabV3+ decoder with channel width 256 and P3, P5 input. Since our ablation models are heavily under-fitted, we increase the training iterations to 270K (3× schedule), tuning learning rate down at 180K and 240K. Following Chen et al.'s strategy <ref type="bibr" target="#b7">[7]</ref>, we use multiscale training with shorter side randomly sampled from [640, 800]. As shown in <ref type="table" target="#tab_7">Table 8</ref>, our BlendMask outperforms both the modified Mask R-CNN with deeper FPN and TensorMask using only half of their training iterations.</p><p>BlendMask is also more efficient. Measured on a V100 GPU, the best R-101 BlendMask runs at 0.07s/im, vs. Ten-sorMask's 0.38s/im, vs. Mask R-CNN's 0.09s/im <ref type="bibr" target="#b7">[7]</ref>. Furthermore, a typical running time of our blender module is merely 0.6ms, which makes the additional time for complex scenes nearly negligible On the contrary, for two-stage Mask R-CNN with more expensive head computation, the inference time increases by a lot if the number of predicted instances grows.</p><p>Real-time setting We design a compact version of our model, BlendMask-RT, to compare with YOLACT <ref type="bibr" target="#b2">[3]</ref>, a real-time instance segmentation method: i) the number of convolution layers in the prediction head is reduced to three, ii) and we merge the classification tower and box tower into one by sharing their features. We use Proto-FPN with four convolution layers with width 128 as the bottom module. The top FPN output P7 is removed because it has little effect on the detecting smaller objects. We train both BlendMask-RT and Mask R-CNN with the ×3 schedule, with shorter side randomly sampled from <ref type="bibr">[440,</ref><ref type="bibr">550]</ref>.</p><p>There are still two differences in the implementation comparing to YOLACT. YOLACT resizes all images to square, changing the aspect ratios of inputs. Also, a paralleled NMS algorithm called Fast NMS is used in YOLACT. We do not adopt these two configurations because they are not conventionally used in instance segmentation researches. In YOLACT, a speedup of 12ms is reported by using Fast NMS. We instead use the Batched NMS in Detectron2, which could be slower than Fast NMS but does not sacrifice the accuracy. Results in <ref type="table">Table 9</ref> shows that BlendMask-RT is 7ms faster and 3.3 AP higher than YOLACT-700. Making our model also competitive under the real-time settings.</p><p>Qualitative results We compare our model with the best available official YOLACT and Mask R-CNN models with ResNet-101 backbone. Masks are illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. Our model yields higher quality masks than Mask R-CNN. The first reason is that we predicts 56 × 56 masks while Mask R-CNN uses 28 × 28 masks. Also our segmentation module mostly utilizes high resolution features that preserve the original aspect-ratio, where Mask R-CNN also uses 28 × 28 features.</p><p>Note that YOLACT has difficulties discriminating instances of the same class close to each other. BlendMask can avoid this typical leakage. This is because its top module provides more detailed instance-level information, guiding the bases to capture position-sensitive information and   suppressing the outside regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussions</head><p>Comparison with Mask R-CNN Similar to Mask R-CNN, we use RoIPooler to locate instances and extract features. We reduce the running time by moving the computation of R-CNN heads before the RoI sampling to generate position-sensitive feature maps. Repeated mask representation and computation for overlapping proposals are avoided. We further simplify the global map representation by replacing the hard alignment in R-FCN <ref type="bibr" target="#b9">[9]</ref> and FCIS <ref type="bibr" target="#b18">[18]</ref> with our attention guided blender, which needs ten times less channels for the same resolution.</p><p>Another advantage of BlendMask is that it can produce higher quality masks, since our output resolution is not restricted by the top-level sampling. Increasing the RoIPooler resolution of Mask R-CNN will introduce the following problem. The head computation increases quadratically with respect to the RoI size. Larger RoIs requires deeper head structures. Different from dense pixel predictions, RoI foreground predictor has to be aware of whole instancelevel information to distinguish foreground from other overlapping instances. Thus, the larger the feature sizes are, the deeper sub-networks is needed.</p><p>Furthermore, it is not very friendly to real-time applications that the inference time of Mask R-CNN is proportional to the number of detections. By contrast, our blender module is very efficient (0.6ms on 1080 Ti). The additional inference time required after increasing the number of detections can be neglected.</p><p>Our blender module is very flexible. Because our toplevel instance attention prediction is just a single convolution layer, it can be an almost free add-on to most modern object detectors. With its accurate instance prediction, it can also be used to refine two-stage instance predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Panoptic Segmentation</head><p>We use the semantic segmentation branch of Panoptic-FPN <ref type="bibr" target="#b16">[16]</ref> to extend BlendMask to the panoptic segmentation task. We use annotations of COCO 2018 panoptic segmentaiton task. All models are trained on train2017 subset and tested on val2017. We train our model with the default FCOS <ref type="bibr" target="#b26">[25]</ref> 3× schedule with scale jitter (shorter image side in [640, 800]. To combine instance and semantic results, we use the same strategy as in Panoptic-FPN, with instance confidence threshhold 0.2 and overlap threshhold </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.4.</head><p>Results are reported in <ref type="table" target="#tab_9">Table 10</ref>. Our model is consistently better than its Mask R-CNN counterpart, Panoptic-FPN. We assume there are three reasons. First, our instance segmentation is more accurate, this helps with both thing and stuff panoptic quality because instance masks are overlaid on top of semantic masks. Second, our pixel-level instance prediction is also generated from a global feature map, which has the same scale as the semantic prediction, thus the two results are more consistent. Last but not least, since the our bottom module shares structure with the semantic segmentation branch, it is easier for the network to share features during the closely related multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">More Qualitative Results</head><p>We visualize qualitative results of Mask R-CNN and BlendMask on the validation set in <ref type="figure" target="#fig_4">Fig. 5</ref>. Four sets of images are listed in rows. Within each set, the top row is the Mask R-CNN results and the bottom is BlendMask. Both models are based on the newly released Detectron2 with use R101-FPN backbone. Both are trained with the 3× schedule. The Mask R-CNN model achieves 38.6% AP and ours 39.5% AP.</p><p>Since this version of Mask R-CNN is a very strong baseline, and both models achieve very high accuracy, it is very difficult to tell the differences. To demonstrate our advantage, we select some samples where Mask R-CNN has trouble dealing with. Those cases include:</p><p>• Large objects with complex shapes (Horse ears, human poses). Mask R-CNN fails to provide sharp borders.</p><p>• Objects in separated parts (tennis players occluded by nets, trains divided by poles). Mask R-CNN tends to include occlusions as false positive or segment targets into separate objects. • Overlapping objects (riders, crowds, drivers). Mask R-CNN gets uncertain on the borders and leaves larger false negative regions. Sometimes, it assigns parts to the wrong objects, such as the last example in the first row.</p><p>Our BlendMask performs better on these cases. 1) Generally, BlendMask utilizes features with higher resolution. Even for the large objects, we use stride-8 features. Thus details are better preserved. 2) As shown in previous illustrations, our bottom module acts as a class agnostic instance segmenter which is very sensitive to borders. 3) Sharing features with the bounding box regressor, our top module is very good at recognizing individual instances. It can generate attentions with flexible shapes to merge the fine-grained segments of bottom module outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Evaluating on LVIS annotations</head><p>To quantify the high quality masks generated by Blend-Mask, we compare our results with on the higher-quality LVIS annotations <ref type="bibr" target="#b12">[12]</ref>.</p><p>Our model is compared to the best high resolution model we are aware of, recent PointRend <ref type="bibr" target="#b17">[17]</ref>, which uses multiple subnets to refine the local features to get higher resolution mask predictions. The description of the evaluation metric can be found in <ref type="bibr" target="#b17">[17]</ref>. <ref type="table">Table 11</ref> shows that the evaluation numbers will improve further given more accurate ground truth annotations. Our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have devised a novel blender module for instancelevel dense prediction tasks which uses both high-level instance and low-level semantic information. It is efficient and easy to integrate with different main-stream detection networks.</p><p>Our framework BlendMask outperforms the carefullyengineered Mask R-CNN without bells and whistles while being 20% faster. Furthermore, the real-time version BlendMask-RT achieves 34.2% mAP at 25 FPS evaluated on a single 1080Ti GPU card. We believe that our Blend-Mask is capable of serving as an alternative to Mask R-CNN <ref type="bibr" target="#b13">[13]</ref> for many other instance-level recognition tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Bottom-Level Bases(b) Top-Level attentions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 -</head><label>3</label><figDesc>Detailed view of learned bases and attentions. The left four images are the bottom-level bases. The right image is the top-level attentions. Colors on each position of the attentions correspond to the weights of the bases, indicating from which part of which base is the mask assembled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>9 -</head><label>9</label><figDesc>Real-time setting comparison of speed and accuracy with other state-of-the-art methods on COCO val2017. Metrics for YOLACT are obtained using their official code and trained model. Mask R-CNN and BlendMask models are trained and measured using Detectron2. Resolution 550 × * means using shorter side 550 in inference. Our fast version of BlendMask significantly outperforms YOLACT in accuracy with on par execution time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 -</head><label>4</label><figDesc>Detailed comparison with other methods. The large image on the left side is the segmentation result of our method. We further zoom in our result and compare against YOLACT<ref type="bibr" target="#b2">[3]</ref> (31.2% mAP) and Mask R-CNN<ref type="bibr" target="#b13">[13]</ref> (36.1% mAP) on the right side. Our masks are overall of higher quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 -</head><label>5</label><figDesc>Selected results of Mask R-CNN (top) and BlendMask (bottom). Both models are based on Detectron2. The Mask R-CNN model is the official 3× R101 model with 38.6 AP. BlendMask model obtains 39.5 AP. Best viewed in digital format with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the bottom-level RoI resolution, • M , the top-level prediction resolution, • K, the number of bases, • bottom module input features, it can either be features from the backbone or the FPN, • sampling method for bottom bases, nearest-neighbour or bilinear pooling, • interpolation method for top-level attentions, nearest neighbour or bilinear upsampling. We represent our models with abbreviation R K M. For example, 28 4 4 represents bottom-level region resolution of 28 × 28, 4 number of bases and 4 × 4 top-level instance attentions. By default, we use backbone features C3 and</figDesc><table /><note>C5 to keep aligned with DeepLabv3+ [6]. Nearest neigh- bour interpolation is used in top-level interpolation, for a fair comparison with FCIS [18]. Bilinear sampling is used in the bottom level, consistent with RoIAlign [13].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>For the ablation experiments, performance and time of our models are measured with one image per batch on one 1080Ti GPU.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell cols="2">Weighted-sum 29.7</cell><cell>52.2</cell><cell>30.1</cell></row><row><cell cols="2">Assembler 30.3</cell><cell>52.5</cell><cell>31.3</cell></row><row><cell cols="2">Blender 31.6</cell><cell>53.4</cell><cell>33.3</cell></row></table><note>Table 1 -Comparison of different strategies for merging top and bot- tom modules. Here the model used is 28 4 4. Weighted-sum is our analogy to YOLACT, reducing the top resolution to 1 × 1. Assembler is our analogy to FCIS, where the number of bases is increased to 16, match- ing each of the region crops without the need of top-level attentions.tom module. For ablation study, all the networks are trained with the 1× schedule of FCOS [25], i.e., 90K iterations, batch size 16 on 4 GPUs, and base learning rate 0.01 with constant warm-up of 1k iterations. The learning rate is re- duced by a factor of 10 at iteration 60K and 80K. Input im- ages are resized to have shorter side 800 and longer side at maximum 1333. All hyperparameters are set to be the same with FCOS [25]. Testing details The unit for inference time is 'ms' in all our tables.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 -</head><label>2</label><figDesc>both merge proposal-based bottom regions to create mask prediction. YOLACT simply performs a weighted sum of the channels of the bottom regions; FCIS assembles crops of position-sensitive masks without modifications. Our blender can be regarded as a generalization where both YOLACT and FCIS merging are special cases: The blender with 1 × 1 top-level resolution degenerates to YOLACT; Resolutions: Performance by varying top-/bottom-level resolutions, with the number of bases K = 4 for all models. Top-level attentions are interpolated with nearest neighbour. Bottom module uses backbone features C3, C5. The performance increases as the attention resolution grows, saturating at resolutions of near 1/4 of the region sizes.</figDesc><table><row><cell>R</cell><cell cols="2">M Time</cell><cell>AP</cell><cell cols="3">APS APM APL</cell></row><row><cell></cell><cell>2</cell><cell>72.7</cell><cell cols="2">30.6 14.3</cell><cell>34.1</cell><cell>42.5</cell></row><row><cell>28</cell><cell>4</cell><cell>72.9</cell><cell cols="2">31.6 14.8</cell><cell>35.2</cell><cell>45.0</cell></row><row><cell></cell><cell>7</cell><cell>73.9</cell><cell cols="2">32.0 15.3</cell><cell>35.6</cell><cell>45.0</cell></row><row><cell></cell><cell>4</cell><cell>72.9</cell><cell cols="2">32.5 14.9</cell><cell>36.1</cell><cell>46.0</cell></row><row><cell>56</cell><cell>7</cell><cell>74.1</cell><cell cols="2">33.1 15.1</cell><cell>36.6</cell><cell>47.7</cell></row><row><cell></cell><cell>14</cell><cell>77.7</cell><cell cols="2">33.3 16.3</cell><cell>36.8</cell><cell>47.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 -</head><label>4</label><figDesc>Bottom feature locations: Performance with bottom resolution 56 × 56, 4 bases and bilinear bottom interpolation. C3, C5 uses features from backbone. P3, P5 uses features from FPN. Bottom feature locations: backbone vs. FPN We compare our bottom module feature sampling locations. By using FPN features, we can improve the performance while reducing the running time (seeTable 4). In later experiments, if not specified, we use P3 and P5 of FPN as our bottom module input.</figDesc><table><row><cell cols="4">Features M Time (ms)</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell>C3, C5</cell><cell>7 14</cell><cell>74.1 77.7</cell><cell></cell><cell>33.1 33.3</cell><cell></cell><cell>54.1 54.1</cell><cell>34.9 35.3</cell></row><row><cell>P3, P5</cell><cell>7 14</cell><cell>72.5 76.4</cell><cell></cell><cell>33.3 33.4</cell><cell></cell><cell>54.2 54.3</cell><cell>35.3 35.5</cell></row><row><cell cols="3">Interpolation M</cell><cell>AP</cell><cell cols="3">AP50 AP75</cell></row><row><cell></cell><cell>Nearest</cell><cell cols="2">7 14 33.4 33.3</cell><cell cols="2">54.2 54.3</cell><cell>35.3 35.5</cell></row><row><cell></cell><cell>Bilinear</cell><cell cols="2">7 14 33.6 33.5</cell><cell cols="2">54.3 54.6</cell><cell>35.7 35.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 -Table 6 -Table 7 -</head><label>567</label><figDesc>Top interpolation: Performance with bottom resolution 56 × 56, 4 bases and bilinear bottom interpolation. Nearest represents nearestneighbour upsampling and bilinear is bilinear interpolation. Bottom Alignment: Performance with 4 bases and bilinear top interpolation. Nearest represents the original RoIPool in Fast R-CNN [11] and bilinear is the RoIAlign in Mask R-CNN [13]. Interpolation method: nearest vs. bilinear In Mask R-CNN [13], RoIAlign plays a crucial role in aligning the pooled features to keep local-coherence. We investigate the effectiveness of bilinear interpolation for bottom RoI sampling and top-level attention re-scaling. As shown in Table 5, changing top interpolation from nearest to bilinear yields a marginal improvement of 0.2 AP.The results of bottom sampling with RoIPool<ref type="bibr" target="#b11">[11]</ref> (nearest) and RoIAlign<ref type="bibr" target="#b13">[13]</ref> (bilinear) are shown inTable 6. For both resolutions, the aligned bilinear sampling could improve the performance by almost 2AP. Using aligned features for the bottom-level is more crucial, since it is where Bottom Time (ms) AP bb Other improvements: We use 56 4 14x14 with bilinear interpolation for all models. '+semantic' is the model with semantic supervision as auxiliary loss. '+128' is the model with bottom module channel size being 256. '+s/4' means using P2,P5 as the bottom input.</figDesc><table><row><cell cols="2">Alignment R</cell><cell>M</cell><cell>AP</cell><cell>AP50 AP75</cell></row><row><cell>Nearest</cell><cell cols="3">28 56 14 31.9 7 30.5</cell><cell>53.0 53.6</cell><cell>31.6 33.4</cell></row><row><cell>Bilinear</cell><cell cols="3">28 56 14 33.6 7 32.4</cell><cell>54.4 54.6</cell><cell>34.5 35.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 -</head><label>8</label><figDesc>Quantitative results on COCO test-dev. We compare our BlendMask against Mask R-CNN and TensorMask. Mask R-CNN* is the modified Mask R-CNN with implementation details in TensorMask<ref type="bibr" target="#b7">[7]</ref>. Models with 'aug.' uses multi-scale training with shorter side range [640, 800]. Speed for Mask R-CNN 1× and BlendMask are measured with maskrcnn benchmark on a single 1080Ti GPU. BlendMask* is implemented with Detectron2, the speed difference is caused by different measuring rules. '+deform convs (interval = 3)' uses deformable convolution in the backbone with interval 3, following<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell cols="5">Method Backbone Epochs Aug. Time (ms)</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>Mask R-CNN [13]</cell><cell></cell><cell>12</cell><cell>97.0</cell><cell></cell><cell>34.6</cell><cell>56.5</cell><cell cols="2">36.6</cell><cell>15.4</cell><cell>36.3</cell><cell>49.7</cell></row><row><cell>Mask R-CNN*</cell><cell></cell><cell>72</cell><cell>97+</cell><cell></cell><cell>36.8</cell><cell>59.2</cell><cell cols="2">39.3</cell><cell>17.1</cell><cell>38.7</cell><cell>52.1</cell></row><row><cell>TensorMask [7]</cell><cell>R-50</cell><cell>72</cell><cell cols="2">400+</cell><cell>35.5</cell><cell>57.3</cell><cell cols="2">37.4</cell><cell>16.6</cell><cell>37.0</cell><cell>49.1</cell></row><row><cell>BlendMask</cell><cell></cell><cell>12</cell><cell>78.5</cell><cell></cell><cell>34.3</cell><cell>55.4</cell><cell cols="2">36.6</cell><cell>14.9</cell><cell>36.4</cell><cell>48.9</cell></row><row><cell>BlendMask</cell><cell></cell><cell>36</cell><cell>78.5</cell><cell></cell><cell>37.0</cell><cell>58.9</cell><cell cols="2">39.7</cell><cell>17.3</cell><cell>39.4</cell><cell>52.5</cell></row><row><cell>BlendMask*</cell><cell></cell><cell>36</cell><cell>74.0</cell><cell></cell><cell>37.8</cell><cell>58.8</cell><cell cols="2">40.3</cell><cell>18.8</cell><cell>40.9</cell><cell>53.6</cell></row><row><cell>Mask R-CNN</cell><cell></cell><cell>12</cell><cell cols="2">118.1</cell><cell>36.2</cell><cell>58.6</cell><cell cols="2">38.4</cell><cell>16.4</cell><cell>38.4</cell><cell>52.1</cell></row><row><cell>Mask R-CNN*</cell><cell></cell><cell>36</cell><cell cols="2">118+</cell><cell>38.3</cell><cell>61.2</cell><cell cols="2">40.8</cell><cell>18.2</cell><cell>40.6</cell><cell>54.1</cell></row><row><cell>TensorMask</cell><cell></cell><cell>72</cell><cell cols="2">400+</cell><cell>37.3</cell><cell>59.5</cell><cell cols="2">39.5</cell><cell>17.5</cell><cell>39.3</cell><cell>51.6</cell></row><row><cell>SOLO [26] +deform convs [26]</cell><cell>R-101</cell><cell>72 72</cell><cell>--</cell><cell></cell><cell>37.8 40.4</cell><cell>59.5 62.7</cell><cell cols="2">40.4 43.3</cell><cell>16.4 17.6</cell><cell>40.6 43.3</cell><cell>54.2 58.9</cell></row><row><cell>BlendMask</cell><cell></cell><cell>36</cell><cell cols="2">101.8</cell><cell>38.4</cell><cell>60.7</cell><cell cols="2">41.3</cell><cell>18.2</cell><cell>41.5</cell><cell>53.3</cell></row><row><cell>BlendMask*</cell><cell></cell><cell>36</cell><cell>94.1</cell><cell></cell><cell>39.6</cell><cell>61.6</cell><cell cols="2">42.6</cell><cell>22.4</cell><cell>42.2</cell><cell>51.4</cell></row><row><cell>+deform convs (interval = 3)</cell><cell></cell><cell>60</cell><cell cols="2">105.0</cell><cell>41.3</cell><cell>63.1</cell><cell cols="2">44.6</cell><cell>22.7</cell><cell>44.1</cell><cell>54.5</cell></row><row><cell cols="2">Method Backbone</cell><cell>NMS</cell><cell cols="4">Resolution Time (ms) AP bb</cell><cell>AP</cell><cell cols="3">AP50 AP75</cell></row><row><cell>YOLACT</cell><cell></cell><cell>Fast</cell><cell>550 × 550</cell><cell>34.2</cell><cell></cell><cell>32.5</cell><cell>29.8</cell><cell></cell><cell>48.3</cell><cell>31.3</cell></row><row><cell>YOLACT</cell><cell>R-101</cell><cell>Fast</cell><cell>700 × 700</cell><cell>46.7</cell><cell></cell><cell>33.4</cell><cell>30.9</cell><cell></cell><cell>49.8</cell><cell>32.5</cell></row><row><cell>BlendMask-RT</cell><cell></cell><cell>Batched</cell><cell>550 ×  *</cell><cell>47.6</cell><cell></cell><cell>41.6</cell><cell>36.8</cell><cell></cell><cell>61.2</cell><cell>42.4</cell></row><row><cell>Mask R-CNN BlendMask-RT</cell><cell>R-50</cell><cell>Batched</cell><cell>550 ×  *</cell><cell>63.4 36.0</cell><cell></cell><cell>39.1 39.3</cell><cell>35.3 35.1</cell><cell></cell><cell>56.5 55.5</cell><cell>37.6 37.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 -</head><label>10</label><figDesc>Panoptic results on COCO val2017. Panoptic-FPN results are from the official Detectron2 implementation, which are improved upon the original published results in [16]. method can benefits from the accurate bottom features and surpasses the high-res PointRend results.</figDesc><table><row><cell cols="2">Method Backbone</cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell cols="4">PQ Th PQ St mIoU AP box</cell><cell>AP</cell></row><row><cell>Panoptic-FPN [16] BlendMask</cell><cell>R-50</cell><cell cols="3">41.5 79.1 50.5 42.5 80.1 51.6</cell><cell>48.3 49.5</cell><cell>31.2 32.0</cell><cell>42.9 43.5</cell><cell>40.0 41.8</cell><cell>36.5 37.2</cell></row><row><cell>Panoptic-FPN [16] BlendMask</cell><cell>R-101</cell><cell cols="3">43.0 80.0 52.1 44.3 80.1 53.4</cell><cell>49.7 51.6</cell><cell>32.9 33.2</cell><cell>44.5 44.9</cell><cell>42.4 44.0</cell><cell>38.5 38.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To make fair comparison with TensorMask, the code base that we use for main result is maskrcnn benchmark. Recently released Detectron2 fixed several issues of maskrcnn benchmark (ROIAlign and paste mask) in the previous repository and the performance is further improved.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Huawei Technologies for the donation of GPU cloud computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up instance segmentation using deep higher-order CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Conf. Machine Vis</title>
		<meeting>British Conf. Machine Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06218</idno>
		<title level="m">YOLACT++: Better real-time instance segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">YOLACT: real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis., abs</title>
		<meeting>Int. Conf. Comp. Vis., abs</meeting>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic instance segmentation with a discriminative loss function. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1708.02551</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Res. Repository</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis., abs</title>
		<meeting>Int. Conf. Comp. Vis., abs</meeting>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">IMP: instance mask projection for high accuracy semantic segmentation of things. arXiv Comput. Res. Repository, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointrend</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08193</idno>
		<title level="m">Image segmentation as rendering</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Microsoft COCO: common objects</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Our model is the last model in Table 8. Our model is 0.2 points higher on COCO and 0.7 points higher on LVIS annotations. Here LVIS AP is COCO mask AP evaluated against the higher-quality LVIS annotations. in context</title>
	</analytic>
	<monogr>
		<title level="m">Table 11 -Comparison with PointRend. Mask R-CNN and PointRend results are quoted from Table</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proc. Eur. Conf. Comp. Vis.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Affinity derivation and graph merge for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="708" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8837" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis., abs</title>
		<meeting>Int. Conf. Comp. Vis., abs</meeting>
		<imprint>
			<date type="published" when="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04488</idno>
		<title level="m">Segmenting objects by locations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Po-larMask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno>arxiv.org/abs/1909.13226. 4</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv Comput. Res. Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
