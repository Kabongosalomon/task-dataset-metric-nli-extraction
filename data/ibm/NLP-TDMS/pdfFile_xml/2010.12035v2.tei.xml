<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
							<email>tabelini@lcad.inf.ufes.br</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidade Federal do Espírito Santo (UFES)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidade Federal do Espírito Santo (UFES)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><forename type="middle">M</forename><surname>Paixão</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Instituto Federal do Espírito Santo (IFES)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Badue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidade Federal do Espírito Santo (UFES)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">F</forename><surname>De Souza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidade Federal do Espírito Santo (UFES)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Oliveira-Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidade Federal do Espírito Santo (UFES)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern lane detection methods have achieved remarkable performances in complex real-world scenarios, but many have issues maintaining real-time efficiency, which is important for autonomous vehicles. In this work, we propose LaneATT: an anchor-based deep lane detection model, which, akin to other generic deep object detectors, uses the anchors for the feature pooling step. Since lanes follow a regular pattern and are highly correlated, we hypothesize that in some cases global information may be crucial to infer their positions, especially in conditions such as occlusion, missing lane markers, and others. Thus, this work proposes a novel anchor-based attention mechanism that aggregates global information. The model was evaluated extensively on three of the most widely used datasets in the literature. The results show that our method outperforms the current state-of-the-art methods showing both higher efficacy and efficiency. Moreover, an ablation study is performed along with a discussion on efficiency trade-off options that are useful in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has been essential for recent advances in numerous areas, especially in autonomous driving <ref type="bibr" target="#b1">[2]</ref>. Many of the deep learning applications in self-driving cars are in their perception systems. To be safe around humans, autonomous vehicles should perceive their surroundings, including the position of other vehicles and themselves. In the end, the more predictable a car's movement is, the safer it will be for its passengers and pedestrians. Thus, it is important for autonomous vehicles to know each lane's exact position, which is the goal of lane detection systems.</p><p>Lane detection models have to overcome various challenges. A model that will be used in a real-world scenario should be robust to several adverse conditions, such as extreme light and weather conditions. Moreover, lane markings can be occluded by other objects (e.g., cars), which is extremely common for self-driving cars. Some approaches, such as polynomial regression models, may also suffer from a data imbalance problem caused by the long-tail effect since cases with sharper curves are less common. Besides, the model not only has to be robust but also efficient. In several applications, lane detection must perform in real-time, or faster to save processing power for other systems, a requirement that many models struggle to cope with.</p><p>There are numerous works in the literature that tackle this problem. Before the advent of deep learning, several methods used more traditional computer vision techniques, such as Hough lines <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref>. More recently, focus has shifted to deep learning approaches with the advance of convolutional neural networks (CNNs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. In this context, the lane detection problem is usually formulated as a segmentation task, where, given an input image, the output is a segmentation map with per-pixel predictions <ref type="bibr" target="#b16">[17]</ref>. Although recent advances in deep learning have enabled the use of segmentation networks in real-time <ref type="bibr" target="#b21">[22]</ref>, various models struggle to to achieve real-time performance. Consequently, the number of backbone options for segmentation-based methods is rather limited. Hence, some recent works have proposed solutions in other directions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. Apart from that, many other issues are common in works on lane detection, such as the need for a post-processing step (usually a heuristic), long training times, and a lack of publicly available source code, which hinders comparisons and reproducibility.</p><p>In this work, we present a method for real-time lane detection that is both faster and more accurate than most stateof-the-art methods. We propose an anchor-based singlestage lane detection model called LaneATT. Its architecture enables the use of a lightweight backbone CNN while maintaining high accuracy. A novel anchor-based attention mechanism to aggregate global information is also proposed. Extensive experimental results are shown on three benchmarks (TuSimple <ref type="bibr" target="#b23">[24]</ref>, CULane <ref type="bibr" target="#b16">[17]</ref> and LLAMAS <ref type="bibr" target="#b2">[3]</ref>), along with a comparison with state-of-the-art methods, a discussion on efficiency trade-offs, and an ablation study of our design choices. In summary, our main contributions are:</p><p>• A lane detection method that is more accurate than existing state-of-the-art real-time methods on a large and complex dataset;</p><p>• A model that enables faster training and inference times than most other models (reaching 250 FPS and almost an order of magnitude less multiply-accumulate operations (MACs) than the previous state-of-the-art);</p><p>• A novel anchor-based attention mechanism for lane detection which is potentially useful in other domains where the objects being detected are correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Although the first lane detection approaches rely on classical computer vision, substantial progress on accuracy and efficiency has been achieved with recent deep learning methods. Thus, this literature review focuses on deep lane detectors. This section first discusses the dominant approaches, which are based on segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15]</ref> or rowwise classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>, and, subsequently, review solutions in other directions. Finally, the lack of reproducibility (a common issue in lane detection works) is discussed.</p><p>Segmentation-based methods. In this approach, predictions are made on a per-pixel basis, classifying each pixel as either lane or background. With the segmentation map generated, a post-processing step is necessary to decode it into a set of lanes. In SCNN <ref type="bibr" target="#b16">[17]</ref>, the authors propose a scheme specifically designed for long thin structures and show its effectiveness in lane detection. However, the method is slow (7.5 FPS), which hinders its applicability in real-world cases. Since larger backbones are one of the main culprits for slower speeds, the authors, in <ref type="bibr" target="#b10">[11]</ref>, proposes a self attention distillation (SAD) module to aggregate contextual information. The module allows the use of a more lightweight backbone, achieving a high-performance while maintaining real-time efficiency. In CurveLanes-NAS <ref type="bibr" target="#b25">[26]</ref>, the authors propose the use of neural architecture search (NAS) to find a better backbone. Although they achieved state-of-the-art results, their NAS is extremely expensive computationally, requiring 5,000 GPU hours per dataset.</p><p>Row-wise classification methods. The row-wise classification approach is a simple way to detect lanes based on a grid division of the input image. For each row, the model predicts the most probable cell to contain a part of a lane marking. Since only one cell is selected on each row, this process is repeated for each possible lane in an image. Similar to segmentation methods, it also requires a post-processing step to construct the set of lanes. The method was first introduced in E2E-LMD <ref type="bibr" target="#b26">[27]</ref>, achieving state-of-the-art results on two datasets. In <ref type="bibr" target="#b19">[20]</ref>, the authors show that it is capable of reaching high speed, although some accuracy is lost. This approach is also used in IntRA-KD <ref type="bibr" target="#b9">[10]</ref>.</p><p>Other approaches. In FastDraw <ref type="bibr" target="#b17">[18]</ref>, the author proposes a novel learning-based approach to decode the lane structures, which avoids the need for clustering post-processing steps (required in segmentation and row-wise classification methods). Although the proposed method is shown to achieve high speeds, it does not perform better than existing state-of-the-art methods in terms of accuracy. The same effect is shown in PolyLaneNet <ref type="bibr" target="#b22">[23]</ref>, where an even faster model, based on deep polynomial regression, is proposed. In that approach, the model learns to output a polynomial for each lane. Despite its speed, the model struggles with the imbalanced nature of lane detection datasets, as evidenced by the high bias towards straight lanes in its predictions. In Line-CNN <ref type="bibr" target="#b12">[13]</ref>, an anchor-based method for lane detection is presented. This model achieves state-of-the-art results on a public dataset and promising results on another that is not publicly available. Despite the real-time efficiency, the model is considerably slower than other approaches. Moreover, the code is not public, which makes the results difficult to reproduce. There are also works addressing other parts of the pipeline of a lane detector. In <ref type="bibr" target="#b11">[12]</ref>, a post-processing method with a focus on occlusion cases is proposed, achieving results considerably higher than other works, but at the cost of notably low speeds (around 4 FPS).</p><p>Reproducibility. As noted in <ref type="bibr" target="#b22">[23]</ref>, many of the cited works do not publish the code to reproduce the results reported <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>, or, in some cases, the code is only partially public <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>. This hinders deeper qualitative and quantitative comparisons. For instance, the two most common metrics to measure a model's efficiency are multiply-accumulate operations (MACs) and frames-per-second (FPS). While the first does not depend on the benchmark platform, it is not always a good proxy for the second, which is the true goal. Therefore, FPS comparisons are also hindered by the lack of source code.</p><p>Unlike most of the previously proposed methods that managed to achieve high speeds at the cost of accuracy, we propose a method that is both faster and more accurate than existing state-of-the-art ones. In addition, the full code to reproduce the reported results is published for the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>LaneATT is an anchor-based single-stage model (like YOLOv3 <ref type="bibr" target="#b20">[21]</ref> or SSD <ref type="bibr" target="#b15">[16]</ref>) for lane detection. An overview of the method is shown in <ref type="figure">Figure 1</ref>. It receives as input RGB images I ∈ R 3×H I ×W I taken from a front-facing camera mounted in a vehicle. The outputs are lane boundary lines (hereafter called lanes, following the usual terminology in the literature). To generate those outputs, a convolutional neural network (CNN), referred to as the backbone, generates a feature map that is then pooled to extract each anchor's features. Those features are combined with a set of global </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image plane</head><p>Anchor-based Feature Pooling <ref type="figure">Figure 1</ref>. Overview of the proposed method. A backbone generates feature maps from an input image. Subsequently, each anchor is projected onto the feature maps. This projection is used to pool features that are concatenated with another set of features created in the attention module. Finally, using this resulting feature set, two layers, one for classification and another for regression, make the final predictions.</p><formula xml:id="formula_0">Anchor i O θ a loc i a glob i ⊕ FC Latt softmax × wi,0 a loc 0 × wi,i−1 a loc i−1 × wi,i+1 a loc i+1 × wi,N anc−1 a loc Nanc−1 . . . . . . + FC Lcls pi = p0 . . . pK FC Lreg ri = l x0 . . . xN pts −1 Anchor i O θ p1 = 0.96 x0 x1 x2 x3 l = 3 + → addition × → multiplication ⊕→ concatenation</formula><p>features produced by an attention module. By combining local and global features, the model can use information from other lanes more easily, which might be necessary in cases with conditions such as occlusion or no visible lane markings. Finally, the combined features are passed to fullyconnected layers to predict the final output lanes. , where y i = i · H I Npts−1 . Since Y is fixed, a lane can then be defined only by its x-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lane and anchor representation</head><formula xml:id="formula_1">coordinates X = {x i } Npts−1 i=0</formula><p>, each x i associated with the respective y i ∈ Y . Since most lanes do not cross the whole image vertically, a start-index s and an end-index e are used to define the valid contiguous sequence of X.</p><p>Likewise Line-CNN <ref type="bibr" target="#b12">[13]</ref>, our method performs anchorbased detection using lines instead of boxes, which means that lanes' proposals are made having these lines as references. An anchor is a "virtual" line in the image plane defined by (i) an origin point O = (x orig , y orig ) (with y orig ∈ Y ) located in one of the borders of the image (except the top border) and (ii) a direction θ. The proposed method uses the same set of anchors as <ref type="bibr" target="#b12">[13]</ref>. This lane and anchor representation satisfies the vast majority of real-world lanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Backbone</head><p>The first stage of the proposed method is feature extraction, which can be performed by any generic CNN, such as a ResNet <ref type="bibr" target="#b8">[9]</ref>. The output of this stage is a feature map F back ∈ R C F ×H F ×W F from which the features for each anchor will be extracted through a pooling process, as described in the next section. For dimensionality reduction, a 1 × 1 convolution is applied onto F back , generating a channel-wise reduced feature map F ∈ R C F ×H F ×W F . This reduction is performed to reduce computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Anchor-based feature pooling</head><p>An anchor defines the points of F that will be used for the respective proposals. Since the anchors are modeled as lines, the interest points for a given anchor are those that intercept the anchor's virtual line (considering the rasterized line reduced to the feature maps dimensions). For every y j = 0, 1, 2, . . . , H F − 1, there will be a single corresponding xcoordinate,</p><formula xml:id="formula_2">x j = 1 tan θ (y j − y orig /δ back ) + x orig /δ back ,<label>(1)</label></formula><p>where (x orig , y orig ) and θ are, respectively, the origin point and slope of the anchor's line, and δ back is the backbone's global stride. Thus, every anchor i will have its corresponding feature vector a loc i ∈ R C F ·H F (column-vector notation) pooled from F that carries local feature information (local features). In cases where a part of the anchor is outside the boundaries of F, a loc i is zero-padded. Notice that the pooling operation is similar to the Fast R-CNN's <ref type="bibr" target="#b7">[8]</ref> region of interest projection (RoI projection), however, instead of using the proposal for pooling, a singlestage detector is achieved by using the anchor itself. Additionally, the RoI pooling layer (used to generate fixed-size features) is not necessary for our method. Comparing to Line-CNN <ref type="bibr" target="#b12">[13]</ref>, that leverages only the feature maps' borders, our method can potentially explore all the feature map, which enables the use of more lightweight backbones with smaller receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Attention mechanism</head><p>Depending on the model architecture, the information carried by the pooled feature vector ends up being mostly local. This is particularly the case for shallower and faster models, which tend to exploit backbones with smaller receptive fields. However, in some cases (such as the ones with occlusion) the local information may not be enough to predict the lane's existence and its position. To address that problem, we propose an attention mechanism that acts on the local features (a loc • ) to produce additional features (a glob • ) that aggregate global information.</p><p>Basically, the attention mechanism structure is composed of a fully-connected layer L att which processes a local feature vector a loc i and outputs a probability (weight) w i,j for every anchor j, j = i. Formally,</p><formula xml:id="formula_3">w i,j =      softmax(L att (a loc i )) j , if j &lt; i 0, if j = i softmax(L att (a loc i )) j−1 , if j &gt; i<label>(2)</label></formula><p>Afterwards, those weights are combined with the local features to produce a global feature vector of same dimension:</p><formula xml:id="formula_4">a glob i = j w i,j a loc j .<label>(3)</label></formula><p>Naturally, the whole process can be implemented efficiently with matrix multiplication, since the same procedure is executed for all anchors. Let N anc be the number of anchors. Let A loc = [a loc 0 , . . . , a loc Nanc−1 ] T be the matrix containing the local feature vectors (as rows) and W = [w i,j ] Nanc×Nanc the weight matrix, w i,j defined in Equation (2). Thus, global features can be computed as:</p><formula xml:id="formula_5">A glob = W A loc .<label>(4)</label></formula><p>Notice that A glob and A loc have the same dimensions, i.e., A glob ∈ R Nanc×C F ·H F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Proposal prediction</head><p>A lane proposal is predicted for each anchor and consists of three main components: (i) K + 1 probabilities (K lane types and one class for "background" or invalid proposal), (ii) N pts offsets (the horizontal distance between the prediction and the anchor's line), and (iii) the length l of the proposal (the number of valid offsets). The start-index (s) for the proposal is directly determined by the y-coordinate of the anchor's origin (y orig , see Section 3.1). Thus, the end-index can be determined as e = s + l − 1.</p><p>To generate the final proposals, local and global information are aggregated by concatenating a loc i and a glob i , producing an augmented feature vector a aug i ∈ R 2·C F ·H F . This augmented vector is fed to two parallel fully-connected layers, one for classification (L cls ) and one for regression (L reg ), which produce the final proposals. L cls predicts p i = {p 0 , . . . , p K+1 } (item i) and L reg predicts r i = l, {x 0 , . . . , x Npts−1 } (items ii and iii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Non-maximum Supression (NMS)</head><p>As usual in anchor-based deep detection, NMS is paramount to reduce the number of false positives. In the proposed method, this procedure is applied both during training and test phases based on the lane distance metric proposed in <ref type="bibr" target="#b12">[13]</ref>. The distance between two lanes X a = {x a i }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Npts i=1</head><p>and</p><formula xml:id="formula_6">X b = {x b i } Npts i=1</formula><p>is computed based on their common valid indices (or y-coordinates). Let s = max(s a , s b ) and e = min(e a , e b ) define the range of those common indices. Thus, the lane distance metric is defined as</p><formula xml:id="formula_7">D(X a , X b ) = 1 e −s +1 · e i=s |x a i − x b i |, e ≥ s +∞, otherwise.<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Model training</head><p>During training, the distance metric in Equation <ref type="formula" target="#formula_7">(5)</ref> is also used to define the positive and the negative anchors. First, the metric is used to measure the distance between every anchor (those not filtered in NMS) and the groundtruth lanes. Subsequently, the anchors with distance (Eq. 5) lower than a threshold τ p are considered positives, while those with distance greater than τ n are considered negatives. Anchors (and their associated proposals) with distance in between those thresholds are disregarded. The remainder N p&amp;n are used in a multi-task loss defined as:</p><formula xml:id="formula_8">L({p i , r i } N p&amp;n −1 i=0 ) = λ i L cls (p i , p * i ) + i L reg (r i , r * i ),<label>(6)</label></formula><p>where p i , r i are the classification and regression outputs for the anchor i, whereas p * i and r * i are the classification and regression targets for the anchor i. The regression loss is computed only with the length l and the x-coordinates values corresponding to indices common to both the proposal and the ground-truth. The common indices (between s and e ) of the x-coordinates are selected similarly to the lane distance (Equation <ref type="formula" target="#formula_7">(5)</ref>) but with e = e gt instead of e = min(e prop , e gt ), where e prop and e gt are the endindexes for the proposal and its associated ground-truth, respectively. If the end-index predicted in the proposal e prop is used, the training may become unstable by converging to degenerate solutions (e.g., e prop might converge to zero). The functions L cls and L reg are the Focal Loss <ref type="bibr" target="#b13">[14]</ref> and the Smooth L1, respectively. If the anchor i is considered negative, its corresponding L reg is equal to 0. The factor λ is used to balance the loss components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Anchor filtering for speed efficiency</head><p>The full set of anchors comprises a total of 2,782 anchors. This elevated number is one of the main factors limiting the model's speed. Since a large number of anchors will not be useful during the training (e.g., some anchors may have a starting point above the horizon line of all images in the training dataset), the set's size can be reduced. To choose which anchors are going to be disregarded in both training and test phases, the method measures the number of times each anchor from the training set is marked as positive (same criteria as in the training). Finally, only the top-N anc marked anchors are kept for further processing (also during test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our method was evaluated on the two most widely used lane detection datasets (TuSimple <ref type="bibr" target="#b23">[24]</ref> and CULane <ref type="bibr" target="#b16">[17]</ref>) and on the recently released benchmark (LLAMAS <ref type="bibr" target="#b2">[3]</ref>). An overview of the datasets can be seen in <ref type="table" target="#tab_0">Table 1</ref>. This section starts describing the efficiency metrics and some of the implementation details. All experiments used the default metric parameters set by the dataset's creator, which are the same used by the related works. The three first subsections discusses the experimental results for each dataset (including the dataset description and the evaluation metrics). The two final subsections address experiments on efficiency tradeoffs and an ablation study on parts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">TuSimple</head><p>Dataset description. TuSimple <ref type="bibr" target="#b23">[24]</ref> is a lane detection dataset containing only highway scenes, a scenario that is usually considered easier than street scenes. Despite that, it is one of the most widely used datasets in lane detection works. All images have 1280 × 720 pixels, with at most 5 lanes.</p><p>Evaluation metrics. On TuSimple, the three standard metrics are false discovery rate (FDR), false negative rate (FNR), and accuracy. The accuracy is defined as</p><formula xml:id="formula_9">Acc = clip C clip clip S clip ,<label>(7)</label></formula><p>where C clip is the number of lane points predicted correctly in the clip and S clip is the total number of points in the clip (or image). For a point prediction to be considered correct, the prediction has to be within 20 pixels the ground truth. For a lane prediction to be considered a true positive (for the FDR and FNR metrics), its number of correct predicted points has to be greater than 85%. We also report the F1 score (hereafter called F1), which is the harmonic mean of the precision and the recall.</p><p>Results. The results of LaneATT on TuSimple, along with other state-of-the-art methods, are shown in <ref type="table">Table 2</ref> and in <ref type="figure" target="#fig_5">Figure 3</ref>   LaneATT <ref type="bibr" target="#b19">[20]</ref> [28] SCNN <ref type="bibr" target="#b16">[17]</ref> EL-GAN <ref type="bibr" target="#b6">[7]</ref> FastDraw <ref type="bibr" target="#b17">[18]</ref> ENet-SAD <ref type="bibr" target="#b10">[11]</ref> Line-CNN <ref type="bibr" target="#b12">[13]</ref> PolyLaneNet <ref type="bibr" target="#b22">[23]</ref> PointLaneNet <ref type="bibr" target="#b4">[5]</ref> Cascaded-CNN <ref type="bibr" target="#b18">[19]</ref> ERFNet-IntRA-KD <ref type="bibr" target="#b9">[10]</ref>    <ref type="table">Table 2</ref>. State-of-the-art results on TuSimple. For a fairer comparison, the FPS of the fastest method ( <ref type="bibr" target="#b19">[20]</ref>) was measured on the same machine and conditions as our method. Additionally, all metrics for this method were computed using the official source code, since only the accuracy was available in the paper. The best and second-best results across methods with source-code available are in bold and underlined, respectively.</p><p>on par with other state-of-the-art methods. However, it is also clear that the results in this dataset are saturated (highvalues) already, probably because its scenes are not complex and the metric is permissive <ref type="bibr" target="#b22">[23]</ref>. This is evidenced by the small difference in performance across methods, in contrast to results in more complex datasets and less permissive metrics (as shown in Section 4.2). Nonetheless, our method is much faster than others. The method proposed in <ref type="bibr" target="#b19">[20]</ref> is the only with speed comparable to ours. Since the FDR and FNR metrics were not reported in their work, we computed them using the published code and reported those metrics.</p><p>Although they achieved high accuracy, the FDR is notably high. For instance, our highest false-positive rate is 5.64%, using the ResNet-122, whereas their lowest is 18.91%, almost four times higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CULane</head><p>Dataset description. CULane <ref type="bibr" target="#b16">[17]</ref>   <ref type="table">Table 3</ref>. State-of-the-art results on CULane. Since the images in the "Cross" category have no lanes, the reported number is the amount of false-positives. For a fairer comparison, we measured the FPS of the fastest method ( <ref type="bibr" target="#b19">[20]</ref>) under the same machine and conditions as ours.</p><p>The best and second-best results across methods with source-code available are in bold and underlined, respectively.</p><p>Evaluation metrics. The only metric is the F1, which is based on the intersection over union (IoU). Since the IoU relies on areas instead of points, a lane is represented as a thick line connecting the respective lane's points. In particular, the dataset's official metric considers the lanes as 30-pixels-thick lines. If a prediction has an IoU greater than 0.5 with a ground-truth lane, it is considered a true positive.</p><p>Results. The results of LaneATT on CULane, along with other state-of-the-art methods, are shown in <ref type="table">Table 3</ref> and in <ref type="figure" target="#fig_5">Figure 3</ref>  . We do not compare to the results shown in <ref type="bibr" target="#b11">[12]</ref>, as the main contribution is a post-processing method that could easily be incorporated to our method, but the source-code is not public. Moreover, it is remarkably slow, which makes the model impractical in real world applications (the full pipeline runs at less than 10 FPS, as reported in their work). In this context, LaneATT achieves the highest F1 among the methods compared while maintaining a high efficiency (+170 FPS) on CULane, a dataset with highly complex scenes. Compared to <ref type="bibr" target="#b19">[20]</ref>, our most lightweight model (with ResNet-18) surpasses their largest (with ResNet-34) by almost 3% of F1 while being much faster (250 vs. 175 FPS on the same machine). Additionally, in "Night" and "Shadow" scenes, our method outperforms all others, including SIM-CycleGAN <ref type="bibr" target="#b14">[15]</ref>, specifically designed for those scenarios. Those results demonstrate both the efficacy and the efficiency of LaneATT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">LLAMAS</head><p>Dataset description. LLAMAS <ref type="bibr" target="#b2">[3]</ref> is a large lane detection dataset with over 100k images. The annotations were not manually made, instead, they were generated using high definition maps. All images are from highway scenarios. The evaluation is based on the CULane's F1, which was computed by the author of the LLAMAS benchmark since the testing set's annotations are not public.</p><p>Results. The results of LaneATT on LLAMAS, along with PolyLaneNet's <ref type="bibr" target="#b22">[23]</ref> results, are shown in <ref type="table">Table 4</ref>. Qualitative results are shown in <ref type="figure" target="#fig_2">Figure 2</ref> (bottom row). Since the benchmark is recent and only PolyLaneNet provided the necessary source code to be evaluated on LLAMAS, it is the only comparable method. As evidenced, LaneATT is able to achieve an F1 greater than 90% in all three backbones. The results can also be seen in the benchmark's website 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Efficiency trade-offs</head><p>Being efficient is crucial for a lane detection model. In some cases, it might even be necessary to trade some accuracy to achieve the application's requirement. In this experiment, some of the possible trade-offs are shown. In particular, different settings of image input size (H I × W I ) and number of anchors (N anc , as described in Section 3.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The results are shown in <ref type="table" target="#tab_3">Table 5</ref>. They show that the number of anchors can be reduced for a slight improvement in terms of efficiency without a large F1 drop. However, if the reduction is too large, the F1 starts to drop considerably. Moreover, if too many anchors are used, the efficacy can also degrade, which might be a consequence of unnecessary anchors disturbing the training. The results are similar for the input size, although the MACs drops are larger. The largest impact of both the number of anchors and the input size is on the training time. During the inference, the proposals are filtered (using a confidence threshold) before the NMS procedure. During the training, there's no such filtering. Since the NMS is one the main bottlenecks of the model, and its running time depends directly on the number of objects, the number of anchors has a much higher impact on the training phase than on the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>This experiment evaluates the impact of each major part (one at a time) of the proposed method: anchor-based pooling, shared layers, focal loss and the attention mechanism. The results are shown in <ref type="table" target="#tab_4">Table 6</ref>. The first row comprises the results for the standard LaneATT, while the following rows show the results for slightly modified versions of the standard model. In the second row, the anchor-based pooling was removed and the same procedure to select features of Line-CNN <ref type="bibr" target="#b12">[13]</ref> was used (i.e., only features from a single point in the feature map were used for each anchor). In the third one, instead of using a single pair of fully-connected layers (L reg and L cls ) for the final prediction, three pairs (six layers) were used, one pair matching one boundary (left, bottom, or right). That is, all anchors starting in the left boundary of the image had its proposals generated by the same pair of layers L L reg and L L cls and similarly for the bottom (L B reg and L B cls ) and the right (L R reg and L R cls ) boundaries. In the fourth one, the Focal Loss was replaced with the Cross Entropy, and in the last one, the attention mechanism was removed.</p><p>The massive drop of performance when the anchor-based pooling procedure is removed shows its importance. This procedure enabled the use of a more lightweight backbone, which was not possible in Line-CNN <ref type="bibr" target="#b12">[13]</ref> without a large performance drop. The results also show that a layer for each boundary of the image is not only unnecessary, but also detrimental to the model's efficiency. Furthermore, using the Focal Loss instead of the Cross Entropy was also shown to be beneficial. Besides, it also eliminates the need for one hyperparameter (the number of negative samples to be used in the loss computation). Finally, the proposed attention mechanism is another modification that significantly increases the model performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>lane is represented by 2D-points with equally-spaced y-coordinates Y = {y i } Npts−1 i=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>LaneATT qualitative results on TuSimple (top row), CU-Lane (middle row), and LLAMAS (bottom row). Blue lines are ground-truth, while green and red lines are true-positives and falsepositives, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(left side). Qualitative results are shown in Figure 2 (top row). As demonstrated, accuracy-wise LaneATT is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Model latency vs. F1 of state-of-the-art methods on CULane and TuSimple. Method F1 (%) Acc (%) FDR (%) FNR (%) FPS MACs (G)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(right side). Qualitative results are shown in Figure 2 (middle row)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overview of the datasets used in this work.Efficiency metrics. Two efficiency-related metrics are reported: frames-per-second (FPS) and multiply-accumulate operations (MACs). One MAC is approx. two floating operations (FLOPs). The FPS is computed using a single image per batch and constant inputs, so the metric is not dependent on I/O operations but only on the model's efficiency.</figDesc><table><row><cell>Dataset</cell><cell>Train</cell><cell>Val.</cell><cell cols="2">Test Max. # of lanes</cell></row><row><cell>TuSimple [24]</cell><cell>3,268</cell><cell>358</cell><cell>2,782</cell><cell>5</cell></row><row><cell>CULane [17]</cell><cell>88,880</cell><cell cols="2">9,675 34,680</cell><cell>4</cell></row><row><cell cols="4">LLAMAS [3] 58,269 20,844 20,929</cell><cell>4</cell></row></table><note>pts = 72, N anc = 1000, τ p = 15 and τn = 20. The used datasets do not provide the lane type (e.g., dashed or solid), thus, we set K = 1 (according Sec- tion 3.5). For more details and parameter values, the code 1 can be accessed, along with each experiment's configuration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>is one of the largest publicly available lane detection datasets, and also one of the most complexes. All the images have 1640 × 590 pixels, and all test images are divided into nine categories, such as crowded, night, absence of visible lines, etc.</figDesc><table><row><cell>Method</cell><cell cols="11">Total Normal Crowded Dazzle Shadow No line Arrow Curve Cross Night FPS MACs (G)</cell></row><row><cell>Source-code unavailable</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[28]</cell><cell>73.10</cell><cell>89.70</cell><cell>76.50</cell><cell>67.40</cell><cell>65.50</cell><cell>35.10</cell><cell>82.20</cell><cell>63.20</cell><cell>68.70</cell><cell>24.0</cell><cell></cell></row><row><cell>FastDraw (ResNet-50) [18]</cell><cell></cell><cell>85.90</cell><cell>63.60</cell><cell>57.00</cell><cell>59.90</cell><cell>40.60</cell><cell>79.40</cell><cell>65.20</cell><cell>7013 57.80</cell><cell>90.3</cell><cell></cell></row><row><cell>PointLaneNet [5]</cell><cell></cell><cell>90.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>71.0</cell><cell></cell></row><row><cell>SpinNet [6]</cell><cell>74.20</cell><cell>90.50</cell><cell>71.70</cell><cell>62.00</cell><cell>72.90</cell><cell>43.20</cell><cell>85.00</cell><cell>50.70</cell><cell>68.10</cell><cell></cell><cell></cell></row><row><cell>R-18-E2E [27]</cell><cell>70.80</cell><cell>90.00</cell><cell>69.70</cell><cell>60.20</cell><cell>62.50</cell><cell>43.20</cell><cell>83.20</cell><cell>70.30</cell><cell>2296 63.30</cell><cell></cell><cell></cell></row><row><cell>R-34-E2E [27]</cell><cell>71.50</cell><cell>90.40</cell><cell>69.90</cell><cell>61.50</cell><cell>68.10</cell><cell>45.00</cell><cell>83.70</cell><cell>69.80</cell><cell>2077 63.20</cell><cell></cell><cell></cell></row><row><cell>R-101-E2E [27]</cell><cell>71.90</cell><cell>90.10</cell><cell>71.20</cell><cell>60.90</cell><cell>68.10</cell><cell>44.90</cell><cell>84.30</cell><cell>70.20</cell><cell>2333 65.20</cell><cell></cell><cell></cell></row><row><cell>ERFNet-E2E [27]</cell><cell>74.00</cell><cell>91.00</cell><cell>73.10</cell><cell>64.50</cell><cell>74.10</cell><cell>46.60</cell><cell>85.80</cell><cell>71.90</cell><cell>2022 67.90</cell><cell></cell><cell></cell></row><row><cell>Source-code available</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCNN [17]</cell><cell>71.60</cell><cell>90.60</cell><cell>69.70</cell><cell>58.50</cell><cell>66.90</cell><cell>43.40</cell><cell>84.10</cell><cell>64.40</cell><cell>1990 66.10</cell><cell>7.5</cell><cell></cell></row><row><cell>ENet-SAD [11]</cell><cell>70.80</cell><cell>90.10</cell><cell>68.80</cell><cell>60.20</cell><cell>65.90</cell><cell>41.60</cell><cell>84.00</cell><cell>65.70</cell><cell>1998 66.00</cell><cell>75</cell><cell></cell></row><row><cell>[20] (ResNet-18)</cell><cell>68.40</cell><cell>87.70</cell><cell>66.00</cell><cell>58.40</cell><cell>62.80</cell><cell>40.20</cell><cell>81.00</cell><cell>57.90</cell><cell cols="2">1743 62.10 322.5</cell><cell></cell></row><row><cell>[20] (ResNet-34)</cell><cell>72.30</cell><cell>90.70</cell><cell>70.20</cell><cell>59.50</cell><cell>69.30</cell><cell>44.40</cell><cell>85.70</cell><cell>69.50</cell><cell cols="2">2037 66.70 175.0</cell><cell></cell></row><row><cell>ERFNet-IntRA-KD [10]</cell><cell>72.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100.0</cell><cell></cell></row><row><cell>SIM-CycleGAN [15]</cell><cell>73.90</cell><cell>91.80</cell><cell>71.80</cell><cell>66.40</cell><cell>76.20</cell><cell>46.10</cell><cell>87.80</cell><cell>67.10</cell><cell>2346 69.40</cell><cell></cell><cell></cell></row><row><cell>CurveLanes-NAS-S [26]</cell><cell>71.40</cell><cell>88.30</cell><cell>68.60</cell><cell>63.20</cell><cell>68.00</cell><cell>47.90</cell><cell>82.50</cell><cell>66.00</cell><cell>2817 66.20</cell><cell></cell><cell>9.0</cell></row><row><cell>CurveLanes-NAS-M [26]</cell><cell>73.50</cell><cell>90.20</cell><cell>70.50</cell><cell>65.90</cell><cell>69.30</cell><cell>48.80</cell><cell>85.70</cell><cell>67.50</cell><cell>2359 68.20</cell><cell></cell><cell>33.7</cell></row><row><cell>CurveLanes-NAS-L [26]</cell><cell>74.80</cell><cell>90.70</cell><cell>72.30</cell><cell>67.70</cell><cell>70.10</cell><cell>49.40</cell><cell>85.80</cell><cell>68.40</cell><cell>1746 68.90</cell><cell></cell><cell>86.5</cell></row><row><cell>LaneATT (ResNet-18)</cell><cell>75.09</cell><cell>91.11</cell><cell>72.96</cell><cell>65.72</cell><cell>70.91</cell><cell>48.35</cell><cell>85.49</cell><cell>63.37</cell><cell>1170 68.95</cell><cell>250</cell><cell>9.3</cell></row><row><cell>LaneATT (ResNet-34)</cell><cell>76.68</cell><cell>92.14</cell><cell>75.03</cell><cell>66.47</cell><cell>78.15</cell><cell>49.39</cell><cell>88.38</cell><cell>67.72</cell><cell>1330 70.72</cell><cell>171</cell><cell>18.0</cell></row><row><cell>LaneATT (ResNet-122)</cell><cell>77.02</cell><cell>91.74</cell><cell>76.16</cell><cell>69.47</cell><cell>76.31</cell><cell>50.46</cell><cell>86.29</cell><cell>64.05</cell><cell>1264 70.81</cell><cell>26</cell><cell>70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Efficiency trade-offs on CULane using the ResNet-34 backbone. "TT" stands for training time in hours.</figDesc><table><row><cell></cell><cell cols="3">F1 (%) Prec. (%) Rec. (%)</cell></row><row><cell>PolyLaneNet [23]</cell><cell>88.40</cell><cell>88.87</cell><cell>87.93</cell></row><row><cell>LaneATT (ResNet-18)</cell><cell>93.46</cell><cell>96.92</cell><cell>90.24</cell></row><row><cell>LaneATT (ResNet-34)</cell><cell>93.74</cell><cell>96.79</cell><cell>90.88</cell></row><row><cell>LaneATT (ResNet-122)</cell><cell>93.54</cell><cell>96.82</cell><cell>90.47</cell></row><row><cell cols="3">Table 4. State-of-the-art results on LLAMAS.</cell><cell></cell></row><row><cell>Modification</cell><cell cols="3">F1 (%) FPS MACs (G) TT (h)</cell></row><row><cell>N anc = 250</cell><cell>68.68 196</cell><cell>17.3</cell><cell>5.7</cell></row><row><cell>N anc = 500</cell><cell>75.45 190</cell><cell>17.4</cell><cell>6.4</cell></row><row><cell>N anc = 750</cell><cell>75.80 181</cell><cell>17.7</cell><cell>7.8</cell></row><row><cell>N anc = 1000</cell><cell>76.66 171</cell><cell>18.0</cell><cell>11.1</cell></row><row><cell>N anc = 1250</cell><cell>75.91 156</cell><cell>18.4</cell><cell>11.5</cell></row><row><cell>H I × W I = 180 × 320</cell><cell>66.74 195</cell><cell>4.8</cell><cell>4.3</cell></row><row><cell>H I × W I = 288 × 512</cell><cell>75.02 186</cell><cell>11.5</cell><cell>7.3</cell></row><row><cell>H I × W I = 360 × 640</cell><cell>76.66 171</cell><cell>18.0</cell><cell>11.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation study results on CULane.</figDesc><table><row><cell>Model</cell><cell cols="2">F1 (%) FPS Params. (M)</cell></row><row><cell>LaneATT (ResNet-34)</cell><cell>76.68 171</cell><cell>22.13</cell></row><row><cell>− anchor-based pooling</cell><cell>64.89 188</cell><cell>21.39</cell></row><row><cell>− shared layers</cell><cell>75.45 142</cell><cell>22.34</cell></row><row><cell>− focal loss</cell><cell>75.54 171</cell><cell>22.13</cell></row><row><cell>− attention mechanism</cell><cell>75.78 196</cell><cell>21.37</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/lucastabelini/LaneATT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https : / / unsupervised -llamas . com / llamas / benchmark_splines</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a real-time single-stage deep lane detection model that outperforms state-of-the-art models, as shown by an extensive comparison with the literature. The model is not only effective but also efficient. On TuSimple, the method achieves the second-highest reported F1 (a difference of only 0.02%) while being much faster than the top-F1 method (171 vs. 30 FPS). On CULane, one of the largest and most complex lane detection datasets, the method establishes a new state-of-the-art among real-time methods in terms of both speed and accuracy (+4.38% of F1 compared to the stateof-the-art method with a similar speed of around 170 FPS). Additionally, the method achieved a high F1 (+93%) on the LLAMAS benchmark on all three backbones evaluated. To achieve those results, along with other modifications, a novel anchor-based attention mechanism was also proposed. The ablation study showed that this addition increased the model's performance (F1 score) significantly when considering the gains obtained by the literature advance in recent years. Additionally, some efficiency trade-offs that are useful in practice were also shown.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real time lane detection for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Assidiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">O</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer and Communication Engineering</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-driving cars: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rânik</forename><surname>Guidolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Vivacqua Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vinicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avelino</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Forechi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>Paixão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Mutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Veronese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">F De</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">113816</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised labeled lane marker dataset generation using maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soussan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">F De</forename><surname>Edilson De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
		<title level="m">Lane Analysis System (ELAS): Dataset and Algorithms. Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Point-LaneNet: Efficient end-to-end CNNs for Accurate Real-Time Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SpinNet: Spinning Convolutional Network for Lane Boundary Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="428" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Embedding Loss Driven Generative Adversarial Networks for Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nóra</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>El-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inter-Region Affinity Distillation for Road Marking Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning lightweight Lane Detection CNNs by Self Attention Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lane Detection using Lane Boundary Marker Network with Road Geometry Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsheen Rafaqat</forename><surname>Hussam Ullah Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wajahat</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aamer</forename><surname>Kazmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaheer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Line-CNN: Endto-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lane Detection in Low-light Conditions Using an Efficient Data Enhancement: Light Conditions Style Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV), 2020</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SSD: Single Shot Multibox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial As Deep: Spatial CNN for Traffic Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FastDraw: Addressing the Long Tail of Lane Detection by Adapting a Sequential Prediction Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lane Detection and Classification using Cascaded CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Pizzati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Allodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Barrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Systems Theory</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ultra Fast Structureaware Deep Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized Con-vNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PolyLaneNet: Lane Estimation via Deep Polynomial Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tusimple</forename><surname>Tusimple</surname></persName>
		</author>
		<ptr target="https://github.com/TuSimple/tusimple-benchmark.Accessed" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end Lane Detection through Differentiable Least-Squares Fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-End Lane Marker Detection via Row-wise Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Seok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesoo</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrack</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duck Hoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR Workshop</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric Constrained Joint Lane Segmentation and Lane Boundary Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust Lane Detection from Continuous Driving Scenes using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
