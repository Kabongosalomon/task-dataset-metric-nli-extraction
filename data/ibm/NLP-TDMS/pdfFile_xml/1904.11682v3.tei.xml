<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoSF: Searching Scoring Functions for Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hong Kong University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong SAR</orgName>
								<orgName type="institution" key="instit3">China ‡ 4Paradigm Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hong Kong University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong SAR</orgName>
								<orgName type="institution" key="instit3">China ‡ 4Paradigm Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
							<email>daiwenyuan@4paradigm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hong Kong University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong SAR</orgName>
								<orgName type="institution" key="instit3">China ‡ 4Paradigm Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<email>leichen@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hong Kong University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong SAR</orgName>
								<orgName type="institution" key="instit3">China ‡ 4Paradigm Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AutoSF: Searching Scoring Functions for Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scoring functions (SFs), which measure the plausibility of triplets in knowledge graph (KG), have become the crux of KG embedding. Lots of SFs, which target at capturing different kinds of relations in KGs, have been designed by humans in recent years. However, as relations can exhibit complex patterns that are hard to infer before training, none of them can consistently perform better than others on existing benchmark data sets. In this paper, inspired by the recent success of automated machine learning (AutoML), we propose to automatically design SFs (AutoSF) for distinct KGs by the AutoML techniques. However, it is non-trivial to explore domainspecific information here to make AutoSF efficient and effective. We firstly identify a unified representation over popularly used SFs, which helps to set up a search space for AutoSF. Then, we propose a greedy algorithm to search in such a space efficiently. The algorithm is further sped up by a filter and a predictor, which can avoid repeatedly training SFs with same expressive ability and help removing bad candidates during the search before model training. Finally, we perform extensive experiments on benchmark data sets. Results on link prediction and triplets classification show that the searched SFs by AutoSF, are KG dependent, new to the literature, and outperform the state-ofthe-art SFs designed by humans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Knowledge Graph (KG) <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>, as a special kind of graph structure with entities as nodes and relations as edges, is important to both data mining and machine learning, and has inspired various downstream applications, e.g., structured search <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>, question answering <ref type="bibr" target="#b24">[25]</ref> and recommendation <ref type="bibr" target="#b51">[52]</ref>. In KGs, each edge is represented as a triplet with form (head entity, relation, tail entity), denoted as (h, r, t). A fundamental issue is how to quantize the plausibility of triplets (h, r, t)s <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b39">[40]</ref>. KG embedding (KGE) has recently emerged and been developed as a promising method serving this purpose <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Basically, given a set of observed triplets, KGE attempts to learn low-dimensional vector representations of entities and relations so that the plausibility of triplets can be quantized. Scoring function (SF), which returns a score for (h, r, t) based on the embeddings, is used to measure the plausibility. Generally, SF is designed and chosen by humans and it has significant effects on embeddings' quality <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>Ever since the invention of KGE, many SFs have been proposed in the literature. Let the embedded vectors of h, r and t be h, r and t respectively. TransE <ref type="bibr" target="#b3">[4]</ref>, a representative embedding model, interprets the triplet (h, r, t) as a translation r from head entity h to tail entity t, i.e. the embeddings satisfy h + r = t. Variants of TransE like TransH <ref type="bibr" target="#b42">[43]</ref> and TransR <ref type="bibr" target="#b20">[21]</ref>, project the embedding vector into different spaces and enables the embedding to model relationships that are one-tomany, many-to-one or many-to-many. These models are categorized into translational distance models (TDMs). However, as proved in <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, TDMs are not fully expressive and their empirical performance is inferior to other models. RESCAL <ref type="bibr" target="#b29">[30]</ref>, DistMult <ref type="bibr" target="#b45">[46]</ref>, ComplEx <ref type="bibr" target="#b38">[39]</ref>, Analogy <ref type="bibr" target="#b23">[24]</ref> and more recently proposed SimplE <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, use a bilinear function h R t to model the plausibility of triplets, where R is a square matrix related to relation embedding. These models belong to the bilinear model (BLMs). Different BLMs use different constrains to regularize the relation matrix R in order to adapt to different datasets. Inspired by the success of deep networks <ref type="bibr" target="#b1">[2]</ref>, some neural network models (NNMs) have also been explored as SFs, e.g., MLP <ref type="bibr" target="#b6">[7]</ref>, NTM <ref type="bibr" target="#b33">[34]</ref>, Neural LP <ref type="bibr" target="#b46">[47]</ref> and ConvE <ref type="bibr" target="#b5">[6]</ref>. Even though neural networks are powerful and have strong expressive ability, the NNMs do not perform well in KGE domain because of not being well-regularized.</p><p>Among the existing SFs, BLM-based ones are the most powerful as indicated by both the state-of-the-art results <ref type="bibr" target="#b18">[19]</ref> and theoretical guarantees on expressiveness <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b40">[41]</ref>. However, since different KGs have distinct patterns in relations <ref type="bibr" target="#b31">[32]</ref>, a SF which adapts well to one KG may not perform consistently well on other KGs. Besides, designing new SFs to outperform state-of-the-art SFs is challenging. Therefore, how to choose and design a good SF for a certain KG is a non-trivial and difficult problem.</p><p>Recently, automated machine learning (AutoML) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b49">[50]</ref> has exhibited its power in many machine learning tasks and applications, e.g. model selection <ref type="bibr" target="#b12">[13]</ref>, image classification <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b50">[51]</ref> and recommendation <ref type="bibr" target="#b47">[48]</ref>. In order to select proper models and hyper-parameters for different tasks, hyperparameter optimization (HPO) has been proposed <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> to effectively and efficiently find better configurations, which previously requires great human efforts. Another hot trend in AutoML is to search better neural networks for deep learning models. Neural architecture search (NAS) <ref type="bibr" target="#b54">[55]</ref> has identified networks with fewer parameters and better performance than networks designed by humans.</p><p>Inspired by the success of AutoML, we aim to design better and novel data-dependent SFs for KGE. Specifically, we propose automated scoring function (AutoSF) which can automatically search an SF for a given KG. It can not only reduce human's effort in designing new SFs, but also make adaptation to different KGs. However, it is not easy to achieve the above goal. When applying AutoML, two important perspectives, i.e. search space, which helps to figure out important properties of the underling problems, and search algorithm, which determines the efficiency of finding better points in the space, need serious consideration. In this work, we have made the following contributions to achieve the goals: </p><formula xml:id="formula_0">• First,</formula><formula xml:id="formula_1">a diag(b) c, where diag(b) = D b ∈ R d×d is the diagonal matrix of b. We denote the complex vector v = v re +iv im ∈ C d with v re , v im ∈ R d . The conjugate of a complex vector is conj(v) = v re − iv im .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Knowledge graph embedding (KGE)</head><p>Given a set of observed (positive) triplets, the goal of KGE is to learn low-dimensional vector representations of entities and relations so that the plausibility measured by f (h, r, t) of observed triplets (h, r, t)s are maximized while those of nonobserved ones are minimized <ref type="bibr" target="#b39">[40]</ref>. To build a KGE model, the most important thing is to design and choose a proper SF f , which measures the triplets' plausibility based on embeddings. Since different SFs have different strengths and weaknesses, the choice of f is critical for the KGE's performance <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b39">[40]</ref>. A large amount of KGE models with popular SFs follow the same framework (Alg.1) <ref type="bibr" target="#b39">[40]</ref> using stochastic gradient descent. At step 5, negative triplets are sampled fromS <ref type="bibr">(h,r,t)</ref> , which contains all non-observed triplets for a current positive triplet (h, r, t), by some fixed distributions <ref type="bibr" target="#b42">[43]</ref> or dynamic sampling schemes <ref type="bibr" target="#b53">[54]</ref>. Next, the gradients are computed based on the given SF and embeddings, and are used to update the model parameters (step 6). Hinge loss <ref type="bibr" target="#b3">[4]</ref> and logistic loss <ref type="bibr" target="#b45">[46]</ref> are popularly used as . In this paper, we use the multiclass loss <ref type="bibr" target="#b18">[19]</ref> since it currently achieves the best performance and has little variance.</p><p>Algorithm 1 Stochastic training of KGE <ref type="bibr" target="#b39">[40]</ref>.</p><p>Input: training set S = {(h, r, t)}, scoring function f and loss function ; <ref type="bibr">1:</ref> initialize embeddings e, r for each e ∈ E and r ∈ R. 2: for i = 1, · · · , T do 3:</p><p>sample a mini-batch Sbatch ⊆ S of size m; <ref type="bibr">4:</ref> for each (h, r, t) ∈ Sbatch do <ref type="bibr">5:</ref> samplem negative tripletsS (h,r,t) ≡ {(hj, r,tj)} for the positive triplet (h, r, t); <ref type="bibr">6:</ref> update embedding parameters based on loss using selected positive and negative triplets; <ref type="bibr">7:</ref> end for 8: end for 9: return embeddings of entities in E and relations in R.</p><p>Existing human-designed SFs mainly fall into three types:</p><p>• Translational distance models (TDMs): The translational approach exploits the distance-based SFs. Inspired by the word analogy results in word embeddings <ref type="bibr" target="#b1">[2]</ref>, the plausibility is measured based on the distance between two entities, after a translation carried out by the relation. In TransE <ref type="bibr" target="#b3">[4]</ref>, the SF is defined by the (negative) distance between h + r and t, i.e. f (h, r, t) = −||h + r − t|| 1 . Other TDMs-based SFs, e.g., TransH <ref type="bibr" target="#b42">[43]</ref>, TransR <ref type="bibr" target="#b11">[12]</ref>, enhance over TransE by introducing extra mapping matrices.</p><p>• BiLinear models (BLMs): SFs in this group exploit the plausibility of a triplet by the product-based similarity. Generally, they share the form as f (h, r, t) = h Rt where R ∈ R d×d is a matrix referring to the embedding of relation r <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. RESCAL <ref type="bibr" target="#b29">[30]</ref> models the embedding of each relation by directly using R. DistMult <ref type="bibr" target="#b45">[46]</ref> overcomes the overfitting problem of RESCAL by constraining R to be diagonal. ComplEx <ref type="bibr" target="#b38">[39]</ref> allows R and h, t to be complex values, which enables handling asymmetric relations. HolE <ref type="bibr" target="#b28">[29]</ref> uses a circular correlation to replace the dot product operation, but is proven to be equivalent to ComplEx <ref type="bibr" target="#b15">[16]</ref>.</p><p>Other variants like Analogy <ref type="bibr" target="#b23">[24]</ref>, SimplE <ref type="bibr" target="#b17">[18]</ref> regularize the matrix R in different ways. output the probability of the triplets based on neural networks which take the entities' and relations' embeddings as inputs. MLP proposed in <ref type="bibr" target="#b6">[7]</ref> and NTN proposed in <ref type="bibr" target="#b33">[34]</ref> are representative neural models. Both of them use a large amount of parameters to combine entities' and relations' embeddings. ConvE <ref type="bibr" target="#b5">[6]</ref> takes advantage of convolutional neural network to increase the interaction among different dimensions of the embeddings. As proved in <ref type="bibr" target="#b40">[41]</ref>, TDMs have less expressive ability than BLMs, which further leads to their inferior empirical performance. Based on the power of deep networks, NNMs are also introduced for KGE. However, due to the huge model complexity and increasing difficulty of training, as well as the lack of domain-specific constraints, their performance is still worse than BLMs <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Therefore, we focus on BLMs in the sequel. The most representative BLMs are listed in Tab. I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Automated machine learning (AutoML)</head><p>Automated machine learning (AutoML) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b49">[50]</ref> has recently exhibited its power in easing the usage of and designing better machine learning models. Basically, AutoML can be regarded as a bi-level optimization problem where we need to update model parameters by the training data sets and tune hyper-parameters by the validation data sets. Regarding the success of AutoML, there are two important perspectives:</p><p>• Search space: This helps to figure out important properties of the underlying learning models and set up the search space for an AutoML problem. First, the space needs to be general enough to cover human wisdom as special cases. However, the space cannot be too general, otherwise searching in the space will be too expensive. • Search algorithm: Unlike convex optimization, there is no universal and efficient optimization tools. Once the search space is determined, efficient algorithms should be developed to search good points in the space. We take NAS and HPO as examples. The search space in NAS is spanned by network operations, e.g., convolution with different sizes, skip-connections. Various tailor-made algorithms, such as reinforcement learning <ref type="bibr" target="#b54">[55]</ref>, evolution algorithms <ref type="bibr" target="#b43">[44]</ref>, and one-shot algorithms <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b50">[51]</ref>, have been proposed for efficient optimization. For HPO, Bayesian optimization <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> is usually customized to search the space made up by the hyper-parameters of the learning tools.</p><p>This paper is the first step towards automated embedding of knowledge graphs. However, such a step is not trivial since previous AutoML methods used in NAS and HPO cannot be directly applied to KGE. The main problem is that we need to explore domain-specific properties in defining the search space and designing efficient search algorithm to achieve effectiveness with less cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE SEARCH PROBLEM</head><p>As mentioned in Sec.II, new designs of SFs have continuously boosted the performance of KGEs in recent years. However, there is no absolute winner among the humandesigned SFs. Besides, as different KGs usually exhibit distinct patterns in relations, how to choose a proper SF to achieve good performance is non-trivial. These raise one question: can we automatically design a SF for a given KG with good performance guarantee? In this part, we define AutoSF as a searching problem and make deep analysis on the search space based on KG properties to address the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AutoSF: Searching for SFs</head><p>Since SF is the crux to KGE and different KG has distinct properties, we are motivated to form the problem of designing new and better SFs as a searching problem. Specifically, we define it as follows:</p><p>Definition 1 (AutoML Problem). Let F (P ; g) be a KGE model (with indexed embeddings P = {h, r, t} and structure g), M (F (P ; g), S) measures the performance (the higher the better) of a KGE model F on a set of triplets S. The problem of searching the SF is formulated as:</p><formula xml:id="formula_2">g * ∈ arg max g∈G M (F (P * ; g), S val ) (1) s.t. P * = arg max P M(F (P ; g), S tra ),<label>(2)</label></formula><p>where G contains all possible choices of g, S tra and S val denote training and validation data sets.</p><p>Same as NAS <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b54">[55]</ref> and HPO <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, AutoSF is formulated as a bi-level optimization problem. We firstly need to train the model to obtain P * (converged model parameters) on the training set S tra by (2), and then search for a better g which is measured by the performance M on the validation set S val by <ref type="bibr" target="#b0">(1)</ref>. However, in this sequel we can see the search space of g and search strategy in AutoSF are fundamentally different from previous AutoML works. They are closely related to KGE's domain and new to the AutoML literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Search space: a unified representation of BLM SFs</head><p>To solve the AutoSF problem, the first question is: what is a good search space G? As discussed in Sec.II-B, the space can neither be too specific nor too general. To motivate a good  search space, let us look at some commonly used SFs (Tab. I) and dig out what are important properties of g.</p><p>As discussed in Sec.II-A, the state-of-the-art performance of KGE models is achieved by BLMs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, thus we limit our scope to them. RESCAL <ref type="bibr" target="#b29">[30]</ref> is not considered since it does not have good scalability <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b38">[39]</ref> and neither empirically perform well. The other models in Tab. I regularize the number of trainable parameters of square matrix R ∈ R d×d to be the same as entity embedding dimensions. Therefore, we constrain the relation embedding size to be the same as the entity's and learn different ways of mapping the relation embedding r ∈ R d into a square matrix R ∈ R d×d . Besides, as the summary of relation types in Tab. I, important properties are symmetric, anti-symmetric, asymmetric and inverse. These are important properties of good SFs. Thus, a search space should be able to handle the symmetric related properties. In addition, as will be discussed in Remark III.1, different SFs in BLMs differ in their way of regularizing the square matrix R. Therefore, we are motivated to adaptively search how to regularize the relational matrix on different KGs.</p><p>To motivate such a space, we can see that there are two main differences among these SFs.</p><p>• The embedding can be either real or complex, e.g. Dist-</p><p>Mult v.s. ComplEx. • When embedding vectors are split, different SFs combine them in distinct manners, e.g., Analogy v.s. SimplE. <ref type="bibr" target="#b38">[39]</ref>. Let the complex embedding h = h re +ih im , where h re , h im ∈ R d (same for r, t), then ComplEx can be expressed as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Dealing with complex embeddings: A complex vector</head><formula xml:id="formula_3">v ∈ C d with v = v re +iv im is composed of a real part v re ∈ R d and an imaginary part v im ∈ R d . To deal with the complex embeddings, we can use 2d-dimensional real vector [v re ,v im ] to represent the d-dimensional complex vector v [1],</formula><formula xml:id="formula_4">Re ( h, r, conj(t) ) = h re , r re , t re + h im , r re , t im + h re , r im , t im − h im , r im , t re . (3)</formula><p>Similarly, DistMult <ref type="bibr" target="#b45">[46]</ref> with 2d-dimensional embeddings can also be denoted by [v re , v im ] and represented as two parts</p><formula xml:id="formula_5">h, r, t = h re , r re , t re + h im , r im , t im .<label>(4)</label></formula><p>2) Dealing with different splits: To make the training parameters consistent, we also use 2d-dimensional real valued embeddings to represent Analogy and SimplE. As given in Tab. I, embeddings in Analogy <ref type="bibr" target="#b23">[24]</ref> are split into a real part h ∈ R d , and a complex parth, which can be denoted as a concatenated real vector [h re ,h im ] ∈ R d in the similar way as ComplEx. And the SF is split as</p><formula xml:id="formula_6">ĥ ,r,t + Re h ,ȓ, conj(t) .<label>(5)</label></formula><p>In SimplE <ref type="bibr" target="#b17">[18]</ref>, two independent embedding vectorsĥ ∈ R d andh ∈ R d are used to represent each entity and relation. The resulting SF becomes</p><formula xml:id="formula_7">ĥ ,r,t + h ,ȓ,t .<label>(6)</label></formula><p>3) The unified representation: In order to deal with the two different partitions, i.e. ComplEx v.s. DistMult and Analogy v.s. SimplE, we split embedding h ∈ R d as h = [h 1 ; h 2 ; h 3 ; h 4 ] (same for r and t) to cover <ref type="formula">(3)</ref>, <ref type="formula" target="#formula_5">(4)</ref>, <ref type="formula" target="#formula_6">(5)</ref> and <ref type="bibr" target="#b5">(6)</ref>. Note that any splits k (with k ≥ 4 and k is even) can be used to cover the SFs in Tab. I. We take k = 4 in order to ensure a tractable search space. The transformation of each SF is then summarized as:</p><formula xml:id="formula_8">DistMult: f (h,r,t)= h1,r1,t1 + h2,r2,t2 + h3,r3,t3 + h4,r4,t4 , ComplEx: f (h,r,t)= h1,r1,t1 + h1,r3,t3 + h3,r1,t3 − h3,r3,t1 + h2,r2,t2 + h2,r4,t4 + h4,r2,t4 − h4,r4,t2</formula><p>, Analogy: f (h,r,t)= h1,r1,t1 + h2,r2,t2 + h3,r3,t3 + h3,r4,t4 + h4,r3,t4 − h4,r4,t3 , SimplE : f (h,r,t)= h1,r1,t3 + h2,r2,t4 + h3,r3,t1 + h4,r4,t2 .</p><p>Based on above formulations, all the scoring functions can be formed as f (h, r, t) = h Rt. Let D r i = diag(r i ) for i ∈ {1,2,3,4}, the forms of R for these SFs can be graphically represented as <ref type="figure" target="#fig_1">Fig. 1</ref>. In this way, we can see that the main difference between the four SFs is their way of filling the 4×4 block matrix (see <ref type="figure" target="#fig_1">Fig. 1(e)</ref>). Based on such a pattern, we identify the search space of BLM-based SFs in Definition 2.</p><p>Definition 2 (Search space G). Let g (r) return a 4 × 4 block matrix, of which the elements in each block is given by</p><formula xml:id="formula_9">[g (r)] ij = diag(a ij ) where a ij ∈ {0, ±r 1 , ±r 2 , ±r 3 , ±r 4 } for i, j ∈ {1, 2, 3, 4}.</formula><p>Then, SFs can be represented by</p><formula xml:id="formula_10">f unified (h, r, t) = i,j h i , a ij , t j = h g (r) t.</formula><p>Remark III.1 (Searching to regularize BLMs). Note that the SFs shown in <ref type="figure">Fig. 5</ref> constrain the relation matrix R in different forms, which can be regarded as different regularization schemes. Viewed in this way, AutoSF aims to search how to regularize the relational matrix that can adapt to different relation properties in different KGs. In addition, the data dependent regularization cannot be easily formed as a  <ref type="bibr" target="#b45">[46]</ref> f (t, r, h) = f (h, r, t) g(r) = g(r) IsSimilarTo, Spouse anti-symmetric <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b38">[39]</ref> f</p><formula xml:id="formula_11">(t, r, h) = −f (h, r, t) g(r) = −g(r)</formula><p>LargerThan, Hypernym general asymmetric <ref type="bibr" target="#b23">[24]</ref> f (t, r, h) = f (h, r, t) g(r) = g(r)</p><p>LocatedIn, Profession inverse <ref type="bibr" target="#b17">[18]</ref> f (t, r, h) = f (h, r , t), r = r g(r) = g(r ) Hypernym, Hyponym constraint in training procedure, which motivates us to use AutoML to search based on validation sets performance.</p><p>Remark III.2 (General Search Space). Due to the recent success of deep networks <ref type="bibr" target="#b14">[15]</ref> and the approximation ability of multilayer perceptron (MLP) <ref type="bibr" target="#b4">[5]</ref>, one may want to use a MLP as F for <ref type="formula" target="#formula_2">(2)</ref>. However, the design of the MLP is also a searching problem, which is very time-consuming <ref type="bibr" target="#b54">[55]</ref>.</p><p>Besides, an arbitrarily large MLP will lead to an extremely large space. As verified in <ref type="bibr" target="#b48">[49]</ref>, the general approximator MLP is a bad choice for NAS and performs worse than those using reinforcement learning <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE SEARCH STRATEGY</head><p>Here, we propose an efficient search strategy to address the AutoSF problem based on domain-specific properties in KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Challenges in algorithm design</head><p>Same as the other AutoML problems, the search problem of AutoSF is black-box, the search space is huge, and each step in the search is very expensive since both model training and evaluation should be involved. These problems have previously been touched by algorithms such as reinforcement learning <ref type="bibr" target="#b54">[55]</ref>, Bayes optimization <ref type="bibr" target="#b12">[13]</ref> and genetic programming <ref type="bibr" target="#b43">[44]</ref>. However, they are not a good choice here since we have domain-specific problems, i.e. expressiveness and invariance, in KGE, which are more challenging.</p><p>1) Expressiveness: It is clear that not all SFs g ∈ G (from Definition 2) are equally good. Expressiveness <ref type="figure">(Definition 3)</ref>, which means f should be better able to handle common relations in KGs , is of big concern for SFs. Their consequent requirements on f and g(r) are summarized in Tab. II.</p><p>Definition 3 (Expressiveness <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>). If f can handle symmetric, anti-symmetric, general asymmetric, and inverse relations, then f is expressive.</p><p>To ensure that f can handle those common relations, we propose Proposition 1. Proposition 1. If g(r) can be symmetric for some r ∈ R d , i.e. g(r) = g(r), and skew-symmetric for some r ∈ R d , i.e. g(r ) = −g(r ). Then the formulated f is expressive. (Proofs in Appendix A).</p><p>With this Proposition and to avoid trivial solutions, we introduce the following constraints on g: (C1). g(r) can be symmetric with proper r and skewsymmetric with proper r . (C2). g(r) has no zero rows/columns, covers all r 1 to r 4 , and has no repeated rows/columns. For (C1), the symmetric property of g(r) determines what kind of relation the given SF can model based on Proposition 1. For (C2), if there are zero rows/columns in g, the corresponding embedding dimensions will be useless. It means that these dimensions will never be optimized during training. The above constraints are important for finding potentially good candidate g ∈ G, and they play a key role in filtering out the bad g's for the design of an efficient search algorithm. As in Definition 3 and Proposition 1, we need to deal with Constraint (C1) for expressiveness. It is challenging since g only represents a structure, however the exact check of (C1) relies on the values in r, which are unknown in advance. Fortunately, we can check it by value assignment. Take the SF in <ref type="figure" target="#fig_3">Fig. 2(a)</ref> for example. We can see that g(r) can be symmetric by assigning r 3 = r 1 and r 4 = r 2 as in <ref type="figure" target="#fig_3">Fig. 2(b)</ref>, and skew-symmetric by setting r 3 = −r 2 and r 4 = −r 1 like in <ref type="figure" target="#fig_3">Fig. 2(c)</ref>. This is the key idea of addressing expressiveness.</p><p>2) Invariance: As defined in Sec.III-B, the embeddings are split into 4 parts, i.e. r = [r 1 ; r 2 ; r 3 ; r 4 ]. Prior to the embedding training, permuting the r i 's will lead to equivalent structure since"1,2,3,4" here are only identity of each component and these components are equivalent at this stage. For example, we can permute r = [r 1 ; r 2 ; r 3 ; r 4 ] into r = [r 2 ; r 1 ; r 3 ; r 4 ]. Even though r 1 and r 2 change their position, the learned embedding could be the same by changing corresponding values after training. Therefore, the structure of SFs is invariance to permutation of r i 's. Similarly, since h and t share the same embedding parameters e, the generated SFs are also equivalent by simultaneously permuting the h i 's and t i 's. Moreover, if we flip the signs of some r i , we can learn equal embeddings by flipping the true value of those r i after training. In summary, there exist three kinds of invariance: permuting entity embedding h i 's and t i 's, permuting relation embedding r i 's, and flipping signs. The example of three cases are given in <ref type="figure" target="#fig_3">Fig. 2(d-f</ref>). Let h, r, t be the embeddings of the SF g 1 andh,r,t be the embedding of another SF g 2 , and g 2 is formed through the invariance changes of g 1 . Then we will have h g 1 (r)t =h g 2 (r)t after the model training. Therefore, it is tedious to train and evaluate the equivalents once we know the performance of one SF among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Progressive greedy search</head><p>As in Sec.III-B, adding one more block into g indicates adding one more nonzero multiplicative term into f , namely</p><formula xml:id="formula_12">f b+1 = f b + s h i , r j , t k ,<label>(7)</label></formula><p>where s ∈ {±1} and i, j, k ∈ {1, 2, 3, 4}. In order to search efficiently, we propose a progressive greedy algorithm based on the inductive rule <ref type="bibr" target="#b6">(7)</ref>, which can significantly cut down the  search space in a stage-wise manner. The intuition of using <ref type="bibr" target="#b6">(7)</ref> to progressively generate SFs it to gradually adjust the relation matrix g(r). However, greedy search usually leads to sub-optimal solutions <ref type="bibr" target="#b37">[38]</ref>, which can be more serious when faced with the expressive and invariance challenges in AutoSF. Therefore, we enhance the greedy search with a filter and a predictor to specifically deal with the expressiveness and invariance discussed in Sec.IV-A. </p><formula xml:id="formula_13">f b ← f b−2 + s1 hi 1 , rj 1 , t k 1 + s2 hi 2 , rj 2 , t k 2 ; 5: if f b satisfies filter Q then H b ← H b ∪ f b ; 6: until H b = N 7:</formula><p>select top-K2 f b s in H b based on the predictor P; <ref type="bibr">8:</ref> train embeddings with the selected f b s; <ref type="bibr">9:</ref> evaluate the obtained embedding from the selected f b s; <ref type="bibr">10:</ref> T b ← add and record f b and their performance; <ref type="bibr">11:</ref> update the predictor P with records in T . 12: end for 13: return desired SFs in T B . 1) Complete procedures: Alg.2 shows our progressive greedy algorithm. As in Definition 2, let the number of nonzero blocks in g be B and the SF in this group be f B . The idea of progressive search is that given the desired B, we start from small blocks b and then gradually add in more blocks until b = B. Thus, we can greedily generate candidates based on the top SFs in T b−2 at step 2-6 to reduce search space. Specifically, we greedily pick up the top-K 1 f b−2 in the previously evaluated models in T b−2 . N candidates will then be generated by adding two more multiplicative term in step 4 to deal with Constraint (C1) since adding one block each step will result in simply lying on the diagonal. All the candidates are generated from b = 4 and are checked through the Filter Q (see Sec.IV-B2) to guarantee Constraint (C2) and avoid training equivalents. Next, we use the predictor P (see Sec.IV-B3) to further select K 2 promising candidates, which will then be trained and evaluated using Alg.1, in step 7 of Alg.2. The training data for P is gradually collected with the trained SFs in T = T 4 ∪ T 6 ∪ · · · at step 10.</p><p>2) Invariance -Using a filter: The filter Q we used in Alg.2 has two functions: 1) deal with Constraint (C2) and 2) remove equivalent structures due to invariance. Constraint (C2) is easy to check, given the structure of g, we can directly map it into a 4×4 substitute matrix and use {0, ±1, ±2, ±2, ±4} to represent [g(r)] ij ∈ {0, ±r 1 , ±r 2 , ±r 3 , ±r 4 }. Then checking requirements in (C2) is a trivial task, i.e. checking if the 4 × 4 substitute matrix satisfies the Constraint (C2).</p><p>For the invariance, once a candidate f b fulfilling Constraint (C2) is generated, we use the invariance property to generate a set of equivalents G f b . Specifically, we can permute the entity parts, relation parts, or flip signs to get</p><formula xml:id="formula_14">4! × 4! × 2 4 = 9, 216 equivalents of f b . If G f b ∩ H b ∩ T b = ∅,</formula><p>we throw f b away since there are equivalent structures in the sampled set H b and history record T b . This step can dramatically help us to reduce the cost in training equivalent structures. Take f 4 as an example, the whole space is reduced from A 4 16 ×2 4 to 5 through the filter, namely there are only five good and unique candidates in f 4 . Besides, we add exception for the condition in step 5 of Alg.2 for f 4 since the number of candidates is smaller than N .</p><p>3) Expressiveness -Constructing a predictor: Even though the filter helps to throw away many unpromising candidates, it does not deal with Constraint (C1). Hence, after collecting N candidates, we use the predictor P to further select K 2 promising ones among them. Considering that the performance of SFs on a specific KG is closely related to how the SF is formed, we can use a learning model, i.e. the predictor P to predict the performance and select good candidates in advance. In general, we need to extract features for points which have been visited by the search algorithm, and then use a learning model to predict validation performance based on those features <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The following are principles a good predictor needs to meet (P1). Correlate well with true performance: the predictor needs not to accurately predict the exact values of validation performance, instead it should rank good candidates over bad ones; (P2). Learn from small samples: as the real performance of each point in the search space is expensive to acquire, the complexity of the predictor should be low so that it can learn from few samples. Based on Principle (P1), the extracted features from g should be closely related to the quality of defined SF. Meanwhile, the features should be cheap to construct, i.e. they should not depend on values of r, which are unknown before training. For (P2), the number of features should be small to guarantee a simple predictor. Therefore, we are motivated to design the symmetry-related features (SRFs), which can effectively capture to what extent g(r) can be symmetric or skew-symmetric (Proposition 2), and has low complexity.</p><p>Similar as the filter, we also use a 4 × 4 substitute matrix to represent g. As in <ref type="figure">Fig. 3, we use</ref> </p><formula xml:id="formula_15">v = [v 1 ; v 2 ; v 3 ; v 4 ]</formula><p>to represent [r 1 ; r 2 ; r 3 ; r 4 ], then the symmetric and skewsymmetric property of g can be checked through g(v)−g(v) and g(v) + g(v) . Since g(v) is a simple 4 × 4 matrix, the checking procedure is very cheap. Then by assigning different values to v (details in Appendix C), a 22-dimensional SRF will be returned. Considering that the correlation of SRFs with SFs' performance is guaranteed under Proposition 2, we can use a simple two-layer MLP (22-2-1) as the predictor P. Other regression models with low complexity may also work here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Search complexity analysis</head><p>There are 16 blocks and each can be filled with 9 different contents {0, ±D r 1 , ±D r 2 , ±D r 3 , ±D r 4 }. Thus the whole space size is 9 16 , which is extremely large. The greedy strategy, predictor and filter cut down the space in different perspectives. Specifically, in each greedy step:</p><formula xml:id="formula_16">• Greedy: Considering that f b is progressively generated on f b−2 for b = 6, 8, . . . , there can be C 2 16−(b−2) × 4 2 × 2 2 candidates (C 2 16−(b−2)</formula><p>is to choose location, 4 2 is for the two r i s and 2 2 is for signs). In comparison, there can be</p><formula xml:id="formula_17">C b 16 × 4 b × 2 b</formula><p>possible SFs in f b . Take b = 6 for example, there are 2 × 10 9 possible candidates. Since f 6 is generated based on the 5 good candidates in f 4 , we reduce the space size from 2 × 10 9 to approximately 3 × 10 4 based on the greedy scheme.</p><p>• Filter: The filter we designed is mainly used to deal with invariance properties. Permuting r i 's leads to 4! = 24 equivalent structures. Simultaneously permuting h i 's and t i 's also gives 24 equivalents. Besides, there are 2 4 = 16 possible signs patterns. Therefore, given a g(r), we can generate at most (there may exist same structures in this set) 24 × 24 × 16 = 9216 equivalent SFs, which should perform the same. Besides, by constraining the SF under Constraint (C2), many bad candidates can also be filtered out. Take f 4 as an example, only 5 candidates are selected to be trained among approximately 700k possible structures.</p><p>• Predictor: Once N candidates are generated, the predictor will select K 2 ones based on their predicted performance. Thus, the reducing ratio of predictor is about N/K 2 . While it is difficult to directly quantize to which extend the three steps together can help to reduce the search space, we can observe the significance of efficiency gained through each component. Besides, we perform an empirical study in Sec.V-E to show the performance gaining of these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with existing AutoML approaches</head><p>The most related work in the AutoML literature is PNAS <ref type="bibr" target="#b21">[22]</ref>, which combines a greedy algorithm with a performance predictor to search a cell structure for the convolutional neural network (CNN). However, the filter is not used in PNAS as the search space for AutoSF is fundamentally different from that of CNN. Besides, PNAS adopts direct one-hot encoding for the predictor, which has a bad empirical performance here (see Sec.V-E1). As for the other AutoML approaches, even though the search problem of AutoSF is similarly defined as HPO <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref> and NAS <ref type="bibr" target="#b9">[10]</ref>, the search space and search algorithm of AutoSF are novel and specifically designed for KGE. There is no direct way for them to deal with the challenges in Sec.IV-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EMPIRICAL STUDY</head><p>All of the algorithms are written in python with PyTorch framework <ref type="bibr" target="#b30">[31]</ref>. Experiments are run on 8 TITAN Xp GPUs.</p><p>A. Experiment setup 1) Datasets: Five data sets, i.e. WN18, FB15k, WN18RR, FB15k237 and YAGO3-10 are considered (statistics in Tab. III). WN18RR and FB15k237 are variants that remove near-duplicate or inverse-duplicate relations from WN18 and FB15k respectively, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b41">[42]</ref>. YAGO3-10 is much larger than the others. These are benchmark datasets, and are popularly used to compare KGE models in the literature <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The number of symmetric, anti-symmetric, inverse pairs and general asymmetric are computed in the following way: Given a relation r, let the number of positive triplets (h, r, t) be n r . (i) If the number of (t, r, h) is larger than 0.9n r , then we regard it as symmetric; (ii) If the number of (t, r, h) is zero and the size of joint set of h and t is at least 0.1n r (this is to ensure that they have same type), we regard as anti-symmetric; (iii) If there exist another relation r that has at least 0.9n r (t, r , h), then r and r are inverse pairs; (iv) others are regarded as general asymmetric. The threshold 0.9 and 0.1 are hand-made and just used to roughly (other values are fine) indicate the relation properties for each data set.</p><p>2) Hyper-parameters: Since the searched embedding models belong to BLMs, we fairly compare different SFs with a fixed set of hyper-parameters. In order to reduce training time, we set the dimension d as 64 during the search procedure. First, we use SimplE <ref type="bibr" target="#b17">[18]</ref> as the benchmark model and  <ref type="table" target="#tab_1">,943  11  86,835  3,034  3,134  4  3  1  3  FB15k237 [37]  14,541  237  272,115  17,535 20,466  33  5  20  179  YAGO3-10 [26]  123,188  37  1,079,040  5,000  5,000  8  0  1  28   TABLE IV  COMPARISON OF THE BEST SF IDENTIFIED BY AUTOSF AND THE STATE-OF-THE-ART SFS. THE BOLD NUMBER MEANS THE BEST PERFORMANCE, AND  THE UNDERLINE MEANS THE SECOND BEST</ref> Besides, we use Adagrad <ref type="bibr" target="#b7">[8]</ref> as the optimizer since it tends to perform better as indicated in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Once a good hyperparameter configuration is selected, we use it to train and evaluate different searched SFs. After the search procedure, we pick up the best SF evaluated by the MRR performance on the validation data set as the searched SF. When comparing the searched SFs with human-designed ones, we increase the dimension from 64 to d ∈ {256, 512, 1024, 2048} as in <ref type="bibr" target="#b18">[19]</ref>. As mentioned in <ref type="bibr" target="#b41">[42]</ref>, KGE models are sensitive to hyperparameters. For a fair comparison, we use the same set of hyper-parameters to train and evaluated different models on each dataset.</p><p>3) Meta Hyper-parameters: The hyper-parameters K 1 , K 2 and N have little influence to the search procedure. We use K 1 = K 2 = 8 and N = 256 for all data sets. Besides, steps 2-11 in Alg.2 run based on an inner loop. We train 8 models in parallel and iterate for 32 times (16 times for YAGO3-10), namely we evaluate 256 f b s for each b &gt; 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with existing SFs on link prediction</head><p>We compare our AutoSF with the state-of-the-art KGE models discussed in Sec.II-A, which are designed by humans, i.e. TransE <ref type="bibr" target="#b3">[4]</ref>, TransH <ref type="bibr" target="#b42">[43]</ref>, and RotatE <ref type="bibr" target="#b34">[35]</ref> from TDMs; NTM <ref type="bibr" target="#b33">[34]</ref>, Neural LP <ref type="bibr" target="#b46">[47]</ref>, and ConvE <ref type="bibr" target="#b5">[6]</ref> from NNMs; TuckER <ref type="bibr" target="#b0">[1]</ref>, HolE/HolEX <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b44">[45]</ref>, Quat <ref type="bibr" target="#b52">[53]</ref>, DistMult <ref type="bibr" target="#b45">[46]</ref>, ComplEx <ref type="bibr" target="#b38">[39]</ref>, Analogy <ref type="bibr" target="#b23">[24]</ref> and SimplE <ref type="bibr" target="#b17">[18]</ref> from BLMs; and a rule-based method AnyBURL <ref type="bibr" target="#b26">[27]</ref>. Hyper-parameters are selected by the MRR value on the validation set.</p><p>Following <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref>, we test KGE's performance based on link prediction. For each triplet (h, r, t) ∈ S, where S is the validation or testing set, we compute the score of (h , r, t) for all h ∈ E and get the rank of h, the same for t based on scores of (h, r, t ) over all t ∈ E, r is not compared as in the literature <ref type="bibr" target="#b39">[40]</ref>. Same as above mentioned papers, we adopt the following metrics: (i) Mean reciprocal ranking</p><formula xml:id="formula_18">(MRR): 1 /|S| |S| i=1 1 /rank i , where rank i , i ∈ {1, .</formula><p>. . , |S|} is a set of ranking results and (ii) H@10:</p><formula xml:id="formula_19">1 /|S| |S| i=1 I (ranki &lt; 10),</formula><p>where I(·) is the indicator function. We report the performance in a "filtered" setting as in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b42">[43]</ref>, where larger MRR and H@10 indicate higher embedding quality. 1) Effectiveness: A comparison of the testing performance of AutoSF and the current state-of-the-art SFs are shown in Tab. IV. Firstly, we can see that there is no absolute winner among the baseline SFs. For example, TuckER is the best on WN18, but is the worst among human-designed BLMs on FB15k. DistMult generally performs worse on the benchmarks except for FB15k237 since it does not follow Proposition 1. A single model is hard to adapt to different KGs. However, AutoSF performs consistently well among these five data sets. i.e. the best among FB15k, WN18RR, FB15k237 and YAGO3-10, and the runner-up on WN18.</p><p>Besides, we plot the learning curves of DistMult, Analogy, ComplEx, SimplE and the best SF searched by AutoSF in <ref type="figure" target="#fig_5">Fig. 4</ref>. As shown, the searched SFs not only outperform baselines, but also converge faster, which may due to these SFs can better capture relations in these datasets. 2) Case study: Distinctiveness: To show the searched SFs are KG-dependent and novel to the literature, we plot them in <ref type="figure">Fig. 5</ref>. It is obvious that these SFs are different from each other, and they are not equivalent regarding invariance properties. As shown in Tab. III, WN18 and FB15k have many symmetric, anti-symmetric relations and inverse relation pairs, the best SF searched on them are very similar and have the same SRF. The other three data sets are more realistic and contains less symmetric, anti-symmetric and inverse relations, thus have different SRFs with fewer entry being non-zero.</p><p>The most special case is FB15k237, which can only be symmetric under (S11). Viewing the values in Tab. IV, we can see that the leading performance on FB15k237 is achieved by DistMult and AutoSF, both of which cannot be skewsymmetric. As given in the statistic information in Tab. III, FB15k237 has relatively fewer anti-symmetric relations. This may explain why skew-symmetric is not that important for g(r). However, SRFs still work for these cases since it can be aware that skew-symmetric property is not that essential and focus more on searching different local structures. Besides, we pick up the best SF searched from one data set and test it on another data set in Tab. V. We can readily find that these SFs get the best performance on the data sets where they are searched. This again demonstrate that SFs found by AutoSF on different KGs are distinct from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with existing SFs on triplet classification</head><p>To further demonstrate the effectiveness of the searched SFs, we do triplet classification as in <ref type="bibr" target="#b42">[43]</ref>. This task is to confirm whether a given (h, r, t) is correct or not and is more helpful in answering yes-or-no questions. The decision rule of classification is as follows: for each (h, r, t), if its score is larger than the relation-specific threshold σ r , which we predict to be positive, otherwise negative. The threshold σ r is determined by maximizing the accuracy of the validation set. We test this task on FB15k, WN18RR and FB15k237, in which the positive and negative triplets are provided. As shown in Tab. VI, searched SFs consistently outperform humandesigned BLMs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with other AutoML approaches</head><p>In this part, we compare AutoSF with the other search algorithms. WN18RR and FB15k237 are used here, and all algorithms share the same set of hyper-parameters. First, to show the effectiveness of the search space in BLM, we train a general approximator (Gen-Approx), i.e. MLP (in Appendix D), on the validation set. Then, AutoSF is compared with Random search and Bayes algorithm [3] on f 6 . As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, the general approximator performs much worse than BLM since it is too flexible to consider domain-specific constraints and easily overfits. For BLM settings, the Bayes algorithm can improve the efficiency upon random search. However, it will easily fall into local optimum and does not take the domain property into account. Among them, AutoSF is the most efficient and has the best any-time performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation study</head><p>We use WN18RR and FB15k237 to illustrate the importance of different components in the proposed searching algorithm.</p><p>1) Filter and predictor: To show the effectiveness of the filter and predictor, we remove them from AutoSF and make comparisons in <ref type="figure">Fig. 7</ref>. As shown, the greedy algorithm is more efficient than random search. Both filter and predictor are important. Removing either the filter or predictor will lead to degenerated efficiency. Besides, compared with Greedy, i.e. no filter and no predictor, they can both improve efficiency through reducing the search space. <ref type="figure">Fig. 7</ref>. Ablation study on the predictor and filter in AutoSF.</p><p>2) SRF features: As in Sec.V-D, one-hot representation <ref type="bibr" target="#b21">[22]</ref> can also be used as an alternative to SRFs. We compare the two kind of features in <ref type="figure">Fig. 8</ref>. For AutoSF (with one-hot), a 96-8-1 fully connected neural network is used, and a 22-2-1 network is used for AutoSF (with SRF). AutoSF (no predictor) is shown here as a baseline and it is the same as that in <ref type="figure">Fig. 7</ref>. <ref type="figure">Fig. 8</ref>. Comparison of the proposed SRF with one-hot encoding in <ref type="bibr" target="#b21">[22]</ref>.</p><p>3) Sensitivity of meta hyper-parameters: There are three meta hyper-parameters N, K 1 , K 2 used in our search Alg.2. The results we reported in previous parts are based on N = 256, K 1 = 8, K 2 = 8. We change the value of N to 128 and 512, K 2 to 4 and 16, and show the searching curve on f 6 in <ref type="figure" target="#fig_7">Fig. 9</ref>. The parameter K 1 that selects top candidates in f b−2 is not compared for b = 6 since there are only 5 candidates in f 4 . As can be seen, all the different settings perform similar and obviously outperform the Greedy baseline. 4) Running time analysis: We show the running time of different components in AutoSF in Tab. VII. First, the filter and the predictor (including SRF computation) take much shorter running time compared with that of the model training. Then, as each greedy step contains 256 model training, the best SFs can be searched within only several hours (on 8 GPUs), except for YAGO3-10 which takes more than one day to evaluate 128 candidates. In comparison, search problem based on reinforcement learning <ref type="bibr" target="#b54">[55]</ref> runs over 4 days across 500 GPUs; genetic programming <ref type="bibr" target="#b43">[44]</ref> takes 17 days on single GPU; and Bayes optimization <ref type="bibr" target="#b12">[13]</ref> trains for several days on CPUs. Thus, the proposed AutoSF makes the search problem on KGE tractable, and it is very efficient in AutoML literature. In addition, since AutoSF search SFs with dimension 64 and then fine-tune the hyper-parameters with d ∈ {256, 512, 1024, 2048} as in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In comparison, the searching cost is comparable with the fine-tune cost which generally needs to train and evaluate hundreds of hyper-parameter settings with large dimension size. In this view, the searching cost is not that expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose AutoSF, an algorithm to automatically design and discover better SFs for KGE. By using a progressive greedy search algorithm enhanced by a filter and a predictor with domain-specific knowledge, AutoSF can efficiently design promising SFs that are KG dependent, new to the literature, and outperform the state-of-the-art SFs designed by humans from the huge search space. In future work, a promising direction is to explore how to efficiently search the network structure for NNMs under domain-specific constraints. The greedy algorithm used in AutoSF somehow limits the exploration in the search space, which is also a potential problem to be addressed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>A graphical illustration of R for existing SFs in Tab. I and the search space of AutoSF. Blank space is for zero matrix and D r i = diag(r i ) , i = 1 . . . 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>skew-sym. (d) permute h i and t i . (e) permute r i . (f) flip sign.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of Invariance and Expressiveness. (a) SimplE model; (b) assign r 3 = r 1 , r 4 = r 2 ; (c) assign r 3 = −r 1 , r 4 = −r 2 ; (d) permute [h 1 ; h 2 ; h 3 ; h 4 ] into [h 1 ; h 3 ; h 2 ; h 4 ] and do the same for t; (e) permute [r 1 ; r 2 ; r 3 ; r 4 ] into [r 1 ; r 4 ; r 2 ; r 3 ]; (f) flip the signs of r 2 and r 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .Proposition 2 .</head><label>32</label><figDesc>Example of generating a feature of SRF. The extracted SRFs (see Appendix C) are (i) invariant to both the permutations and flipping signs of blocks in R and (ii) give predictions related to symmetric or antisymmetric properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison on clock time (in hours) of model training v.s testing MRR between search SFs (by AutoSF) and human-designed ones. YAGO3-10. Fig. 5. A graphical illustration of SFs identified by our AutoSF on each data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of AutoSF with other AutoML approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Comparison of different meta hyper-parameters. Greedy is added here as a contrast. 256 models are evaluated in each setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>we make an important observation over existing SFs, which allows us to represent the BLM-based SFs in a unified form. Based on the unified representation, we formulate the design of SFs as an AutoML problem (i.e. AutoSF), and set up the corresponding search space. The space is not only specific enough to cover good SFs designed by humans, but also general enough to include novel SFs not visited in the literature.</figDesc><table /><note>• Second, we observe it is common that different KGs have distinct properties on relations that are symmetric, asym- metric, inverse, etc. This inspires us to conduct domain- specific analysis on the KGE models, and design constraints to effectively guide subsequent searches in the space.• Third, we propose a progressive greedy algorithm to search through such a space. We further build a filter to avoid train- ing redundant SFs and a predictor with specifically designed symmetry-related features (SRF) to select promising SFs. The search algorithm can significantly reduce the search space size by capturing the domain specific properties of candidate SFs.• Finally, experimental results on five popular benchmarks on link prediction and triplet classification tasks demonstrate that the SFs searched by AutoSF outperform the start-of- the-art SFs designed by humans. In addition, the searched SFs are KG dependent and new to the literature. We further conduct case study on the searched SFs to provide means for analyzing KGs, which can inspire better understanding of embedding techniques for future researches. Notations. We denote vectors by lowercase boldface, and matrix by uppercase boldface. A KG contains a set of triplets S = {(h, r, t)} with h, t ∈ E and r ∈ R, where E and R are the set of entities and relations, respectively. For simplicity, the embeddings are represented by letters of indices in boldface, e.g. h, r, t are embeddings of h, r, t, respectively, and h, t share the same set of embedding parameters e. a, b, c =d i=1 a i ·b i ·c i is the triple dot product and can be alternatively represented as</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EXISTING</head><label>I</label><figDesc>SFS COVERED BY OUR SEARCH SPACE. FOR ANALOGY AND SIMPLE, THE EMBEDDING SPLITS INTO TWO PARTS, I.E. h = [ĥ ,h ] AND d =d +d (SAME FOR r AND t). THE RELATION TYPES ARE SUMMARIZED IN TAB. II.</figDesc><table><row><cell>scoring function</cell><cell>embeddings</cell><cell>definition</cell><cell>relation types that can model</cell></row><row><cell>DistMult [46]</cell><cell>symmetric h, r, t ∈ R d</cell><cell>h, r, t</cell><cell>symmetric</cell></row><row><cell>ComplEx [39] / HolE [29]</cell><cell>h, r, t ∈ C d</cell><cell>Re( h, r, conj(t) )</cell><cell>symmetric, anti-symmetric, asymmetric, inverse</cell></row><row><cell cols="2">Analogy [24]ĥ,r,t ∈ Rd,h,ȓ,t ∈ Cd</cell><cell>ĥ ,r,t + Re h ,ȓ, conj(t)</cell><cell>symmetric, anti-symmetric, asymmetric, inverse</cell></row><row><cell cols="2">SimplE [18] / CP [19]ĥ,r,t ∈ R d ,h,ȓ,t ∈ R d</cell><cell>ĥ ,r,t + h ,ȓ,t</cell><cell>symmetric, anti-symmetric, asymmetric, inverse</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMMON</head><label>II</label><figDesc>RELATIONS IN KGS AND RESULTING REQUIREMENTS ON f , AND SEARCH CANDIDATES IN g(r).</figDesc><table><row><cell>common relations</cell><cell>requirements on f</cell><cell>requirements on g(r)</cell><cell>examples from WN18/FB15K</cell></row><row><cell>symmetric</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III STATISTICS</head><label>III</label><figDesc>OF THE DATA SETS USED IN EXPERIMENTS. "SYM" AND "ANTI-SYM" DENOTES THE SYMMETRIC AND ANTI-SYMMETRIC RELATIONS.</figDesc><table><row><cell>data set</cell><cell>#entity</cell><cell>#relation</cell><cell>#train</cell><cell>#valid</cell><cell>#test</cell><cell cols="4">#sym #anti-sym #inverse #general</cell></row><row><cell>WN18 [4]</cell><cell>40,943</cell><cell>18</cell><cell>141,442</cell><cell>5,000</cell><cell>5,000</cell><cell>4</cell><cell>7</cell><cell>7</cell><cell>0</cell></row><row><cell>FB15k [4]</cell><cell>14,951</cell><cell>1,345</cell><cell>484,142</cell><cell cols="2">50,000 59,071</cell><cell>66</cell><cell>38</cell><cell>556</cell><cell>685</cell></row><row><cell>WN18RR [6]</cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. DISTMULT, COMPLEX, ANALOGY AND SIMPLE ARE OBTAINED FROM OUR IMPLEMENTATION, OTHERS ARE COPIED FROM THE CORRESPONDING REFERENCE PAPER. STD IS LESS THAN 0.001, THUS NOT REPORTED. L2 penalty λ in [10 −5 , 10 −1 ], decay rate in [0.99, 1.0], batch size m in {256, 512, 1024}. All the models are trained until converge to avoid the influence of different convergence speed.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell>FB15k</cell><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell>FB15k237</cell><cell></cell><cell cols="2">YAGO3-10</cell><cell></cell></row><row><cell>type</cell><cell>model</cell><cell cols="3">MRR H@1 H@10</cell><cell cols="12">MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10</cell></row><row><cell>TDM</cell><cell>TransE [54]</cell><cell>0.500</cell><cell>-</cell><cell>94.1</cell><cell>0.495</cell><cell>-</cell><cell>77.4</cell><cell>0.178</cell><cell>-</cell><cell>45.1</cell><cell>0.256</cell><cell>-</cell><cell>41.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TransH [54]</cell><cell>0.521</cell><cell>-</cell><cell>94.5</cell><cell>0.452</cell><cell>-</cell><cell>76.6</cell><cell>0.186</cell><cell>-</cell><cell>45.1</cell><cell>0.233</cell><cell>-</cell><cell>40.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>RotatE [35]</cell><cell cols="2">0.949 94.4</cell><cell>95.9</cell><cell cols="2">0.797 74.6</cell><cell>88.4</cell><cell>0.476</cell><cell>42.8</cell><cell>57.1</cell><cell cols="2">0.338 24.1</cell><cell>53.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NNM</cell><cell>NTN [46]</cell><cell>0.53</cell><cell>-</cell><cell>66.1</cell><cell>0.25</cell><cell></cell><cell>41.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Neural LP [47] 0.94</cell><cell>-</cell><cell>94.5</cell><cell>0.76</cell><cell>-</cell><cell>83.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.24</cell><cell>-</cell><cell>36.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ConvE [6]</cell><cell cols="2">0.942 93.5</cell><cell>95.5</cell><cell cols="2">0.745 67.0</cell><cell>87.3</cell><cell>0.46</cell><cell>39.</cell><cell>48.</cell><cell cols="2">0.316 23.9</cell><cell>49.1</cell><cell>0.52</cell><cell>45.</cell><cell>66.</cell></row><row><cell>BLM</cell><cell>TuckER [1]</cell><cell cols="2">0.953 94.9</cell><cell>95.8</cell><cell cols="2">0.795 74.1</cell><cell>89.2</cell><cell cols="2">0.470 44.3</cell><cell>52.6</cell><cell>0.358</cell><cell>26.6</cell><cell>54.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>HolEX [45]</cell><cell cols="2">0.938 93.0</cell><cell>94.9</cell><cell cols="2">0.800 75.0</cell><cell>88.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>QuatE [53]</cell><cell cols="2">0.950 94.5</cell><cell>95.9</cell><cell cols="2">0.782 71.1</cell><cell>90.0</cell><cell>0.488</cell><cell>43.8</cell><cell>58.2</cell><cell cols="2">0.348 24.8</cell><cell>55.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DistMult</cell><cell cols="2">0.821 71.7</cell><cell>95.2</cell><cell cols="2">0.817 77.7</cell><cell>89.5</cell><cell cols="2">0.443 40.4</cell><cell>50.7</cell><cell cols="2">0.349 25.7</cell><cell>53.7</cell><cell cols="2">0.552 47.6</cell><cell>69.4</cell></row><row><cell></cell><cell>ComplEx</cell><cell cols="2">0.951 94.5</cell><cell>95.7</cell><cell>0.831</cell><cell>79.6</cell><cell>90.5</cell><cell cols="2">0.471 43.0</cell><cell>55.1</cell><cell cols="2">0.347 25.4</cell><cell>54.1</cell><cell>0.566</cell><cell>49.1</cell><cell>70.9</cell></row><row><cell></cell><cell>Analogy</cell><cell cols="2">0.950 94.6</cell><cell>95.7</cell><cell cols="2">0.829 79.3</cell><cell>90.5</cell><cell cols="2">0.472 43.3</cell><cell>55.8</cell><cell cols="2">0.348 25.6</cell><cell>54.7</cell><cell cols="2">0.565 49.0</cell><cell>71.3</cell></row><row><cell></cell><cell>SimplE/CP</cell><cell cols="2">0.950 94.5</cell><cell>95.9</cell><cell cols="2">0.830 79.8</cell><cell>90.3</cell><cell cols="2">0.468 42.9</cell><cell>55.2</cell><cell cols="2">0.350 26.0</cell><cell>54.4</cell><cell cols="2">0.565 49.1</cell><cell>71.0</cell></row><row><cell cols="2">AnyBURL [27]</cell><cell>0.95</cell><cell>94.6</cell><cell>95.9</cell><cell>0.83</cell><cell>80.8</cell><cell>87.6</cell><cell>0.48</cell><cell>44.6</cell><cell>55.5</cell><cell>0.31</cell><cell>23.3</cell><cell>48.6</cell><cell>0.54</cell><cell>47.7</cell><cell>47.3</cell></row><row><cell></cell><cell>AutoSF</cell><cell>0.952</cell><cell>94.7</cell><cell>96.1</cell><cell cols="2">0.853 82.1</cell><cell>91.0</cell><cell cols="2">0.490 45.1</cell><cell>56.7</cell><cell cols="2">0.360 26.7</cell><cell>55.2</cell><cell cols="2">0.571 50.1</cell><cell>71.5</cell></row><row><cell cols="8">tune hyper-parameters with the help of HyperOpt, a hyper-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">parameter optimization framework based on TPE [3]. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">searching ranges are given as follows: learning rate η in [0, 1],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V MRRS</head><label>V</label><figDesc>OF APPLYING SF SEARCHED FROM ONE DATA SET (INDICATED BY EACH ROW) ON ANOTHER DATA SET (INDICATED BY EACH COLUMN).</figDesc><table><row><cell></cell><cell>WN18</cell><cell cols="4">FB15k WN18RR FB15k237 YAGO3-10</cell></row><row><cell>WN18</cell><cell>0.952</cell><cell>0.841</cell><cell>0.473</cell><cell>0.349</cell><cell>0.561</cell></row><row><cell>FB15k</cell><cell>0.950</cell><cell>0.853</cell><cell>0.470</cell><cell>0.350</cell><cell>0.563</cell></row><row><cell>WN18RR</cell><cell>0.951</cell><cell>0.833</cell><cell>0.490</cell><cell>0.345</cell><cell>0.568</cell></row><row><cell>FB15k237</cell><cell>0.894</cell><cell>0.781</cell><cell>0.462</cell><cell>0.360</cell><cell>0.565</cell></row><row><cell cols="2">YAGO3-10 0.885</cell><cell>0.835</cell><cell>0.466</cell><cell>0.352</cell><cell>0.571</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF SEARCHED SFS WITH THE STATE-OF-THE-ART SFS ON ACCURACY (IN %) FOR TRIPLET CLASSIFICATION. STD&lt;0.2.</figDesc><table><row><cell></cell><cell cols="3">FB15k WN18RR FB15k237</cell></row><row><cell>DistMult</cell><cell>80.8</cell><cell>84.6</cell><cell>79.8</cell></row><row><cell>Analogy</cell><cell>82.1</cell><cell>86.1</cell><cell>79.7</cell></row><row><cell>ComplEx</cell><cell>81.8</cell><cell>86.6</cell><cell>79.6</cell></row><row><cell>SimplE</cell><cell>81.5</cell><cell>85.7</cell><cell>79.6</cell></row><row><cell>AutoSF</cell><cell>82.7</cell><cell>87.7</cell><cell>81.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII RUNNING</head><label>VII</label><figDesc>TIME ANALYSIS. WE SHOW THE RUNNING TIME (MIN) PER GREEDY STEP (STEP 2-11 IN ALG.2). APART FROM STEP 2-6 (FILTER), STEP 7,10-11 (PREDICTOR), STEP 8 (TRAIN) AND STEP 9 (EVALUATION), ALL OTHER STEPS TAKE LESS THAN 0.1 MINUTES.</figDesc><table><row><cell>steps</cell><cell>filtering 2-6</cell><cell>predictor 7,10-11</cell><cell>train 8</cell><cell>evaluate 9</cell></row><row><cell>WN18</cell><cell>15.9±0.5</cell><cell>1.8±0.1</cell><cell>475.9±9.5</cell><cell>41.3±0.8</cell></row><row><cell>FB15K</cell><cell>16.8±0.7</cell><cell>1.9±0.1</cell><cell>886.3±21.8</cell><cell>153.7±3.9</cell></row><row><cell>WN18RR</cell><cell>16.1±1.0</cell><cell>1.8±0.1</cell><cell>271.4±5.1</cell><cell>27.9±0.5</cell></row><row><cell>FB15k237</cell><cell>16.6±1.1</cell><cell>1.9±0.1</cell><cell>439.2±11.2</cell><cell>63.5±1.9</cell></row><row><cell>YAGO3-10</cell><cell>16.6±0.9</cell><cell>1.7±0.1</cell><cell>1631.1±85.5</cell><cell>141.9±8.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The work is partially supported by the Hong Kong RGC GRF Project 16202218, CRF project C6030-18G, AOE project AoE/E-603/18, the National Science Foundation of China </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Proposition 1</head><p>Proof. We consider the four cases separately:</p><p>• symmetric relations: If g(r) = g(r) for some r ∈ R d , then given a triplet (h, r, t), f (t, r, h) = t g(r)h = t g(r)h = h g(r) t = h g(r)t = f (h, r, t), which means g(r) can handle symmetric relations. • anti-symmetric relations: If g(r ) = −g(r ) for some r ∈ R d , then given a triplet (h, r , t), f (t, r , h) = t g(r )h = t g(r )h = h g(r ) t = −h g(r )t = −f (h, r , t), which means g(r ) can handle anti-symmetric relations. • general asymmetric relation: Since D r i = diag(r i ) , i = 1 . . . 4, then for any scalar w ∈ R, D (wr) i = diag(wr i ) = wD r i . This leads to g(wr) = wg(r). Similarly, we have g(w 1 r + w 2 r ) = w 1 g(r) + w 2 g(r ) for scalar w 1 , w 2 . If g(r) = g(r) for some r ∈ R d and g(r ) = −g(r ) for another r ∈ R d , then for any general assymetric relation r asym , let r asym = w 1 r + w 2 r . We have</p><p>Similarly, we can obtain</p><p>Then, for any value of the pair f (h, r asym , t) and f (t, r asym , h), there exist appropriate scalars w 1 , w 2 by solving <ref type="bibr" target="#b7">(8)</ref> and <ref type="bibr" target="#b8">(9)</ref> to obtain r asym = w 1 r + w 2 r . • inverse relations: Let r a and r b be two relations, and assume r a = w 1 r + w 2 r and r b = w 3 r + w 4 r given g(r) = g(r), g(r ) = −g(r ) based on general asymmetric property. Let w 1 = w 3 and w 2 = −w 4 , then</p><p>This means r a and r b are a pair of inverse relations. Thus, we obtain the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Proposition 2</head><p>Proof. (i) For each case (S1-11), the SRF is generated based on permutation and flipping signs of the 4 basic values</p><p>Thus, no matter how the structure g changes due to permutation or flipping signs, they will lead to the same feature. Once the matrix g(v) can be symmetric or anti-symmetric under one assignment Si, its corresponding feature will not change regardless of permutation or flipping signs. (ii) The SRF is generated based on the symmetric and skew-symmetric property and each dimension corresponds to a specific case of symmetric or skew-symmetric. Then the predictor can learn higher weights to the dimensions correlates the data's symmetric property well. Besides, this pattern can be easily learned through a few samples. For each (S1)-(S11), we use permutation and flipping the signs based on the given examples to check if g(v) can be symmetric or skew-symmetric under each case. As a result, a 11×2 = 22 dimensional SRF returns with little extra cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Design of SRFs Remark A.1 (SRF). Let the 1-dimensional degeneration of</head><p>The 11 cases exhaustively enumerate the possible conditions of g(·) being symmetric or skew-symmetric. What we care most is what kind of symmetric properties g(r) can be under these cases. Some data sets may need more features being 1 if it has more symmetric, anti-symmetric and inverse relations like FB15k, but some may not like FB15k237. The process of SRF generation is given in Alg.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 SRF generation for each (S1-11)</head><p>Input: the structure of g, SRFi=[0,0] for i = 1 . . . 11; <ref type="bibr">1:</ref> for v in the assignment candidates of Si through permuting and flipping signs do </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Details of Taking MLP as G</head><p>To ensure quick training and testing <ref type="bibr" target="#b5">[6]</ref>, we use two fullyconnected neural networks as the MLP. Specifically, to predict the tail entity, we use N N 1 to combine h and r into v = N N 1 (h, r). Then we use the dot product of v and t as the score. To test the head entity, another network N N 2 is built in similar way and final score is N N 2 (t, r), h . The two networks share the same structure (128-64-64) and are trained jointly based on Alg.1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Approximation with artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Csáji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Dept. Science, Eotvos Lorand Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient benchmarking of hyperparameter optimizers via surrogates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BOHB: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1436" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transition-based knowledge graph embedding with relational mapping properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACLIC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient and robust automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>The MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the equivalence of holographic and complex embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="554" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<title level="m">Automated Machine Learning: Methods, Systems, Challenges</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SimplE embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Knowledge representation learning: A quantitative review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10901</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jonathon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural networkbased question answering over knowledge graphs on word and character level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lukovnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1211" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anytime bottom-up rule learning for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chekol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Knowledge graph refinement: A survey of approaches and evaluation methods. Semantic web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="489" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Introducing the knowledge graph: Things, not strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Official Google blog</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AMiner: Mining deep knowledge from big scholar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International WWW Committee</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="373" to="373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on CVSMC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Greed is good: Algorithmic results for sparse approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIT</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2231" to="2242" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4735" to="4772" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TKDE</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On multi-relational link prediction with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">On evaluating embedding models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06410</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Genetic CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Expanding holographic embeddings for knowledge completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4496" to="4506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2319" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Efficient neural interaction function search for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Searching for interaction functions in collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.12091</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno>Arxiv: 1810.13306</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Differentiable neural architecture search via proximal iterations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13577</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Quaternion knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">NSCaching: simple and efficient negative sampling for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="614" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
