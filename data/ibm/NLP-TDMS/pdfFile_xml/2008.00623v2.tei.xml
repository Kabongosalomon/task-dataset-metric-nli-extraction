<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 DELIGHT: DEEP AND LIGHT-WEIGHT TRANSFORMER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 DELIGHT: DEEP AND LIGHT-WEIGHT TRANSFORMER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a deep and light-weight transformer, DeLighT, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation, a deep and lightweight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Attention-based transformer networks <ref type="bibr" target="#b0">(Vaswani et al., 2017)</ref> are widely used for sequence modeling tasks, including language modeling and machine translation. To improve performance, models are often scaled to be either wider, by increasing the dimension of hidden layers, or deeper, by stacking more transformer blocks. For example, T5 (Raffel et al., 2019) uses a dimension of 65K and GPT-3 (Brown et al., 2020) uses 96 transformer blocks. However, such scaling increases the number of network parameters significantly (e.g., T5 and GPT-3 have 11 billion and 175 billion parameters, respectively), and complicates learning, i.e., these models either require very large training corpora <ref type="bibr" target="#b1">(Raffel et al., 2019;</ref><ref type="bibr" target="#b3">Devlin et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref> or careful regularization <ref type="bibr" target="#b4">(Hinton et al., 2012;</ref><ref type="bibr" target="#b5">Wan et al., 2013;</ref><ref type="bibr" target="#b6">Merity et al., 2018a)</ref>. In this paper, we introduce a new parameter-efficient attention-based architecture that can be easily scaled to be both wide and deep. Our Deep and Light-weight Transformer architecture, DeLighT, extends the transformer architecture of Vaswani et al. (2017) and delivers similar or better performance with significantly fewer parameters and operations. At the heart of DeLighT is the DeLighT transformation that uses the group linear transformations (GLTs) of Mehta et al. ( <ref type="formula">2018)</ref> with an expand-reduce strategy for varying the width and depth of the DeLighT block efficiently. Since GLTs are local by nature, the DeLighT transformation uses feature shuffling, which is analogous to channel shuffling in convolutional networks <ref type="bibr" target="#b8">(Zhang et al., 2018)</ref>, to share information between different groups. Such wide and deep representations facilitate replacing the multi-head attention and feed-forward layers in transformers with single headed attention and light-weight feed-forward layers, reducing total network parameters and operations. Importantly, unlike transformers, the DeLighT transformation decouples the depth and width from the input size, allowing us to allocate parameters more efficiently across blocks by using shallower and narrower DeLighT blocks near the input and deeper and wider DeLighT blocks near the output.</p><p>We demonstrate that DeLighT models achieve similar or better performance than transformer models with significantly fewer parameters and operations, on two common sequence modeling tasks, (i) machine translation and (ii) language modeling. On the low resource WMT'16 En-Ro machine translation dataset, DeLighT attains transformer performance using 2.8× fewer parameters. On the high resource WMT'14 En-Fr dataset, DeLighT delivers better performance (+0.4 BLEU score) with 1.8× fewer parameters than baseline transformers. Similarly, on language modeling, DeLighT matches the performance of Transformer-XL (Dai et al., 2019) with 1.5× fewer parameters arXiv:2008.00623v2 [cs.</p><p>LG] 11 Feb 2021</p><p>Published as a conference paper at ICLR 2021 on the WikiText-103 dataset. Our source code is open-source and is available at: https://github.com/ sacmehta/delight</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Improving transformers: Several methods have been introduced to improve the transformer architecture. The first line of research addresses the challenge of computing self attention on long input sequences <ref type="bibr" target="#b10">(Child et al., 2019;</ref><ref type="bibr" target="#b11">Kitaev et al., 2020;</ref><ref type="bibr" target="#b12">Beltagy et al., 2020)</ref>. These methods can be combined with our architecture. The second line of research focuses on explaining multi-head attention <ref type="bibr" target="#b13">(Raganato and Tiedemann, 2018;</ref><ref type="bibr" target="#b14">Brunner et al., 2020)</ref>. They show that increasing the number of transformer heads can lead to redundant representations <ref type="bibr" target="#b15">(Voita et al., 2019a;</ref><ref type="bibr" target="#b16">Michel et al., 2019)</ref> and using fixed attention heads with predefined patterns <ref type="bibr" target="#b17">(Raganato et al., 2020)</ref> or synthetic attention matrices <ref type="bibr" target="#b18">(Tay et al., 2020)</ref> improves performance. The third line of research focuses on improving transformers by learning better representations <ref type="bibr" target="#b19">(Wu et al., 2019;</ref><ref type="bibr" target="#b21">So et al., 2019)</ref>. These works aim to improve the expressiveness of transformers using different transformations -for example, using convolutions <ref type="bibr" target="#b19">(Wu et al., 2019;</ref><ref type="bibr" target="#b22">Gehring et al., 2017)</ref>, gated linear units , or multi-branch feature extractors <ref type="bibr" target="#b21">(So et al., 2019;</ref>. Our work falls into this category. Unlike previous works, we show that it is possible to efficiently allocate parameters both at the block-level using the DeLighT transformation and across blocks using block-wise scaling.</p><p>Model scaling: Model scaling is a standard method to improve the performance of sequence models <ref type="bibr" target="#b0">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b1">Raffel et al., 2019;</ref><ref type="bibr" target="#b24">Lan et al., 2020;</ref><ref type="bibr" target="#b3">Devlin et al., 2019;</ref><ref type="bibr" target="#b25">Shoeybi et al., 2019;</ref><ref type="bibr" target="#b26">Tan and Le, 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>. Model dimensions are increased in width-wise scaling <ref type="bibr" target="#b0">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b3">Devlin et al., 2019)</ref> while more blocks (e.g., Transformer blocks) are stacked in depth-wise scaling <ref type="bibr" target="#b25">(Shoeybi et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020;</ref><ref type="bibr" target="#b27">Wang et al., 2019)</ref>. In both cases (and their combination), parameters inside each block of the network are the same, which may lead to a sub-optimal solution. To further improve the performance of sequence models, this paper introduces block-wise scaling that allows for variably-sized blocks and efficient allocation of parameters in the network. Our results show that (1) shallower and narrower DeLighT blocks near the input and deeper and wider DeLighT blocks near the output deliver the best performance, and (2) models with block-wise scaling coupled with model scaling achieve better performance compared to model scaling alone. We note that convolutional neural networks (CNNs) also learn shallower and narrower representations near the input and deeper and wider representations near the output. Unlike CNNs (e.g., ResNet of <ref type="bibr" target="#b28">He et al. 2016</ref>) that perform a fixed number of operations at each convolutional layer, the proposed block-wise scaling uses a variable number of operations in each layer and block.</p><p>Improving sequence models: There is also significant recent work on other related methods for improving sequence models, including (1) improving accuracy using better token-level representations -for example, using BPE <ref type="bibr" target="#b29">(Sennrich et al., 2016)</ref>, adaptive inputs  and outputs <ref type="bibr" target="#b31">(Grave et al., 2017a)</ref>, and DeFINE <ref type="bibr" target="#b32">(Mehta et al., 2020)</ref>, and (2) improving efficiency -for example, using compression <ref type="bibr" target="#b33">(Chen et al., 2018;</ref><ref type="bibr">Sun et al., 2020)</ref>, pruning <ref type="bibr" target="#b35">(Han et al., 2016;</ref><ref type="bibr" target="#b36">Voita et al., 2019b)</ref>, and distillation <ref type="bibr" target="#b37">(Hinton et al., 2015;</ref><ref type="bibr" target="#b38">Sanh et al., 2019)</ref>. The closest to our work is the DeFINE transformation, which also learns representations using an expand-reduce strategy. The key difference between the DeFINE transformation ( <ref type="figure">Figure 1c</ref>) and the DeLighT transformation <ref type="figure">(Figure 1d</ref>) is that the DeLighT transformation more efficiently allocates parameters within expansion and reduction layers. Unlike DeFINE, which uses fewer groups in group linear transformations to learn wider representations, DeLighT transformation uses more groups to learn wider representations with fewer parameters. The DeLighT transformation achieves comparable performance to the DeFINE transformation but with significantly fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DELIGHT: DEEP AND LIGHT-WEIGHT TRANSFORMER</head><p>A standard transformer block <ref type="figure">(Figure 1a</ref>) comprises of multi-head attention that uses a query-keyvalue decomposition to model relationships between sequence tokens, and a feed forward network (FFN) to learn wider representations. Multi-head attention obtains query Q, key K, and value V by applying three projections to the input, each consisting of h linear layers (or heads) that map the second reduces the dimensions from d f to d m . The depth of a transformer block is 4, consisting of (1) three parallel branches for queries, keys, and values, (2) a fusion layer that combines the output of multiple heads, and (3) two sequential linear layers in the FFN. In general, transformer-based networks sequentially stacks transformer blocks to increase network capacity and depth.</p><formula xml:id="formula_0">d m -dimensional input into a d h -dimensional space, where d h = d m /h is the head dimension.</formula><p>This paper extends the transformer architecture and introduces a deep and light-weight transformer, DeLighT. Our model uses a deep and light-weight expand-reduce transformation, DeLighT transformation (Section 3.1), that enables learning wider representations efficiently. It also enables replacing multi-head attention and feed forward network (FFN) layers with single-head attention and a light-weight FFN (Section 3.2). DeLighT transformation decouples attention dimensions from the depth and width, allowing us to learn representations efficiently using block-wise scaling instead of uniform stacking of transformer blocks (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DELIGHT TRANSFORMATION</head><p>DeLighT transformation maps a d m dimensional input vector into a high dimensional space (expansion) and then reduces it down to a d o dimensional output vector (reduction) using N layers of the group transformations of <ref type="bibr" target="#b7">Mehta et al. (2018)</ref>, as shown in <ref type="figure">Figure 1d</ref>. During these expansion and reduction phases, DeLighT transformation uses group linear transformations (GLTs) because they learn local representations by deriving the output from a specific part of the input and are more efficient than linear transformations. To learn global representations, the DeLighT transformation shares information between different groups in the group linear transformation using feature shuffling, analogous to channel shuffling in convolutional networks <ref type="bibr" target="#b8">(Zhang et al., 2018)</ref>.</p><p>A standard approach to increase the expressivity and capacity of transformers is to increase the input dimensions, d m . However, increasing d m linearly also increases the number of operations in multihead attention (O(n 2 d m ), where n is the sequence length) in a standard transformer block ( <ref type="figure">Figure  1a</ref>). In contrast, to increase the expressivity and capacity of the DeLighT block, we increase the depth and width of its intermediate DeLighT transformations using expansion and reduction phases. This enables us to use smaller dimensions for computing attention, requiring fewer operations. (5) maximum groups g max in a GLT. In the expansion phase, the DeLighT transformation projects the d m -dimensional input to a high-dimensional space, d max = w m d m , linearly using N 2 layers. In the reduction phase, the DeLighT transformation projects the d max -dimensional vector to a d o -dimensional space using the remaining N − N 2 GLT layers. Mathematically, we define the output Y at each GLT layer l as:</p><formula xml:id="formula_1">Y l = F X, W l , b l , g l , l = 1 F H X, Y l−1 , W l , b l , g l , Otherwise<label>(1)</label></formula><p>where W l = W l 1 , · · · , W l g l and b l = b l 1 , · · · , b l g l are the learnable weights and biases of group linear transformation F with g l groups at the l-th layer. Briefly, the F function takes the input X or H X, Y l−1 and splits into g l non-overlapping groups such that X = X 1 , · · · , X g l . The function F then linearly transforms each X i with weights W l i and bias b l i to produce output Y l i = X i W l i + b l i . The outputs of each group Y l i are then concatenated to produce the output Y l . The function H first shuffles the output of each group in Y l−1 and then combines it with the input X using the input mixer connection of <ref type="bibr" target="#b32">Mehta et al. (2020)</ref> to avoid vanishing gradient problems. <ref type="figure">Figure  2</ref> visualizes the expansion phase in the DeLighT transformation with group linear transformation, feature shuffling, and the input mixer connection.</p><p>The number of groups at the l-th GLT in DeLighT transformation are computed as:</p><formula xml:id="formula_2">g l = min(2 l−1 , g max ), 1 ≤ l ≤ N/2 g N −l , Otherwise<label>(2)</label></formula><p>In our experiments, we use g max = dm 32 so that each group has at least 32 input elements. DeLighT layer and single head attention: Let us assume we have a sequence of n input tokens, each of dimensionality d m . These n, d m -dimensional inputs are first fed to the DeLighT transformation to produce n, d o -dimensional outputs, where d o &lt; d m . These n, d o -dimensional outputs are then projected simultaneously using three linear layers to produce d o -dimensional queries Q, keys K, and values V. We then model contextual relationships between these n tokens using scaled dot-product attention (Eq. 3). To enable the use of residual connections <ref type="bibr" target="#b28">(He et al., 2016)</ref>, the d o -dimensional outputs of this attention operation are linearly projected into a d m -dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DELIGHT BLOCK</head><formula xml:id="formula_3">Attention(K, Q, V) = softmax QK T √ d o V<label>(3)</label></formula><p>We hypothesize that the ability of DeLighT to learn wider representations allows us to replace multi-head attention with single-head attention. The computational costs for computing attention in Light-weight FFN: Similar to FFNs in transformers, this block also consists of two linear layers.</p><formula xml:id="formula_4">Block-wise Uniform Input Output Input Output B blocks N 0 = N min N B−1 = N max (see Eq. 4) N B−1 =N N 0 =N (a) Uniform vs. block-wise B0 B1 B2 B3 B4 B5 B6 B7</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder blocks</head><p>Since the DeLighT block has already incorporated wider representations using the DeLighT transformation, it allows us to invert the functionality of FFN layers in the transformer. The first layer reduces the dimensionality of the input from d m to d m /r while the second layer expands the dimensionality from d m /r to d m , where r is the reduction factor (see <ref type="figure">Figure 1b</ref>). Our light-weight FFN reduces the number of parameters and operations in the FFN by a factor of rd f /d m . In the standard transformer, the FFN dimensions are expanded by a factor of 4. 1 In our experiments, we used r = 4. Thus, the light-weight FFN reduces the number of parameters in the FFN by 16×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block depth:</head><p>The DeLighT block stacks (1) a DeLighT transformation with N GLTs, (2) three parallel linear layers for key, query, and value, (3) a projection layer, and (4) two linear layers of a light-weight FFN. Thus, the depth of DeLighT block is N + 4. Compared to the standard transformer block (depth is 4), DeLighT block is deeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BLOCK-WISE SCALING</head><p>Standard methods for improving the performance of sequence models include increasing the model dimensions (width scaling), stacking more blocks (depth scaling), or both. However, such scaling is not very effective on small datasets. For example, when a Transformer-Base (d m = 512) network is replaced with Transformer-Large (d m = 1024) on the WMT'16 En-Ro corpus, the number of parameters increases by approximately 4× while the performance does not change appreciably (BLEU: 34.28 vs. 34.35). We hypothesize that this happens because scaling model width and depth allocates parameters uniformly across blocks, which may lead to learning redundant parameters. To create deep and wide networks, we extend model scaling to the block level (see <ref type="figure" target="#fig_2">Figure 3</ref>).</p><p>Scaling the DeLighT block: The DeLighT block learns deep and wide representations using the DeLighT transformation, whose depth and width are controlled by two configuration parameters: the number of GLT layers N and the width multiplier w m , respectively ( <ref type="figure" target="#fig_2">Figure 3a</ref>). These configuration parameters allow us to increase the number of learnable parameters inside the DeLighT block independently of the input d m and output d o dimensions. Such calibration is not possible with the standard transformer block because their expressiveness and capacity are a function of the input (input dimension = number of heads × head dimension). Here, we introduce block-wise scaling that creates a network with variably-sized DeLighT blocks, allocating shallower and narrower DeLighT blocks near the input and deeper and wider DeLighT blocks near the output.</p><p>To do so, we introduce two network-wide configuration parameters: minimum N min and maximum N max number of GLTs in a DeLighT transformation. For the b-th DeLighT block, we compute the number of GLTs N b and the width multiplier w b m in a DeLighT transformation using linear scaling (Eq. 4). With this scaling, each DeLighT block has a different depth and width <ref type="bibr">(Figure 3a)</ref>.</p><formula xml:id="formula_5">N b = N min + (N max − N min ) b B − 1 , w b m = w m + (N max − N min ) b N min (B − 1) , 0 ≤ b ≤ B − 1 (4)</formula><p>Here, B denotes the number of DeLighT blocks in the network. We add superscript b to number of GLT layers N and width multiplier w m to indicate that these parameters are for the b-th block.</p><p>Network depth: The depth of transformer block is fixed, i.e., 4. Therefore, previous works <ref type="bibr" target="#b1">(Raffel et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020;</ref><ref type="bibr" target="#b27">Wang et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We evaluate the performance of DeLighT on two standard sequence modeling tasks: (1) machine translation (Section 4.1) and <ref type="formula" target="#formula_2">(2)</ref>    (1) the number of parameters is the same and (2) the performance is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">RESULTS</head><p>Comparison with baseline transformers: 3 points yet with 13 million fewer parameters and 3 billion fewer operations (see <ref type="table" target="#tab_3">Table 2</ref>).</p><p>Particularly interesting are the performance comparisons of DeLighT with the baseline transformers of <ref type="bibr" target="#b0">Vaswani et al. (2017)</ref> and its neural search variant, i.e., Evolved Transformer of <ref type="bibr" target="#b21">So et al. (2019)</ref>, at two different parametric settings on WMT'14 En-De corpora in <ref type="figure" target="#fig_3">Figure 4</ref>. For small models (&lt; 10 M parameters), DeLighT models delivers better performance and for attaining the same performance as these models, DeLighT models requires fewer parameters.</p><p>Comparison with state-of-the-art methods: Most state-of-the-art methods have evaluated the performance on WMT'14 En-De while some have also evaluated on IWSLT'14 De-En. <ref type="table">Table  3</ref> compares the performance of DeLighT with state-of-the-art methods on these two corpora.</p><p>DeLighT delivers similar or better performance than existing methods. It is important to note that existing methods have improved baseline transformers with different design choices -for example, the asymmetric encoder-decoder structure <ref type="bibr" target="#b27">(Wang et al., 2019)</ref> and neural architecture search <ref type="bibr" target="#b21">(So et al., 2019)</ref>. We believe that DeLighT, in the future, would also benefit from such design choices.</p><p>Scaling up DeLighT models: <ref type="figure" target="#fig_4">Figure 5</ref> shows the performance of DeLighT models improves with increase in network parameters; suggesting their ability to learn representations across different corpora, including low-resource.  <ref type="table">Table 3</ref>: Comparison with state-of-the-art methods on machine translation corpora. DeLighT delivers similar or better performance than state-of-the-art models with fewer parameters. Here, † indicates that the network uses neural architecture search (NAS) and ‡ indicates that full network parameters are not reported. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LANGUAGE MODELING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and evaluation:</head><p>We evaluate on the WikiText-103 dataset <ref type="bibr" target="#b47">(Merity et al., 2017</ref>) that has 103M/217K/245K tokens for training, validation, and testing. It has a word-level vocabulary of about 260K tokens. Following recent works <ref type="bibr" target="#b9">Dai et al., 2019)</ref>, we report performance in terms of perplexity (lower is better) on the test set.</p><p>Architecture: We use the transformer-based decoder architecture of  with B DeLighT blocks. We use w m =2, N min =4, and N max =12. We scale d m using values {384, 512, 784, 1024} for increasing network parameters. For simplicity, we set B = N max . Following standard practice, we use adaptive input (Baevski and Auli, 2019) as a look-up table and adaptive output <ref type="bibr" target="#b31">(Grave et al., 2017a)</ref> as the classification layer with one head (head dimension is 128) and two tails (tail dimensions are 64 and 32). We also share weights between the input and the output layers.</p><p>Training: We follow the training setup of , except that we train our models on 8 NVIDIA Tesla V100 GPUs for 100K iterations with a context length of 512 and an effective batch size of 64K tokens. We use Adam during training and use a context length of 480 during test.</p><p>Results: <ref type="table" target="#tab_7">Table 4b</ref> compares the performance of DeLighT with previous methods on WikiText-103. <ref type="table" target="#tab_7">Table 4a</ref> plots the variation of perplexity with number of parameters for DeLighT and Transformer-XL <ref type="bibr" target="#b9">(Dai et al., 2019</ref>) -which outperforms other transformer-based implementations (e.g., . Both tables show that DeLighT delivers better performance than state-of-the-art methods (including Transformer-XL) and it does this using a smaller context length and significantly fewer parameters, suggesting that the DeLighT transformation helps learn strong contextual relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS AND DISCUSSIONS ON COMPUTATIONAL EFFICIENCY</head><p>Training time and memory consumption: <ref type="table" target="#tab_8">Table 5</ref> compares the training time and memory consumption of DeLighT with baseline transformers. For an apples-to-apples comparisons, we implemented the Transformer unit without NVIDIA's dedicated CUDA kernel, and trained both transformer and DeLighT full-precision networks for 30K iterations on 16 NVIDIA V100 GPUs. The transformer and DeLighT models took about 37 and 23 hours for training and consumed about 12.5 GB and 14.5 GB of GPU memory, respectively (R1 vs. R2). When we enabled the dedicated CUDA kernel provided by APEX library 3 for multi-head attention in Transformers, the training time of the    transformer model reduced from 37 to 16 hours while we did not observe any significant change in memory consumption. Motivated by this observation, we implemented dedicated CUDA kernels for grouping and ungrouping functions in GLTs (see Appendix E). With these changes, training time and GPU memory consumption of DeLighT reduced by about 4 hours and 3 GB, respectively. We emphasize that grouping, linear transformation, feature shuffling, and ungrouping, can be implemented efficiently using a single CUDA kernel. In future, we expect a dedicated CUDA kernel for these operations would further reduce the memory consumption as well as training/inference time.</p><p>Regularization: <ref type="table" target="#tab_9">Table 6</ref> shows that DeLighT delivers similar performance to baseline transformers, but with fewer parameters and less regularization. This suggests that learning representations with better transformation functions alleviates the need for dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper introduces a deep and light-weight transformer architecture, DeLighT, that efficiently allocates parameters both within the DeLighT block and across DeLighT blocks. Compared to state-of-the-art transformer models, DeLighT models are (1) deep and light-weight and (2) deliver similar or better performance. In the future, we plan to apply DeLighT to other tasks, including language model pre-training, question answering, and language generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DELI G HT ARCHITECTURES FOR LANGUAGE MODELING AND MACHINE TRANSLATION</head><p>DeLighT architectures for language modeling and machine translation are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. For language modeling, we follow the architecture in  while for machine translation, we follow the architecture in <ref type="bibr" target="#b0">Vaswani et al. (2017)</ref>.</p><p>Language modeling: <ref type="figure" target="#fig_5">Figure 6a</ref> shows the architecture for language modeling. The architecture stacks B DeLighT blocks, the configuration of each block is determined using block-wise scaling. Each block has three sub-layers. The first layer is a DeLighT transformation that learns representations in high-dimensional space. The second layer is a single-head attention that encodes contextual relationships. The third layer is a position-wise light-weight feed-forward network. Similar to <ref type="bibr" target="#b0">Vaswani et al. (2017)</ref>, we employ a residual connections <ref type="bibr" target="#b28">(He et al., 2016)</ref>. Similar to previous works <ref type="bibr" target="#b9">Dai et al., 2019)</ref>, we use tied adaptive input  and adaptive softmax <ref type="bibr" target="#b31">(Grave et al., 2017a)</ref> to map tokens to vectors and vectors to tokens, respectively.</p><p>Machine translation: <ref type="figure" target="#fig_5">Figure 6b</ref> shows the architecture for machine translation. The encoder stacks B DeLighT blocks, the configuration of each block is determined using block-wise scaling. Similar to language modeling, each encoder block has three sub-layers. The first layer is a DeLighT transformation that learns representations in high-dimensional space. The second layer is a single-head attention that encodes contextual relationships. The third layer is a position-wise light-weight feed-forward network. Similar to <ref type="bibr" target="#b0">Vaswani et al. (2017)</ref>, we employ a residual connections <ref type="bibr" target="#b28">(He et al., 2016)</ref>. We use learnable look-up  to vectors. Similar to the encoder, the decoder also stacks B blocks. Decoder blocks are identical to encoder blocks, except that they have an additional source-target single-head attention unit before the light-weight FFN.</p><p>Keys and values in source-target single-head attention unit are projections over the encoder output. We use standard learnable look-up table to map tokens to vectors and linear classification layer to map vectors to tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GROUP LINEAR TRANSFORMATION WITH INPUT-MIXER CONNECTION</head><p>Group linear transformation (GLT) F splits a dm-dimensional input X into g non-overlapping groups such that X = Concat(X1, · · · , Xg), where Xi is a dm g -dimensional vector. Xi's are then simultaneously transformed using g linear transforms Wi ∈ R dm g × do g to produce g outputs Yi = XiWi. Yi's are then concatenated to produce the final do-dimensional output Y = Concat(Y1, · · · , Yg). <ref type="figure" target="#fig_7">Figure 7a</ref> shows an example of GLT in the expansion phase of DeLighT transformation. For illustrative purposes, we have used the same dimensions in this example. Recall that as we go deeper in the expansion phase, the number of groups increases. In this example, the first layer has one group, the second layer has two groups and the third layer has four groups. GLTs learns group-specific representations and are local. To allow GLT to learn global representations, we use feature shuffle. An example of GLT with feature shuffle is shown in <ref type="figure" target="#fig_7">Figure 7b</ref>. Furthermore, training deep neural networks by merely stacking linear or group linear (with or without feature shuffle) is challenging because of vanishing gradient problem. Residual connections introduced by <ref type="bibr" target="#b28">He et al. (2016)</ref> mitigates this problem and helps train deep neural networks. However, such connections cannot be employed when input and output dimensions are not the same (e.g., during the expansion and reduction phases in DeLighT transformation). To stabilize the training and learn deeper representations, we use input-mixer connection of <ref type="bibr" target="#b32">Mehta et al. (2020)</ref>. <ref type="figure" target="#fig_7">Figure 7c</ref> shows an example of GLT with feature shuffle and input mixer connection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MULTIPLICATION-ADDITION OPERATIONS IN DELI G HT</head><p>The DeLighT block is built using linear transformations, GLTs, and scaled dot-product attention. Total number of multiplication-addition operations (MACs) in a network is an accumulation of these individual operations.</p><p>Let n denotes the number of source tokens, m denotes the number of target tokens, dm denotes the input dimension, do denotes the output dimension, and g denotes the number of groups in GLT. The procedure for counting MACs for each of these operations is described below.</p><p>Group linear transformation (GLT): GLT F has g learnable matrices Wi ∈ R dm g × do g . Therefore, GLT learns dmdo g parameters and performs dmdo g MACs to transform dm-dimensional input to do-dimensional output. Following a standard practice, e.g., ResNet of <ref type="bibr" target="#b28">He et al. (2016)</ref>, we count addition and multiplication as one operation instead of two because these operations can be fused in recent hardwares. Importantly, when g = 1, the GLT is the same as linear transformation.</p><p>Self-attention in DeLighT: The scaled dot-product self-attention in DeLighT is defined as:</p><formula xml:id="formula_6">Attention(K, Q, V) = softmax QK T √ do V<label>(5)</label></formula><p>where Q ∈ R n×do , K ∈ R n×do , V ∈ R n×do denotes query, key, and value, respectively.</p><p>The attention operation involves two dot-products. The first dot product between Q and K while the second dot product is between the output of first dot product and V. Both dot products require don 2 MACs. Therefore, total number of MACs in computing scaled dot-product self-attention are 2don 2 .</p><p>In case of a source-target attention (as in machine translation), K's and V's are from the source (encoder) and Q's are incrementally decoded (one token at a time). Therefore, the number of MACs required to decode m target tokens given n source tokens are m k=1 2kndo . <ref type="table" target="#tab_11">Table 7</ref> studies the impact of DeLighT block parameters on the WikiText-103 dataset, namely (1) minimum number of GLTs Nmin, (2) maximum number of GLTs Nmax, (3) width multiplier wm, and (4) model dimension dm (see <ref type="figure">Figure 1b</ref>). <ref type="figure" target="#fig_8">Figure 8, Figure 9</ref>, and <ref type="figure" target="#fig_10">Figure 10</ref> shows the impact of the DeLighT transformation, feature shuffling, and the light-weight FFN. <ref type="table" target="#tab_13">Table 8</ref> shows the effect of position of DeLighT transformation in the DeLighT block while <ref type="figure" target="#fig_13">Figure 12</ref> shows the effect of scaling DeLighT networks. We choose the WikiText-103 dataset for ablations because it has very large vocabulary compared to other datasets (267K vs. 30-40K), allowing us to test the ability under large vocabulary sizes. The performance is reported in terms of perplexity (lower is better) on the validation set. In our ablation studies, we used the same settings for training as in Section 4.2 except that we train only for 50K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ABLATIONS ON THE WIKITEXT-103 DATASET</head><p>DeLighT block: Overall, <ref type="table" target="#tab_11">Table 7</ref> shows that scaling depth and width using DeLighT transformation and block-wise scaling improves performance. We make following observations: a) Block-wise scaling (R4, R5) delivers better performance compared to uniform scaling (R1-R3). For instance, DeLighT with Nmin = 4 and Nmax = 8 (R4) is 1.25× shallower than DeLighT with Nmin = 8 and Nmax = 8 (R2), but delivers better performance with a similar number of parameters and operations. Scaling wm improves performance (R2 vs. R3), however, the improvement is significantly lower than for the model with block-wise scaling (R3 vs. R5). This suggests that non-uniform distribution of parameters across blocks allows the network to learn better representations. b) Different ratios between Nmax and Nmin yields different results. We observe significant performance improvements when the ratio is greater than or equal to two. For example, when we scale Nmax N min from 2 to 3 (R6 vs. R8), the perplexity improves by ∼5 points with only a moderate increase in network parameters. On the other hand, when the Nmax N min is close to 1 (R6 vs. R7), performance does not change appreciably. This is likely because the allocation of parameters across blocks is close to uniform (Eq. 4). This is consistent with our previous observation. c) Learning shallower and narrower representations near the input and deeper and wider representations near the output achieves better performance. For example, when we scaled Nmax from 8 to 12 for Nmin = 4 (R6, R8), DeLighT delivered better performance with a similar number of parameters compared to a model with Nmin = 6 (R7, R9). This is likely because the ratio of Nmax and Nmin is higher when Nmin = 4, which helps allocate parameters per block more effectively. d) Deeper and wider representations near the input and shallower and narrower representations near the output hurts performance (R13 vs. R16). e) Scaling width using wm and dm improves performance (R10-R15), however, their impact is different. For example, when we scale wm and dm by two, the rate of increase in number of parameters and operations is more rapid with dm compared to wm. DeLighT's ability to learn wider representations in different ways may be useful in selecting application specific models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of DeLighT transformation:</head><p>We replace DeLighT transformation in the DeLighT block ( <ref type="figure">Figure  1b)</ref> with (1) the DeFINE transformation and (2) a stack of linear layers. <ref type="figure" target="#fig_8">Figure 8</ref> shows that DeLighT transformation delivers similar performance with significantly fewer parameters compared to the DeFINE unit and linear layers. In these experiments, the settings are the same as R13-R15 <ref type="table" target="#tab_11">(Table 7)</ref>, except, Nmax = 8, because models with a stack of linear layers learn too many parameters.</p><p>Feature shuffling: <ref type="figure" target="#fig_9">Figure 9</ref> shows that feature shuffling improves the performance of DeLighT by 1-2 perplexity points. Here, we use the same settings as in R13-R15 <ref type="table" target="#tab_11">(Table 7)</ref>.</p><p>Light-weight FFN: <ref type="figure" target="#fig_10">Figure 10</ref> shows the impact of varying the reduction factor r in the light-weight FFN. We use the same settings as in R13 <ref type="table" target="#tab_11">(Table 7)</ref>. We did not observe any significant drop in performance until r = 4. Beyond r = 4, we see a drop in performance (perplexity increases by ∼2 points). In such cases, the inner dimensions of the light-weight FFN are very small and hurt performance. Notably, the light-weight FFN with r = 2 2 delivered the same performance as r = 2 −2 , but with 1.28× fewer network parameters. At r = 2 −2 , the light-weight FFN is the same as the FFN in <ref type="bibr" target="#b0">Vaswani et al. (2017)</ref>. This suggests that the ability of     DeLighT networks with block-wise scaling delivers better performance across different settings. Lower perplexity value means better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block-wise Uniform</head><formula xml:id="formula_7">N min =4, N max =4, Mean=4 N min =8, N max =8, Mean=8 N min =4, N max =8, Mean=5.6 N min =4, N max =12,</formula><p>DeLighT transformation to learn representations in high-dimensional spaces efficiently allows us to reduce the computational burden on the FFN.</p><p>We also tested removing the light-weight FFN and while it reduced parameters by ∼0.5-1 M, performance dropped by about 2-3 perplexity points across different parametric settings.</p><p>Uniform vs. block-wise scaling: <ref type="figure" target="#fig_12">Figure 11</ref> compares the performance of DeLighT with uniform and blockwise scaling. For a given model dimension dm, DeLighT models with block-wise scaling delivers better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position of DeLighT transformation:</head><p>We studied three configurations for the DeLighT transformation on the WikiText-103 validation set <ref type="table" target="#tab_13">(Table 8</ref>):</p><p>(1) DeLighT transformation followed by single-headed attention and light-weight FFN, (2) single-headed attention followed by DeLighT transformation, and (3) single-headed attention followed by DeLighT transformation and light-weight FFN. For similar number of parameters, we found that (2) and (3) drops the performance of (1) significantly across different parametric settings. This suggests that deeper and wider representations helps learn better contextual representations; allowing us to replace multi-headed attention with single-headed attention.</p><p>Scaling up DeLighT: <ref type="figure" target="#fig_13">Figure 12</ref> shows the results of DeLighT models obtained after varying configuration parameters of DeLighT transformations (Nmin={4, 6}, Nmax={8, 12}, wm={2, 3, 4}, and dm={256, 384, 512}). We can see that scaling one configuration parameter (e.g., dm) while keeping other configuration parameters constant (e.g., Nmin, Nmax, and wm) consistently improves performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configuration Parameters Perplexity</head><p>DeLighT   This work investigates relationships between Nmin, Nmax, wm, and dm, manually. We believe that a more principled approach, such as compound scaling of <ref type="bibr" target="#b26">Tan and Le (2019)</ref>, that establishes relationships between these parameters would produce more efficient and accurate models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E SOURCE CODE FOR GROUP LINEAR TRANSFORMATION</head><p>The source code for implementing group linear transformation (GLT) in PyTorch is shown in Listing 1. The source code for efficiently implementing the grouping function in GLT is shown in Listing 2. Since the ungrouping kernel is similar to grouping kernel, we have not shown it here.</p><p>The reshape and transpose operations in naive PyTorch implementation for grouping and ungrouping are replaced with a dedicated CUDA kernels, resulting in reduced memory footprint and faster training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Formally, the Figure 2 :</head><label>the2</label><figDesc>DeLighT transformation is controlled by five configuration parameters: (1) number of GLT layers N , (2) width multiplier w m , (3) input dimension d m , (4) output dimension d o , and Example illustrating the expansion phase in the DeLighT transformation that uses GLTs, feature shuffling, and an input mixer connection, to learn deeper and wider representations efficiently. For illustrative purposes, we have used the same input and output dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>1b shows how we integrate DeLighT transformation into the transformer block to improve its efficiency. The d m -dimensional inputs are first fed to the DeLighT transformation to produce d o -dimensional outputs, where d o &lt; d m . These d o -dimensional outputs are then fed into a single head attention, followed by a light-weight FFN to model their relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Distribution of parameters and operations within each block Block-wise scaling efficiently allocates parameters and operations across blocks, leading to shallower and narrower DeLighT blocks near the input and deeper and wider DeLighT blocks near the output. In (b), DeLighT networks with both uniform (N =Nmin=Nmax=8) and block-wise (Nmin=4, Nmax=8) scaling have about 16.7 M parameters and perform 3.5 B operations (computed for a sequence length of n = 30), however, the DeLighT network with block-wise scaling delivered 2 points better perplexity. the standard transformer and the DeLighT block are O(d m n 2 ) and O(d o n 2 ) respectively, where d o &lt; d m . Therefore, the DeLighT block reduces the cost for computing attention by a factor of d m /d o . In our experiments, we used d o = d m /2, thus requiring 2× fewer multiplication-addition operations as compared to the transformer architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of DeLighT with Transformers and Evolved Transformers at two different settings, on the WMT'14 En-De corpus:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Scaling up DeLighT models. The performance of DeLighT improves with an increase in the number of network parameters, across different corpora, including low-resource (WMT'16 En-Ro).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Sequence modeling with DeLighT. Here, green color hexagon represents the DeLighT transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>This figure visualizes different variants of group linear transformations that are used in the DeLighT transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Impact of different transformations. DeLighT transformations are more parametric efficient than DeFINE and linear transformations. Lower perplexity value means better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Impact of feature shuffling. Feature shuffling allows us to learn representations from global information and improves performance. Lower perplexity value means better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Impact of reduction factor r in light-weight FFN. The ability of DeLighT transformation to learn representations in high-dimensional spaces efficiently allows us to reduce the computational burden on the FFN. Lower perplexity value means better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Uniform vs. block-wise scaling. (a) contrasts the uniform and block-wise scaling methods. (b) compares the results of DeLighT with uniform and block-wise scaling methods on the WikiText-103 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Scaling up DeLighT. Scaling one configuration parameter (e.g., dm) while keeping other configuration parameters constant (e.g., Nmin, Nmax, and wm) consistently improves performance. The numbers on top of each bar represents network parameters (in million). Lower value of perplexity means better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The FFN consists of two linear layers, where the first expands the dimensions from d m to d f and the</figDesc><table><row><cell>Multi-head Attention Feed Forward Network (FFN)</cell><cell>Query dm dh Attention ops: O(d m n 2 )</cell><cell>Key dm dh Attention Concat dm dm Add dm df =4dm dm Add FFN params: Value dm dh 8d 2 m Depth = 4</cell><cell>transformation with Single-head Attention DeLighT Light-weight FFN</cell><cell>N b (Eq. 4) Query do do Attention ops: O(d o n 2 )</cell><cell>dm w b m dm Key do do= dm 2 Attention do dm Add dm dm/4 dm Add Depth = 4 + N b Value do do FFN params: d 2 m 2</cell><cell>Output (do-dimensional) Input (dm-dimensional) Output (do-dimensional) (c) DeFINE transformation Expansion Reduction No. of layers (depth) = N Input (dm-dimensional) Expansion Reduction No. of layers (depth) = N</cell></row><row><cell></cell><cell cols="2">(a) Transformer block</cell><cell></cell><cell cols="2">(b) DeLighT block</cell><cell>(d) DeLighT transformation</cell></row></table><note>Figure 1: (a, b) Block-wise comparison between the standard transformer block of Vaswani et al. (2017) and the DeLighT block. In the DeLighT transformation, the number of operations in computing attention are reduced by half while the number of parameters (and operations) in the FFN are reduced by 16×. Transformations with learnable parameters ( Linear and DeLighT ) are shown in color. The shape of linear transformations indicate their operation (expansion, reduction, etc.). (c, d) compares the DeFINE transformation (Mehta et al., 2020) with the DeLighT transformation. Compared to the DeFINE transformation, the DeLighT transformation uses group linear transformations (GLTs) with more groups to learn wider representations with fewer parameters. Different colors are used to show groups in GLTs. For simplicity, feature shuffling is not shown in (d).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>have associated the depth of transformer-based networks with the number of transformer blocks. In DeLighT, we present a different perspective to learn deeper representations, wherein each block is variably-sized. To compute the network depth, we use the standard definition across different domains, including computer vision (e.g., ResNet of<ref type="bibr" target="#b28">He et al. 2016</ref>) and theoretical machine learning(Telgarsky, 2016). These works measures network depth as the number of sequential learnable layers (e.g., convolution, linear, or group linear). Similarly, the depth of DeLighT and transformer networks with B blocks is</figDesc><table><row><cell>B−1 b=0 (N b + 4) and 4B, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>En dataset, we replicate the setup of<ref type="bibr" target="#b19">Wu et al. (2019)</ref> and<ref type="bibr" target="#b40">Edunov et al. (2018)</ref>, which uses 160K/7K/7K sentence pairs for training, validation, and testing with a joint BPE vocabulary of about 10K tokens, respectively. For the WMT'14 English-German (En-De) dataset, we follow the setup of<ref type="bibr" target="#b0">Vaswani et al. (2017)</ref>. The dataset has 3.9M/39K/3K sentence pairs for training, validation, and testing respectively with a joint BPE vocabulary size of 44K. 2 For the WMT'14 English-French (En-Fr) dataset, we replicate the setup of<ref type="bibr" target="#b22">Gehring et al. (2017)</ref>, which uses 36M/27K/3K sentence pairs for training, validation, and testing respectively with a joint BPE vocabulary size of 44K. The performance is evaluated in terms of BLEU<ref type="bibr" target="#b42">(Papineni et al., 2002)</ref> (higher is better) on the test set. We follow<ref type="bibr" target="#b19">Wu et al. (2019)</ref> for beam search related hyper-parameters. We follow the symmetric encoder-decoder architecture of<ref type="bibr" target="#b0">Vaswani et al. (2017)</ref> with sinusoidal positional encodings. Both the encoder and the decoder have B DeLighT blocks. Decoder blocks are identical to the encoder blocks(Figure 1b), except that they have an additional source-target single-head attention unit before the light-weight FFN. In the source-target single-head attention unit, keys and values are projections over the encoder output (full details in Appendix A). In our experiments, we use w m = 2, N min = 4, and N max = 8 for WMT'16 En-Ro, WMT'14 En-De, and WMT'14 En-Fr; resulting in 222 layer deep DeLighT networks. For IWSLT'14 De-En, we used w m = 1, N min = 3, and N max = 9 for IWSLT'14 De-En; resulting in 289 layer deep network. For simplicity, we set B = N max . We use a learnable look-up table that maps every token in the vocabulary to a 128-dimensional vector. We implement our models using Fairseq<ref type="bibr" target="#b43">(Ott et al., 2019)</ref> and use their provided scripts for data pre-processing, training, and evaluation.Training: For IWSLT'14 De-En models, we follow the setup of<ref type="bibr" target="#b19">Wu et al. (2019)</ref> and train all our models for 50K iterations with a batch size of 4K tokens on a single NVIDIA GTX 1080 GPU. For WMT'16 En-Ro, we follow the training setup of<ref type="bibr" target="#b44">Ghazvininejad et al. (2019)</ref> and train models for 100K iterations on 16 NVIDIA Tesla V100 GPUs with an effective batch size of 64K tokens. Comparison with baseline transformers on machine translation corpora. DeLighT models require significantly fewer parameters to achieve similar performance. Here, † and ‡ indicate the best reported transformer baselines from<ref type="bibr" target="#b19">Wu et al. (2019)</ref> and<ref type="bibr" target="#b44">Ghazvininejad et al. (2019)</ref>, respectively.</figDesc><table><row><cell>language modeling (Section 4.2).</cell></row><row><cell>4.1 MACHINE TRANSLATION</cell></row></table><note>Datasets and evaluation: We benchmark DeLighT models on four datasets: (1) IWSLT'14 German-English (De-En), (2) WMT'16 English-Romanian (En-Ro), (3) WMT'14 English-German (WMT'14 En-De), and (4) WMT'14 English-French (WMT'14 En-Fr). For the IWSLT'14 De-Architecture:For WMT'14 En-De and WMT'14 En-Fr, we follow the training set-up of Wu et al. (2019) and train our models on 16 V100 GPUs for 30K and 50K iterations, respectively. We use Adam (Kingma and Ba, 2015) to minimize cross entropy loss with a label smoothing value of 0.1 during training. For a fair comparison, we trained baseline transformer models using the same training set-up.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>BLEU (WMT'14 En-De)</cell><cell>20 22 24 26 28</cell><cell>0</cell><cell>10 Parameters (in million) 20 30 40 50 60 1.8x fewer parameters +1.4 BLEU DeLighT Transformer Evolved Trans.</cell></row></table><note>DeLighT networks are deep, light- weight and efficient as compared to transformers. BLEU score is reported on the WMT'14 En-Fr dataset. To compute network depth, we count the number of sequential layers in the network (Sec- tion 3.3). We used 20 source and 20 target tokens for computing multiplication-addition operations (MACs). See Appendex C for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>compares the performance of DeLighT with the baseline transformers of Vaswani et al. (2017) on different corpora. DeLighT delivers better performance with fewer parameters than transformers, across different corpora. Specifically, on low-resource (WMT'16 En-Ro) and high resource (WMT'14 En-De &amp; WMT'14 En-Fr) corpora, DeLighT delivers similar or better performance with 2.8× and 1.8× fewer parameters, respectively. When the number of parameters are increased, DeLighT outperforms transformers. For example, on WMT'14 En-Fr dataset, DeLighT is 3.7× deeper than transformers and improves its BLEU score by 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on the WikiText-103 dataset. Compared to Transformer-XL, DeLighT delivers similar or better performance (lower perplexity) with fewer parameters. † For Transformer-XL, we reproduce results using the official source code. For evaluating Transformer-XL with a context length of 480, we set the mem_len hyper-parameter to 480 in the official evaluation scripts.</figDesc><table><row><cell cols="2">Row # Model</cell><cell cols="2"># Params (in million) (WMT'14 En-Fr) BLEU</cell><cell>Training Memory time (in GB)</cell></row><row><cell>R1</cell><cell>Transformer (unoptimized)</cell><cell>67 M</cell><cell>39.2</cell><cell>37 hours 12.5 GB</cell></row><row><cell>R2</cell><cell>DeLighT (unoptimized)</cell><cell>54 M</cell><cell>40.5</cell><cell>23 hours 14.5 GB</cell></row><row><cell>R3</cell><cell>Transformer (w/ Apex optimized)</cell><cell>67 M</cell><cell>39.2</cell><cell>16 hours 11.9 GB</cell></row><row><cell>R4</cell><cell>DeLighT (w/ optimized grouping)</cell><cell>54 M</cell><cell>40.5</cell><cell>19 hours 11.5 GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison with baseline transformers in terms of training speed and memory consumption. In R4, we implemented CUDA kernels for grouping and ungrouping functions only (see Appendix E). We expect DeLighT to be more efficient with a single and dedicated CUDA kernel for grouping, transformation, feature shuffling, and ungrouping. Memory consumption is measured on a single NVIDIA GP100 GPU (16 GB memory) with a maximum of 4096 tokens per batch and without any gradient accumulation.</figDesc><table><row><cell>Model</cell><cell cols="2">Dropout BLEU</cell></row><row><cell>Transformer (62 M)</cell><cell>0.10</cell><cell>27.3</cell></row><row><cell>Transformer (62 M)</cell><cell>0.30</cell><cell>27.7</cell></row><row><cell>DeLighT (37 M)</cell><cell>0.05</cell><cell>27.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>DeLighT requires less regularization as compared to baseline transformers (Dataset: WMT'14 En-De).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablations on different aspects of the DeLighT block, including uniform vs. block-wise scaling, depth scaling, and width scaling. Rows partially highlighted in color have the same configuration (repeated for illustrating results). Our experimental setup is similar to Section 4, except that we train our models for 50K iterations. Multiplication and addition operations (MACs) are computed for 20 time steps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Effect of the position of DeLighT transformation. Lower value of perplexity means better performance.</figDesc><table><row><cell>128</cell><cell>256</cell><cell>384</cell><cell>512</cell></row><row><cell cols="4">Model dimension (d m )</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Transformer-base uses dm=512 and d f =2048 while Transformer-large uses dm=1024 and d f =4096.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use training and validation data that is compatible with the Tensor2Tensor library<ref type="bibr" target="#b41">(Vaswani et al., 2018)</ref> in order to have fair comparisons with recent works (e.g., Evolved Transformer).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/NVIDIA/apex</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This research was supported by ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF (IIS-1616112, IIS1252835), and an Allen Distinguished Investigator Award. Authors would also like to thank members of the UW-NLP and the H2Lab at The University of Washington for their valuable feedback and comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Published as a conference paper at ICLR 2021  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyyGPP0TZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramidal recurrent unit for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An analysis of encoder representations in transformer-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On identifiability in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gino</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJg1f6EFDB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fixed encoder self-attention patterns in transformerbased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10260</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lite transformer with long-short range attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5877" to="5886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Édouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Groupreduce: Block-wise low-rank approximation for neural language model shrinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mobilebert: a compact task-agnostic bert for resource-limited devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Representation Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Workshop on Energy Efficient Machine Learning and Cognitive Computing -NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">Matus Telgarsky. Benefits of depth in neural networks. COLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
		<ptr target="http://arxiv.org/abs/1803.07416" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6114" to="6123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Latent alignment and variational attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9712" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An analysis of neural language modeling at multiple scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08240</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
