<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PhyAAt: Physiology of Auditory Attention to Speech Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikesh</forename><surname>Bajaj</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Intelligent Sensing</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science (EECS)</orgName>
								<orgName type="institution">Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<region>Queen</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dipartimento di Ingegneria Navale</orgName>
								<orgName type="department" key="dep2">Elettronica e delle Telecomunicazioni (DITEN)</orgName>
								<orgName type="laboratory">Elios Lab</orgName>
								<orgName type="institution">University of Genoa</orgName>
								<address>
									<settlement>Elettrica</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Requena</forename><surname>Carrión</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Intelligent Sensing</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science (EECS)</orgName>
								<orgName type="institution">Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<region>Queen</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Bellotti</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dipartimento di Ingegneria Navale</orgName>
								<orgName type="department" key="dep2">Elettronica e delle Telecomunicazioni (DITEN)</orgName>
								<orgName type="laboratory">Elios Lab</orgName>
								<orgName type="institution">University of Genoa</orgName>
								<address>
									<settlement>Elettrica</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PhyAAt: Physiology of Auditory Attention to Speech Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Auditory attention · Physiological signals · EEG · GSR · PPG · Attention of speech · Predictive modeling · PhyAAt</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Auditory attention to natural speech is a complex brain process. Its quantification from physiological signals can be valuable to improving and widening the range of applications of current braincomputer-interface systems, however it remains a challenging task. In this article, we present a dataset of physiological signals collected from an experiment on auditory attention to natural speech. In this experiment, auditory stimuli consisting of reproductions of English sentences in different auditory conditions were presented to 25 non-native participants, who were asked to transcribe the sentences. During the experiment, 14 channel electroencephalogram, galvanic skin response, and photoplethysmogram signals were collected from each participant. Based on the number of correctly transcribed words, an attention score was obtained for each auditory stimulus presented to subjects. A strong correlation (p &lt;&lt; 0.0001) between the attention score and the auditory conditions was found. We also formulate four different predictive tasks involving the collected dataset and develop a feature extraction framework. The results for each predictive task are obtained using a Support Vector Machine with spectral features, and are better than chance level. The dataset has been made publicly available for further research, along with the python library phyaat to facilitate the preprocessing, modeling, and reproduction of the results presented in this paper. The dataset and other resources are shared on webpage -https://phyaat.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Auditory attention is a cognitive process of great importance that still remains poorly understood <ref type="bibr" target="#b0">[1]</ref>. It plays a vital role in many human activities, from aircraft flying to learning in a classroom environment. Thus, understanding attention can be important to improve the effectiveness, experience, and performance of many tasks <ref type="bibr" target="#b1">[2]</ref>. A better understanding of auditory attention could also lead to improved brain-computer-interface (BCI) systems that use emotional and mental states to execute actions.</p><p>Auditory attention and auditory perception are closely related processes; while perception is the process of interpreting information, attention is the process by which discrete pieces of information are selected or discarded <ref type="bibr" target="#b2">[3]</ref>. Auditory perception is a complex brain process that involves encoding sounds into patterns of neural activity <ref type="bibr" target="#b3">[4]</ref>. This process can also modulate other physiological responses, such as the heart rate and the sympathetic tone. The brain activity can be investigated in the Electroencephalogram (EEG), and the heart rate and sympathetic response can be extracted from Photoplethysmogram (PPG) and Galvanic Skin Response (GSR) signals, respectively. Other physiological modalities such as the Electromyogram, the Electrooculogram, and the Electrocardiogram are also available.</p><p>Physiological signals are used in many applications from healthcare to BCI system. Datasets of physiological signals are available for a wide variety of applications, from paralysis <ref type="bibr" target="#b4">[5]</ref> and epilepsy <ref type="bibr" target="#b5">[6]</ref> studies, to applications for patients in a complete locked-in state <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> to emotion recognition <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref> and serious games <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>. Widely used EEG datasets for BCI applications include motor imagery datasets <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>. There are several attention related datasets, including datasets for covert and overt visual attention <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref> and for auditory-visual attention shift <ref type="bibr" target="#b24">[24]</ref>. Specifically for auditory attention based on EEG signals, an auditory oddball paradigm has been presented, where the response of participants to oddball sounds inserted in streams of sinusoidal tones was analysed <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Experiment Design</head><p>Among the wide variety of physiological datasets available, to the best of our knowledge there is a lack of datasets designed to study auditory attention to natural speech. With the goal of supporting future research in the field of auditory attention, we have designed an experiment based on the dichotic listening task <ref type="bibr" target="#b27">[27]</ref>, and collected a dataset of physiological signals that include 14 channel EEG, PPG and GSR. The dataset is available to the wider community for it to be used freely 1 . Based on this dataset, a model could be trained to predict from physiological signals the attention level of individuals in different scenarios, for instance during the delivery of a lecture. This dataset does not restrict itself to auditory attention studies and can also be used to investigate other mechanisms involved in brain auditory processing of natural speech. One example of such applications is the design of the difficulty levels of a game based on the level of auditory attention <ref type="bibr" target="#b28">[28]</ref>.</p><p>The rest of the article is organised as follows. Section 2 explains the experimental design, materials and procedures. Section 3 describes the collected dataset, labels and file structure. In Section 4, we analyse the correlation between attention score and auditory conditions. In Section 5, we formulate the four predictive tasks and Section 6 describes the framework to extract the features from physiological responses. Section 7 demonstrates the results of the predictive tasks using the data from one subject. Section 8 concludes the presented work and discusses several directions for future research. The details of resources made available are explained in the Resources section at the end of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of the experiment design</head><p>An experiment based on diochotic listening task was designed to create the dataset for auditory attention analysis. The main components of this experiment are shown in <ref type="figure">Figure 1</ref>. Unlike the diochotic listening task, only one audio stimuli per trial was presented to a participant. Audio stimuli were reproduced under three different auditory conditions (N ,S, and L) and following previous studies, the attention score A for each trial was computed from a transcribed audio message, by counting the number of correctly identified words <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref>. While the establishing attention score, minor spelling errors and typos were ignored such as did/dids, beautiful/beutiful, dog/dogs. For the entire duration of the experiment, three different modalities of physiological responses (R) were recorded at the sampling rate of 128Hz. The details of the methods and materials are explained in subsequent subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stimuli</head><p>A total collection of 5000 audio files, each containing semantically correct English language sentence was obtained from the Tatoeba Project <ref type="bibr" target="#b33">[33]</ref>. The length of stimuli ranges from 3 to 13 words per sentence. All the stimuli were reproduced by the same speaker to avoid variations in physical properties such as pitch, rate of speech, etc. From the collection of semantic stimuli, a total 1700 non-semantic stimuli were created by suitably inserting random isolated words in between. The following are examples of semantic and non-semantic stimuli:</p><p>Tx1: I am going to study. Tx2: I would like to read some books about the Beatles.</p><p>Tx3: Let's touch enjoyable go. Tx4: I have a hey big are we dog.</p><p>In the above examples, stimuli Tx1 and Tx2 are semantic and stimuli Tx3 and Tx4 are non-semantic stimuli generated by inserting the words highlighted in bold font. From each group of semantic and non-semantic stimuli, six additional groups of stimuli were created by adding different levels of background noise. The signal to noise ratios (SNR) of each background noise level were -6 dB, -3 dB, 0 dB, 3 dB, 6 dB and noise-free (∞ dB). In summary, we created two groups of audio stimuli, semantic and non-semantic, of lengths ranging from 3 to 13 words and six different levels of background noise was added to each audio stimuli during reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Physiological signals</head><p>We recorded three types of physiological responses from each participant, namely EEG, GSR, and PPG. A 14 channel Emotiv Epoc wireless device was used for recording EEG signals <ref type="bibr" target="#b34">[34]</ref>. The EEG signals were collected using the API provided by the device manufacturer with the highest available sampling rate of 128Hz. The DC component was removed from EEG signals while recording. Two small copper plates placed on the index finger and middle finger were used to measure the GSR. The resulting GSR signal was low-pass filtered and recorded at a rate of 128 Hz. Both the raw and low-pass filtered signals were recorded.</p><p>A pulse sensor [35] positioned on the ring finger was used to record the PPG response, which measures microvascular blood volume changes. Pulse rate and the inter-beat interval (IBI) were obtained from the PPG response using software code provided by the sensor manufacturer <ref type="bibr" target="#b35">[36]</ref>.</p><p>In summary, 19 signals streams were recorded from each participant, i.e. 14 EEG channels, 2 GSR streams (raw and low-pass filtered) and 3 PPG streams (raw signal, pulse rate, and IBI). All the physiological responses were labeled accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Participants</head><p>A total of 25 university students, 21 male, 4 female, 1 left handed, 24 right handed, participated in our study. All the participants were non-native English speakers (i.e. their first language was other than English) and had no known auditory processing disorder. The age of the participants ranged from 16 to 34 years. The majority of nationalities among the participants were Indian and Italian and the majority of the first languages were Arabic, Italian and Malayalam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Experiment procedure</head><p>After mounting all the sensors e.g. EEG headset, GSR plates, and PPG pulse sensor, participant were provided with a passive ear-phone and presented with a computer interface. Participants were then asked to enter demographic information that included nationality, gender, age-group, and first language. Participants were also asked to rate their English language skills in terms of listening, writing, reading, and speaking. Once all the information was entered, participants could initiate the experiment.</p><p>A total of 144 stimuli were randomly selected without replacement, for each participant. Out of 144, 72 were selected from the semantic group and 72 from the non-semantic group. In each group, the 72 stimuli were equally split into six background noise levels, i.e. 12 stimuli were assigned to each noise level. Furthermore, the 12 stimuli within each noise level had different lengths. Specifically, 5 stimuli were short with an average of 4 words (L1), 4 stimuli had an average of 8 words (L2), and 3 stimuli were long, with an average of 12 words (L3). The chosen lengths followed closely previous studies <ref type="bibr" target="#b29">[29]</ref>. The variation in length within each category was ±1 word and we assumed that this variation did not have a significant effect on perception, as suggested by previous studies <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>The experiment thus consisted of 144 trials per participant, during which they had to transcribe one auditory stimulus. Each trial was divided into three tasks, namely Listening, Writing, and Resting. The timeline of a trial is shown in  <ref type="figure" target="#fig_0">Figure 2b</ref>. A participant could actively start a trial by pressing the play button on the computer interface, after which a listening task would start by reproducing one of the 144 audio stimuli. Once the audio was finished, participants were allowed to write the response in a text-box. Participants were not allowed to replay any stimuli and the interface remained disabled while stimuli were being played. After the transcription was written, participants could press the submit button to save the response and end the writing task. To start new trial, participants had to press the play button again. The time period between writing and the next listening task was labeled as resting.</p><p>Participant were explained the entire procedure beforehand. The average time taken for a participant to complete the experiment was 40 ± 10 minutes. <ref type="figure" target="#fig_0">Figure 2a</ref> shows one of the participants conducting the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collected dataset</head><p>The collected dataset includes the physiological responses of the 25 participants. Signals and time segments are conveniently labeled so as to identify auditory conditions and tasks, and the computed attention scores for each stimuli is also included. A segment of physiological responses and corresponding labels is shown in <ref type="figure">Figure 3</ref> and includes the 14 EEG channels, the low-pass GSR signal and the raw PPG response. Listening, writing, and resting segments are identified by different background colors (green, white, and blue, respectively) and the attention score for listening segments is also shown. A summary of collected dataset is described in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>As shown in <ref type="figure">Figure 4</ref>, the collected dataset is structured in a directory containing 25 sub-directories, one for each participant, named as S1, S2, ... S25. Each sub-directory contains two files namely S[x]_Signals.csv and S[x]_Textscores.csv, where [x] is the participant ID, e.g. S1 corresponds to the participant 1. The signal file includes 19 streams of signals, time-stamped, and three labels, namely Label_N, Label_S, and Label_T, corresponding to noise level, semanticity, and task respectively. For the noise level ∞ dB, the numerical value 1000 is used. Semanticity is encoded as 0 and 1, for semantic and non-semantic respectively. Tasks are encoded as 0, 1, and 2 for listening, writing, and resting respectively. The labels before first trial are set to -1. The text score file includes the attention score (Label_A) for each trial, along with noise level, semanticity, length of stimulus, and timestamp. The dataset has been anonymised and the time-stamp in each files are set to the same time, i.e. 00 hours. The dataset also contains the demographic information and English level self-rating score of each participant. Finally, a ReadMe file is included.</p><p>A python library phyaat has been created to allow users to download the dataset and extract the segments with the corresponding labels to use for predictive modeling. User guide and instructions are also provided 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of Attention Score</head><p>The Spearman rank correlation between the attention score and the auditory condition (i.e. noise level, semanticity, and length of stimulus) was carried out and its results are shown in <ref type="figure">Figure 5</ref>. It can be observed that attention score has a strong positive correlation (r = 0.7) with the background noise level and a negative correlation with the semancity (r = −0.24) and length of the stimulus (r = −0.23), all with a p-value p &lt; 0.0001. Furthermore, a repeated-measure ANOVA test was performed on the attention score considering each auditory conditions individually and jointly. The <ref type="figure">Figure 3</ref>: A segment of collected physiological responses with corresponding labels. Green color background is corresponds to listening task, white for writing, and blue for resting.</p><p>p-values were found to be low (p &lt;&lt; 0.001), suggesting a significant impact of the auditory conditions on the attention scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Formulation of predictive tasks</head><p>Based on the analysis of the attention score and the experiment design <ref type="figure">(Figure 1</ref>), four different predictive tasks have been formulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">T1: Attention score prediction</head><p>In this task, the objective is to estimate the auditory attention level A of a participant based on the physiological signals recorded during the listening segment. Given a set of features F r extracted from the listening segment this objective can be formulated as finding a model f A (·) such that the attention score can be approximated by the quantity</p><formula xml:id="formula_0">A = f A (F r )<label>(1)</label></formula><p>The model f A (·) is defined as the one minimising the expected risk  where L(·, ·) is the chosen loss function and E[·] is the expectation operator. The choice of the loss function can be defined as the mean square error, the mean absolute error or a combination of the two, such as the Huber loss.</p><formula xml:id="formula_1">E(f A ) = E[L(A, f A (F r ))],<label>(2)</label></formula><p>The analysis of the attention score (c.f. Section 4) shows that there is a strong correlation between attention score and auditory conditions, such as noise level and semanticity. Therefore our knowledge about the auditory conditions can be informative for predicting the auditory attention. Although in general auditory conditions are not known, in our controlled experiment we do have access to their exact values. Consequently, we can formulate a predictive task for the auditory attention, that use the value of the auditory conditions as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">T2: Noise level prediction</head><p>Assuming that the background noise level influences the physiological responses of a person, a model f N (·) can be obtained to provide an estimation N of the noise level experienced by a subject, based on the features extracted from the physiological responses during a listening segment,</p><formula xml:id="formula_2">N = f N (F r )<label>(3)</label></formula><p>Since the defined noise level is an ordinal quantity, this predictive task can be formulated as a regression problem with a loss functions similar to the ones suggested for the predictive task T1. Alternatively, this predictive task can be formulated as a classification problems, where each of the six levels of noise constitute different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">T3: Semanticity of stimulus prediction</head><p>Assuming that the semanticity of a stimulus S modulates the physiological responses of a subject, a model f S (·) can be obtained to estimate the semanticity experienced by subject from the set of features F R :</p><formula xml:id="formula_3">S = f S (F r )<label>(4)</label></formula><p>Since semanticity is a binary valued auditory condition, this task can be formulated as a classification problem. Choices of loss functions include the cross-entropy or Hinge loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">T4: LWR classification</head><p>Tasks T1, T2 and T3 are based on listening segments only and therefore assume that the subject not engaged in any activities other than listening. However, there are scenarios where we might be interested in evaluating the activity that a subject might be engaged in. The objective of the LWR-classification task is to predict which activity T is being performed by the subject, considering listening, writing or resting as the candidate activities. Similar to other predictive tasks, this can be modeled as to predict T for unseen data, as follows</p><formula xml:id="formula_4">T = f T (F r )<label>(5)</label></formula><p>where f T (F r ) is a model that uses a set of features F r extracted from a segment of physiological responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Feature extraction framework</head><p>In the context of the experiment described in Section 2.5, a segment corresponds to a continuous time interval during which a complete task (listening, reading or resting) is executed. By processing the physiological signals recorded during an entire segment, a set of features F r can be extracted and used as an input in any of the predictive tasks T1 through T4. Other time intervals that do not map to complete tasks could also be used to extract features. We will call these time intervals windows, to distinguish them from our definition of segment. A graphical illustration of segment-wise and window-wise feature extraction is shown in <ref type="figure" target="#fig_2">Figures 6a and 6b</ref>, respectively. Different models could be built to estimate the target predictions based on features extracted from short windows or from an entire segment. An interesting question is to determine the minimum window length that will produce a set of features F r that contain enough information to produce an accurate prediction.</p><p>By allowing moving windows, window-wise feature extraction could be used to train temporal models, such as Dynamic Bayesian Network or RNN, for predictive tasks T1 and T3. Features extracted from each window could be modelled as non-Independent and Identically Distributed (IID) samples. In the case of predictive tasks T2 and T4, window-wise feature extraction could be used to train temporal or non-temporal models and the set of features extracted from each window could be treated as IID samples instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results of predictive tasks</head><p>In this section a basic SVM model is created for each of the predictive tasks defined in Section 5, by using data from a single subject, namely S1. First the EEG signals from subject S1 are pre-processed by using a high-pass filter with cutoff frequency of &gt; 0.5Hz and artifact are removed by using an automatic and tunable wavelet approach <ref type="bibr" target="#b38">[39]</ref>   Since the sample size for each task (i.e. 143 for T1, T2, T3 and 143 × 3 = 429 for T4) is small, K-Fold cross validation with K = 10 folds is used and to avoid overfitting a regularization parameter C is included. The average training and testing performance for each task together with the final value of the parameter C are reported in <ref type="table" target="#tab_2">Table 2</ref> and shown in <ref type="figure">Figure 7</ref>. It can be observed from <ref type="figure">Figure 7</ref> that performance for each predictive task is better than chance level measure. Since the class ratio is balanced for classification tasks, the chance level is 1/number of classes (e.g. 1/2 for T3 and 1/3 for T4). For regression, the standard deviation σ is shown in <ref type="figure">Figure 7</ref>. These results are reproducible and the corresponding python scripts are provided as part of the phyaat library. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future work</head><p>In this paper, we have presented a dataset of physiological signals for predictive analysis of auditory attention to natural speech. Our dataset is novel in its kind and contains three different physiological signals, namely EEG, GSR, and PPG. Signals were recorded during an experiment conducted with 25 subjects, which involved three tasks (listening, writing and resting) executed in 144 trials . In the listening stage, subjects were presented with an audio stimulus, which is to be transcribed in following writing stage, followed by a resting before the subsequent trial. In each trial, audio stimuli were presented in different auditory conditions, which included background noise level, semanticity, and length of the stimulus. The attention score from transcription of each trial was computed. Based on the collected dataset and experiment design, four predictive tasks have been formulated and two approaches of feature extraction have been proposed. For demonstration purposes, SVM models using EEG spectral features have been created for each predictive tasks using EEG data from a single subject. The correlation analysis of the attention score and the results of the predictive tasks demonstrate the potential of the collected dataset. <ref type="figure">Figure 7</ref>: Results of basic modeling using SVM for each predictive task with subject S1</p><p>In order to advance the research in the field of auditory attention, the collected dataset has been made openly available to the wider research community, together with supporting tools (See Resources section below) To improve the performance of the predictive tasks, more sophisticated models such as DBN, RNN and LSTM can be used with spectral, wavelet-based and other features. Our dataset provides an excellent opportunity to investigate the auditory attention mechanisms of the brain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) A participant during the experiment (b) Timeline of a trial Experiment procedure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 Figure 4 :Figure 5 :</head><label>545</label><figDesc>Writing, listening, reading and speaking skill of English language, rating scale 1-File structure of dataset directory (a) Noise level (b) Semanticity (c) Length of stimulus Correlation of attention score and auditory conditions, p &lt; 0.0001</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Feature extraction framework Classifier. Mean absolute error (MEA) and accuracy are used as performance metrics for regression (T1 &amp; T2) and classification (T3 &amp; T4), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset summary</figDesc><table><row><cell>Types of signal</cell><cell></cell><cell>EEG, GSR, PPG</cell></row><row><cell>EEG channels</cell><cell></cell><cell>14 channels; AF3, AF4, F3, F4, FC5, FC6,</cell></row><row><cell></cell><cell></cell><cell>F7, F8, T7, T8, P7, P8, O1, O2</cell></row><row><cell cols="3">Total signals streams 19 streams; 14 from EEG, 2 from GSR-in-</cell></row><row><cell></cell><cell></cell><cell>stant sample and moving averaged, 3 from</cell></row><row><cell></cell><cell></cell><cell>PPG -raw signal, beats per minutes (BPM),</cell></row><row><cell></cell><cell></cell><cell>interval between beats (IBI)</cell></row><row><cell>Sampling rate</cell><cell></cell><cell>128Hz</cell></row><row><cell>Participants</cell><cell></cell><cell>25 participants; 21 male, 4 female</cell></row><row><cell cols="2">Number of stimuli</cell><cell>144 randomly selected stimuli, 72 semantic</cell></row><row><cell>per participants</cell><cell></cell><cell>and 72 non-semantic</cell></row><row><cell>Independent</cell><cell>vari-</cell><cell>Noise level, Semanticity, Length of stimu-</cell></row><row><cell>ables</cell><cell></cell><cell>lus</cell></row><row><cell>Noise levels</cell><cell></cell><cell>6 levels; -6, -3, 0, 3, 6 and ∞ dB SNR</cell></row><row><cell cols="2">Semanticity levels</cell><cell>2 levels; 0-semantic, 1-non-semantic</cell></row><row><cell cols="2">Length of stimulus</cell><cell>3 to 13 words per stimulus, grouped into</cell></row><row><cell></cell><cell></cell><cell>three categories; L1 (small), L2 (medium)</cell></row><row><cell></cell><cell></cell><cell>and L3 (long)</cell></row><row><cell cols="2">Average duration of a</cell><cell></cell></row><row><cell>stimuli</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>with default parameter setting, namely db3, β = 0.1, N = 128 and λ s (·). After the pre-processing stage, six spectral features are extracted from each segment from the 14 EEG channels. The spectral features correspond to the total power in the</figDesc><table><row><cell>(a) Segment-wise feature extraction</cell><cell>(b) Window-wise feature extraction</cell></row></table><note>commonly used EEG frequency bands, namely delta (0.5-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-30 Hz), low gamma (30-47 Hz) and high gamma (47-64 Hz).The spectral power in each frequency band is computed based on Welch's method. In summary, a total of 84 spectral features (6 × 14 = 84) are extracted from each EEG segment. During the modeling stage, SVM with RBF kernel are used. Task T1 and T2 are modeled with a Support Vector Regressor. In task T2, noise level ∞ dB is mapped to 10 dB. Task T3 and T4 are modeled with Support Vector</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of basic modeling using SVM for each predictive task with subject S1</figDesc><table><row><cell>Predictive Task</cell><cell>SVM</cell><cell cols="3">Training Testing Metric</cell></row><row><cell cols="2">T1: Attention score C = 10</cell><cell>23.80</cell><cell>29.65</cell><cell>MAE</cell></row><row><cell>T2: Noise level</cell><cell>C = 1</cell><cell>4.07</cell><cell>4.75</cell><cell></cell></row><row><cell>T3: Semanticity</cell><cell>C = 2</cell><cell>0.95</cell><cell>0.56</cell><cell>Accuracy</cell></row><row><cell>T4: LWR</cell><cell>C = 2</cell><cell>0.97</cell><cell>0.81</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">https://phyaat.github.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://phyaat.github.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://phyaat.github.io/introduction</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resources</head><p>The dataset and supporting source code is distributed as a python library phyaat (https://pypi.org/project/phyaat/) on the Python Package Index (PyPI) platform. A user guide and scripts to reproduce the results presented in this paper, can be downloaded from the project homepage (https://phyaat.github.io/). Supporting functions in the library include pre-processing, feature extraction and modeling functions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The development of auditory attention in children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilary</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Molholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Bioscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="120" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A selective review of selective attention research from the past century</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Driver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="78" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Elizabeth Styles. Attention, perception and memory: an integrated introduction</title>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Methods for the analysis of auditory processing in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frédéric E Theunissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thane</forename><surname>Fremouw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="187" to="207" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brain-computer interfaces and communication in paralysis: extinction of goal directed thinking in completely paralysed patients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Birbaumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical neurophysiology</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2658" to="2666" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonrandomness, nonlinear dependence, and nonstationarity of electroencephalographic recordings from epilepsy patients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaspar</forename><surname>Ralph G Andrzejak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rummel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46206</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SSVEP-based Bremen-BCI interface-boosting information transfer rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Volosyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neural engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36020</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An auditory brain-computer interface evoked by natural speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ma Lopez-Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Pelayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neural engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36013</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auditory brain-computer interfaces for complete locked-in patients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Ma Lopez-Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco Pelayo</forename><surname>Ron-Angevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Work-Conference on Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="378" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Seok Won Bang, and Sang Ryong Kim. Emotion recognition system using short-term monitoring of physiological signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Kyung Hwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical and biological engineering and computing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="427" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From physiological signals to emotions: Implementing and comparing selected methods for feature extraction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>André</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="940" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotion assessment: Arousal evaluation using EEG&apos;s and peripheral physiological signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Chanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kronegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Grandjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multimedia content representation, classification and security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="530" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deap: A database for emotion analysis; using physiological signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Muhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Seok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashkan</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Touradj</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition in music listening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Hong</forename><surname>Yuan-Pin Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzyy-Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Lin</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyh-Kang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Horng</forename><surname>Jeng-Ren Duann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1798" to="1806" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emotion assessment from physiological signals for adaptation of game difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Chanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Rebetez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireille</forename><surname>Bétrancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1052" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">EEG-based &quot;serious&quot; games design for medical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sourina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Khoa</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cyberworlds (cw), 2010 international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="270" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A review of the use of psychophysiological methods in game research. journal of gaming &amp; virtual worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Matias Kivikangas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inger</forename><surname>Cowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>Salminen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Järvelä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravaja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="181" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BCI Competition 2008-Graz data set A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller-Putz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schlögl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfurtscheller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
		<respStmt>
			<orgName>Institute for Knowledge Discovery (Laboratory of Brain-Computer Interfaces ; Graz University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Braincomputer communication: motivation, aim, and impact of exploring a virtual apartment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Keinrath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Pfurtscheller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="482" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Open access dataset for eeg+ nirs single-trial classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyoung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Alexander Von Lühmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do-Won</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichai</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jeong</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1735" to="1745" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A covert attention P300-based brain-computer interface: Geospell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Aloise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Aricò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Riccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serenella</forename><surname>Salinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donatella</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Febo</forename><surname>Cincotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ergonomics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="538" to="551" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Influence of P300 latency jitter on event related potential-based brain-computer interface performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aricò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aloise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cincotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neural engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35008</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Brain-computer interfacing using modulations of alpha activity induced by covert shifts of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matthias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Treder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><forename type="middle">M</forename><surname>Bahramisharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Gerven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blankertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroengineering and rehabilitation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modality-specificity of sensory aging in vision and audition: evidence from event-related potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rčeponienė</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Westerfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain research</title>
		<imprint>
			<biblScope unit="volume">1215</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An auditory oddball brain-computer interface for binary choices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Halder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andreoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Nijboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sc Kleih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birbaumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kübler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="516" to="523" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new auditory multi-class brain-computer interface paradigm: spatial hearing as an informative cue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martijn</forename><surname>Schreuder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tangermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">9813</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The corpus callosum in dichotic listening studies of hemispheric asymmetry: a review of clinical and experimental evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Westerhausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Hugdahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1044" to="1054" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auditory attention, implications for serious game design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikesh</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Bellotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesùs</forename><surname>Requena Carriòn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">De</forename><surname>Gloria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Games and Learning Alliance</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aural perception of different syntactic structures and lengths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Compton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and speech</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="87" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Performance on a free-recall verbal dichotic listening task and cerebral dominance determined by the carotid amytal test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhn</forename><surname>Gaddes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="747" to="753" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Individual differences in working memory capacity and divided attention in dichotic listening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Ra</forename><surname>Colflesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="699" to="703" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dichotic listening performance and frontal lobe function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Hugdahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Benke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Brain Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tatoeba : open collaborative multilingual sentence dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trang</forename><surname>Ho</surname></persName>
		</author>
		<idno>2017/01/19</idno>
		<ptr target="https://tatoeba.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<idno>2017/01/19</idno>
		<ptr target="https://www.emotiv.com/epoc/" />
	</analytic>
	<monogr>
		<title level="j">Emotiv Epoc EEG headset</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Source code for pulse sensor, github repository</title>
		<idno>2017/01/19</idno>
		<ptr target="https://github.com/WorldFamousElectronics/PulseSensor_Amped_Arduino" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The primacy model: a new model of immediate serial recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Norris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">761</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Word-length effects in immediate memory: Overwriting trace decay theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Neath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James S Nairne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="441" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic and tunable algorithm for eeg artifact removal using wavelet decomposition with applications in predictive modeling during auditory tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikesh</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Jesús Requena Carrión</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Bellotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alesandro De</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gloria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">101624</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
