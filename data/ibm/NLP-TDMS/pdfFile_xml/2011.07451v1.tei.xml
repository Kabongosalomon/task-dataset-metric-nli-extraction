<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coresets for Robust Training of Neural Networks against Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-11-15">15 Nov 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
							<email>baharan@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California Los Angeles</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
							<email>kaidicao@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coresets for Robust Training of Neural Networks against Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-15">15 Nov 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern neural networks have the capacity to overfit noisy labels frequently found in real-world datasets. Although great progress has been made, existing techniques are limited in providing theoretical guarantees for the performance of the neural networks trained with noisy labels. Here we propose a novel approach with strong theoretical guarantees for robust training of deep networks trained with noisy labels. The key idea behind our method is to select weighted subsets (coresets) of clean data points that provide an approximately low-rank Jacobian matrix. We then prove that gradient descent applied to the subsets do not overfit the noisy labels. Our extensive experiments corroborate our theory and demonstrate that deep networks trained on our subsets achieve a significantly superior performance compared to state-of-the art, e.g., 6% increase in accuracy on CIFAR-10 with 80% noisy labels, and 7% increase in accuracy on mini Webvision 1 .</p><p>We conduct experiments on noisy versions of CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b21">[22]</ref> with noisy labels generated by random flipping the original ones, and the mini Webvision datasets <ref type="bibr" target="#b23">[24]</ref> which is a benchmark consisting of images crawled from websites, containing real-world noisy labels. Empirical results demonstrate that the robustness of deep models trained by CRUST is superior to state-of-theart baselines, e.g. 6% increase in accuracy on CIFAR-10 with 80% noisy labels, and 7% increase in accuracy on mini Webvision. We note that CRUST achieves state-of-the-art performance without the need for training any auxiliary model or utilizing an extra clean dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Additional Related Work</head><p>In practice, deeper and wider neural networks generalize better <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b47">48]</ref>. Theoretically, recent results proved that when the number of hidden nodes is polynomial in the size of the dataset, neural network parameters stay close to their initialization, where the training landscape is almost linear <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>, or convex and semi-smooth <ref type="bibr" target="#b1">[2]</ref>. For such networks, (stochastic) gradient descent with random initialization can almost always drive the training loss to 0, and overfit any (random or noisy) labeling of the data. Importantly, these results utilize the property that the Jacobian of the neural network is well-conditioned at a random initialization if the dataset is sufficiently diverse.</p><p>More closely related to our work is the recent result of <ref type="bibr" target="#b31">[32]</ref> which proved that along the directions associated with large singular values of a neural network Jacobian, learning is fast and generalizes well. In contrast, early stopping can help with generalization along directions associated with small</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of deep neural networks relies heavily on the quality of training data, and in particular accurate labels of the training examples. However, maintaining label quality becomes very expensive for large datasets, and hence mislabeled data points are ubiquitous in large real-world datasets <ref type="bibr" target="#b20">[21]</ref>. As deep neural networks have the capacity to essentially memorize any (even random) labeling of the data <ref type="bibr" target="#b48">[49]</ref>, noisy labels have a drastic effect on the generalization performance of deep neural networks. Therefore, it becomes crucial to develop methods with strong theoretical guarantees for robust training of neural networks against noisy labels. Such guarantees become of the utmost importance in safety-critical systems, such as aircraft, autonomous cars, and medical devices.</p><p>There has been a great empirical progress in robust training of neural networks against noisy labels. Existing directions mainly focus on: estimating the noise transition matrix <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref>, designing robust loss functions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref>, correcting noisy labels <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>, using explicit regularization techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>, and selecting or reweighting training examples <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>. In general, estimating the noise transition matrix is challenging, correcting noisy labels is vulnerable to overfitting, and designing robust loss functions or using explicit regularization cannot achieve state-of-the-art performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b50">51]</ref>. Therefore, the most promising methods rely on selecting or reweighting training examples by knowledge distillation from auxiliary models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>, or exploiting an extra clean labelled dataset containing no noisy labels <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref>. In practice, training reliable auxiliary models could be challenging, and relying on an extra dataset is restrictive as it requires the training and extra dataset to follow the same distribution. Nevertheless, the major limitation of the state-of-the-art methods is their inability to provide theoretical guarantees for the performance of neural networks trained with noisy labels.</p><p>There has been a few recent efforts to theoretically explain the effectiveness of regularization and early stopping in generalization of over-parameterized neural networks trained on noisy labels <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>. Specifically, Hu et al. <ref type="bibr" target="#b15">[16]</ref> proved that when width of the hidden layers is sufficiently large (polynomial in the size of the training data), gradient descent with regularization by distance to initialization corresponds to kernel ridge regression using the Neural Tangent Kernel (NTK). Kernel ridge regression performs comparably to early-stopped gradient descent <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref>, and leads to a generalization guarantee in presence of noisy labels. In another work, Li et al. <ref type="bibr" target="#b22">[23]</ref> proved that under a rich (clusterable) dataset model, a one-hidden layer neural network trained with gradient descent first fits the correct labels, and then starts to overfit the noisy labels. This is consistent with the previous empirical findings showing that deep networks tend to learn simple examples first, then gradually memorize harder instances <ref type="bibr" target="#b4">[5]</ref>. In practice, however, regularization and early-stopping provide robustness only under relatively low levels of noise (up to 20% of noisy labels) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Here we develop a principled technique, CRUST, with strong theoretical guarantees for robust training of neural networks against noisy labels. The key idea of our method is to carefully select subsets of clean data points that allow the neural network to effectively learn from the training data, but prevent it to overfit noisy labels. To find such subsets, we rely on recent results that characterize the training dynamics based on properties of neural network Jacobian matrix containing all its first-order partial derivatives. In particular, (1) learning along prominent singular vectors of the Jacobian is fast and generalizes well, while learning along small singular vectors is slow and leads to overfitting; and (2) label noise falls on the space of small singular values and impedes generalization <ref type="bibr" target="#b31">[32]</ref>. To effectively and robustly learn from the training data, CRUST efficiently finds subsets of clean and diverse data points for which the neural network has an approximately low-rank Jacobian matrix.</p><p>We show that the set of medoids of data points in the gradient space that minimizes the average gradient dissimilarity to all the other data points satisfies the above properties. To avoid overfitting noisy labels, CRUST iteratively extracts and trains on the set of updated medoids. We prove that for large enough coresets and a constant fraction of noisy labels, deep networks trained with gradient descent on the medoids found by CRUST do not overfit the noisy labels. We then explain how mixing up <ref type="bibr" target="#b50">[51]</ref> the centers with a few other data points reduces the error of gradient descent updates on the coresets. Effectively, clean coresets found by CRUST improve the generalization performance by reducing the ratio of noisy labels and their alignment with the space of small singular values. singular values. This is consistent with prior results proving the effectiveness of regularization and early stopping for providing robustness against noisy labels <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>. These results, however, are restricted to unrealistically wide networks, and in practice are only effective under low levels of noise.</p><p>On the other hand, our method, CRUST, provides rigorous guarantees for robust training of arbitrary deep neural networks against noisy labels, by efficiently extracting subsets of clean data points that provide an approximately low-rank Jacobian matrix during the training. Effectively, the extracted subsets do not allow the network to overfit noise, and hence CRUST can quickly train a model that generalizes well. Unlike existing analytical results that are limited to a neighborhood around a random initialization, our method captures the change in Jacobian structure of deep networks for arbitrary parameter values during training. As a result, it achieves state-of-the-art performance both under mild as well as severe noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Setting: Learning from Noisy Labeled Data</head><p>In this section we formally describe the problem of learning from datasets with noisy labels. Suppose we have a dataset</p><formula xml:id="formula_0">D = {(x i , y i )} n i=1 ⊂ R d × R,</formula><p>where (x i , y i ) denotes the i-th sample with input x i ∈ R d and its observed label y i ∈ R. We assume that the labels {y i } n i=1 belong to one of C classes. Specifically, <ref type="bibr" target="#b0">1]</ref>. We further assume that the labels are separated with margin δ ≤ ν r − ν s for all r, s ∈ [C], r ≠ s. Suppose we only observe inputs and their noisy labels {y i } n i=1 , but do not observe true labels {ỹ i } n i=1 . For each class 1 ≤ j ≤ C, a fraction of the labels associated with that class are assigned to another label chosen from {ν j } C j=1 . Let f (W , x) be an L-layer fully connected neural network with scalar output, where x ∈ R d is the input and W = (W <ref type="bibr" target="#b0">(1)</ref> , ⋯, W (L) ) is all the network parameters. Here, W (l) ∈ R d l ×d l−1 is the weight matrix in the l-th layer (d 0 = d, d L = 1). For simplicity, we assume all the parameters are aggregated in a vector, i.e., W ∈ R m , where m = ∑ L l=2 d l × d l−1 . Suppose that the network is trained by minimizing the squared loss over the noisy training dataset D = {(x i , y i )} n i=1 :</p><formula xml:id="formula_1">y i ∈ {ν 1 , ν 2 , ⋯, ν C } with {ν j } C j=1 ∈ [−1,</formula><formula xml:id="formula_2">L(W ) = 1 2 i∈V (y i − f (W , x i )) 2 ,<label>(1)</label></formula><p>where V ={1, ⋯, n} is the set of all training examples. We apply gradient descent with a constant learning rate η, starting from an initial point W 0 to minimize L(W ). The iterations take the form</p><formula xml:id="formula_3">W τ +1 = W τ − η∇L(W τ , X), ∇L(W , X) = J T (W , X)(f (W , X) − y),<label>(2)</label></formula><p>where J (W , X) ∈ R n×m is the Jacobian matrix associated with the nonlinear mapping f defined as</p><formula xml:id="formula_4">J (W , X) = ∂f (W , x 1 ) ∂W ⋯ ∂f (W , x n ) ∂W T .<label>(3)</label></formula><p>The goal is to learn a function f ∶ R d → R (in the form of a neural network) that can predict the true labels {ỹ i } n i=1 on the dataset D. In the rest of the paper, we use X = (x 1 , ⋯, x n ), y = (y 1 , ⋯, y n ) T ,ỹ = (ỹ 1 , ⋯,ỹ n ) T . Furthermore, we use X S , y S , and J (W , X S ) to denote inputs, labels, and the Jacobian matrix associated with elements in a subset S ⊆ V of data points, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach: CRUST</head><p>In this section we present our main results. We first introduce our method CRUST that selects subsets of clean data points that has an approximately low-rank Jacobian and do not allow gradient descent to overfit noisy labels. Then, we show how mixing up the subsets with a few other data points can further reduce the error of gradient descent updates, and improve the generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extracting Clean Subsets with Approximately Low-rank Jacobian</head><p>The key idea of our method is to carefully select subsets of data points that allow the neural network to effectively learn from the clean training data, but prevent it to overfit noisy labels. Recent result on optimization and generalization of neural networks show that the Jacobian of typical neural networks exhibits an approximately low-rank structure, i.e., a number of singular values are large and the remaining majority of the spectrum consists of small singular values. Consequently, the Jacobian spectrum can be split into information space I, and nuisance space N , associated with the large and small singular values <ref type="bibr" target="#b31">[32]</ref>. Formally, for complementary subspaces S − , S + ⊂ R n , and for all unit norm vectors v ∈ S + , w ∈ S − , and scalars 0 ≤ µ ≪ α ≤ β, we have</p><formula xml:id="formula_5">α ≤ J T (W , X)v 2 ≤ β, and J T (W , X)w 2 ≤ µ.<label>(4)</label></formula><p>There are two key observations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>: (1) While learning over the low-dimensional information space is fast and generalizes well, learning over the high-dimensional nuisance space is slow and leads to overfitting; and (2) The generalization capability and dynamics of training is dictated by how well the label, y, and residual vector, r = f (W , X) − y, are aligned with the information space. If the residual vector is very well aligned with the singular vectors associated with the top singular values of J (W , X), the gradient update, ∇L(W ) = J T (W , X)r, significantly reduces the misfit allowing substantial reduction in the training error. Importantly, the residual and label of most of the clean data points fall on the information space, while the residual and label of noisy data points fall on the nuisance space and impede training and generalization.</p><p>To avoid overfitting noisy labels, one can leverage the first observation above, and iteratively selects subsets S of k data points that provide the best rank-k approximation to the Jacobian matrix. In doing so, gradient descent applied to the subsets cannot overfit the noisy labels. Formally:</p><formula xml:id="formula_6">S * (W ) = arg min S⊆V J T (W , X) − P S J T (W , X) 2 s.t. S ≤ k,<label>(5)</label></formula><p>where J (W , X S ) ∈ R k×m is the set of k rows of the Jacobian matrix associated to X S , and P S = J T (W , X S )J (W , X S ) denotes the projection onto the k-dimensional space spanned by the rows of J (W , X S ). Existing techniques to find the best subset of k rows or columns from an n × m matrix have a computational complexity of poly(n, m, k) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>, where n, m are the number of data points and parameters in the network. Note that the subset S * (W ) depends on the parameter vector W and a new subset should be extracted after every parameter update. Furthermore, calculating the Jacobian matrix requires backpropagation on the entire dataset which could be very expensive for deep networks. Therefore, the computational complexity of the above methods becomes prohibitive for over-parameterized neural networks trained on large datasets. Most importantly, while this approach prohibits overfitting, it does not help identifying the clean data points.</p><p>To achieve a good generalization performance, our approach takes advantage of both the above mentioned observations. In particular, our goal is to find representative subsets of k diverse data points with clean labels that span the information space I, and provide an approximately low-rank Jacobian matrix. The important observation is that as nuisance space is very high dimensional, data points with noisy labels spread out in the gradient space. In contrast, information space is lowdimensional and data points with clean labels that have similar gradients cluster closely together. The set of most centrally located clean data points in the gradient space can be found by solving the following k-medoids problem:</p><formula xml:id="formula_7">S * (W ) ∈ arg min S⊆V i∈V min j∈S d ij (W ) s.t. S ≤ k,<label>(6)</label></formula><p>where d ij (W ) = ∇L(W , x i ) − ∇L(W , x j ) 2 is the pairwise dissimilarity between gradients of data points i and j. Note that the above formulation does not provide the best rank-k approximation of the Jacobian matrix. However, as the k-medoids objective selects a diverse set of clean data points, the minimum singular value of the Jacobian of the selected subset projected over the subspace S + , i.e., σ min (J (W , X * S ), S + ), will be large. Next, we weight the derivative of every medoid j ∈ S * by the size of its corresponding cluster r j = ∑ i∈V I[j = arg min s∈S * d is ] to create the weighted Jacobian matrix J r (W , X S * ) = diag([r 1 , ⋯, r k ])J(W , X S * ) ∈ R k×m . We can establish the following upper and lower bounds on the singular values σ i∈[k] (J r (W , X S * ), S + ) of the weighted Jacobian over S + :</p><formula xml:id="formula_8">√ r min σ min (J (W , X S * ), S + ) ≤ σ i∈[k] (J r (W , X S * ), S + ) ≤ √ r max J (W , X S * ) ,<label>(7)</label></formula><p>where r min = min j∈[k] r j and r max = max j∈[k] r j , and we get an error of ǫ in approximating the largest singular value of the neural network Jacobian, ǫ ≤ √ r max J (W , X S * ) − J (W , X) . Now, we apply gradient descent updates in Eq. (2) to the weighted Jacobian J r (W , X S * ) of the k extracted medoids:</p><formula xml:id="formula_9">W τ +1 = W τ − ηJ T r (W , X S * )(f (W , X S * ) − y S * ),<label>(8)</label></formula><p>Note that we still need backpropagation on the entire dataset to be able to compute pairwise dissimilarities d ij . For neural networks, it is shown that the variation of the gradient norms is mostly captured by the gradient of the loss w.r.t. the input to the last layer of the network <ref type="bibr" target="#b18">[19]</ref>. This argument can be used to efficiently upper-bound the normed difference between pairwise gradient dissimilarities <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_10">d ij (W ) = ∇L(W , x i ) − ∇L(W , x j ) 2 ≤ c 1 Σ ′ L (z L i )∇ L i L − Σ ′ L (z L j )∇ L j L 2 + c 2 ,<label>(9)</label></formula><p>where Σ ′ L (z L i )∇ L i L is gradient of the loss function L w.r.t. the input to the last layer L for data point i, and c 2 , c 2 are constants. The above upper-bound is marginally more expensive to calculate than the value of the loss since it can be computed in a closed form in terms of z L . Hence,</p><formula xml:id="formula_11">d u ij = Σ ′ L (z L i )∇ L i L − Σ ′ L (z L j )∇ L j L 2</formula><p>can be efficiently calculated. We note that although the upperbounds d u ij have a lower dimensionality than d ij , noisy data points still spread out in this lowerdimensional space, and hence are not selected as medoids. This is confirmed by out experiments ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>).</p><p>Having upper-bounds on the pairwise gradient dissimilarities, we can efficiently find a near-optimal solution for problem (6) by turning it into a submodular maximization problem. A set function</p><formula xml:id="formula_12">F ∶ 2 V → R + is submodular if F (S ∪ {e}) − F (S) ≥ F (T ∪ {e}) − F (T ), for any S ⊆ T ⊆ V and e ∈ V ∖ T . F is monotone if F (e S) ≥ 0 for any e ∈ V ∖S and S ⊆ V .</formula><p>Minimizing the objective in Problem <ref type="formula" target="#formula_7">(6)</ref> is equivalent to maximizing the following submodular facility location function:</p><formula xml:id="formula_13">S * (W ) ∈ arg max S⊆V, S ≤k F (S, W ), F (S, W ) = i∈V max j∈S d 0 − d ′ ij (W ),<label>(10)</label></formula><formula xml:id="formula_14">where d 0 is a constant satisfying d 0 ≥ d u ij (W ), for all i, j ∈ V .</formula><p>For maximizing the above monotone submodular function, the classical greedy algorithm provides a constant (1 − 1 e)-approximation. The greedy algorithm starts with the empty set S 0 = ∅, and at each iteration t, it chooses an element e ∈ V that maximizes the marginal utility</p><formula xml:id="formula_15">F (e S t ) = F (S t ∪ {e}) − F (S t ). Formally, S t = S t−1 ∪ {arg max e∈V F (e S t−1 )}.</formula><p>The computational complexity of the greedy algorithm is O(nk). However, its complexity can be reduced to O( V ) using stochastic methods <ref type="bibr" target="#b28">[29]</ref>, and can be further improved using lazy evaluation <ref type="bibr" target="#b27">[28]</ref> and distributed implementations <ref type="bibr" target="#b30">[31]</ref>. Note that this complexity does not involve any backpropagation as we use the upper-bounds calculated in Eq. (9). Hence, the subsets can be found very efficiently, in parallel from all classes. Unlike majority of the existing techniques for robust training against noisy labels that has a large computational complexity, robust training with CRUST is even faster than training on the entire dataset. We also note that Problem (10) can be addressed in the streaming scenario for very large datasets <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our experiments confirm that CRUST can successfully find almost all the clean data points (c.f. <ref type="figure" target="#fig_0">Fig.  1 (a)</ref>). The following theorem guarantees that for a small fraction ρ of noisy labels in the selected subsets, deep networks trained with gradient descent do not overfit the noisy labels.</p><p>Theorem 4.1 Assume that we apply gradient descent on the least-squares loss in Eq. (2) to train a neural network on a dataset with noisy labels. Furthermore, suppose that the Jacobian mapping is L-smooth 2 . Assume that the dataset has a label margin of δ, and coresets found by CRUST contain a fraction of ρ &lt; δ 8 noisy labels. If the coresets approximate the Jacobian matrix by an error of at most ǫ ≤ O(</p><formula xml:id="formula_16">δα 2 kβ log( √ k ρ) ), where α = √ rminσmin(J (W , X S )), β = J (W , X) +ǫ, then for L ≤ αβ L √ 2k and step size η = 1 2β 2 , after τ ≥ O( 1 ηα 2 log( √ n ρ ))</formula><p>iterations the network classifies all the selected elements correctly.</p><p>The proof can be found in the Appendix. Note that the elements of the selected subsets are mostly clean, and hence the noise ratio ρ is much smaller in the subsets compared to the entire dataset. Very recently, <ref type="bibr" target="#b31">[32]</ref> showed that the classification error of neural networks trained on noisy datasets of size n is controlled by the portion of the labels that fall over the nuisance space, i.e., Π N (y) √ n. Coresets of size k selected by CRUST are mostly clean. For such subsets, the label vector is mostly aligned with the information space, and thus Π N (y S ) √ k is smaller. Our method improves the generalization performance by extracting subsets S of size k for which</p><formula xml:id="formula_17">Π N (y S ) √ k ≤ Π N (y) √ n.</formula><p>While defer the formal generalization proof to future work, our</p><formula xml:id="formula_18">Algorithm 1 CORESETS FOR ROBUST TRAINING AGAINST NOISY LABELS (CRUST) Input: The noisy set D = {(x i , y i )} n i=1 , number of iterations T . Output: Output model parameters W T . 1: for τ = 1, ⋯, T do 2: S τ = ∅. 3: for c ∈ {1, ⋯, C} do 4: U τ c = {(x i , y i ) ∈ D f (W τ , x i ) = ν c }, n c = U τ c n.</formula><p>▷ Classify based on predictions. <ref type="bibr" target="#b4">5</ref>:</p><formula xml:id="formula_19">d u ij =upper-bounded pairwise gradient dissimilarities for i, j ∈ U τ c ▷ Eq. 9. 6: S τ c = {k⋅n c -medoids from U τ c using d u ij } ▷</formula><p>The greedy algorithm. <ref type="bibr">7:</ref> for j ∈ S τ c do 8:</p><formula xml:id="formula_20">V τ j = {i ∈ U τ c j = arg min v∈S τ c d u iv } 9: R τ j = small random sample from V τ j . 10:D τ j =Mixup (x j , y j ) with {(x i , y i ) i ∈ R τ j } ▷ Eq. (11)</formula><p>. <ref type="bibr" target="#b10">11</ref>: Update the parameters W τ using weighted gradient descent on S τ . ▷ Eq. (2). <ref type="bibr" target="#b15">16</ref>: end for experiments show that even under severe noise (80% noisy labels), CRUST successfully finds the clean data points and achieves a superior generalization performance (c.f. <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><formula xml:id="formula_21">r i = V τ j R τ j , ∀i ∈ R τ j ▷ Coreset</formula><p>Next, we discuss how to reduce the error of backpropagation on the weighted centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Further Reducing the Error of Coresets</head><p>There are two potential sources of error during weighted gradient descent updates in Eq. <ref type="bibr" target="#b7">(8)</ref>. First, we have an ǫ error in estimating the prominent singular value of the Jacobian matrix. And second, although the k-medoids formulation selects centers of clustered clean data points in the gradient space, there is still a small chance, in particular early in training process when the gradients are more uniformly distributed, that the coresets contain some noisy labels. Both errors can be alleviated if we slightly relax the constraint of training on exact feature vectors and their labels, and allow training on combinations of every center j ∈S with a few examples in its corresponding cluster V j . This is the idea behind mixup <ref type="bibr" target="#b50">[51]</ref>. It extends the training distribution with convex combinations of pairs of examples and their labels. For every cluster V j , we select a few data points R j ⊂V j ∖ {j}, R j ≪ V j uniformly at random, and for every data point (x i ,y i ), i ∈R j , we mix it up with the corresponding center (x j ,y j ), j ∈S * , to get the setD j of mixed up points:</p><formula xml:id="formula_22">D j = {(x,ŷ) x = λx i + (1 − λ)x j ,ŷ = λy i + (1 − λ)y j ∀i ∈ R j },<label>(11)</label></formula><p>where λ ∼ Beta(α, α) ∈ [0, 1] and α ∈ R + . The mixup hyper-parameter α controls the strength of interpolation between feature-label pairs. Our experiments show that the subsets chosen by CRUST contain mostly clean data points, but may contain some noisy labels early during the training ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>). Mixing up the centers with as few as one example from the corresponding cluster, i.e. R j = 1, can reduce the effect of potential noisy labels in the selected subsets. Therefore, mixup can further help improving the generalization performance, as is confirmed by our experiments ( <ref type="table" target="#tab_2">Table  2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Iteratively Reducing Noise</head><p>The subsets found in Problem (10) depend on the parameter vector W and need to be updated during the training. Let W τ be the parameter vector at iteration τ . At update time τ ∈ [T ], we first classify data points based on the updated predictions y τ = f (W τ , X). We denote by U τ c = {(x i , y i ) ∈ D f (W τ , x i ) = ν c } the set of data points labeled as ν c in iteration τ . Then, we find S(W τ ) by greedily extracting k⋅n c medoids from each class, where n c = U c n is the fraction of data points in class c ∈ [C]. Finding separate coresets from each class can further help to not cluster together noisy data points spread out in the nuisance space, and improves the accuracy of the extracted coresets. Next, we partition the data points in every class to by assigning every data point to its closest medoid. Formally, for partition V τ j we have V τ j = {i ∈ U τ c j = arg min j∈S τ d u ij }. Finally, we take a small random sample R τ j from every partition V τ j , and for every data point i ∈ R τ j we mix it up with the corresponding medoid j ∈ S(W τ ) according to Eq. (11), and add the generated setD τ j of mixed up data points to the training set. In our experiments we use R τ j = 1. At update time τ , we train on the union of sets generated by mixup, i.e.</p><formula xml:id="formula_23">D τ = {D τ 1 ∪ ⋯ ∪D τ k }, where every data point i ∈D τ j is weighted by r i = V τ j R τ j .</formula><p>The pseudo code of CRUST is given in Algorithm 1. Note that while CRUST updates the coreset during the training, as almost all the clean data points are contained in the coresets in every iteration, gradient descent can successfully contract their residuals in Theorem 4.1 and fit the correct labels. Since CRUST finds a new subset at every iteration τ , we need to use α = min τ r τ min σ min (J (W τ , X S τ )), and β = max τ √ r τ max J (W τ , X S τ ) in Theorem 4.1, where α and β are the minimum and maximum singular values of the Jacobian of the subsets weighted by r τ found by CRUST, during T steps of gradient descent updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our method on artificially corrupted versions of CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b21">[22]</ref> with controllable degrees of label noise, as well as a real-world large-scale dataset mini WebVision <ref type="bibr" target="#b23">[24]</ref>, which contains real noisy labels. Our algorithm is developed with PyTorch <ref type="bibr" target="#b32">[33]</ref>. We use 1 Nvidia GTX 1080 Ti for all CIFAR experiments and 4 for training on the mini WebVision dataset.</p><p>Baselines. We compare our approach with multiple state-of-the-art methods for robust training against label corruption. (1) F-correction <ref type="bibr" target="#b33">[34]</ref> first naively trains a neural network using ERM, then estimates the noise transition matrix T . T is then used to construct a corrected loss function with which the model will be retrained. (2) MentorNet <ref type="bibr" target="#b16">[17]</ref> first pretrains a teacher network to mimic a curriculum. The student network is then trained with the sample reweighting scheme provided by the teacher network. (4) D2L <ref type="bibr" target="#b25">[26]</ref> reduces the effect of noisy labels on learning the true data distribution after learning rate annealing using corrected labels. (3) Decoupling <ref type="bibr" target="#b26">[27]</ref> trains two networks simultaneously, and the two networks only train on a subset of samples that do not have the same prediction in every mini batch. (5) Co-teaching <ref type="bibr" target="#b13">[14]</ref> also maintains two networks in the training time. Each network selects clean data (samples with small loss) and guide the other network to train on its selected clean subset. (6) INCV <ref type="bibr" target="#b8">[9]</ref> first estimates the noise transition matrix T through cross validation, then applies iterative Co-teaching by including samples with small losses. (7) T-Revision <ref type="bibr" target="#b45">[46]</ref> designs a deep-learning-based risk-consistent estimator to tune the transition matrix accurately. (8) L_DMI <ref type="bibr" target="#b46">[47]</ref> proposes information theoretic noise-robust loss function based on generalized mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Empirical results on artificially corrupted CIFAR</head><p>We first evaluate our method on CIFAR-10 and CIFAR-100, which contain 50,000 training images and 10,000 test images of size 32 × 32 with 10 and 100 classes, respectively. We follow testing protocol adopted in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>, by considering both symmetric and asymmetric label noise. Specifically, we test noise ratio of 0.2, 0.5, 0.8 for symmetric noise, and 0.4 for asymmetric noise.</p><p>In our experiments, we train ResNet-32 <ref type="bibr" target="#b14">[15]</ref> for 120 epochs with a minibatch of 128. We use SGD with an initial learning rate of 0.1 and decays at epoch 80, 100 by a factor of 10 to optimize the objective, with a momentum of 0.9 and weight decay of 5 × 10 −4 . We only use simple data augmentation following <ref type="bibr" target="#b14">[15]</ref>: we first pad 4 pixels on every side of the image, and then randomly crop a 32 × 32 image from the padded image. We flip the image horizontally with a probability of 0.5. For CRUST, we select coresets of size 50% of the size of the dataset unless otherwise stated. <ref type="table" target="#tab_1">Table 1</ref>. Our proposed method CRUST outperforms all the baselines in terms of average test accuracy. While INCV attempts to find a subset of the training set with heuristics, our theoretically-principled method can successfully distinguish data points with correct labels from those with noisy labels, which results in a clear improvement across all different settings. It can be seen that CRUST achieves a consistent improvement by an average of 3.15%, under various symmetric and asymmetric noisy scenarios, compared to the strongest base-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We report top-1 test accuracy of various methods in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation study on each component</head><p>Here we investigate the effect of each component of CRUST and its importance, for robust training on CIFAR-10 with 20% and 50% symmetric noise. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results.</p><p>Effect of the coreset. Based on the empirical results, the coreset plays an important role in improving the generalization of the trained networks. It is worthwhile noticing that by greedily finding the coreset based on the gradients, we can outperform INCV already. This clearly corroborate our theory and shows the effectiveness of CRUST in filtering the noise and extracting data points with correct labels, compared to other heuristics. It also confirms our argument that although upper-bounded gradient dissimilarities in Eq. (9) has a much lower dimensionality compared to the exact gradients, noisy data points still spread out in the gradient space. Therefore, CRUST can successfully identify central data points with clean labels in the gradient space.</p><p>Effect of mixup and model update. As discussed in Sec. 4.2, mixup can reduce the bias of estimating the full gradient with the coreset. Moreover, finding separate coresets from each class can further help filtering noisy labels, and improves the accuracy of the extracted coresets. At the beginning of every epoch, CRUST updates the predictions based on the current model parameters and extract corsets from every class separately. We observed that both components, namely mixup and extracting coresets separately from each class based on the predictions of the model being trained, further improve the generalization and hence the final accuracy.</p><p>Size of the coresets. <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrates training curve for CIFAR-10 with 50% symmetric noise. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> shows the accuracy of coresets of size 30%, 50%, and 70% selected by CRUST. We observe that for various sizes of coresets, the number of noisy centers decreases over time. Furthermore, the fraction of correct labels in the coresets (label accuracy) decreases when the size of the selected centers increases from 30% to 70%. This demonstrates that CRUST identifies clean data points  first. <ref type="figure" target="#fig_0">Fig. 1 (b), (c)</ref> show the train and test accuracy, when training with CRUST on coresets of various sizes. We can see that coresets of size 30% achieve a lower accuracy as they are too small to accurately estimate the spectrum of the information space and achieve a good generalization performance. Coresets of size 70% achieve a lower training and test accuracy compared to coresets of size 50%. As 50% of the labels are noisy, subsets of size 70% contain at least 20% noisy labels and the model eventually overfits the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Empirical results on mini WebVision</head><p>WebVision is real-world dataset with inherent noisy labels <ref type="bibr" target="#b23">[24]</ref>. It contains 2.4 million images crawled from Google and Flickr that share the same 1000 classes from the ImageNet dataset. The noise ratio in classes varies from 0.5% to 88% ( <ref type="figure">Fig. 4</ref> in <ref type="bibr" target="#b23">[24]</ref> shows the noise distribution). We follow the setting in <ref type="bibr" target="#b16">[17]</ref> and create a mini WebVision that consists of the top 50 classes in the Google subset with 66,000 images. We use both WebVision and ImageNet test sets for testing the performance of the model trained on coresets of size 50% of the data found by CRUST. We train InceptionResNet-v2 <ref type="bibr" target="#b38">[39]</ref> for 90 epochs with a starting learning rate of 0.1. We anneal the learning rate at epoch 30 and 60, respectively. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. It can be seen that our method consistently outperforms other baselines, and achieves an average of 5% improvement in the test accuracy, compared to INCV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a novel approach with strong theoretical guarantees for robust training of neural networks against noisy labels. Our method, CRUST, relies on the following key observations: (1) Learning along prominent singular vectors of the Jacobian is fast and generalizes well, while learning along small singular vectors is slow and leads to overfitting; and (2) The generalization capability and dynamics of training is dictated by how well the label and residual vector are aligned with the information space. To achieve a good generalization performance and avoid overfitting, CRUST iteratively selects subsets of clean data points that provide an approximately low-rank Jacobian matrix.</p><p>We proved that for a constant fraction of noisy labels in the subsets, neural networks trained with gradient descent applied to the subsets found by CRUST correctly classify all its data points. At the same time, our method improves the generalization performance of the deep network by decreasing the portion of noisy labels that fall over the nuisance space of the network Jacobian. Our extensive experiments demonstrated the effectiveness of our method in providing robustness against noisy labels. In particular, we showed that deep networks trained on the our subsets achieve a significantly superior performance, e.g., 6% increase in accuracy on CIFAR-10 with 80% noisy labels, and 7% increase in accuracy on mini Webvision, compared to state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Deep neural networks achieve impressive results in a wide variety of domains, including vision and speech recognition. The quality of the trained deep models on such datasets increases logarithmically with the size of the data <ref type="bibr" target="#b37">[38]</ref>. This improvement, however, is contingent on the availability of reliable and accurate labels. In practice, collecting large high quality datasets is often very expensive and time-consuming. For example, labeling of medical images depends on domain experts and hence is very resource-intensive. In some applications, it necessitate obtaining consensus labels or labels from multiple experts and methods for aggregating those annotations to get the ground truth labels <ref type="bibr" target="#b17">[18]</ref>. In some domains, crowd-sourcing methods are used to obtain labels from non-experts. An alternative solution is automated mining of data, e.g., from the Internet by using different image-level tags that can be regarded as labels. These solutions are cheaper and more time-efficient than human annotations, but label noise in such datasets is expected to be higher than in expert-labeled datasets. Noisy labels have a drastic effect on the generalization performance of deep neural networks. This prevents deep networks from being employed in real-world noisy scenarios, in particular in safety critical applications such as aircraft, autonomous cars, and medical devices.</p><p>State-of-the art methods for training deep networks with noisy labels are mostly heuristics and cannot provide theoretical guarantees for the robustness of the trained model in presence of noisy labels. Failure of such systems can have a drastic effect in sensitive and safety critical applications. Our research provides a principled method for training deep networks on real-world datasets with noisy labels. Our proposed method, CRUST, is based on the recent advances in theoretical understanding of neural networks, and provides theoretical guarantee for the performance of the deep networks trained with noisy labels. We expect our method to have a far-reaching impact in deployment of deep neural networks in real-world systems. We believe our research will be beneficial for deep learning in variety of domains, and do not have any societal or ethical disadvantages.</p><p>Our main contribution is to show that for any dataset, clean data points cluster together in the gradient space and hence medoids of the gradients (1) have clean labels, and (2) provide a low-rank approximation of the Jacobian, J , of an arbitrary deep network. Hence, training on the medoids is robust to noisy labels. Our analysis of the residuals during gradient descent builds on the analysis of <ref type="bibr" target="#b22">[23]</ref>, but generalize it to arbitrary deep networks without the need for over-parameterization, and random initialization. Crucially, <ref type="bibr" target="#b22">[23]</ref> relies on the following assumptions to show that gradient descent with early stopping is robust to label noise, with a high probability: (1) data X ⊂ R n×d has K clusters, <ref type="bibr" target="#b1">(2)</ref> neural net f has one hidden layer with k neurons, i.e., f =φ(XW T )ν, (3) output weights ν are fixed to half +1 √ k, and half −1 √ k, (4) network is over-parameterized, i.e., k ≥ K 4 , where K= O(n), (5) the input-to-hidden weights W 0 have random Gaussian initialization. Indeed, from the clusterable data assumption it easily follows that the neural network covariance</p><formula xml:id="formula_24">Σ(X) = E W φ ′ (XW T )φ ′ (W T X) ⊙ XX T = 1 k E W 0 J (W 0 )J T (W 0 )</formula><p>] is low-rank, and hence early stopping prevents overfitting noisy labels. In contrast, our results holds for arbitrary deep networks without relying on the above assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs for Theorems</head><p>The following Corollary builds upon the meta Theorem 7.2 from <ref type="bibr" target="#b22">[23]</ref> and captures the contraction of the residuals during gradient descent updates on a dataset with corrupted labels, when the Jacobian is exactly low-rank. The original theorem in <ref type="bibr" target="#b22">[23]</ref> is based on the assumptions that the fraction of corrupted labels in the dataset is small, and the dataset is clusterable. Hence S + is dictated by the membership of data points to different clusters. In contrast, our method CRUST applies gradient descent only to the selected subsets. Note that the elements of the selected subsets are mostly clean, and hence the extracted subsets have a significantly smaller fraction of noisy labels. The elements selected earlier by CRUST are medoids of the main clusters in the gradient space. As we keep selecting new data points, we extract medoids of the smaller groups of data points within the main clusters. Therefore, in our method the support subspace S + is defined by the assignment of the elements of the extracted subsets to the main clusters.</p><p>More specifically, assume that there are K &lt; k main clusters in the gradient space. We denote the set of central elements of the main clusters byS. For a subset S ⊆ V of size k selected by CRUST and upper-bounds d u on gradient dissimilarities from Eq. (9), let Λ ℓ = {i ∈ [k] ℓ = arg min s∈S d u is } be the set of elements in S that are closest to an element ℓ ∈S, i.e., they lie within the main cluster l in the gradient space. Then, S + is characterized by</p><formula xml:id="formula_25">S + = {v ∈ R k v i1 = v i2 for all i 1 , i 2 ∈ Λ ℓ and for all 1 ≤ ℓ ≤ K}.<label>(12)</label></formula><p>The following corollary captures the contraction of the residuals during gradient descent on subsets found by CRUST, when the Jacobian is low-rank. In Theorem 4.1, we characterize the contraction of the residuals, when the Jacobian is approximately low-rank, i.e., over S − the spectral norm is small but nonzero. . Assume that the weighted Jacobian of the subset S found by CRUST is low-rank, i.e., for all v ∈ S + and w ∈ S − with unit Euclidean norm, and β ≥ α &gt; 0 we have that α ≤ J T r (W , X S )v ℓ2 ≤ β and J T r (W , X S )w ℓ2 = 0. Moreover, assume that the Jacobian mapping J (W, X S ) associated to the nonlinear mapping f is L-smooth, i.e., for all W 1 ,</p><formula xml:id="formula_26">W 2 ∈ R m we have J (W 2 ) − J (W 1 ) ≤ L W 2 − W 1</formula><p>ℓ2 . <ref type="bibr" target="#b2">3</ref> Also let y s ,ỹ S , e = y S −ỹ S ∈ R k denote the corrupted and uncorrupted labels associated with the selected subset, and label corruption, respectively. Furthermore, suppose the initial residual f (W 0 , X S ) −ỹ S with respect to the uncorrupted labels obey f (W 0 , X S ) −ỹ S ∈ S + . Then, iterates of gradient descent updates of the from (2) with a learning rate η ≤ 1 2β 2 min 1, αβ</p><formula xml:id="formula_27">L r 0 ℓ 2 , obey W τ − W 0 ℓ2 ≤ 4 r 0 ℓ2 α = 4 f (W 0 , X S ) − y S ℓ2 α .</formula><p>Furthermore, if λ &gt; 0 is a precision level obeying λ ≥ Π S+ (e) ℓ∞ , then running gradient descent updates of the from (2) with a learning rate η ≤ 1 2β 2 min 1, αβ</p><formula xml:id="formula_28">L r 0 ℓ 2 , after τ ≥ 5 ηα 2 log r 0 ℓ 2 λ</formula><p>iterations, W τ achieves the following error bound with respect to the true labels</p><formula xml:id="formula_29">f (W τ , X S ) −ỹ S ℓ∞ ≤ 2λ.</formula><p>Finally, if e has at most s nonzeros and S + is γ-diffused i.e., for any vector v ∈ S + we have v ℓ∞ ≤ γ n v ℓ2 for some γ &gt; 0, then using λ = Π S+ (e) ℓ∞ , we get</p><formula xml:id="formula_30">f (W τ , X S ) −ỹ S ℓ∞ ≤ 2 Π S+ (e) ℓ∞ ≤ γ √ s n e ℓ2 .</formula><p>Lemma A.2 Let {(x i , y i )} i∈S be the subset selected by CRUST, and {ỹ i } i∈S be the corresponding noiseless labels. Note that the elements of the selected subsets are mostly clean, but may contain a smaller fraction ρ of noisy labels. Let J (W , X S ) be the Jacobian matrix corresponding to the selected subset which is rank k, and S + be its column space. Then, the difference between noiseless and noisy labels satisfy the bound</p><formula xml:id="formula_31">Π S+ (y S −ỹ S ) ℓ∞ ≤ 2ρ.</formula><p>Proof The proof is similar to that of Lemma 8.10 in <ref type="bibr" target="#b22">[23]</ref>, but using S + as defined in Eq. (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Theorem 4.1</head><p>We first consider the case where the set S ⊆ V of k weighted medoids found by CRUST approximates the largest singular value of the neural network Jacobian by an error of ǫ = 0. Therefore, we can apply Corollary A.3 to characterize the behavior of gradient descent on the weighted subsets found by CRUST. The following Corollary summarizes the results:</p><p>Corollary A.3 Assume that we apply gradient descent on the least-squares loss in Eq.</p><p>(2) to train a neural network on subsets found by CRUST from a dataset with class labels {ν j } C j=1 ∈ [−1, 1], label margin δ. Suppose that the Jacobian mapping is L-smooth, and let α = min τ r τ min σ min (J (W τ , X S τ )), and β = max τ √ r τ max J (W τ , X S τ ) be the minimum and maximum singular values of the Jacobian of the subsets weighted by r τ found by CRUST, during τ steps of gradient descent updates. If subsets contain a fraction of ρ ≤ δ 8 noisy labels, using step size η = 1 2β 2 min(1, αβ L √ 2k ), after τ = O( 1 ηα 2 log( √ k ρ )) iterations, the neural network classifies all the selected elements correctly.</p><p>Proof Fix a vector v and letp = J r (W , X S )v and p = J (W , X S )v. Entries ofp multiply the entries of p somewhere between r min and r max . This establishes the upper and lower bounds on the singular values of J r (W , X S ) over S + in terms of the singular values of J (W , X S ). I.e., √ r min σ min (J (W , X S * ),</p><formula xml:id="formula_32">S + ) ≤ σ i∈[k] (J r (W , X S * ), S + ) ≤ √ r max J (W , X S * ) .<label>(13)</label></formula><p>Therefore, we get that α = min τ r τ </p><formula xml:id="formula_33">min σ min (J (W τ , X τ S )), β = max τ √ r τ max J (W τ , X τ S</formula><formula xml:id="formula_34">Π S+ (y −ỹ) ℓ∞ ≤ 2ρ.</formula><p>Substituting the values corresponding to α, β in Theorem 4.1, we get that after</p><formula xml:id="formula_35">5 ηα 2 log( r 0 ℓ2 2ρ ) ≤ 5 ηα 2 log( √ 2k 2ρ ) ≤ τ<label>(14)</label></formula><p>gradient descent iterations, the error bound with respect to the true labels is f (W τ , X S ) −ỹ S ℓ∞ ≤ 2ρ. If gradient clusters are roughly balanced, i.e., there are O(k K ′ ) data points in each cluster, we get that for all gradient iterations with</p><formula xml:id="formula_36">5 ηα 2 log( r 0 ℓ2 2ρ ) ≤ 5 ηα 2 log( √ 2k 2ρ ) = O( K ηkσ 2 min J (W , X S ) log( √ k ρ )) ≤ τ,<label>(15)</label></formula><p>the infinity norm of the residual obeys (using</p><formula xml:id="formula_37">λ = Π S+ (e) ℓ∞ ≤ 2ρ) f (W , X S ) −ỹ ℓ∞ ≤ 4ρ.</formula><p>This implies that if ρ ≤ δ 8, the labels predicted by the network are away from the correct labels by less than δ 2, hence the elements of the subsets (including noisy ones) will be classified correctly.</p><p>A.2 Completing the Proof of Theorem 4.1</p><p>Next, we consider the case where weighted subsets found by CRUST approximate the prominent singular value of the Jacobian matrix by an error of at most ǫ. Here, we characterize the behavior of gradient descent by comparing the iterations with and without error.</p><p>In particular, starting from W 0 =W 0 consider the gradient descent iterations on the weighted subsetsS that estimating the largest singular value of the neural network Jacobian without an error,</p><formula xml:id="formula_38">W τ +1 =W τ − ηJ T r (W τ , XSτ )(f (W τ , X) − ySτ ),<label>(16)</label></formula><p>and gradient descent iterations on the weighted subsets S with an error of at most ǫ in estimating the largest singular value of the neural network Jacobian,</p><formula xml:id="formula_39">W τ +1 = W τ − ηJ T r (W τ , X S τ )(f (W τ , X S τ ) − y S τ ).<label>(17)</label></formula><p>To proceed with the proof, we use the following short hand notations for residuals and Jacobian matrix in iteration τ of gradient descent:</p><formula xml:id="formula_40">r τ = f (W τ , X S τ ) − y S τ ,r τ = f (W τ , XSτ ) − ySτ (18) J τ = J r (W τ , X S τ ), J τ +1,τ = J r (W τ +1 , W τ , X S τ ),<label>(19)</label></formula><p>J τ = J r (W τ , X S τ ),J τ +1,τ = J r (W τ +1 ,W τ , XSτ ) (20)</p><formula xml:id="formula_41">d τ = W τ −W τ F , p τ = r τ −r τ F ,<label>(21)</label></formula><p>where J r (W 1 , W 2 , X S ) denotes the average weighted neural network Jacobian at subset X S , i.e., J r (W 1 , W 2 , X S ) = 1 0 J r (αW 1 + (1 − α)W 2 , X S )dα.</p><p>We first proof the following Lemma that bounds the normed difference between J r (W 1 ,W 2 , XS) and J r (W 1 , W 2 , X S ).</p><p>Lemma A.4 Let X S , XS be the subset of data points found by CRUST, that approximates the Jacobian matrix on the entire data by and error of 0 and ǫ, respectively. Given parameters W 1 , W 2 ,W 1 ,W 2 , we have that</p><formula xml:id="formula_42">J r (W 1 , W 2 , X S ) − J r (W 1 ,W 2 , XS) ≤ ( W 1 − W 1 F + W 2 − W 2 F 2 + ǫ).</formula><p>Proof Given W ,W , we can write J r (W , X S ) − J r (W , XS) ≤ J r (W , X S ) − J r (W , X S ) + J r (W , XS) − J r (W , X S )</p><formula xml:id="formula_43">(22) ≤ L W −W + ǫ.<label>(23)</label></formula><p>To get the result on J (W 1 , W 2 , X S ) − J r (W 1 ,W 2 , XS) , we integrate</p><formula xml:id="formula_44">J (W 1 , W 2 , X S ) − J r (W 1 ,W 2 , XS) ≤ 1 0 (L α(W 1 − W 1 ) + (1 − α)(W 1 − W 1 ) F + ǫ)dα<label>(24)</label></formula><formula xml:id="formula_45">≤ L( W 1 − W 1 F + W 2 − W 2 F ) 2 + ǫ.<label>(25)</label></formula><p>Now, applying Lemma A.4, we have</p><formula xml:id="formula_46">J r (W τ , X S τ ) − J r (W τ , XSτ ) ≤ L W τ − W τ F + ǫ ≤ Ld τ + ǫ (26) J r (W τ +1 , W τ , X S τ ) − J r (W τ +1 ,W τ , XSτ ) ≤ L(d τ + d τ +1 ) 2 + ǫ.<label>(27)</label></formula><p>Following this and since the normed noiseless residual is non-increasing and satisfies r τ ℓ2 ≤ r 0 ℓ2 , we can write</p><formula xml:id="formula_47">W τ +1 = W τ − ηJ τ r τ ,W τ +1 =W τ − η(J τ ) Trτ (28) W τ +1 −W τ +1 F ≤ W τ −W τ F + η J τ −J τ r τ ℓ2 + η J τ r τ −r τ ℓ2 ,<label>(29)</label></formula><formula xml:id="formula_48">d τ +1 ≤ d τ + η (Ld τ + ǫ r 0 ℓ2 + βp τ ).<label>(30)</label></formula><p>For the residual we have</p><formula xml:id="formula_49">r τ +1 = r τ − f (W τ , X S ) + f (W τ +1 , X S )<label>(31)</label></formula><formula xml:id="formula_50">= r τ + J τ +1,τ (W τ +1 − W τ ) (32) = r τ − ηJ τ +1,τ (J τ ) T r τ ,<label>(33)</label></formula><p>where in Eq. (33) we used W τ +1 − W τ = η∇L(W τ ) = η(J τ ) T r τ . Furthermore, we can write</p><formula xml:id="formula_51">r τ +1 −r τ +1 = (r τ −r τ ) − η(J τ +1,τ −J τ +1,τ )(J τ ) T r τ (34) − ηJ τ +1,τ (J τ ) T − (J τ ) T r τ − ηJ τ +1,τ (J τ ) T (r τ −r τ )<label>(35)</label></formula><formula xml:id="formula_52">= I − ηJ τ +1,τ (J τ ) T (r τ −r τ ) − η(J τ +1,τ −J τ +1,τ )(J τ ) T r τ (36) − ηJ τ +1,τ (J τ ) T − (J τ ) T r τ .<label>(37)</label></formula><p>Using I ⪰J τ +1,τ (J τ ) T β 2 ⪰ 0, we have r τ +1 −r τ +1 ℓ2 ≤ r τ −r τ ℓ2 + ηβ r τ ℓ2 (L(3d τ + d τ +1 ) 2 + 2ǫ)</p><p>≤ r τ −r τ ℓ2 + ηβ( r 0 ℓ2 + p τ )(L(3d τ + d τ +1 ) 2 + 2ǫ),</p><p>where we used r τ ℓ2 ≤ p τ + r 0 ℓ2 and (I − ηJ τ +1,τ (J τ ) T )v ℓ2 ≤ v ℓ2 which follows from the contraction of the residual. This implies that p τ +1 ≤ p τ + ηβ( r 0 ℓ2 + p τ )(L(3d τ + d τ +1 ) 2 + 2ǫ).</p><p>We use a similar inductive argument as that of Theorem 8.10 in <ref type="bibr" target="#b22">[23]</ref>. The claim is that if for all t ≤ τ 0 , we have (using r 0 ℓ2 ≤ Θ)</p><formula xml:id="formula_56">ǫ ≤ O( α 2 β log( √ k ρ)</formula><p>) ≤ O( kσ 2 min (J (W , X S )) Kβ log( √ k ρ) ), and L ≤ 2 5τ 0 ηΘ(1 + 8ητ 0 β 2 )) ,</p><p>then it follows that</p><formula xml:id="formula_58">p t ≤ 8tηǫΘβ, d t ≤ 2tηǫΘ(1 + 8ητ 0 β 2 ) ≤ 20tη 2 τ 0 ǫΘβ 2 ≤ O(tη 2 τ 0 k 3 2 ǫ).<label>(42)</label></formula><p>The proof is by induction. Suppose for t ≤ τ 0 − 1, we have that</p><formula xml:id="formula_59">p t ≤ 8tηǫΘβ ≤ Θ, d t ≤ 2tηǫΘ(1 + 8ητ 0 β 2 ).<label>(43)</label></formula><p>At t + 1, from (30) we know that</p><formula xml:id="formula_60">d t+1 − d t η ≤ Ld t Θ + ǫΘ + 8τ 0 ηβ 2 ǫΘ<label>(44)</label></formula><p>Now, using L ≤ 2 5τ0ηΘ(1+8ητ0β 2 )) ≤ 1 2ητ0Θ from (41), and replacing d t from (43) into (44) we get</p><formula xml:id="formula_61">d t+1 − d t η ≤ Ld t Θ + ǫΘ + 8τ 0 ηβ 2 ǫΘ ? ≤ 2ǫΘ(1 + 8ητ 0 β 2 ).<label>(45)</label></formula><p>This establishes the induction for d t+1 .</p><p>To show the induction on p t , following (8.64) and using p t ≤ Θ, we need p t+1 − p t η ≤ βΘ(L(3d τ + d τ +1 ) + 4ǫ)</p><formula xml:id="formula_62">? ≤ 8ǫΘβ (46) L(3d τ + d τ +1 ) + 4ǫ ? ≤ 8ǫ<label>(47)</label></formula><formula xml:id="formula_63">L(3d τ + d τ +1 ) ? ≤ 4ǫ (48) 10Lτ 0 η(1 + 8ητ 0 β 2 )Θ ? ≤ 4,<label>(49)</label></formula><p>where in the last inequality we used 3d t + d t+1 ≤ 10τ 0 ηǫΘ(1 + 8ητ 0 β 2 ). Note that η = ). Hence, if αβ L √ 2k ≥ 1, we get that η = 1 2β 2 ≥ 1 τ0β 2 , and thus ητ 0 β 2 ≥ 1. This allows upper-bounding 3d t + d t+1 . Now, for L we have L≤ 2 5τ 0 ηΘ(1 + 8ητ 0 β 2 )) .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Training curve for CIFAR-10 with 50% symmetric noise. (a) The fraction of correct labels in the coreset (label accuracy) with respect to epochs. (b) The accuracy on the whole training set with respect to epochs. (c) The accuracy on test set with respect to epochs. Here we consider coresets of size 30%, 50%, and 70% of the CIFAR-10 training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Corollary A. 1 1 2</head><label>11</label><figDesc>Consider a nonlinear least squares problem of the form L(W , X) = f (W, X) − y) 2 ℓ2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 2β 2</head><label>12</label><figDesc>min(1, αβ L √ 2k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Average test accuracy (5 runs) on CIFAR-10 and CIFAR-100. The best test accuracy is marked in bold. CRUST achieves up to 6% improvement (3.15% in average) over the strongest baseline INCV. We note the superior performance of CRUST under 80% label noise.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Noise Type</cell><cell></cell><cell>Sym</cell><cell></cell><cell>Asym</cell><cell>Sym</cell><cell></cell><cell>Asym</cell></row><row><cell>Noise Ratio</cell><cell>20</cell><cell>50</cell><cell>80</cell><cell>40</cell><cell>20</cell><cell>50</cell><cell>40</cell></row><row><cell cols="8">F-correction 85.1 ± 0.4 76.0 ± 0.2 34.8 ± 4.5 83.6 ± 2.2 55.8 ± 0.5 43.3 ± 0.7 42.3 ± 0.7 Decoupling 86.7 ± 0.3 79.3 ± 0.6 36.9 ± 4.6 75.3 ± 0.8 57.6 ± 0.5 45.7 ± 0.4 43.1 ± 0.4 Co-teaching 89.1 ± 0.3 82.1 ± 0.6 16.2 ± 3.2 84.6 ± 2.8 64.0 ± 0.3 52.3 ± 0.4 47.7 ± 1.2 MentorNet 88.4 ± 0.5 77.1 ± 0.4 28.9 ± 2.3 77.3 ± 0.8 63.0 ± 0.4 46.4 ± 0.4 42.4 ± 0.5 D2L 86.1 ± 0.4 67.4 ± 3.6 10.0 ± 0.1 85.6 ± 1.2 12.5 ± 4.2 5.6 ± 5.4 14.1 ± 5.8 INCV 89.7 ± 0.2 84.8 ± 0.3 52.3 ± 3.5 86.0 ± 0.5 60.2 ± 0.2 53.1 ± 0.4 50.7 ± 0.2 T-Revision 79.3 ± 0.5 78.5 ± 0.6 36.2 ± 1.6 76.3 ± 0.8 52.4 ± 0.3 37.6 ± 0.3 32.3 ± 0.4 L_DMI 84.3 ± 0.4 78.8 ± 0.5 20.9 ± 2.2 84.8 ± 0.7 56.8 ± 0.4 42.2 ± 0.5 39.5 ± 0.4 CRUST 91.1 ± 0.2 86.3 ± 0.3 58.3 ± 1.8 88.8 ± 0.4 65.2 ± 0.2 56.4 ± 0.4 53.0 ± 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Component</cell><cell></cell><cell></cell><cell cols="2">Noise Ratio</cell></row><row><cell cols="4">coreset w/ label coreset w/ pred. w/o mixup w/ mixup</cell><cell>20</cell><cell>50</cell></row><row><cell>✓ ✓</cell><cell>✓ ✓</cell><cell>✓ ✓</cell><cell>✓ ✓</cell><cell cols="2">90.21 84.92 90.48 85.23 90.71 85.57 91.12 86.27</cell></row></table><note>Ablation study on CIFAR-10 with 20% and 50% symmetric noise. ✓ indicates the corre- sponding component. coreset w/label and coreset w/label correspond to finding coresets separately from every class based on their observed noisy labels, or labels predicted by the model being trained.line INCV. Interestingly, CRUST achieves the largest improvement of 6% over INCV, under sever 80% label noise. This shows the effectiveness of our method in extracting data points with clean labels from a large number of noisy data points, compared to other baselines.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy on mini WebVision. The best test accuracy is marked in bold. CRUST achieves up to 7.16% improvement (6.46% in average) in top-1 accuracy over the strongest baseline INCV.</figDesc><table><row><cell></cell><cell>WebVision</cell><cell>ImageNet</cell></row><row><cell>Method</cell><cell cols="2">Top-1 Top-5 Top-1 Top-5</cell></row><row><cell cols="3">F-correction 61.12 82.68 57.36 82.36</cell></row><row><cell>Decoupling</cell><cell cols="2">62.54 84.74 58.26 82.26</cell></row><row><cell cols="3">Co-teaching 63.58 85.20 61.48 84.70</cell></row><row><cell>MentorNet</cell><cell cols="2">63.00 81.40 57.80 79.92</cell></row><row><cell>D2L</cell><cell cols="2">62.68 84.00 57.80 81.36</cell></row><row><cell>INCV</cell><cell cols="2">65.24 85.34 61.60 84.98</cell></row><row><cell>CRUST</cell><cell cols="2">72.40 89.56 67.36 87.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>) . Moreover, we have f ∶ R d → [−1, 1], and class labels {ν j } C j=1 ∈ [−1, 1]. Hence, for every element i ∈ S, we have f (W , x i ) − y i ≤ 2, and the upper-bound on the initial misfit is r 0</figDesc><table><row><cell>l2 ≤</cell><cell>√</cell><cell>2k.</cell></row><row><cell>Now, using Lemma A.2, we know that</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at https://github.com/snap-stanford/crust. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that, if ∂J (W ,X) ∂W is continuous, the smoothness condition holds over any compact domain (albeit for a possibly large L).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that, if ∂J (W ) ∂W is continuous, the smoothness condition holds over any compact domain (albeit for a possibly large L).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of DARPA under Nos. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This concludes the induction since the condition on L is satisfied. Now, from <ref type="bibr" target="#b43">(44)</ref> and using ητ 0 β 2 ≥ 1 we get</p><p>Finally, from <ref type="formula">(43)</ref> </p><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we calculate the misclassification error. From Corollary A.3 and Eq. (43) we have that for</head><p>To calculate the classification rate, we denote the residual vectorsr τ = f (W τ , XSτ ) −ỹ S τ and r τ = f (W τ , X S τ ) −ỹ S τ . Now, we count the number of entries of r τ that is larger than the label margin δ 2 in absolute value. Let I be the set of entries satisfying this condition. For i ∈ I we have r τ i ≥ δ 2. Therefore,</p><p>(55) Thus, the sum of the entries with a larger error than the label margin is r τ −r τ ℓ1 ≥ I γδ 2.</p><p>(56) Consequently, we have</p><p>Hence, the total number of errors is at most</p><p>For the network to classify all the data points correctly, we need I ≤ c ′ ǫ .</p><p>(59)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning and generalization in overparameterized neural networks, going beyond two layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6155" to="6166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy column subset selection: New bounds and distributed algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Altschuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahab</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2539" to="2548" />
		</imprint>
	</monogr>
	<note>Afshin Rostamizadeh, and Morteza Zadimoghaddam</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Streaming submodular maximization: Massive data summarization on the fly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwinkumar</forename><surname>Badanidiyuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="671" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Heteroskedastic and imbalanced deep learning with adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15766</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A generalization theory of gradient descent for learning overparameterized deep relu networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01384</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Ben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Input sparsity time low-rank approximation via ridge leverage score sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Michael B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Musco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1758" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Understanding generalization of deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11368</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep learning with noisy labels: exploring techniques and remedies in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davood</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gholipour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02911</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Not all samples are created equal: Deep learning with importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2525" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On approximation guarantees for greedy low rank optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Embracing error to enable rapid crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI conference on human factors in computing systems</title>
		<meeting>the 2016 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3167" to="3179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11680</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accelerated greedy algorithms for maximizing submodular set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Minoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1978" />
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vondrák, and Andreas Krause. Lazier than lazy greedy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwinkumar</forename><surname>Baharan Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Badanidiyuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Coresets for data-efficient training of machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Baharan Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01827</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed submodular maximization: Identifying representative elements in massive data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2049" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards moderate overparameterization: global convergence guarantees for training shallow neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Information Theory</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Early stopping and non-parametric regression: an optimal data-dependent stopping rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvesh</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="335" to="366" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Imae for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Early stopping for kernel boosting algorithms: A general analysis with localized complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6065" to="6075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6838" to="6849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6225" to="6236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Distilling effective supervision from severe label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
