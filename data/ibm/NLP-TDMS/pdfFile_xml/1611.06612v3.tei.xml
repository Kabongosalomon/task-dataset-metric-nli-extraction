<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<email>ian.reid@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new stateof-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a crucial component in image understanding. The task here is to assign a unique label (or category) to every single pixel in the image, which can be considered as a dense classification problem. The related problem of so-called object parsing can usually be cast as semantic segmentation. Recently, deep learning methods, and in particular convolutional neural networks (CNNs), e.g., VGG <ref type="bibr" target="#b41">[42]</ref>, Residual Net <ref type="bibr" target="#b23">[24]</ref>, have shown remarkable results in recognition tasks. However, these approaches exhibit clear limitations when it comes to dense prediction in tasks like dense depth or normal estimation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>  and semantic segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5]</ref>. Multiple stages of spatial pooling and convolution strides reduce the final image prediction typically by a factor of 32 in each dimension, thereby losing much of the finer image structure.</p><p>One way to address this limitation is to learn deconvolutional filters as an up-sampling operation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36]</ref> to generate high-resolution feature maps. The deconvolution operations are not able to recover the low-level visual features which are lost after the down-sampling operation in the convolution forward stage. Therefore, they are unable to output accurate high-resolution prediction. Low-level visual information is essential for accurate prediction on the boundaries or details. The method DeepLab recently proposed by Chen et al. <ref type="bibr" target="#b5">[6]</ref> employs atrous (or dilated) convolutions to account for larger receptive fields without downscaling the image. DeepLab is widely applied and represents state-ofthe-art performance on semantic segmentation. This strategy, although successful, has at least two limitations. First, it needs to perform convolutions on a large number of detailed (high-resolution) feature maps that usually have highdimensional features, which are computational expensive. Moreover, a large number of high-dimensional and highresolution feature maps also require huge GPU memory resources, especially in the training stage. This hampers the computation of high-resolution predictions and usually limits the output size to 1/8 of the original input. Second, dilated convolutions introduce a coarse sub-sampling of features, which potentially leads to a loss of important details.</p><p>Another type of methods exploits features from intermediate layers for generating high-resolution prediction, e.g., the FCN method in <ref type="bibr" target="#b35">[36]</ref> and Hypercolumns in <ref type="bibr" target="#b21">[22]</ref>. The intuition behind these works is that features from middle layers are expected to describe mid-level representations for object parts, while retaining spatial information. This information is though to be complementary to the features from early convolution layers which encode low-level spatial visual information like edges, corners, circles, etc., and also complementary to high-level features from deeper layers which encode high-level semantic information, including object-or category-level evidence, but which lack strong spatial information.</p><p>We argue that features from all levels are helpful for semantic segmentation. High-level semantic features helps the category recognition of image regions, while low-level visual features help to generate sharp, detailed boundaries for high-resolution prediction. How to effectively exploit middle layer features remains an open question and deserves more attentions. To this end we propose a novel network architecture which effectively exploits multi-level features for generating high-resolution predictions. Our main contributions are as follows:</p><p>1. We propose a multi-path refinement network (Re-fineNet) which exploits features at multiple levels of abstraction for high-resolution semantic segmentation. RefineNet refines low-resolution (coarse) semantic features with fine-grained low-level features in a recursive manner to generate high-resolution semantic feature maps. Our model is flexible in that it can be cascaded and modified in various ways.</p><p>2. Our cascaded RefineNets can be effectively trained end-to-end, which is crucial for best prediction performance. More specifically, all components in Re-fineNet employ residual connections <ref type="bibr" target="#b23">[24]</ref> with identity mappings <ref type="bibr" target="#b24">[25]</ref>, such that gradients can be directly propagated through short-range and long-range residual connections allowing for both effective and efficient end-to-end training. To facilitate future research, we release both source code and trained models for our RefineNet. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>CNNs become the most successful methods for semantic segmentation in recent years. The early methods in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> are region-proposal-based methods which classify region proposals to generate segmentation results. Recently fully convolution network (FCNNs) based based methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> show effective feature generation and end-toend training, and thus become the most popular choice for semantic segmentation. FCNNs have also been widely applied in other dense-prediction tasks, e.g., depth estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref>, image restoration <ref type="bibr" target="#b13">[14]</ref>, image super-resolution <ref type="bibr" target="#b11">[12]</ref>. The proposed method here is also based on fully convolution-style networks.</p><p>FCNN based methods usually have the limitation of lowresolution prediction. There are a number of proposed techniques which addressed this limitation and aim to generate high-resolution predictions. The atrous convolution based approach DeepLab-CRF in <ref type="bibr" target="#b4">[5]</ref> directly output a middleresolution score map then applies the dense CRF method <ref type="bibr" target="#b26">[27]</ref> to refine boundaries by leveraging color contrast information. CRF-RNN <ref type="bibr" target="#b46">[47]</ref> extends this approach by implementing recurrent layers for end-to-end learning of the dense CRF and FCNN. Deconvolution methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b1">2]</ref> learn deconvolution layers to up-sample the low-resolution predictions. The depth estimation method <ref type="bibr" target="#b33">[34]</ref> employs super-pixel pooling to output high-resolution prediction.</p><p>There are several existing methods which exploit middle layer features for segmentation. The FCN method in <ref type="bibr" target="#b35">[36]</ref> adds prediction layers to middle layers to generate prediction scores at multiple resolutions. They average the multi-resolution scores to generate the final prediction mask. Their system is trained in a stage-wise manner rather than end-to-end training. The method Hypercolumn <ref type="bibr" target="#b21">[22]</ref> merges features from middle layers and learns dense classification layers. Their method employs stage-wise training instead of end-to-end training. The method Seg-Net <ref type="bibr" target="#b1">[2]</ref> and U-Net <ref type="bibr" target="#b39">[40]</ref> apply skip-connections in the deconvolution architecture to exploit the features from middle layers.</p><p>Although there are a few existing work, how to effectively exploit middle layer features remains an open question. We propose a novel network architecture, RefineNet, to address this question. The network architecture of Re-fineNet is clearly different from existing methods. Re-fineNet consists of a number of specially designed components which are able to refine the coarse high-level semantic features by exploiting low-level visual features. In particular, RefineNet employs short-range and long-range residual connections with identity mappings which enable effective end-to-end training of the whole system, and thus help to archive good performance. Comprehensive empirical results clearly verify the effectiveness of our novel network architecture for exploiting middle layer features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Before presenting our approach, we first review the structure of fully convolutional networks for semantic segmentation <ref type="bibr" target="#b35">[36]</ref> in more detail and also discuss the recent dilated convolution technique <ref type="bibr" target="#b5">[6]</ref> which is specifically designed to generate high-resolution predictions.</p><p>Very deep CNNs have shown outstanding performance on object recognition problems. Specifically, the recently proposed Residual Net (ResNet) <ref type="bibr" target="#b23">[24]</ref> has shown step-change improvements over earlier architectures, and ResNet models pre-trained for ImageNet recognition tasks are publicly available. Because of this, in the following we adopt ResNet as our fundamental building block for semantic segmentation. Note, however, that replacing it with any other deep network is straightforward.</p><p>Since semantic segmentation can be cast as a dense classification problem, the ResNet model can be easily modified for this task. This is achieved by replacing the single label prediction layer with a dense prediction layer that outputs the classification confidence for each class at every pixel. This approach is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>(a). As can be seen, during the forward pass in ResNet, the resolution of the feature maps (layer outputs) is decreased, while the feature depth, i.e. the number of feature maps per layer (or channels) is increased. The former is caused by striding during convolutional and pooling operations.</p><p>The ResNet layers can be naturally divided into 4 blocks according to the resolution of the output feature maps, as shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. Typically, the stride is set to 2, thus reducing the feature map resolution to one half when passing from one block to the next. This sequential sub-sampling has two effects: first it increases the receptive field of convolutions at deeper levels, enabling the filters to capture more global and contextual information which is essential for high quality classification; second it is necessary to keep the training efficient and tractable because each layer comprises a large number of filters and therefore produces an output which has a corresponding number of channels, thus there is a trade-off between the number of channels and resolution of the feature maps. Typically the final feature map output ends up being 32 times smaller in each spatial dimension than the original image (but with 1000s of channels). This low-resolution feature map loses important visual details captured by early low-level filters, resulting in a rather coarse segmentation map. This issue is a well-known limi-tation of deep CNN-based segmentation methods.</p><p>An alternative approach to avoid lowering the resolution while retaining a large receptive field is to use dilated (atrous) convolution. This method introduced in <ref type="bibr" target="#b5">[6]</ref>, has the state-of-the-art performance on semantic segmentation. The sub-sampling operations are removed (the stride is changed from 2 to 1), and all convolution layers after the first block use dilated convolution. Such a dilated convolution (effectively a sub-sampled convolution kernel) has the effect of increasing the receptive field size of the filters without increasing the number of weights that must be learned (see illustration in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>). Even so, there is a significant cost in memory, because unlike the image subsampling methods, one must retain very large numbers of feature maps at higher resolution. For example, if we retain all channels in all layers to be at least 1/4 of the original image resolution, and consider a typical number of filter channels to be 1024, then we can see that the memory capacity of even high-end GPUs is quickly swamped by very deep networks. In practice, therefore, dilation convolution methods usually have a resolution prediction of no more than 1/8 size of the original rather than 1/4, when using a deep network.</p><p>In contrast to dilated convolution methods, in this paper we propose a means to enjoy both the memory and computational benefits of deresolving, while still able to produce effective and efficient high-resolution segmentation prediction, as described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We propose a new framework that provides multiple paths over which information from different resolutions and via potentially long-range connections, is assimilated using a generic building block, the RefineNet. <ref type="figure" target="#fig_1">Fig. 2</ref>(c) shows one possible arrangement of the building blocks to achieve our goal of high resolution semantic segmentation. We begin by describing the multi-path refinement arrangement in Sec. 3.1 followed by a detailed description of each Re-fineNet block in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Path Refinement</head><p>As noted previously, we aim to exploit multi-level features for high-resolution prediction with long-range residual connections. RefineNet provides a generic means to fuse coarse high-level semantic features with finer-grained low-level features to generate high-resolution semantic feature maps. A crucial aspect of the design ensures that the gradient can be effortlessly propagated backwards through the network all the way to early low-level layers over longrange residual connections, ensuring that the entire network can be trained end-to-end.</p><p>For our standard multi-path architecture, we divide the pre-trained ResNet (trained with ImageNet) into 4 blocks   </p><formula xml:id="formula_0">... (a) (b) (c) (d)</formula><p>ReLU Sum <ref type="figure">Figure 3</ref>. The individual components of our multi-path refinement network architecture RefineNet. Components in RefineNet employ residual connections with identity mappings. In this way, gradients can be directly propagated within RefineNet via local residual connections, and also directly propagate to the input paths via long-range residual connections, and thus we achieve effective end-to-end training of the whole system. according to the resolutions of the feature maps, and employ a 4-cascaded architecture with 4 RefineNet units, each of which directly connects to the output of one ResNet block as well as to the preceding RefineNet block in the cascade. Note, however, that such a design is not unique. In fact, our flexible architecture allows for a simple exploration of different variants. For example, a RefineNet block can accept input from multiple ResNet blocks. We will analyse a 2-cascaded version, a single-block approach as well as a 2-scale 7-path architecture later in Sec. 4.3.</p><p>We denote RefineNet-m as the RefineNet block that connects to the output of block-m in ResNet. In practice, each ResNet output is passed through one convolutional layer to adapt the dimensionality. Although all RefineNets share the same internal architecture, their parameters are not tied, allowing for a more flexible adaptation for individual levels of detail. Following the illustration in <ref type="figure" target="#fig_1">Fig. 2</ref>(c) bottom up, we start from the last block in ResNet, and connect the output of ResNet block-4 to RefineNet-4. Here, there is only one input for RefineNet-4, and RefineNet-4 serves as an extra set of convolutions which adapt the pre-trained ResNet weights to the task at hand, in our case, semantic segmentation. In the next stage, the output of RefineNet-4 and the ResNet block-3 are fed to RefineNet-3 as 2-path inputs. The goal of RefineNet-3 is to use the high-resolution features from ResNet block-3 to refine the low-resolution feature map output by RefineNet-4 in the previous stage. Similarly, RefineNet-2 and RefineNet-1 repeat this stage-wise refinement by fusing high-level information from the later layers and high-resolution but low-level features from the earlier ones. As the last step, the final high-resolution feature maps are fed to a dense soft-max layer to make the final prediction in the form of a dense score map. This score map is then up-sampled to match the original image using bilinear interpolation.</p><p>The entire network can be efficiently trained end-to-end. It is important to note that we introduce long-range residual connections between the blocks in ResNet and the Re-fineNet modules. During the forward pass, these long-range residual connections convey the low-level features that encode visual details for refining the coarse high-level feature maps. In the training step, the long-range residual connections allow direct gradient propagation to early convolution layers, which helps effective end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RefineNet</head><p>The architecture of one RefineNet block is illustrated in <ref type="figure">Fig. 3(a)</ref>. In the multi-path overview shown in <ref type="figure" target="#fig_1">Fig 2(c)</ref>, RefineNet-1 has one input path, while all other RefineNet blocks have two inputs. Note, however, that our architecture is generic and each Refine block can be easily modified to accept an arbitrary number of feature maps with arbitrary resolutions and depths.</p><p>Residual convolution unit. The first part of each Re-fineNet block consists of an adaptive convolution set that mainly fine-tunes the pretrained ResNet weights for our task. To that end, each input path is passed sequentially through two residual convolution units (RCU), which is a simplified version of the convolution unit in the original ResNet <ref type="bibr" target="#b23">[24]</ref>, where the batch-normalization layers are removed (cf . <ref type="figure">Fig. 3(b)</ref>). The filter number for each input path is set to 512 for RefineNet-4 and 256 for the remaining ones in our experiments.</p><p>Multi-resolution fusion. All path inputs are then fused into a high-resolution feature map by the multi-resolution fusion block, depicted in <ref type="figure">Fig. 3(c)</ref>. This block first applies convolutions for input adaptation, which generate feature maps of the same feature dimension (the smallest one among the inputs), and then up-samples all (smaller) feature maps to the largest resolution of the inputs. Finally, all features maps are fused by summation. The input adaptation in this block also helps to re-scale the feature values appropriately along different paths, which is important for the subsequent sum-fusion. If there is only one input path (e.g., the case of RefineNet-4 in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>), the input path will directly go through this block without changes.</p><p>Chained residual pooling. The output feature map then goes through the chained residual pooling block, schematically depicted in <ref type="figure">Fig. 3(d)</ref>. The proposed chained residual pooling aims to capture background context from a large image region. It is able to efficiently pool features with multiple window sizes and fuse them together using learnable weights. In particular, this component is built as a chain of multiple pooling blocks, each consisting of one max-pooling layer and one convolution layer. One pooling block takes the output of the previous pooling block as input. Therefore, the current pooling block is able to re-use the result from the previous pooling operation and thus access the features from a large region without using a large pooling window. If not further specified, we use two pooling blocks each with stride 1 in our experiments.</p><p>The output feature maps of all pooling blocks are fused together with the input feature map through summation of residual connections. Note that, our choice to employ residual connections also persists in this building block, which once again facilitates gradient propagation during training. In one pooling block, each pooling operation is followed by convolutions which serve as a weighting layer for the summation fusion. It is expected that this convolution layer will learn to accommodate the importance of the pooling block during the training process.</p><p>Output convolutions. The final step of each RefineNet block is another residual convolution unit (RCU). This results in a sequence of three RCUs between each block. To reflect this behavior in the last RefineNet-1 block, we place two additional RCUs before the final softmax prediction step. The goal here is to employ non-linearity operations on the multi-path fused feature maps to generate features for further processing or for final prediction. The feature dimension remains the same after going through this block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Identity Mappings in RefineNet</head><p>Note that all convolutional components of the RefineNet have been carefully constructed inspired by the idea behind residual connections and follow the rule of identity mapping <ref type="bibr" target="#b24">[25]</ref>. This enables effective backward propagation of the gradient through RefineNet and facilitates end-to-end learning of cascaded multi-path refinement networks.</p><p>Employing residual connections with identity mappings allows the gradient to be directly propagated from one block to any other blocks, as was recently shown by <ref type="bibr" target="#b24">[25]</ref>. This concept encourages to maintain a clean information path for shortcut connections, so that these connections are not "blocked" by any non-linear layers or components. Instead, non-linear operations are placed on branches of the main information path. We follow this guideline for developing the individual components in RefineNet, including all convolution units. It is this particular strategy that allows the multi-cascaded RefineNet to be trained effectively. Note that we include one non-linear activation layer (ReLU) in the chained residual pooling block. We observed that this ReLU is important for the effectiveness of subsequent pooling operations and it also makes the model less sensitive to changes in the learning rate. We observed that one single ReLU in each RefineNet block does not noticeably reduce the effectiveness of gradient flow.</p><p>We have both short-range and long-range residual connections in RefineNet. Short-range residual connections refer to local shot-cut connections in one RCU or the residual pooling component, while long-range residual connections refer to the connection between RefineNet modules and the ResNet blocks. With long-range residual connections, the gradient can be directly propagated to early convolution layers in ResNet and thus enables end-to-end training of all network components.</p><p>The fusion block fuses the information of multiple shortcut paths, which can be considered as performing summation fusion of multiple residual connections with necessary dimension or resolution adaptation. In this aspect, the role of the multi-resolution fusion block here is analogous to the role of the "summation" fusion in a conventional residual convolution unit in ResNet. There are certain layers in RefineNet, and in particular within the fusion block, that perform linear feature transformation operations, like linear feature dimension reduction or bilinear up-sampling. These layers are placed on the shortcut paths, which is similar to the case in ResNet <ref type="bibr" target="#b23">[24]</ref>. As in in ResNet, when a shortcut connection crosses two blocks, it will include a convolution layer in the shortcut path for linear feature dimension adaptation, which ensures that the feature dimension matches the subsequent summation in the next block. Since only linear transformation are employed in these layers, gradients still can be propagated through these layers effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To show the effectiveness of our approach, we carry out comprehensive experiments on seven public datasets, which include six popular datasets for semantic segmentation on indoors and outdoors scenes (NYUDv2, PASCAL VOC 2012, SUN-RGBD, PASCAL-Context, Cityscapes, ADE20K MIT), and one dataset for object parsing called Person-Part. The segmentation quality is measured by the intersection-over-union (IoU) score <ref type="bibr" target="#b15">[16]</ref>, the pixel accuracy and the mean accuracy <ref type="bibr" target="#b35">[36]</ref> over all classes. As commonly done in the literature, we apply simple data augmentation during training. Specifically, we perform random scaling (ranging from 0.7 to 1.3), random cropping and horizontal flipping of the images. If not further specified, we apply test-time multi-scale evaluation, which is a common practice in segmentation methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref>. For multi-scale evaluation, we average the predictions on the same image across different scales for the final prediction. We also present an ablation experiment to inspect the impact of various components and an alternative 2-cascaded version of our model. Our system is built on MatConvNet <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object Parsing</head><p>We first present our results on the task of object parsing, which consists of recognizing and segmenting object parts. We carry out experiments on the Person-Part dataset <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> which provides pixel-level labels for six person parts including Head, Torso, Upper/Lower Arms and Upper/Lower Legs. The rest of each image is considered background. There are training 1717 images and 1818 test images. We use four pooling blocks in our chained residual pooling for this dataset.</p><p>We compare our results to a number of state-of-the-art methods, listed in <ref type="table">Table 1</ref>. The results clearly demonstrate the improvement over previous works. In particular, we significantly outperform the the recent DeepLab-v2 approach <ref type="bibr" target="#b5">[6]</ref> which is based on dilated convolutions for highresolution segmentation, using the same ResNet as initialization. In <ref type="table">Table 2</ref>, we present an ablation experiment to quantify the influence of the following components: Network depth, chained residual pooling and multi-scale evaluation (Msc Eva), as described earlier. This experiment shows that each of these three factors can improve the overall performance. Qualitative examples of our object parsing on this dataset are shown in <ref type="figure" target="#fig_4">Fig.4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Segmentation</head><p>We now describe our experiments on dense semantic labeling on six public benchmarks and show that our Re-fineNet outperforms previous methods on all datasets.</p><p>NYUDv2. The NYUDv2 dataset <ref type="bibr" target="#b40">[41]</ref> consists of 1449 RGB-D images showing interior scenes. We use the segmentation labels provided in <ref type="bibr" target="#b18">[19]</ref>, in which all labels are mapped to 40 classes. We use the standard training/test split with 795 and 654 images, respectively. We train our models only on RGB images without using the depth information. Quantitative results are shown in <ref type="table">Table 3</ref>. Our RefineNet achieves new state-of-the-art result on the NYUDv2 dataset.</p><p>Similar to the object parsing task above, we also perform ablation experiments on the NYUDv2 dataset to evaluate the effect of different settings. The results are presented in <ref type="table">Table 2</ref>. Once again, this study demonstrates the benefits of adding the proposed chained residual pooling component and deeper networks, both of which consistently improve the performance as measured by IoU.</p><p>PASCAL VOC 2012 <ref type="bibr" target="#b15">[16]</ref> is a well-known segmentation dataset which includes 20 object categories and one background class. This dataset is split into a training set, a validation set and a test set, with 1464, 1449 and 1456 images each. Since the test set labels are not publicly available, all reported results have been obtained from the VOC evaluation server. Following the common convention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b34">35]</ref>, the training set is augmented by additional annotated VOC images provided in <ref type="bibr" target="#b20">[21]</ref> as well as with the training data from the MS COCO dataset <ref type="bibr" target="#b30">[31]</ref>. We compare our RefineNet on the PASCAL VOC 2012 test set with a number of competitive methods, showing superior performance. We use dense CRF method in <ref type="bibr" target="#b26">[27]</ref> for further refinement for this dataset, which gives marginal improvement of 0.1% on the validation set. Since dense CRF only brings very minor improvement on our high-resolution prediction, we do not apply it on other datasets.</p><p>The detailed results for each category and the mean IoU scores are shown in <ref type="table" target="#tab_4">Table 5</ref>. We achieve an IoU score of 83.4, which is the best reported result on this challenging dataset to date. <ref type="bibr" target="#b1">2</ref> We outperform competing methods in almost all categories. In particular, we significantly outperform the method DeepLab-v2 <ref type="bibr" target="#b5">[6]</ref> which is the currently best known dilation convolution method and uses the same ResNet-101 network as initialization. Selected prediction examples are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>Cityscapes <ref type="bibr" target="#b8">[9]</ref> is a very recent dataset on street scene images from 50 different European cities. This dataset provides fine-grained pixel-level annotations of roads, cars, pedestrians, bicycles, sky, etc. The provided training set has 2975 images and the validation set has 500 images. In  total, 19 classes are considered for training and evaluation. The test set ground-truth is withheld by the organizers, and we evaluate our method on the their evaluation server. The test results are shown in <ref type="table">Table 4</ref>. In this challenging setting, our architecture again outperforms previous methods. A few test images along with ground truth and our predicted semantic maps are shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. PASCAL-Context. The PASCAL-Context <ref type="bibr" target="#b36">[37]</ref> dataset provides the segmentation labels of the whole scene for the PASCAL VOC images. We use the segmentation labels which contain 60 classes (59 object categories plus background) for evaluation as well as the provided training/test splits. The training set contains 4998 images and the test set has 5105 images. Results are shown in <ref type="table">Table 6</ref>. Even without additional training data and with the same underlying ResNet architecture with 101 layers, we outperform the previous state-of-the-art achieved by DeepLab. <ref type="bibr" target="#b42">[43]</ref> is a segmentation dataset that contains around 10, 000 RGB-D indoor images and provides pixel labeling masks for 37 classes. Results are shown in <ref type="table">Table  7</ref>. Our method outperforms all existing methods by a large margin across all evaluation metrics, even though we do not make use of the depth information for training.  <ref type="table">Table 8</ref>. Our method clearly outperforms the baseline methods described in <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUN-RGBD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Variants of cascaded RefineNet</head><p>As discussed earlier, our RefineNet is flexible in that it can be cascaded in various manners for generating various architectures. Here, we discuss several variants of our Re-fineNet. Specifically, we present the architectures of using a single RefineNet, a 2-cascaded RefineNet and a 4-cascaded RefineNet with 2-scale ResNet. The architectures of all three variants are illustrated in <ref type="figure" target="#fig_7">Fig. 7</ref>. The architecture of 4cascaded RefineNet is already presented in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. Please note that this 4-cascaded RefineNet model is the one used in all other experiments.</p><p>The single RefineNet model is the simplest variant of our network. It consists of only one single RefineNet block, which takes all four inputs from the four blocks of ResNet and fuses all-resolution feature maps in a single process. The 2-cascaded version is similar our main model (4cascaded) from <ref type="figure" target="#fig_1">Fig. 2(c)</ref>, but employs only two RefineNet modules instead of four. The bottom one, RefineNet-2, has two inputs from ResNet blocks 3 and 4, and the other one has three inputs, two coming from the remaining ResNet blocks and one from RefineNet-2. For the 2-scale model in <ref type="figure" target="#fig_7">Fig. 7</ref>(c), we use 2 scales of the image as input and respectively 2 ResNets to generate feature maps; the input image is scaled to a factor of 1.2 and 0.6 and fed into 2 independent ResNets.</p><p>The evaluation results of these variants on the NYUD dataset are shown in <ref type="table">Table 9</ref>. This experiment demonstrates that the 4-cascaded version yields better performance than the 2-cascaded and 1-cascaded version, and using 2-scale image input with 2 ResNet is better than using 1-scale input. This is expected due to the larger capacity of the network. However, it also results in longer training times. Hence, we resort to using the single-scale 4-cascaded version as the standard architecture in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented RefineNet, a novel multi-path refinement network for semantic segmentation and object parsing. The cascaded architecture is able to effectively combine high-level semantics and low-level features to produce high-resolution segmentation maps. Our design choices are inspired by the idea of identity mapping which facilitates gradient propagation across long-range connections and thus enables effective end-to-end learning. We outperform all previous works on seven public benchmarks, setting a new mark for the state of the art in semantic labeling.  an ARC Laureate Fellowship (FL130100102).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example results of our method on the task of object parsing (left) and semantic segmentation (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>t i -P a t h R e fi n e m e n t Comparison of fully convolutional approaches for dense classification. Standard multi-layer CNNs, such as ResNet (a) suffer from downscaling of the feature maps, thereby losing fine structures along the way. Dilated convolutions (b) remedy this shortcoming by introducing atrous filters, but are computationally expensive to train and quickly reach memory limits even on modern GPUs. Our proposed architecture that we call RefineNet (c) exploits various levels of detail at different stages of convolutions and fuses them to obtain a high-resolution prediction without the need to maintain large intermediate feature maps. The details of the RefineNet block are outlined in Sec. 3 and illustrated inFig 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Chained</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Our prediction examples on Person-Parts dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Our prediction examples on VOC 2012 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Our prediction examples on Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of 3 variants of our network architecture: (a) single RefineNet, (b) 2-cascaded RefineNet and (c) 4-cascaded RefineNet with 2-scale ResNet. Note that our proposed RefineNet block can seamlessly handle different numbers of inputs of arbitrary resolutions and dimensions without any modification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Residual Pooling</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Adaptive Conv.</cell><cell></cell><cell></cell><cell></cell><cell cols="4">RefineNet</cell></row><row><cell>Multi-path input</cell><cell>...</cell><cell>2x RCU 2x RCU 2x RCU ...</cell><cell>Multi-resolution Fusion</cell><cell>Chained Residual Pooling</cell><cell cols="3">1x RCU Output Conv.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RCU: Residual Conv Unit ReLU 3x3 Conv ReLU 3x3 Conv</cell><cell cols="2">Multi-resolution Fusion 3x3 Conv 3x3 Conv Upsample Upsample ...</cell><cell>Sum</cell><cell>5x5 Pool</cell><cell>3x3 Conv</cell><cell>5x5 Pool Sum</cell><cell>3x3 Conv</cell><cell>Sum 5x5 Pool . . .</cell><cell>3x3 Conv</cell><cell>Sum</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Object parsing results on the Person-Part dataset. Our method achieves the best performance (bold). Ablation experiments on NYUDv2 and Person-Part.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">method IoU</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Attention [7] 56.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">HAZN [45] 57.5</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">LG-LSTM [29] 58.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Graph-LSTM [28] 60.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">DeepLab [5] 62.8</cell><cell></cell></row><row><cell></cell><cell cols="3">DeepLab-v2 (Res101) [6] 64.9</cell><cell></cell></row><row><cell></cell><cell cols="3">RefineNet-Res101 (ours) 68.6</cell><cell></cell></row><row><cell cols="5">Initialization Chained pool. Msc Eva NYUDv2 Person-Parts</cell></row><row><cell>ResNet-50</cell><cell>no</cell><cell>no</cell><cell>40.4</cell><cell>64.1</cell></row><row><cell>ResNet-50</cell><cell>yes</cell><cell>no</cell><cell>42.5</cell><cell>65.7</cell></row><row><cell>ResNet-50</cell><cell>yes</cell><cell>yes</cell><cell>43.8</cell><cell>67.1</cell></row><row><cell>ResNet-101</cell><cell>yes</cell><cell>no</cell><cell>43.6</cell><cell>67.6</cell></row><row><cell>ResNet-101</cell><cell>yes</cell><cell>yes</cell><cell>44.7</cell><cell>68.6</cell></row><row><cell>ResNet-152</cell><cell>yes</cell><cell>yes</cell><cell>46.5</cell><cell>68.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Segmentation results on NYUDv2 (40 classes). Segmentation results on the Cityscapes test set. our method achieves the best performance.</figDesc><table><row><cell cols="5">method training data pixel acc. mean acc. IoU</cell></row><row><cell>Gupta et al. [20]</cell><cell>RGB-D</cell><cell>60.3</cell><cell>-</cell><cell>28.6</cell></row><row><cell>FCN-32s [36]</cell><cell>RGB</cell><cell>60.0</cell><cell>42.2</cell><cell>29.2</cell></row><row><cell>FCN-HHA [36]</cell><cell>RGB-D</cell><cell>65.4</cell><cell>46.1</cell><cell>34.0</cell></row><row><cell>Context [30]</cell><cell>RGB</cell><cell>70.0</cell><cell>53.6</cell><cell>40.6</cell></row><row><cell>RefineNet-Res152</cell><cell>RGB</cell><cell>73.6</cell><cell>58.9</cell><cell>46.5</cell></row><row><cell></cell><cell></cell><cell>Method IoU</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">FCN-8s [36] 65.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>DPN [35] 66.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Dilation10 [46] 67.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Context [30] 71.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">LRR-4x [17] 71.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DeepLab [5] 63.1</cell><cell></cell><cell></cell></row><row><cell cols="3">DeepLab-v2(Res101) [6] 70.4</cell><cell></cell><cell></cell></row><row><cell cols="3">RefineNet-Res101 (ours) 73.6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Results on the PASCAL VOC 2012 test set (IoU scores). Our RefineNet archives the best performance (IoU 83.4). 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 DeconvNet [38] 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 72.5 CRF-RNN [47] 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7 BoxSup [10] 89.8 38.0 89.2 68.9 68.0 89.6 83.0 87.7 34.4 83.6 67.1 81.5 83.7 85.2 83.5 58.6 84.9 55.8 81.2 70.7</figDesc><table><row><cell>Method</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>potted</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mean</cell></row><row><cell>FCN-8s [36]</cell><cell cols="21">76.8 75.2</cell></row><row><cell>DPN [35]</cell><cell cols="20">89.0 61.6 87.7 66.8 74.7 91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8 85.6 85.4 63.6 87.3 61.3 79.4 66.4</cell><cell>77.5</cell></row><row><cell>Context [30]</cell><cell cols="8">94.1 40.7 84.1 67.8 75.9 93.4 84.3 88.4</cell><cell cols="12">42.5 86.4 64.7 85.4 89.0 85.8 86.0 67.5 90.2 63.8 80.9 73.0</cell><cell>78.0</cell></row><row><cell>DeepLab [5]</cell><cell cols="20">89.1 38.3 88.1 63.3 69.7 87.1 83.1 85.0 29.3 76.5 56.5 79.8 77.9 85.8 82.4 57.4 84.3 54.9 80.5 64.1</cell><cell>72.7</cell></row><row><cell>DeepLab2-Res101 [6]</cell><cell cols="20">92.6 60.4 91.6 63.4 76.3 95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1 87.0 87.4 63.3 88.3 60.0 86.8 74.5</cell><cell>79.7</cell></row><row><cell>CSupelec-Res101 [4]</cell><cell>92.9</cell><cell cols="12">61.2 91.0 66.3 77.7 95.3 88.9 92.4 33.8 88.4 69.1 89.8 92.9</cell><cell cols="5">87.7 87.5 62.6 89.9 59.2</cell><cell cols="2">87.1 74.2</cell><cell>80.2</cell></row><row><cell>RefineNet-Res101</cell><cell cols="2">94.9 60.2</cell><cell cols="2">92.8 77.5</cell><cell cols="3">81.5 95.0 87.4</cell><cell cols="2">93.3 39.6</cell><cell cols="3">89.3 73.0 92.7</cell><cell cols="3">92.4 85.4 88.3</cell><cell cols="5">69.7 92.2 65.3 84.2 78.7</cell><cell>82.4</cell></row><row><cell>RefineNet-Res152</cell><cell cols="3">94.7 64.3 94.9</cell><cell cols="2">74.9 82.9</cell><cell cols="6">95.1 88.5 94.7 45.5 91.4 76.3</cell><cell cols="3">90.6 91.8 88.1</cell><cell cols="5">88.0 69.9 92.3 65.9 88.7</cell><cell>76.8</cell><cell>83.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .Table 8 .Table 9 .</head><label>689</label><figDesc>Segmentation results on PASCAL-Context dataset (60 classes). Our method performs the best. We only use the VOC training images. Segmentation results on the ADE20K dataset (150 classes) val set. our method achieves the best performance. Evaluations of 4 variants of cascaded RefineNet: single RefineNet, 2-cascaded RefineNet, 4-cascaded RefineNet, 4cascaded RefineNet with 2-scale ResNet on the NYUDv2 dataset. We use the 4-cascaded version as our main architecture throughout all experiments in the paper because this turns out to be the best compromise between accuracy and efficiency.ADE20K MIT<ref type="bibr" target="#b47">[48]</ref> is a newly released dataset for scene parsing which provides dense labels of 150 classes on more than 20K scene images. The categories include a large variety of objects (e.g., person, car, etc.) and stuff (e.g., sky, road, etc.). The provided validation set consisting of 2000 images is used for quantitative evaluation. Results are shown in</figDesc><table><row><cell></cell><cell>Method</cell><cell>Extra train data</cell><cell>IoU</cell><cell></cell></row><row><cell></cell><cell>O2P [3]</cell><cell>-</cell><cell>18.1</cell><cell></cell></row><row><cell></cell><cell>CFM [11]</cell><cell>-</cell><cell>34.4</cell><cell></cell></row><row><cell></cell><cell>FCN-8s [36]</cell><cell>-</cell><cell>35.1</cell><cell></cell></row><row><cell></cell><cell>BoxSup [10]</cell><cell>-</cell><cell>40.5</cell><cell></cell></row><row><cell></cell><cell>HO-CRF [1]</cell><cell>-</cell><cell>41.3</cell><cell></cell></row><row><cell></cell><cell>Context [30]</cell><cell>-</cell><cell>43.3</cell><cell></cell></row><row><cell cols="4">DeepLab-v2(Res101) [6] COCO (∼100K) 45.7</cell><cell></cell></row><row><cell cols="2">RefineNet-Res101 (ours)</cell><cell>-</cell><cell>47.1</cell><cell></cell></row><row><cell cols="2">RefineNet-Res152 (ours)</cell><cell>-</cell><cell>47.3</cell><cell></cell></row><row><cell cols="5">Table 7. Segmentation results on SUN-RGBD dataset (37 classes).</cell></row><row><cell cols="5">We compare to a number of recent methods. Our RefineNet sig-</cell></row><row><cell cols="3">nificantly outperforms the existing methods.</cell><cell></cell><cell></cell></row><row><cell cols="5">Method Train data Pixel acc. Mean acc. IoU</cell></row><row><cell>Liu et al. [32]</cell><cell>RGB-D</cell><cell>−</cell><cell>10.0</cell><cell>−</cell></row><row><cell>Ren et al. [39]</cell><cell>RGB-D</cell><cell>−</cell><cell>36.3</cell><cell>−</cell></row><row><cell>Kendall et al. [26]</cell><cell>RGB</cell><cell>71.2</cell><cell>45.9</cell><cell>30.7</cell></row><row><cell>Context [30]</cell><cell>RGB</cell><cell>78.4</cell><cell>53.4</cell><cell>42.3</cell></row><row><cell>RefineNet-Res101</cell><cell>RGB</cell><cell>80.4</cell><cell>57.8</cell><cell>45.7</cell></row><row><cell>RefineNet-Res152</cell><cell>RGB</cell><cell>80.6</cell><cell>58.5</cell><cell>45.9</cell></row><row><cell></cell><cell></cell><cell>Method IoU</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">FCN-8s [36] 29.4</cell><cell></cell></row><row><cell></cell><cell cols="3">SegNet [2] 21.6</cell><cell></cell></row><row><cell></cell><cell cols="3">DilatedNet [5, 46] 32.3</cell><cell></cell></row><row><cell></cell><cell cols="3">Cascaded-SegNet [48] 27.5</cell><cell></cell></row><row><cell cols="4">Cascaded-DilatedNet [48] 34.9</cell><cell></cell></row><row><cell cols="4">RefineNet-Res101 (ours) 40.2</cell><cell></cell></row><row><cell cols="4">RefineNet-Res152 (ours) 40.7</cell><cell></cell></row><row><cell></cell><cell cols="4">Variant Initialization Msc Eva IoU</cell></row><row><cell cols="2">single RefineNet</cell><cell>ResNet-50</cell><cell>no</cell><cell>40.3</cell></row><row><cell cols="2">2-cascaded RefineNet</cell><cell>ResNet-50</cell><cell>no</cell><cell>40.9</cell></row><row><cell cols="2">4-cascaded RefineNet</cell><cell>ResNet-50</cell><cell>no</cell><cell>42.5</cell></row><row><cell cols="2">4-cascaded 2-scale RefineNet</cell><cell>ResNet-50</cell><cell>no</cell><cell>43.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our source code will be available at https://github.com/ guosheng/refinenet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The result link to the VOC evaluation server: http://host. robots.ox.ac.uk:8080/anonymous/B3XPSK.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research was supported by the Australian Research Council through the Australian Centre for Robotic Vision (CE140100016). C. Shen's participation was supported by an ARC Future Fellowship (FT120100969). I. Reid's participation was supported by</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03339</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07063</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04510</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE T. Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<idno>abs/1502.07411</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">MatConvNet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06881</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1608.05442</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
