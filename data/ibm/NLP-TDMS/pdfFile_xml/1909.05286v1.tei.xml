<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frustratingly Easy Natural Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Pan</surname></persName>
							<email>panl@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Yorktown Heights</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishav</forename><surname>Chakravarti</surname></persName>
							<email>rchakravarti@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Yorktown Heights</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Ferritto</surname></persName>
							<email>aferritto@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Yorktown Heights</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
							<email>mrglass@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Yorktown Heights</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
							<email>gliozzo@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Yorktown Heights</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
							<email>roukos@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Yorktown Heights</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
							<email>raduf@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Yorktown Heights</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Yorktown Heights</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Frustratingly Easy Natural Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing literature on Question Answering (QA) mostly focuses on algorithmic novelty, data augmentation, or increasingly large pre-trained language models like XLNet and RoBERTa. Additionally, a lot of systems on the QA leaderboards do not have associated research documentation in order to successfully replicate their experiments. In this paper, we outline these algorithmic components such as Attentionover-Attention, coupled with data augmentation and ensembling strategies that have shown to yield state-of-the-art results on benchmark datasets like SQuAD, even achieving super-human performance. Contrary to these prior results, when we evaluate on the recently proposed Natural Questions benchmark dataset, we find that an incredibly simple approach of transfer learning from BERT outperforms the previous state-of-the-art system trained on 4 million more examples than ours by 1.9 F1 points. Adding ensembling strategies further improves that number by 2.3 F1 points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>A relatively new field in the open domain question answering (QA) community is machine reading comprehension (MRC) which aims to read and comprehend a given text, and then answer questions based on it. MRC is one of the key steps for natural language understanding (NLU). MRC also has wide applications in the domain of conversational agents and customer service support. Among the most widely worked on MRC benchmark datasets are the Stanford SQuAD v1.1 ) and v2.0 (Rajpurkar, Jia, and Liang 2018) datasets. Recent MRC research has explored transfer learning from large pre-trained language models like <ref type="bibr">BERT (Devlin et al. 2019)</ref> and XLNet <ref type="bibr" target="#b6">(Yang et al. 2019)</ref> which have solved the tasks in less than a year since their inception. Hence, we argue that harder benchmark MRC challenges are needed. In addition, the SQuAD datasets both suffer from observational bias: the datasets contain questions and answers provided by annotators who have read the given passage first and then created a question given the context. Other datasets like NarrativeQA * Equal Contribution. † Corresponding author. <ref type="bibr" target="#b2">(Kočiskỳ et al. 2018)</ref> and HotpotQA <ref type="bibr" target="#b6">(Yang et al. 2018)</ref> are similarly flawed.</p><p>In this paper, we focus on a new benchmark MRC dataset called Natural Questions (NQ) <ref type="bibr" target="#b3">(Kwiatkowski et al. 2019)</ref> which does not possess the above bias. The NQ queries were sampled from Google search engine logs according to a variety of handcrafted rules to filter for "natural questions" that are potentially answerable by a Wikipedia article. This is a key differentiator from past datasets where observation bias is a concern due to the questions having been generated after seeing an article or passage containing the answer <ref type="bibr" target="#b3">(Kwiatkowski et al. 2019)</ref>. Also, systems need to extract a short and a long answer (paragraphs which would contain the short answer). The dataset shows a human upper bound of 76% on the short answer and 87% on the long answer selection tasks. Since the task has been recently introduced and is bias-free, the authors claim that matching human performance on this task will require significant progress in natural language understanding.</p><p>The contributions of our paper include: • Algorithmic novelties: We add an Attention-overattention (AoA) <ref type="bibr" target="#b1">(Cui et al. 2017</ref>) layer on top of BERT during model finetuning, which gives us the best single model performance on NQ. We also perform a linear combination of BERT output layers instead of using the last layer only. Additionally, we show empirically that an incredibly simple transfer learning strategy of finetuning the pre-trained BERT model on SQuAD first and then on NQ can nearly match the performance of further adding the complex AoA layer. • Smarter Data Augmentation: We show that a simple but effective data augmentation strategy that shuffles the training data helps outperform the previous state-of-theart (SOTA) system trained on 4 million additional synthetically generated QA data. • Ensembling Strategies: We describe several methods that can combine the output of single MRC systems to further improve performance on a leaderboard. Most previous work that obtains "super-human" 1 performance on the 1 <ref type="bibr" target="#b5">Rajpurkar, Jia, and Liang (2018)</ref> note that human performance is likely somewhat underestimated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Most recent MRC systems are predominantly BERT-based as is evident on leaderboards for SQuAD v1.1 and v2.0, Hot-potQA and Natural Questions. "Super-human" results are achieved by adding additional components on top of BERT or BERT-like models such as XLNet. Among them, XLNet + SG-Net Verifier (Zhang et al. 2019) adds a syntax layer, and BERT + DAE + AoA adds an AoA component as shown on the SQuAD leaderboard.</p><p>Another common technique is data augmentation by artificially generating more questions to enhance the training data. , an improvement over Alberti, , combine models of question generation with answer extraction and filter results to ensure round-trip consistency. This technique helped them gather an additional 4 million synthetic training examples which provides SOTA performance on the NQ task.</p><p>Top submissions on the aforementioned leaderboards are usually ensemble results of single systems, yet the underlying ensemble technique is rarely documented. Even the most popular system, BERT + N-Gram Masking + Synthetic Self-Training (ensemble) (Devlin et al. 2019), does not provide their ensemble strategies. In this paper, we describe our recipe for various ensemble strategies together with algorithmic improvements and data augmentation to produce SOTA results on the NQ dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>In this section, we first describe BERT-FOR-QA, the model our system is built upon, and two algorithmic improvements on top of it. (1) Attention-over-Attention (AoA) <ref type="bibr" target="#b1">(Cui et al. 2017)</ref>, as an attention mechanism, combines query-todocument and document-to-query attentions by computing a document-level attention that is weighted by the importance of query words. This technique gives SOTA performance on SQuAD. (2) Inspired by the success of ELMo , we use a linear combination of all the BERT encoded layers instead of only the last layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-for-QA</head><formula xml:id="formula_0">L = [h L 1 , h L 2 , . . . , h L T ]. h L 1 , . . . , h L T = BERT (x 1 , . .</formula><p>. , x T ) BERT LARGE consists of 24 Transformer layers (L = 24), each with 16 heads and h L t ∈ R 1024 while BERT BASE is smaller, (L = 12, each layer with 12 heads and h L t ∈ R 768 ). As an important preprocessing step for BERT, special markup tokens [CLS] and [SEP] are added; one to the beginning of the input sequence and the other to the end. In cases like MRC, where there are two separate input sequences, one for the question and the other for the given context, an additional [SEP] is added in between the two to form a single sequence.</p><p>BERT-FOR-QA adds three dense layers followed by a softmax on top of BERT for answer extraction:</p><formula xml:id="formula_1">b = sof tmax(W 1 H L ), e = sof tmax(W 2 H L ) and a = sof tmax(W 3 h L [CLS] ), where W 1 , W 2 ∈ R 1×1024 , W 3 ∈ R 5×1024 , H L ∈ R N ×1024 , and h L [CLS] ∈ R 1024 . t b</formula><p>and t e denote the probability of the t th token in the sequence being the answer beginning and end, respectively. These three layers are trained during the finetuning stage. The NQ task requires not only a prediction for short answer beginning/end offsets, but also a (containing) longer span of text that provides the necessary context for that short answer. Following prior work from Alberti, Lee, and Collins (2019), we only optimize for short answer spans and then identify the bounds of the containing HTML span as the long answer prediction 2 . We use the hidden state of the [CLS] token to classify the answer type ∈ [short answer, long answer, yes, no, null answer], so y a denotes the probability of the y th answer type being correct. Our loss function is the averaged cross entropy on the two answer pointers and the answer type classifier:</p><formula xml:id="formula_2">L N Q = − 1 3 T t=1 1(b t ) log t b + T t=1 1(e t ) log t e + Y y=1 1(a y ) log y a ,</formula><p>where 1(b) and 1(e) are one-hot vectors for the groundtruth beginning and end positions, and 1(a) for the groundtruth answer type. During decoding, the span over argmax of b and argmax of e is picked as the predicted short answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-over-Attention</head><p>AoA was originally designed for cloze-style question answering, where a phrase in a short passage of text is removed in forming a question. Let Q be a sequence of question tokens [q 1 , . . . , q m ], and C a sequence of context tokens [c 1 , . . . , c n ]. AoA first computes a attention matrix:</p><formula xml:id="formula_3">M = CQ T ,<label>(1)</label></formula><p>where C ∈ R n×h , Q ∈ R m×h , and M ∈ R n×m . In our case, the hidden dimension is h = 1024. Next, it separately performs on M a column-wise softmax α = sof tmax(M T ) and a row-wise softmax β = sof tmax(M). Each row i of matrix α represents the document-level attention regarding q i (query-to-document attention), and each row j of matrix β represents the query-level attention regarding c j (document-to-query attention). To combine the two attentions, β is first row-wise averaged:</p><formula xml:id="formula_4">β = 1 n n j=1 β j<label>(2)</label></formula><p>The resulting vector can be viewed as the average importance of each q i with respect to C, and is used to weigh the document-level attention α.</p><formula xml:id="formula_5">s = α T β T (3)</formula><p>The final attention vector s ∈ R N represents document-level attention weighted by the importance of query words.</p><p>In our work, we use AoA by adding an two-headed AoA layer into the BERT-for-QA model and this layer is trained together with the answer extraction layer during the finetuning stage. Concretely, the combined question and context hidden representation H L from BERT is first separated to H Q and H C 3 , followed by two linear projections of H Q and H C respectively to H Q i and H C i , i ∈ {1, 2}:</p><formula xml:id="formula_6">H Q i = H Q W Q i ,<label>(4)</label></formula><formula xml:id="formula_7">H C i = H C W C i ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">H Q , H Q i ∈ R M ×1024 ; H C , H C i ∈ R N ×1024 ; and W Q i , W C i ∈ R 1024×1024</formula><p>. Therefore, the AoA layer adds about 2.1 million parameters on top of BERT which already has 340 million. Next, we feed H C 1 and H Q 1 into AoA calculation specified in Equation <ref type="formula" target="#formula_3">(1)</ref> to <ref type="formula">(3)</ref> to get the attention vector s 1 for head 1. The same procedure is applied to H Q 2 and H C 2 to get s 2 for head 2. Lastly, s 1 and s 2 are combined with b and e respectively via two weighted sum operations for answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Layer Combination</head><p>So far, we have described using the last layer from the BERT output [h L 1 , . . . , h L n ] as input to downstream layers. We also experiment with combining all the BERT output layers into one representation. Following , we create a trainable vector v ∈ R L and apply softmax over it, yielding w = sof tmax(v). The output layers are linearly combined as follows:</p><formula xml:id="formula_9">h i = L l=1</formula><p>w l h l i v is jointly trained with parameters in BERT-for-QA. h i is then used as input to the final answer extraction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>Our models follow the now common approach of starting with the pre-trained BERT language model and then finetune over the NQ dataset with an additional QA sequence prediction layer as described in previous section. As mentioned in (Alberti, Lee, and Collins 2019), we also find it helpful to run additional task specific pre-training of the underlying BERT language model before starting with the finetuning step with the target NQ dataset. The following two subsections discuss different pre-training and data augmentation strategies employed to try and improve the overall performance of the models. Note that unless we specify otherwise, we are referring to the "large" version of BERT. BERT-for-QA with SQuAD 2.0 finetunes BERT on the supervised task of SQuAD 2.0 as initial pre-training. The intuition is that this allows the model to become more domain and task aware than vanilla BERT. Alberti, Lee, and Collins (2019) similarly leverage SQuAD 1.1 to pre-train the network for NQ. However, we found better results using SQuAD 2.0, likely because of SQuAD 2.0's incorporation of unanswerable questions which also exist in NQ.</p><p>In our future work, we intend to explore the effect of these pre-trainings on additional language models including RoBERTa (Liu et al. 2019) and XLNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>As noted in a number of works such as (Yatskar 2018), and <ref type="bibr" target="#b2">(Dhingra, Pruthi, and Rajagopal 2018)</ref>, model performance in the MRC literature has benefited from finetuning the model with labeled examples from either human annotated or synthetic data augmentation from similar tasks (often with the final set of mini batch updates relying exclusively on data from the target domain as described in the transfer learning tutorial by Ruder et al. <ref type="formula" target="#formula_3">(2019)</ref>). In fact, Alberti et al. (2019) achieve prior SOTA results for the NQ benchmark by adding 4 million synthetically generated QA examples. In this paper, we similarly try to introduce both synthetically generated as well as human labelled data from other related MRC tasks during NQ training.</p><p>Synthetic Data: Sentence Order Shuffling (SOS) The SOS strategy shuffles the ordering of sentences in the paragraphs containing short answer annotations from the NQ training set. The strategy was attempted based on the observation that preliminary Bert-for-QA models showed a bias towards identifying candidate short answer spans from earlier in the paragraph rather than later in the paragraph (which may be a feature of how Wikipedia articles are written and the types of answerable questions that appear in the NQ dataset). This is similar in spirit to the types of perturbations introduced by Zhou, Zhang, and Jiang (2019) for SQuAD 2.0 based on observed biases in the SQuAD dataset. Note that this strategy is much simpler than the genuine text generation strategy employed by  to produce the previous SOTA results for NQ which we intend to explore further in future work.</p><p>Data from other MRC Tasks We attempt to leverage human annotated data from three different machine reading comprehension (MRC) datasets for data augmentation:   <ref type="formula" target="#formula_4">(2)</ref> based on question-answer similarity to the NQ dataset.</p><p>For similarity based sampling, we follow a strategy similar to <ref type="bibr" target="#b5">Xu et al. (2018)</ref>. Specifically, we train a BERT-for-Sequence-Classification model using the Huggingface Py-Torch implementation of BERT 4 . The model accepts question tokens (discarding question marks since those do not appear in NQ) as the first text segment and short answer tokens (padded or truncated to 50 to limit maximum sequence length) as the second text segment. The model is trained with cross entropy loss to predict the source dataset for the question-answer pair using the development set from the three augmentation candidate datasets as well as target NQ development set.</p><p>Once trained, the predicted likelihood of an example being from the NQ dataset is calculated for all questionanswer pairs from the three augmentation candidate training datasets and used to order the examples by similarity for the purposes of sampling 5 . As would be expected, the most "similar" question-answer pairs were from SQuAD 2.0 (˜80% of the sampled data came from SQuAD 2.0) since the task is well aligned with the NQ task while Trivi-aQA question-answer pairs tended to be least "similar" (onlỹ 9.5% of the sampled data came from TriviaQA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset</head><p>The NQ dataset provides 307,373 queries for training, 7,830 queries for development, and 7,842 queries for testing (with the test set only being accessible through a public leaderboard submission).</p><p>For each question, crowd sourced annotators also provide start and end offsets for short answer spans 6 within the Wikipedia article, if available, as well as long answer spans (which is generally the most immediate HTML paragraph, list, or table span containing the short answer span), if available <ref type="bibr" target="#b3">(Kwiatkowski et al. 2019)</ref>.</p><p>Similar to other MRC datasets such as SQuAD 2.0, the NQ dataset forces models to make an attempt at "knowing what they don't know" by requiring a confidence score with each prediction. The evaluation script 7 , then calculates the optimal threshold at which the system will "choose" to provide an answer. The resulting F1 scores for Short Answer (SA) and Long Answer (LA) predictions are used as our headline metric.</p><p>The "partial un-answerability" and "natural generation" aspects of this dataset along with the recency of the task's publication make it an attractive dataset for evaluating model architecture and training choices (with lots of headroom between human performance and the best performing automated system).</p><p>The training itself is carried out using the Huggingface PyTorch implementation of BERT which supports starting from either BERT BASE or BERT LARGE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Optimization</head><p>The primary hyperparameter settings for the models discussed in the Model Architecture section are derived from (Alberti, Lee, and Collins 2019) with the exception of the following:</p><p>1. Stride -Following the implementation of the BERTfor-QA model in (Devlin et al. 2019), we accommodate BERT's pre-trained input size constraint of 512 tokens by splitting larger sequences into multiple spans over the Wikipedia article text using a sliding window. We experiment with multiple stride lengths to control for both experiment latency (shorter strides results in a larger number of spans per article) as well as F1 performance.</p><p>2. Negative Instance Sub-Sampling -Another consequence of splitting each Wikipedia article into multiple spans is that most spans of the article do not contain the correct short answer (only 65% of the questions are answerable by a short span and, of these, 90% contain a single correct answer span in the article with an average span length of only 4 words). As a result, there is a severe imbalance in the number of positive to negative (i.e. no answer) spans of text. The authors of (Alberti, Lee, and Collins 2019) address the imbalance during training by sub-sampling negative instances at a rate of 2%. We emulate this sub-sampling behavior when generating example spans for answerable questions. However, based on the observation that our preliminary BERT BASE models tended to be overconfident for unanswerable questions, we vary the sampling rate between answerable and unanswerable questions.</p><p>3. Batch Size &amp; Learning Rate -These parameters were tuned for each experiment using the approach outlined in (Smith 2018) where we evaluate a number of batch sizes and learning rates on a randomly selected 20% subset of the NQ training and development data. During experimentation, we did find that slight changes in learning rate can have a couple of points impact on the final F1 scores. Further work is needed to improve robustness of learning rate selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembling</head><p>In addition to optimizing for single model performance, in this section we outline a number of strategies that we investigated for ensembling models as is common for top ranking leaderboard submissions in MRC 8 . In order to formally compare approaches we partition the NQ dev set into "devtrain" and "dev-test" by taking the first three dev files for the "train" set and using the last two for the "test" set (the original dev set for NQ is partitioned into 5 files for distribution). This yields "train" and "test" sets of 4,653 and 3,177 examples (query-article pairs) respectively. For each ensembling strategy considered we search for the best k-model ensemble over the "train" set and then evaluate on the "test" set. For these experiments we use k = 4 as this is the number of models that we can decode in 24 hours on a Nvidia R Tesla R P100 GPU, which is the limit for the NQ leaderboard.</p><p>We examine two types of ensembling experiments: (i) ensembling the same model trained with different seeds and (ii) ensembling different model architectures and (pre-)training data. Ensembling the same model trained on different seeds attempts to smooth the variance to produce a stronger result. On the other hand ensembling different models attempts to find models that may not be the strongest individually but harmonize well to produce strong results.</p><p>To generate the ensembled predictions for an example, we combine the top-20 candidate long and short answers from each system in the ensemble 9 . To combine systems we take the arithmetic mean 10 of the scores for each long and short span predicted by at least one system. For spans which are only predicted by a subset of models, a score of zero is imputed for the remaining models. The predicted long/short span is then the span with the greatest arithmetic mean.</p><p>Seed experiments We investigate ensembling the best single model, selected as the model with greatest sum of short and long answer F1 scores, trained with k unique seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Model Ensembling Experiments</head><p>In our investigation of ensembling multiple models we greedy and exhaustive search strategies for selecting models from a pool of candidate models consisting of various configurations described in the Model Training and Model Architecture sections. The candidate pool also contains multiple instances of the same model training and architecture configuration, but with different learning rates (as mentioned in the previous section, we found that slight changes in learning rate can affect the final performance by a couple of F1 points):</p><p>Exhaustive Search During exhaustive search, we consider all n k ensembles of k candidates from our group of n models. After searching all possible ensembles we return two ensembles: (i) the ensemble with the highest long answer F1 score and (ii) the ensemble with the highest short answer F1 score. Given the combinatorial complexity, we limit the search to the top 20 best performing models. We select the top models using the same approach as in our seed experiments (i.e. the ones with the greatest sum of short and long answer F1 scores).</p><p>Greedy Search For the greedy approach we consider all 41 BERT LARGE models that we had trained during experimentation and greedily build an ensemble of size k from this model set, optimizing for either short or long answer performance. We refer to the ensembles created in this way as S and L respectively.</p><p>We construct S by greedily building 1, 2, ..., k model ensembles optimizing for short answer F1. In case adding some of the models decreased our short answer performance, we take the first i ≤ k models of S which give the highest short answer F1. The same is done for L when optimizing for long answers.</p><p>To build the long answer ensemble (when optimizing for short answer performance), we check to see which subset of S results in the best long answer performance. More formally we create L = arg max x∈P(L) F 1 L (x) where F 1 L (X) is the long answer F1 for the ensemble created with the models in X. A corresponding approach is used to create S when optimizing for long answers.</p><p>Finally, we join the predictions for short and long answers together by taking the short answer and long answer predictions from our short and long answer model sets respectively. If for an example a null long answer is predicted, we also predict a null short answer regardless of what S predicted as there are no short answers for examples which do not have a long answer in NQ <ref type="bibr" target="#b3">(Kwiatkowski et al. 2019)</ref>.</p><p>Duplicate Answer Span Aggregation A consequence of splitting large paragraphs into multiple overlapping is that, often, a single system for a single example will generate identical answer spans multiple times in its top 20 predictions. In order to produce a unique prediction score for each answer span from each system, we experiment with the following aggregation strategies on the vector P of scores for a given answer span.  <ref type="figure">Figure 1</ref>: Effect of stride length (in tokens) on the NQ Short Answer Dev Set F1 Performance</p><formula xml:id="formula_10">• Max = max |P | i=1 P i 53.</formula><formula xml:id="formula_11">• Reciprocal Rank Sum = |P | i=1 P i * 1 i • Exponential Sum = |P | i=1 P i * β i−1 for some constant β (we use β = 0.5). • Noisy-Or = 1 − |P | i=1 (1 − P i )</formula><p>For the last three strategies 11 (reciprocal rank sum, exponential sum, and noisy-or), we additionally experiment with score normalization using a logistic regression model that was trained to predict top 1 precision based on the top score 12 using the "dev-train" examples. We use the scikitlearn (Pedregosa et al. 2011) implementation of logistic regression (with stratified 5-fold cross-validation to select the L2 regularization strength).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Stride Rather than using a stride length of 128 tokens as was done by ) and (Alberti, Lee, and Collins 2019), we find that increasing the stride to 192 improves the final F1 score while also reducing the number of spans and, thus, the training time. See figure 1 for experimental results showing a 0.9% gain by increasing the stride length to 192 on some preliminary Bert-for-QA models.</p><p>Further increases seem to deteriorate the performance which may be a function of the size of the relevant context in Wikipedia articles, though additional work is required to better explore context size selection approaches given the document text.</p><p>Negative Instance Sub-Sampling As per table 2, performance initially improves as we sample negative instances at slightly higher rates than the 2% level used in (Alberti, Lee, and Collins 2019), but eventually begins to deteriorate when the sampling rate is increased too much. Performance can be improved further by sampling at a slightly lower rate of 1% for answerable questions and at higher rate of 4% for unanswerable questions. Overall, this change provides a boost of 0.8% in SA F1 over the setting used in (Alberti, Lee, and Collins 2019) on preliminary BERT BASE -for-QA models.</p><p>Pre-Training As per table 1, pre-training on SQuAD 2.0 from the WWM model provides the best single BERT-for-QA model on the target NQ dataset. So we use apply this pre-training strategy to the additional model architectures discussed earlier: AoA and Layer Combo.</p><p>Model Architecture Given our best pre-training strategy of the WWM model on SQuAD 2.0, we show in table 1 that adding the AoA layer during the finetuning stage of our target dataset of NQ yields the best single model performance. Linearly combining the BERT output layers shows a slight improvement over BERT-for-QA for SA but the same amount of drop for LA.</p><p>Data Augmentation As seen in table 1, a naive strategy of simply shuffling the examples from the aforementioned strategies into the first 80% of mini batches during the finetuning phase did not provide significant improvements in single model performance over BERT +W W M . This may indicate that the NQ dataset is sufficiently large so as to not require additional examples. Instead, pre-training the base model on a similar task like SQuAD 2.0 on top of the WWM BERT model seems to be the best strategy for maximizing single model performance and outperforms the previous SOTA: a BERT model trained with 4 million additional synthetic question answer pairs. Another interesting result is that, even the simpler (sentence shuffling) and less data intense (307,373 examples) data augmentation strategy (BERT +W W M w/ SOS) outperforms the previous SOTA model's use of 4 million synthetic question answer generation model. <ref type="table" target="#tab_7">Table 3</ref> shows there is a benefit to ensembling multiple versions of the same model trained with different random seeds at training time. Specifically, there is a gain of roughly 2.5% in both SA and LA F1 by ensembling four models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed Experiments</head><p>Multiple Model Ensembling Experiments As shown in table 3, we find that ensembling a diverse set of models can provide an additional 1% boost in SA F1 and a 1.2% boost in LA F1 over simply ensembling the same model configuration with different random seeds during training.</p><p>Specifically, performing a greedy search and optimizing for long answer performance appears to generalize best to the dev-test set. We hypothesize that the reasons for the superior generalization of the greedy approach over exhaustive is that exhaustive search is "overfitting" to the examples in dev-train. Another potential cause of the better generalization of greedy is that it can search more candidates due to the decreased computational complexity.</p><p>Similarly we hypothesize the reason optimizing for long answer F1 generalizes better for short and long answers is    due to the strict definition of correctness for Natural Questions which requires exact span matching <ref type="bibr" target="#b3">(Kwiatkowski et al. 2019)</ref>. In our final search over all ensembles using the greedy (long answer) search, the algorithm selects an ensemble consisting of the following models: (1) BERT W W M + SQuAD 2 PT + AoA (2) BERT W W M + SQuAD 2 PT (3) BERT W W M + SQuAD 2 PT (4) BERT SSP T . So only one of the chosen model configurations is that of the single best performing model. The remaining models, though outperformed as individual models, provide a boost over multiple random seed variations of the best single model configuration.</p><p>Duplicate Answer Span Aggregation <ref type="table" target="#tab_8">Table 4</ref> shows further experimentation with the greedy long answer ensembling strategy where we vary the aggregation strategies for duplicate answer span predictions. We find that using max aggregation results in the best short answer F1 whereas using normalized noisy-or aggregation results in the best long answer F1. Therefore, for our final submission, we use a combination strategy of producing short answer predictions using a greedy long answer search with max score for duplicate spans and long answer predictions using a greedy long answer search with noisy-or scores for duplicate spans.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1.</head><label></label><figDesc>SQuAD 2.0 -˜130,000 crowd sourced question and answer training pairs derived from Wikipedia paragraphs. 2. NewsQA (Trischler et al. 2016) -˜100,000 crowd sourced question and answer training pairs derived from news articles. 3. TriviaQA (Joshi et al. 2017) -˜78,000 question and answers authored by trivia enthusiasts which were subsequently associated with wikipedia passages (potentially) containing the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1909.05286v1 [cs.CL] 11 Sep 2019 leaderboard fail to outline their ensembling techniques.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Given a token sequence X = [x 1 , x 2 , . . . , x T ], BERT, a deep Transformer (Vaswani et al. 2017) network, outputs a sequence of contextualized token representations H</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1. BERT with Whole Word Masking (WWM) is one of the default BERT pre-trained models that has the same model structure as the original BERT model, but masks whole words instead of word pieces for the Masked Language Model pre-training task. 2. BERT with Span Selection Pre-Training (SSPT) uses an unsupervised auxiliary QA specific task proposed by Glass et al. (2019) to further train the BERT model. The task generates synthetic cloze style queries by masking out terms (named entities or noun phrases) in a sentence. Then answer bearing passages are extracted from the Wikipedia corpus using BM25 based information retrieval (Robertson 2009). This allows us to pre-train all layers of the BERT model including the answer extraction weights by training the model to extract the answer term from the selected passage. 3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Short &amp; long answer F1 performance of BERT-for-QA models on NQ dev. We abbreviate pre-training with PT.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SA F1 LA F1</cell></row><row><cell>Prior Work</cell><cell></cell><cell></cell></row><row><cell>DecAtt + Doc Reader</cell><cell></cell><cell>31.4</cell><cell>54.8</cell></row><row><cell>(Parikh et al. 2016)</cell><cell></cell><cell></cell></row><row><cell cols="2">BERT w/ SQuAD 1.1 PT</cell><cell>52.7</cell><cell>64.7</cell></row><row><cell cols="2">(Alberti, Lee, and Collins 2019)</cell><cell></cell></row><row><cell cols="2">BERT w/ 4M Synthetic Data</cell><cell>55.1</cell><cell>65.9</cell></row><row><cell cols="2">Augmentation (Alberti et al. 2019)</cell><cell></cell></row><row><cell cols="2">This Work (Pre-Training)</cell><cell></cell></row><row><cell>BERT W W M</cell><cell></cell><cell>55.35</cell><cell>66.04</cell></row><row><cell>BERT SSP T</cell><cell></cell><cell>54.83</cell><cell>66.75</cell></row><row><cell cols="2">BERT W W M + SQuAD 2 PT</cell><cell>56.95</cell><cell>67.28</cell></row><row><cell cols="2">BERT W W M + SQuAD 2 PT</cell><cell>57.15</cell><cell>67.08</cell></row><row><cell>+ Layer Combo</cell><cell></cell><cell></cell></row><row><cell cols="3">BERT W W M + SQuAD 2 PT + AoA 57.22</cell><cell>68.24</cell></row><row><cell cols="2">This Work (Data Augmentation)</cell><cell></cell></row><row><cell>BERT W W M w/ SOS</cell><cell></cell><cell>55.81</cell><cell>66.67</cell></row><row><cell cols="2">BERT W W M w/ 21K Random</cell><cell>54.05</cell><cell>66.23</cell></row><row><cell cols="2">Examples from MRC Tasks</cell><cell></cell></row><row><cell cols="2">BERT W W M w/ 21K Similar</cell><cell>55.18</cell><cell>66.34</cell></row><row><cell cols="2">Examples from MRC Tasks</cell><cell></cell></row><row><cell cols="2">BERT W W M w/ 100K Similar</cell><cell>54.68</cell><cell>65.82</cell></row><row><cell cols="2">Examples from MRC Tasks</cell><cell></cell></row><row><cell cols="4">Neg Sampling Rate Neg Sampling Rate SA F1</cell></row><row><cell>for Answerable</cell><cell cols="2">for Un-Answerable</cell></row><row><cell>1%</cell><cell>1%</cell><cell></cell><cell>45.22</cell></row><row><cell>2%</cell><cell>2%</cell><cell></cell><cell>46.20</cell></row><row><cell>4%</cell><cell>4%</cell><cell></cell><cell>46.45</cell></row><row><cell>5%</cell><cell>5%</cell><cell></cell><cell>45.94</cell></row><row><cell>1%</cell><cell>4%</cell><cell></cell><cell>47.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Performance on NQ dev using a preliminary BERT BASE -for-QA model with varying sub-sampling</figDesc><table><row><cell>SA F1 LA F1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ensemble performance on NQ dev-test</figDesc><table><row><cell>Aggregation Strategy</cell><cell cols="2">SA F1 LA F1</cell></row><row><cell>Max</cell><cell cols="2">0.5971 0.7084</cell></row><row><cell cols="3">Reciprocal Rank Sum 0.5728 0.7066</cell></row><row><cell>Exponential Sum</cell><cell cols="2">0.5826 0.7040</cell></row><row><cell>Noisy-Or</cell><cell>0.573</cell><cell>0.715</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Performance on NQ dev-test for varying aggregation strategies for duplicate answer spans (using greedy long answer search)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The candidate long answer HTML spans are provided as part of the preprocessed data for NQ.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Superscript L is dropped here for notation convenience; we use the last layer L = 24 from the BERT output.Pre-TrainingWe explore three types of BERT parameter pre-trainings prior to finetuning on the NQ corpus:</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/huggingface/pytorch-transformers.5  The BERT-for-Sequence-Classification model achieves 90% accuracy at detecting the dataset source for a given query-answer pair.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Instead of short answer spans, annotators have marked 1% of the questions with a simple Yes/No. We leave it as future work to detect and generate answers for these types of queries.7  The evaluation script is provided by Google at https://github. com/google-research-datasets/natural-questions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The top ranking submissions for SQuAD 2.0, TriviaQA, and HotpotQA are all ensemble models as of this paper's writing.9  We empirically find that considering 20 is better than considering fewer candidates (e.g. 5 or 10).10  We have experimented with other approaches such as median, geometric mean, and harmonic mean; however these are omitted here as they resulted in much lower scores than arithmetic mean.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Using un-normalized versions of sum and noisy-or causes dramatic deterioration.12  Though we experimented with additional input features such as query length and mean score across top 20, we omit results as performance does not improve over simple logistic regression.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We outline MRC algorithms that yield SOTA performance on benchmark datasets like SQuAD and show that a very simple approach involving transfer learning reaches the same performance while being computationally inexpensive. We also show that the same simple approach has strong empirical performance and yields the new SOTA on the NQ task as it outperforms a QA system trained on 4 million examples when ours was trained on only 307,373 (i.e. the size of the original NQ training set). Our future work will involve adding larger pre-trained language models like RoBERTa and XLNet.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthetic QA corpora generation with roundtrip consistency. CoRR abs/1906.05416</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [alberti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A BERT baseline for the natural questions</title>
		<meeting><address><addrLine>Alberti, Lee, and Collins</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv:1901.08634 1-4.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cui</surname></persName>
		</author>
		<idno>593-602. ACL</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pruthi</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferritto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Shrivatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno>abs/1705.03551. [Kočiskỳ et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="j">The NarrativeQA reading comprehension challenge. TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Simple and effective semisupervised question answering</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural Questions: a benchmark for question answering research. TACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Deep contextualized word representations</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang ; Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<idno>abs/1611.09830</idno>
		<ptr target="Multi-tasklearningformachinereadingcomprehen-sion.CoRRabs/1809.06963" />
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL: Tutorials, 15-18. Minneapolis, Minnesota: ACL</title>
		<meeting>of NAACL: Tutorials, 15-18. Minneapolis, Minnesota: ACL</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems. Xu et al. 2018</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<idno>arXiv:1908.05147</idno>
	</analytic>
	<monogr>
		<title level="m">SG-Net: Syntax-guided machine reading comprehension</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>XLNet: Generalized autoregressive pretraining for language understanding. CoRR abs/1906.08237. [Yatskar 2018] Yatskar, M. 2018. A qualitative comparison of CoQA, SQuAD 2.0 and QuAC. CoRR abs/1809.10735. [Zhang et al. 2019. Ensemble BERT with data augmentation and linguistic knowledge on SQuAD 2.0</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
