<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple and Deep Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Simple and Deep Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the over-smoothing problem.</p><p>In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: Initial residual and Identity mapping. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi-and fullsupervised tasks. Code is available at https: //github.com/chennnM/GCNII.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph convolutional networks (GCNs) <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref> generalize convolutional neural networks (CNNs) <ref type="bibr">(Le-Cun et al., 1995)</ref> to graph-structured data. To learn the graph representations, the "graph convolution" operation applies the same linear transformation to all the neighbors of a node followed by a nonlinear activation function. In recent years, GCNs and their variants <ref type="bibr" target="#b6">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b38">Veličković et al., 2018)</ref> have been successfully applied to a wide range of applications, including social analysis <ref type="bibr" target="#b32">(Qiu et al., 2018;</ref><ref type="bibr" target="#b21">Li &amp; Goldwasser, 2019)</ref>, traffic prediction <ref type="bibr" target="#b10">(Guo et al., 2019;</ref>, biology <ref type="bibr" target="#b8">(Fout et al., 2017;</ref><ref type="bibr" target="#b36">Shang et al., 2019)</ref>, recommender systems <ref type="bibr" target="#b45">(Ying et al., 2018)</ref>, and com- puter vision <ref type="bibr" target="#b48">(Zhao et al., 2019;</ref><ref type="bibr" target="#b27">Ma et al., 2019</ref>).</p><p>Despite their enormous success, most of the current GCN models are shallow. Most of the recent models, such as GCN <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref> and GAT <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref>, achieve their best performance with 2-layer models. Such shallow architectures limit their ability to extract information from high-order neighbors. However, stacking more layers and adding non-linearity tends to degrade the performance of these models. Such a phenomenon is called over-smoothing <ref type="bibr" target="#b24">(Li et al., 2018b)</ref>, which suggests that as the number of layers increases, the representations of the nodes in GCN are inclined to converge to a certain value and thus become indistinguishable. ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref> solves a similar problem in computer vision with residual connections, which is effective for training very deep neural networks. Unfortunately, adding residual connections in the GCN models merely slows down the over-smoothing problem <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref>; deep GCN models are still outperformed by 2-layer models such as GCN or GAT.</p><p>Recently, several works try to tackle the problem of oversmoothing. JKNet <ref type="bibr" target="#b43">(Xu et al., 2018)</ref> uses dense skip connections to combine the output of each layer to preserve the locality of the node representations. Recently, DropEdge <ref type="bibr" target="#b33">(Rong et al., 2020)</ref> suggests that by randomly removing out a few edges from the input graph, one can relieve the impact of over-smoothing. Experiments <ref type="bibr" target="#b33">(Rong et al., 2020)</ref> suggest that the two methods can slow down the performance drop as we increase the network depth. However, for semi-supervised tasks, the state-of-the-art results are still achieved by the shallow models, and thus the benefit brought by increasing the network depth remains in doubt.</p><p>On the other hand, several methods combine deep propagation with shallow neural networks. SGC <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> attempts to capture higher-order information in the graph by applying the K-th power of the graph convolution matrix in a single neural network layer. PPNP and APPNP <ref type="bibr" target="#b17">(Klicpera et al., 2019a)</ref> replace the power of the graph convolution matrix with the Personalized PageRank matrix to solve the over-smoothing problem. GDC <ref type="bibr" target="#b18">(Klicpera et al., 2019b)</ref> further extends APPNP by generalizing Personalized PageRank <ref type="bibr" target="#b29">(Page et al., 1999)</ref> to an arbitrary graph diffusion process. However, these methods perform a linear combination of neighbor features in each layer and lose the arXiv:2007.02133v1 <ref type="bibr">[cs.</ref>LG] 4 Jul 2020 powerful expression ability of deep nonlinear architectures, which means they are still shallow models.</p><p>In conclusion, it remains an open problem to design a GCN model that effectively prevents over-smoothing and achieves state-of-the-art results with truly deep network structures. Due to this challenge, it is even unclear whether the network depth is a resource or a burden in designing new graph neural networks. In this paper, we give a positive answer to this open problem by demonstrating that the vanilla GCN <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref> can be extended to a deep model with two simple yet effective modifications. In particular, we propose Graph Convolutional Network via Initial residual and Identity mapping (GCNII), a deep GCN model that resolves the over-smoothing problem. At each layer, initial residual constructs a skip connection from the input layer, while identity mapping adds an identity matrix to the weight matrix. The empirical study demonstrates that the two surprisingly simple techniques prevent over-smoothing and improve the performance of GCNII consistently as we increase its network depth. In particular, the deep GCNII model achieves new state-of-the-art results on various semisupervised and full-supervised tasks.</p><p>Second, we provide theoretical analysis for multi-layer GCN and GCNII models. It is known <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> that by stacking k layers, the vanilla GCN essentially simulates a K-th order of polynomial filter with predetermined coefficients.  points out that such a filter simulates a lazy random walk that eventually converges to the stationary vector and thus leads to over-smoothing. On the other hand, we prove that a K-layer GCNII model can express a polynomial spectral filter of order K with arbitrary coefficients. This property is essential for designing deep neural networks. We also derive the closed-form of the stationary vector and analyze the rate of convergence for the vanilla GCN. Our analysis implies that nodes with high degrees are more likely to suffer from over-smoothing in a multi-layer GCN model, and we perform experiments to confirm this theoretical conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Notations. Given a simple and connected undirected graph G = (V, E) with n nodes and m edges. We define the self-looped graphG = (V,Ẽ) to be the graph with a self-loop attached to each node in G. We use {1, . . . , n} to denote the node IDs of G andG, and d j and d j + 1 to denote the degree of node j in G andG, respectively. Let A denote the adjacency matrix and D the diagonal degree matrix. Consequently, the adjacency matrix and diagonal degree matrix ofG is defined to beÃ = A+I andD = D+I, respectively. Let X ∈ R n×d denote the node feature matrix, that is, each node v is associated with a d-dimensional feature vector X v . The normalized graph Laplacian matrix is defined as L = I n − D −1/2 AD −1/2 , which is a symmetric positive semidefinite matrix with eigendecomposition UΛU T ,. Here Λ is a diagonal matrix of the eigenvalues of L, and U ∈ R n×n is a unitary matrix that consists of the eigenvectors of L. The graph convolution operation between signal x and filter g γ (Λ) = diag(γ) is defined as g γ (L) * x = Ug γ (Λ)U T x, where the parameter γ ∈ R n corresponds to a vector of spectral filter coefficients.</p><p>Vanilla GCN. <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref> and <ref type="bibr" target="#b6">(Defferrard et al., 2016)</ref> suggest that the graph convolution operation can be further approximated by the K-th order polynomial of Laplacians</p><formula xml:id="formula_0">Ug θ (Λ)U T x ≈ U K =0 θ Λ U x = K =0 θ L x,</formula><p>where θ ∈ R K+1 corresponds to a vector of polynomial coefficients. The vanilla GCN <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref> sets K = 1, θ 0 = 2θ and θ 1 = −θ to obtain the convolution operation g θ * x = θ I + D −1/2 AD −1/2 x. Finally, by the renormalization trick, <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref> replaces the matrix I+D −1/2 AD −1/2 by a normalized versionP = D −1/2ÃD−1/2 = (D + I n ) −1/2 (A + I n )(D + I n ) −1/2 . and obtains the Graph Convolutional Layer</p><formula xml:id="formula_1">H ( +1) = σ P H ( ) W ( ) .<label>(1)</label></formula><p>Where σ denotes the ReLU operation.</p><p>SGC <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> shows that by stacking K layers, GCN corresponds to a fixed polynomial filter of order K on the graph spectral domain ofG. In particular, letL = I n −D −1/2ÃD−1/2 denote the normalized graph Laplacian matrix of the self-looped graphG. Consequently, applying a K-layer GCN to a signal x corre- <ref type="bibr">et al., 2019)</ref> also shows that by adding a self-loop to each node,L effectively shrinks the underlying graph spectrum.</p><formula xml:id="formula_2">sponds to D −1/2ÃD−1/2 K x = I n −L K x. (Wu</formula><p>APPNP. <ref type="bibr" target="#b17">(Klicpera et al., 2019a)</ref> uses Personalized PageRank to derive a fixed filter of order K. Let f θ (X) denote the output of a two-layer fully connected neural network on the feature matrix X, PPNP's model is defined as</p><formula xml:id="formula_3">H = α I n − (1 − α)Ã −1 f θ (X).</formula><p>(2)</p><p>Due to the property of Personalized PageRank, such a filter preserves locality and thus is suitable for classification tasks. <ref type="bibr" target="#b17">(Klicpera et al., 2019a)</ref> also proposes APPNP, which</p><formula xml:id="formula_4">replaces α I n − (1 − α)Ã −1</formula><p>with an approximation derived by a truncated power iteration. Formally, APPNP with K-hop aggregation is defined as</p><formula xml:id="formula_5">H ( +1) = (1 − α)P H ( ) + αH (0) ,<label>(3)</label></formula><p>where H (0) = f θ (X). By decoupling feature transformation and propagation, PPNP and APPNP can aggregate information from multi-hop neighbors without increasing the number of layers in the neural network.</p><p>JKNet. The first deep GCN framework is proposed by <ref type="bibr" target="#b43">(Xu et al., 2018)</ref>. At the last layer, JKNet combines all previous representations H (1) , . . . , H (K) to learn representations of different orders for different graph substructures. <ref type="bibr" target="#b43">(Xu et al., 2018)</ref> proves that 1) a K-layer vanilla GCN model simulates random walks of K steps in the selflooped graphG and 2) by combining all representations from the previous layers, JKNet relieves the problem of over-smoothing.</p><p>DropEdge A recent work <ref type="bibr" target="#b33">(Rong et al., 2020)</ref> suggests that randomly removing some edges fromG retards the convergence speed of over-smoothing. LetP drop denote the renormalized graph convolution matrix with some edge removed at random, the vanilla GCN equipped with DropEdge is defined as</p><formula xml:id="formula_6">H ( +1) = σ P drop H ( ) W ( ) .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GCNII Model</head><p>It is known <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> that by stacking K layers, the vanilla GCN simulates a polynomial filter K =0 θ L x of order K with fixed coefficients θ on the graph spectral domain ofG. The fixed coefficients limit the expressive power of a multi-layer GCN model and thus leads to oversmoothing. To extend GCN to a truly deep model, we need to enable GCN to express a K order polynomial filter with arbitrary coefficients. We show this can be achieved by two simple techniques: Initial residual connection and Identity mapping. Formally, we define the -th layer of GCNII as</p><formula xml:id="formula_7">H ( +1) =σ (1−α )PH ( ) +α H (0) (1−β )I n +β W ( ) ,<label>(5)</label></formula><p>where α and β are two hyperparameters to be discussed later. Recall thatP =D −1/2ÃD−1/2 is the graph convolution matrix with the renormalization trick. Note that compared to the vanilla GCN model (equation <ref type="formula" target="#formula_1">(1)</ref>), we make two modifications: 1) We combine the smoothed represen-tationPH ( ) with an initial residual connection to the first layer H (0) ; 2) We add an identity mapping I n to the -th weight matrix W ( ) .</p><p>Initial residual connection. To simulate the skip connection in ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref>, <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref> proposes residual connection that combines the smoothed representationPH ( ) with H ( ) . However, it is also shown in <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017</ref>) that such residual connection only partially relieves the over-smoothing problem; the performance of the model still degrades as we stack more layers.</p><p>We propose that, instead of using a residual connection to carry the information from the previous layer, we construct a connection to the initial representation H (0) . The initial residual connection ensures that that the final representation of each node retains at least a fraction of α from the input layer even if we stack many layers. In practice, we can simply set α = 0.1 or 0.2 so that the final representation of each node consists of at least a fraction of the input feature. We also note that H (0) does not necessarily have to be the feature matrix X. If the feature dimension d is large, we can apply a fully-connected neural network on X to obtain a lower-dimensional initial representation H (0) before the forward propagation.</p><p>Finally, we recall that APPNP <ref type="bibr" target="#b17">(Klicpera et al., 2019a)</ref> employs a similar approach to the initial residual connection in the context of Personalized PageRank. However, <ref type="bibr" target="#b17">(Klicpera et al., 2019a)</ref> also shows that performing multiple nonlinearity operations to the feature matrix will lead to overfitting and thus results in the performance drop. Therefore, APPNP applies a linear combination between different layers and thus remains a shallow model. This suggests that the idea of initial residual alone is not sufficient to extend GCN to a deep model. Identity mapping. To amend the deficiency of APPNP, we borrow the idea of identity mapping from ResNet. At the -th layer, we add an identity matrix I n to the weight matrix W ( ) . In the following, we summarize the motivations for introducing identity mapping into our model.</p><p>• Similar to the motivation of ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref>, identity mapping ensures that a deep GCNII model achieves at least the same performance as its shallow version does. In particular, by setting β sufficiently small, deep GCNII ignores the weight matrix W ( ) and essentially simulates APPNP (equation <ref type="formula" target="#formula_5">(3)</ref>).</p><p>• It has been observed that frequent interaction between different dimensions of the feature matrix <ref type="bibr" target="#b17">(Klicpera et al., 2019a)</ref> degrades the performance of the model in semi-supervised tasks. Mapping the smoothed rep-resentationPH ( ) directly to the output reduces such interaction.</p><p>• Identity mapping is proved to be particularly useful in semi-supervised tasks. It is shown in <ref type="bibr" target="#b12">(Hardt &amp; Ma, 2017</ref>) that a linear ResNet of the form H ( +1) = H ( ) W ( ) + I n satisfies the following properties: 1) The optimal weight matrices W (l) have small norms;</p><p>2) The only critical point is the global minimum. The first property allows us to put strong regularization on W to avoid over-fitting, while the later is desirable in semi-supervised tasks where training data is limited.</p><p>• <ref type="bibr" target="#b28">(Oono &amp; Suzuki, 2020)</ref> theoretically proves that the node features of a K-layer GCNs will converge to a subspace and incur information loss. In particular, the rate of convergence depends on s K , where s is the maximum singular value of the weight matrices W ( ) , = 0, . . . , K − 1. By replacing W ( ) with (1 − β )I n + β W ( ) and imposing regularization on W ( ) , we force the norm of W ( ) to be small. Consequently, the singular values of (1 − β )I n + β W ( ) will be close to 1. Therefore, the maximum singular value s will also be close to 1, which implies that s K is large, and the information loss is relieved.</p><p>The principle of setting β is to ensure the decay of the weight matrix adaptively increases as we stack more layers.</p><p>In practice, we set β = log( λ + 1) ≈ λ , where λ is a hyperparameter.</p><p>Connection to iterative shrinkage-thresholding. Recently, there has been work on optimization-inspired network structure design <ref type="bibr" target="#b46">(Zhang &amp; Ghanem, 2018;</ref><ref type="bibr" target="#b30">Papyan et al., 2017)</ref>. The idea is that a feedforward neural network can be considered as an iterative optimization algorithm to minimize some function, and it was hypothesized that better optimization algorithms might lead to better network structure <ref type="bibr" target="#b22">(Li et al., 2018a)</ref>. Thus, theories in numerical optimization algorithms may inspire the design of better and more interpretable network structures. As we will show next, the use of identity mappings in our structure is also well-motivated from this. We consider the LASSO objective: min x∈R n 1 2 Bx − y 2 2 + λ x 1 . Similar to compressive sensing, we consider x as the signal we are trying to recover, B as the measurement matrix, and y as the signal we observe. In our setting, y is the original feature of a node, and x is the node embedding the network tries to learn. As opposed to standard regression models, the design matrix B is unknown parameters and will be learned through back propagation. So, this is in the same spirit as the sparse coding problem, which has been used to design and to analyze CNNs <ref type="bibr" target="#b30">(Papyan et al., 2017)</ref>. Iterative shrinkage-thresholding algorithms are effective for solving the above optimization problem, in which the update in the (t + 1)th iteration is:</p><formula xml:id="formula_8">x t+1 = P µtλ x t − µ t B T Bx t + µ t B T y ,</formula><p>Here µ t is the step size, and P β (·) (with β &gt; 0) is the entry-wise soft thresholding function:</p><formula xml:id="formula_9">P θ (z) =    z − θ, if z ≥ θ 0, if |z| &lt; θ z + θ, if z ≤ −θ .</formula><p>Now, if we reparameterize −B T B by W, the above update formula becomes quite similar to the one used in our method. More spopposeecifically, we have x t+1 = P µtλ (I + µ t W)x t + µ t B T y , where the term µ t B T y corresponds to the initial residual, and I+µ t W corresponds to the identity mapping in our model <ref type="formula" target="#formula_7">(5)</ref>. The soft thresholding operator acts as the nonlinear activation function, which is similar to the effect of ReLU activation. In conclusion, our network structure, especially the use of identity mapping is well-motivated from iterative shrinkage-thresholding algorithms for solving LASSO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Spectral Analysis</head><p>4.1. Spectral analysis of multi-layer GCN.</p><p>We consider the following GCN model with residual connection:</p><formula xml:id="formula_10">H ( +1) = σ P H ( ) + H ( ) W ( ) .<label>(6)</label></formula><p>Recall thatP =D −1/2ÃD−1/2 is the graph convolution matrix with the renormalization trick.  points out that equation <ref type="formula" target="#formula_10">(6)</ref> simulates a lazy random walk with the transition matrix In+D −1/2ÃD−1/2 2 . Such a lazy random walk eventually converges to the stationary state and thus leads to over-smoothing. We now derive the closedform of the stationary vector and analyze the rate of such convergence. Our analysis suggests that the converge rate of an individual node depends on its degree, and we conduct experiments to back up this theoretical finding. In particular, we have the following Theorem. Theorem 1. Assume the self-looped graphG is connected.</p><p>Let h (K) = In+D −1/2ÃD−1/2 2 K ·x denote the representation by applying a K-layer renormalized graph convolution with residual connection to a graph signal x. Let λG denote the spectral gap of the self-looped graphG, that is, the least nonzero eigenvalue of the normalized Laplaciañ L = I n −D −1/2ÃD−1/2 . We have 1) As K goes to infinity, h (K) converges to π = D 1/2 1,x 2m+n · D 1/2 1, where 1 denotes an all-one vector.</p><p>2) The convergence rate is determined by</p><formula xml:id="formula_11">h (K) = π ± n i=1 x i · 1 − λ 2 G 2 K · 1.<label>(7)</label></formula><p>Recall that m and n are the number of nodes and edges in the original graph G. We use the operator ± to indicate that for each entry h (K) (j) and π(j), j = 1, . . . , n,</p><formula xml:id="formula_12">h (K) (j) − π(j) ≤ n i=1 x i · 1 − λ 2 G 2 K .</formula><p>The proof of Theorem 1 can be found in the supplementary materials. There are two consequences from Theorem 1. First of all, it suggests that the K-th representation of GCN h (K) converges to a vector π = D 1/2 1,x 2m+n ·D 1/2 1. Such convergence leads to over-smoothing as the vector π only carries the two kinds of information: the degree of each node, and the inner product between the initial signal x and vector D 1/2 1.</p><p>Convergence rate and node degree. Equation <ref type="formula" target="#formula_11">(7)</ref> suggests that the converge rate depends on the summation of feature entries n i=1 x i and the spectral gap λG. If we take a closer look at the relative converge rate for an individual node j, we can express its final representation h (K) (j) as</p><formula xml:id="formula_13">h (K) (j)= d j + 1    n i=1 √ d i +1 2m+n x i ± n i=1 x i 1− λ 2 G 2 K d j + 1    .</formula><p>This suggests that if a node j has a higher degree of d j (and hence a larger d j + 1), its representation h (K) (j) converges faster to the stationary state π(j). Based on this fact, we make the following conjecture. Conjecture 1. Nodes with higher degrees are more likely to suffer from over-smoothing.</p><p>We will verify Conjecture 1 on real-world datasets in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spectral analysis of GCNII</head><p>We consider the spectral domain of the self-looped graph G. Recall that a polynomial filter of order K on a graph signal x is defined as K =0 θ L x, whereL is the normalized Laplacian matrix ofG and θ k 's are the polynomial coefficients. <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> proves that a K-layer GCN simulates a polynomial filter of order K with fixed coefficients θ. As we shall prove later, such fixed coefficients limit the expressive power of GCN and thus leads to oversmoothing. On the other hand, we show a K-layer GCNII model can express a K order polynomial filter with arbitrary coefficients. Theorem 2. Consider the self-looped graphG and a graph signal x. A K-layer GCNII can express a K order polynomial filter K =0 θ L x with arbitrary coefficients θ.</p><p>The proof of Theorem 2 can be found in the supplementary materials. Intuitively, the parameter β allows GCNII to simulate the coefficient θ of the polynomial filter.</p><p>Expressive power and over-smoothing. The ability to express a polynomial filter with arbitrary coefficients is essential for preventing over-smoothing. To see why this is the case, recall that Theorem 1 suggests a K-layer vanilla GCN simulates a fixed K-order polynomial filterP K x, wherẽ P is the renormalized graph convolution matrix. Oversmoothing is caused by the fact thatP K x converges to a distribution isolated from the input feature x and thus incuring gradient vanishment. DropEdge <ref type="bibr" target="#b33">(Rong et al., 2020)</ref> slows down the rate of convergence, but eventually will fail as K goes to infinity.</p><p>On the other hand, Theorem 2 suggests that deep GCNII converges to a distribution that carries information from both the input feature and the graph structure. This property alone ensures that GCNII will not suffer from oversmoothing even if the number of layers goes to infinity. More precisely, Theorem 2 states that a K-layer GCNII can express h (K) = K =0 θ L · x with arbitrary coefficients θ. Since the renormalized graph convolution matrixP = I n −L, it follows that K-layer GCNII can express h (K) = K =0 θ P · x with arbitrary coefficients θ . Note that with a proper choice of θ , h (K) can carry information from both the input feature and the graph structure even with K going to infinity. For example, APPNP <ref type="bibr" target="#b17">(Klicpera et al., 2019a)</ref> and GDC <ref type="bibr" target="#b18">(Klicpera et al., 2019b)</ref> set θ i = α(1−α) i for some constant 0 &lt; α &lt; 1. As K goes to infinity, h (K) = K =0 θ P · x converges to the Personalized PageRank vector of x, which is a function of both the adjacency matrixÃ and the input feature vector x. The difference between GCNII and APPNP/GDC is that 1) the coefficient vector theta in our model is learned from the input feature and the label, and 2) we impose a ReLU operation at each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Other Related Work</head><p>Spectral-based GCN has been extensively studied for the past few years. <ref type="bibr" target="#b25">(Li et al., 2018c)</ref> improves flexibility by learning a task-driven adaptive graph for each graph data while training. <ref type="bibr" target="#b42">(Xu et al., 2019)</ref> uses the graph wavelet basis instead of the Fourier basis to improve sparseness and locality. Another line of works focuses on the attention-based GCN model <ref type="bibr" target="#b38">(Veličković et al., 2018;</ref><ref type="bibr" target="#b37">Thekumparampil et al., 2018;</ref>, which learn the edge weights at each layer based on node features. (Abu-El-Haija et al., 2019) learn neighborhood mixing relationships by mixing of neighborhood information at various distances but still uses a two-layer model. <ref type="bibr" target="#b9">(Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b20">Lee et al., 2019)</ref> devote to extend pooling operations to graph neural network. For unsupervised information, <ref type="bibr" target="#b39">(Velickovic et al., 2019)</ref> train graph convolutional encoder through maximizing mutual information. <ref type="bibr" target="#b31">(Pei et al., 2020)</ref> build structural neighborhoods in the latent space of graph embedding for aggregation to extract more structural information. <ref type="bibr" target="#b5">(Dave et al., 2019</ref>) uses a single representation vector to capture both topological information and nodal attributes in graph embedding. Many of the sampling-based methods proposed to improve the scalability of GCN. <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref> uses a fixed size of neighborhood samples through layers, <ref type="bibr" target="#b14">Huang et al., 2018)</ref> propose efficient variants based on importance sampling. <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref> construct minibatch based on graph clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we evaluate the performance of GCNII against the state-of-the-art graph neural network models on a wide variety of open graph datasets.</p><p>Dataset and experimental setup. We use three standard citation network datasets Cora, Citeseer, and Pubmed <ref type="bibr" target="#b35">(Sen et al., 2008)</ref> for semi-supervised node classification. In these citation datasets, nodes correspond to documents, and edges correspond to citations; each node feature corresponds to the bag-of-words representation of the document and belongs to one of the academic topics. For full-supervised node classification, we also include Chameleon <ref type="bibr" target="#b34">(Rozemberczki et al., 2019)</ref>, Cornell, Texas, and Wisconsin <ref type="bibr" target="#b31">(Pei et al., 2020)</ref>. These datasets are web networks, where nodes and edges represent web pages and hyperlinks, respectively. The feature of each node is the bag-of-words representation of the corresponding page. For inductive learning, we use Protein-Protein Interaction (PPI) networks <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref>, which contains 24 graphs. Following the setting of previous work <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref>, we use 20 graphs for training, 2 graphs for validation, and the rest for testing. Statistics of the datasets are summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Besides GCNII (5), we also include GCNII*, a variant of GCNII that employs different weight matrices for the smoothed representationPH ( ) and the initial residual H (0) . Formally, the ( + 1)-th layer of GCNII* is defined as As mentioned in Section 3, we set β = log( λ + 1) ≈ λ/ , where λ is a hyperparameter.</p><formula xml:id="formula_14">H ( +1) = σ (1 − α )PH ( ) (1 − β )I n + β W ( ) 1 + +α H (0) (1 − β )I n + β W ( ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Semi-supervised Node Classification</head><p>Setting and baselines. For the semi-supervised node classification task, we apply the standard fixed training/validation/testing split <ref type="bibr" target="#b44">(Yang et al., 2016)</ref> on three datasets Cora, Citeseer, and Pubmed, with 20 nodes per class for training, 500 nodes for validation and 1,000 nodes for testing. For baselines, we include two recent deep GNN models: JKNet <ref type="bibr" target="#b43">(Xu et al., 2018)</ref> and DropEdge <ref type="bibr" target="#b33">(Rong et al., 2020)</ref>. As suggested in <ref type="bibr" target="#b33">(Rong et al., 2020)</ref>, we equip DropEdge on three backbones: GCN <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref>, JKNet <ref type="bibr" target="#b43">(Xu et al., 2018)</ref> and IncepGCN <ref type="bibr" target="#b33">(Rong et al., 2020)</ref>. We also include three state-of-the-art shallow models: GCN <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref> and APPNP <ref type="bibr" target="#b17">(Klicpera et al., 2019a)</ref>.</p><p>We use the Adam SGD optimizer <ref type="bibr" target="#b15">(Kingma &amp; Ba, 2015)</ref> with a learning rate of 0.01 and early stopping with a patience of 100 epochs to train GCNII and GCNII*. We set α = 0.1 and L 2 regularization to 0.0005 for the dense layer on all datasets. We perform a grid search to tune the other hyper-parameters for models with different depths based on the accuracy on the validation set. More details of hyperparameters are listed in the supplementary materials.</p><p>Comparison with SOTA. <ref type="table" target="#tab_2">Table 2</ref> reports the mean classification accuracy with the standard deviation on the test nodes of GCN and GCNII after 100 runs. We reuse the metrics already reported in <ref type="bibr" target="#b7">(Fey &amp; Lenssen, 2019)</ref> for GCN, GAT, and APPNP, and the best metrics reported in <ref type="bibr" target="#b33">(Rong et al., 2020)</ref> for JKNet, JKNet(Drop) and Incep(Drop). Our results successfully demonstrate that GCNII and GCNII* achieves new state-of-the-art performance across all three datasets. Notably, GCNII outperforms the previous stateof-the-art methods by at least 2%. It is also worthwhile to note that the two recent deep models, JKNet and IncepGCN with DropEdge, do not seem to offer significant advantages over the shallow model APPNP. On the other hand, our A detailed comparison with other deep models. <ref type="table" target="#tab_3">Table 3</ref> summaries the results for the deep models with various numbers of layers. We reuse the best-reported results for JKNet, JKNet(Drop) and Incep(Drop) 1 . We observe that on Cora and Citeseer, the performance of GCNII and GCNII* consistently improves as we increase the number of layers. On Pubmed, GCNII and GCNII* achieve the best results with 16 layers, and maintain similar performance as we increase the network depth to 64. We attribute this quality to the identity mapping technique. Overall, the results suggest that with initial residual and identity mapping, we can resolve the over-smoothing problem and extend the vanilla GCN into a truly deep model. On the other hand, the performance of GCN with DropEdge and JKNet drops rapidly as the number of layers exceeds 32, which means they still suffer from over-smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Full-Supervised Node Classification</head><p>We now evaluate GCNII in the task of full-supervised node classification. Following the setting in <ref type="bibr" target="#b31">(Pei et al., 2020)</ref>, we use 7 datasets: Cora, Citeseer, Pubmed, Chameleon, 1 https://github.com/DropEdge/DropEdge </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method PPI</head><p>GraphSAGE <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref> 61.2 VR-GCN <ref type="bibr" target="#b2">(Chen et al., 2018b)</ref> 97.8 GaAN  98.71 GAT <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref> 97.3 JKNet <ref type="bibr" target="#b43">(Xu et al., 2018)</ref> 97.6 GeniePath  98.5 Cluster-GCN <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref> 99.36 GCNII 99.53 ± 0.01 GCNII* 99.56 ± 0.02</p><p>Cornell, Texas, and Wisconsin. For each datasets, we randomly split nodes of each class into 60%, 20%, and 20% for training, validation and testing, and measure the performance of all models on the test sets over 10 random splits, as suggested in <ref type="bibr" target="#b31">(Pei et al., 2020)</ref>. We fix the learning rate to 0.01, dropout rate to 0.5 and the number of hidden units to 64 on all datasets and perform a hyper-parameter search to tune other hyper-parameters based on the validation set.</p><p>Detailed configuration of all model for full-supervised node classification can be found in the supplementary materials.</p><p>Besides the previously mentioned baselines, we also include three variants of Geom-GCN <ref type="bibr" target="#b31">(Pei et al., 2020)</ref> as they are the state-of-the-art models on these datasets. <ref type="table" target="#tab_5">Table 5</ref> reports the mean classification accuracy of each model. We reuse the metrics already reported in <ref type="bibr" target="#b31">(Pei et al., 2020)</ref> for GCN, GAT, and Geom-GCN. We observe that GCNII and GCNII* achieves new state-of-the-art results on 6 out of 7 datasets, which demonstrates the superiority of the deep GCNII framework. Notably, GCNII* outperforms APPNP by over 12% on the Wisconsin dataset. This result suggests that by introducing non-linearity into each layer, the predictive power of GCNII is stronger than that of the linear model APPNP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Inductive Learning</head><p>For the inductive learning task, we apply 9-layer GCNII and GCNII* models with 2048 hidden units on the PPI dataset. We fix the following sets of hyperparameters: α = 0.5, λ = 1.0 and learning rate of 0.001. Due to the large volume of training data, we set the dropout rate to 0.2 and the weight decay to zero. Following <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref>, we also add a skip connection from the -th layer to the ( + 1)-th layer of GCNII and GCNII* to speed up the convergence of the training process. We compare GCNII with the following state-of-the-art methods: GraphSAGE <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref>, VR-GCN <ref type="bibr" target="#b2">(Chen et al., 2018b)</ref>, GaAN , GAT <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref>, JKNet (Xu et al.,  <ref type="formula" target="#formula_1">(16)</ref> 2018), GeniePath , Cluster-GCN <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref>. The metrics are summarized in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>In concordance with our expectations, the results show that GCNII and GCNII* achieve new state-of-the-art performance on PPI. In particular, GCNII achieves this performance with a 9-layer model, while the number of layers with all baseline models are less or equal to 5. This suggests that larger predictive power can also be leveraged by increasing the network depth in the task of inductive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Over-Smoothing Analysis for GCN</head><p>Recall that Conjecture 1 suggests that nodes with higher degrees are more likely to suffer from over-smoothing. To verify this conjecture, we study how the classification accuracy varies with node degree in the semi-supervised node classification task on Cora, Citeseer, and Pubmed. More specifically, we group the nodes of each graph according to their degrees. The i-th group consists of nodes with degrees in the range [2 i , 2 i+1 ) for i = 0, . . . , ∞. For each group, we report the average classification accuracy of GCN with residual connection with various network depths in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We have the following observations. First of all, we note that the accuracy of the 2-layer GCN model increases with the node degree. This is as expected, as nodes with higher degrees generally gain more information from their neighbors. However, as we extend the network depth, the accuracy of high-degree nodes drops more rapidly than that of lowdegree nodes. Notably, GCN with 64 layers is unable to classify nodes with degrees larger than 100. This suggests that over-smoothing indeed has a greater impact on nodes with higher degrees. <ref type="figure">Figure 2</ref> shows the results of an ablation study that evaluates the contributions of our two techniques: initial residual con-nection and identity mapping. We make three observations from <ref type="figure">Figure 2</ref>: 1) Directly applying identity mapping to the vanilla GCN retards the effect of over-smoothing marginally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Ablation Study</head><p>2) Directly applying initial residual connection to the vanilla GCN relieves over-smoothing significantly. However, the best performance is still achieved by the 2-layer model. 3) Applying identity mapping and initial residual connection simultaneously ensures that the accuracy increases with the network depths. This result suggests that both techniques are needed to solve the problem of over-smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose GCNII, a simple and deep GCN model that prevents over-smoothing by initial residual connection and identity mapping. The theoretical analysis shows that GC-NII is able to express a K order polynomial filter with arbitrary coefficients. For vanilla GCN with multiple layers, we provide theoretical and empirical evidence that nodes with higher degrees are more likely to suffer from oversmoothing. Experiments show that the deep GCNII model achieves new state-of-the-art results on various semi-and full-supervised tasks. Interesting directions for future work include combining GCNII with the attention mechanism and analyzing the behavior of GCNII with the ReLU operation. Proof. For simplicity, we assume the signal vector x to be non-negative. Note that we can convert x into a nonnegative input layer H (0) by a linear transformation. We consider a weaker version of GCNII by fixing α = 0.5 and fixing the weight matrix (1 − β )I n + β W ( ) to be γ I n , where γ is a learnable parameter. We have H (l+1) = σ D −1/2ÃD−1/2 H ( ) + x γ I n .</p><p>Since the input feature x is non-negative, we can remove the ReLU operation:</p><formula xml:id="formula_15">H ( +1) = γ D −1/2ÃD−1/2 H ( ) + x = γ I n −L · H ( ) + x .</formula><p>Consequently, we can express the final representation as</p><formula xml:id="formula_16">H (K−1) = K−1 =0 K−1 k=K− −1 γ k I n −L x. (8)</formula><p>On the other hand, a polynomial filter of graphG can be expressed as Switching the order of summation follows that a K-order polynomial fiter K−1 k=0 θ kL k x can be expressed as</p><formula xml:id="formula_17">K−1 k=0 θ kL k x= K−1 =0 K−1 k= θ k (−1) k I n −L x.<label>(9)</label></formula><p>To show that GCNII can express an arbitrary K-order polynomial filter, we need to prove that there exists a solution γ , = 0, . . . , K − 1 such that the corresponding coefficients of I n −L in equations (8) and (9) are equivalent. More precisely, we need to show the following equation system has a solution γ , = 0, . . . , K − 1. Since the left-hand side is a partial product of γ k from K − − 1 to K − 1, we can solve the equation system by</p><formula xml:id="formula_18">γ K− −1 = K−1 k= θ k (−1) k K−1 k= −1 θ k (−1) −1 k − 1 ,<label>(10)</label></formula><p>for = 1, . . . , K − 1 and γ K−1 = K−1 k=0 θ k . Note that the above solution may fail when K−1 k= −1 θ k (−1) −1 k −1 = 0. In this case, we can set γ K− −1 sufficiently large so that equation <ref type="formula" target="#formula_1">(10)</ref> is still a good approximation. We also note that this case is rare because it implies that the K-order filter ignores all features from the -hop neighbors. This proves that a K-layer GCNII can express the K-th order polynomial filter k i=0 θ i L i x with arbitrary coefficients θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof of Theorem 1</head><p>To prove Theorem 1, we need the following Cheeger Inequality <ref type="bibr" target="#b4">(Chung, 2007)</ref> for lazy random walks.</p><p>Lemma 1 ( <ref type="bibr" target="#b4">(Chung, 2007)</ref>). Let p (K) i = In+ÃD −1 2 K e i is the K-th transition probability vector from node i on connected self-looped graphG. Let λG denote the spectral gap ofG. The j-th entry of p (K) i can be bounded by</p><formula xml:id="formula_19">p (K) i (j) − d j + 1 2m + n ≤ d j + 1 d i + 1 1 − λ 2 G 2 K .</formula><p>Proof of Theorem 1. Note that I n =D −1/2D1/2 , we have h (K) = I n +D −1/2ÃD−1/2 2 K · x = D −1/2 I n +ÃD −1 2 D 1/2 K · x =D −1/2 I n +ÃD −1 2 K · D 1/2 x .</p><p>We expressD 1/2 x as linear combination of standard basis:</p><formula xml:id="formula_20">D 1/2 x = (D + I n ) 1/2 x = n i=1 x(i) d i + 1 · e i ,</formula><p>it follows that</p><formula xml:id="formula_21">h (K) =D −1/2 I n +ÃD −1 2 K · n i=1 x(i) d i + 1 · e i = n i=1</formula><p>x(i) d i + 1 ·D −1/2 I n +ÃD −1 2 K · e i .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Semi-supervised node classification accuracy v.s. degree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>θ k (−1) k , k = 0, . . . , K − 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>School of Information, Renmin University of China 2 Gaoling School of Articial Intelligence, Renmin University of China 3 Beijing Key Lab of Big Data Management and Analysis Methods 4 MOE Key Lab of Data Engineering and Knowledge Engineering 5 School of Data Science, Fudan University 6 Alibaba Group. Correspondence to: Zhewei Wei &lt;zhewei@ruc.edu.cn&gt;. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Classes Nodes</cell><cell cols="2">Edges Features</cell></row><row><cell>Cora</cell><cell>7</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>6</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell></row><row><cell>Pubmed</cell><cell cols="2">3 19,717</cell><cell>44,338</cell><cell>500</cell></row><row><cell>Chameleon</cell><cell>4</cell><cell>2,277</cell><cell>36,101</cell><cell>2,325</cell></row><row><cell>Cornell</cell><cell>5</cell><cell>183</cell><cell>295</cell><cell>1,703</cell></row><row><cell>Texas</cell><cell>5</cell><cell>183</cell><cell>309</cell><cell>1,703</cell></row><row><cell>Wisconsin</cell><cell>5</cell><cell>251</cell><cell>499</cell><cell>1,703</cell></row><row><cell>PPI</cell><cell cols="3">121 56,944 818,716</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Summary of classification accuracy (%) results on Cora, Citeseer, and Pubmed. The number in parentheses corresponds to the number of layers of the model.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>GCN</cell><cell>81.5</cell><cell>71.1</cell><cell>79.0</cell></row><row><cell>GAT</cell><cell>83.1</cell><cell>70.8</cell><cell>78.5</cell></row><row><cell>APPNP</cell><cell>83.3</cell><cell>71.8</cell><cell>80.1</cell></row><row><cell>JKNet</cell><cell>81.1 (4)</cell><cell>69.8 (16)</cell><cell>78.1 (32)</cell></row><row><cell cols="2">JKNet(Drop) 83.3 (4)</cell><cell>72.6 (16)</cell><cell>79.2 (32)</cell></row><row><cell cols="2">Incep(Drop) 83.5 (64)</cell><cell>72.7 (4)</cell><cell>79.5 (4)</cell></row><row><cell>GCNII</cell><cell cols="3">85.5 ± 0.5 (64) 73.4 ± 0.6 (32) 80.2 ± 0.4 (16)</cell></row><row><cell>GCNII*</cell><cell cols="3">85.3 ± 0.2 (64) 73.2 ± 0.8 (32) 80.3 ± 0.4 (16)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Summary of classification accuracy (%) results with various depths.</figDesc><table><row><cell cols="2">Dataset Method</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>Layers 16</cell><cell>32</cell><cell>64</cell></row><row><cell></cell><cell>GCN</cell><cell cols="6">81.1 80.4 69.5 64.9 60.3 28.7</cell></row><row><cell></cell><cell cols="7">GCN(Drop) 82.8 82.0 75.8 75.7 62.5 49.5</cell></row><row><cell></cell><cell>JKNet</cell><cell cols="6">-80.2 80.7 80.2 81.1 71.5</cell></row><row><cell>Cora</cell><cell cols="7">JKNet(Drop) -83.3 82.6 83.0 82.5 83.2</cell></row><row><cell></cell><cell>Incep</cell><cell cols="6">-77.6 76.5 81.7 81.7 80.0</cell></row><row><cell></cell><cell>Incep(Drop)</cell><cell cols="6">-82.9 82.5 83.1 83.1 83.5</cell></row><row><cell></cell><cell>GCNII</cell><cell cols="6">82.2 82.6 84.2 84.6 85.4 85.5</cell></row><row><cell></cell><cell>GCNII*</cell><cell cols="6">80.2 82.3 82.8 83.5 84.9 85.3</cell></row><row><cell></cell><cell>GCN</cell><cell cols="6">70.8 67.6 30.2 18.3 25.0 20.0</cell></row><row><cell></cell><cell cols="7">GCN(Drop) 72.3 70.6 61.4 57.2 41.6 34.4</cell></row><row><cell></cell><cell>JKNet</cell><cell cols="6">-68.7 67.7 69.8 68.2 63.4</cell></row><row><cell>Citeseer</cell><cell cols="7">JKNet(Drop) -72.6 71.8 72.6 70.8 72.2</cell></row><row><cell></cell><cell>Incep</cell><cell cols="6">-69.3 68.4 70.2 68.0 67.5</cell></row><row><cell></cell><cell>Incep(Drop)</cell><cell cols="6">-72.7 71.4 72.5 72.6 71.0</cell></row><row><cell></cell><cell>GCNII</cell><cell cols="6">68.2 68.9 70.6 72.9 73.4 73.4</cell></row><row><cell></cell><cell>GCNII*</cell><cell cols="6">66.1 67.9 70.6 72.0 73.2 73.1</cell></row><row><cell></cell><cell>GCN</cell><cell cols="6">79.0 76.5 61.2 40.9 22.4 35.3</cell></row><row><cell></cell><cell cols="7">GCN(Drop) 79.6 79.4 78.1 78.5 77.0 61.5</cell></row><row><cell></cell><cell>JKNet</cell><cell cols="6">-78.0 78.1 72.6 72.4 74.5</cell></row><row><cell>Pubmed</cell><cell cols="7">JKNet(Drop) -78.7 78.7 79.1 79.2 78.9</cell></row><row><cell></cell><cell>Incep</cell><cell cols="6">-77.7 77.9 74.9 OOM OOM</cell></row><row><cell></cell><cell>Incep(Drop)</cell><cell cols="6">-79.5 78.6 79.0 OOM OOM</cell></row><row><cell></cell><cell>GCNII</cell><cell cols="6">78.2 78.8 79.3 80.2 79.8 79.7</cell></row><row><cell></cell><cell>GCNII*</cell><cell cols="6">77.7 78.2 78.8 80.3 79.8 80.1</cell></row><row><cell cols="8">method achieves this result with a 64-layer model, which</cell></row><row><cell cols="7">demonstrates the benefit of deep network structures.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Summary of Micro-averaged F1 scores on PPI.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Mean classification accuracy of full-supervised node classification.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Cite.</cell><cell>Pumb.</cell><cell>Cham.</cell><cell>Corn.</cell><cell>Texa.</cell><cell>Wisc.</cell></row><row><cell>GCN</cell><cell>85.77</cell><cell>73.68</cell><cell>88.13</cell><cell>28.18</cell><cell>52.70</cell><cell>52.16</cell><cell>45.88</cell></row><row><cell>GAT</cell><cell>86.37</cell><cell>74.32</cell><cell>87.62</cell><cell>42.93</cell><cell>54.32</cell><cell>58.38</cell><cell>49.41</cell></row><row><cell cols="2">Geom-GCN-I 85.19</cell><cell>77.99</cell><cell>90.05</cell><cell>60.31</cell><cell>56.76</cell><cell>57.58</cell><cell>58.24</cell></row><row><cell cols="2">Geom-GCN-P 84.93</cell><cell>75.14</cell><cell>88.09</cell><cell>60.90</cell><cell>60.81</cell><cell>67.57</cell><cell>64.12</cell></row><row><cell cols="2">Geom-GCN-S 85.27</cell><cell>74.71</cell><cell>84.75</cell><cell>59.96</cell><cell>55.68</cell><cell>59.73</cell><cell>56.67</cell></row><row><cell>APPNP</cell><cell>87.87</cell><cell>76.53</cell><cell>89.40</cell><cell>54.3</cell><cell>73.51</cell><cell>65.41</cell><cell>69.02</cell></row><row><cell>JKNet</cell><cell cols="2">85.25 (16) 75.85 (8)</cell><cell cols="3">88.94 (64) 60.07 (32) 57.30 (4)</cell><cell cols="2">56.49 (32) 48.82 (8)</cell></row><row><cell>JKNet(Drop)</cell><cell cols="2">87.46 (16) 75.96 (8)</cell><cell cols="3">89.45 (64) 62.08 (32) 61.08 (4)</cell><cell cols="2">57.30 (32) 50.59 (8)</cell></row><row><cell>Incep(Drop)</cell><cell>86.86 (8)</cell><cell>76.83 (8)</cell><cell>89.18 (4)</cell><cell>61.71 (8)</cell><cell cols="2">61.62 (16) 57.84 (8)</cell><cell>50.20 (8)</cell></row><row><cell>GCNII</cell><cell cols="4">88.49 (64) 77.08 (64) 89.57 (64) 60.61 (8)</cell><cell cols="3">74.86 (16) 69.46 (32) 74.12 (16)</cell></row><row><cell>GCNII*</cell><cell cols="4">88.01 (64) 77.13 (64) 90.30 (64) 62.48 (8)</cell><cell cols="3">76.49 (16) 77.84 (32) 81.57</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by National Natural Science Foundation of China (No. 61832017, No. 61932001  and No. 61972401 </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We note that In+ÃD −1 2 K · e i = p (K) i is the K-th transition probability vector of a random walk from node i. By Lemma 1, the j-th entry of p (K) i can be bounded by</p><p>or equivalently,</p><p>Therefore, we can express the j-th entry of h (K) as</p><p>This proves</p><p>and the Theorem follows. <ref type="table">Table 6</ref> summarizes the training configuration of GCNII for semi-supervised. L 2 d and L 2c denote the weight decay for dense layer and convolutional layer respectively. The searching hyper-parameters include numbers of layers, hidden dimension, dropout, λ and L 2c regularization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyper-parameters details</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixhop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Four proofs for the cheeger inequality and graph partition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCM</title>
		<meeting>ICCM</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">378</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neuralbrane: Neural bayesian personalized ranking for attributed network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="131" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention based spatial-temporal graph convolutional networks for traffic flow forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity matters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoding social information with graph convolutional networks forpolitical perspective detection in news media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Optimization algorithm inspired deep neural network structure design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01638</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Predicting path failure in time-evolving graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<editor>KDD. ACM</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geniepath</surname></persName>
		</author>
		<title level="m">Graph neural networks with adaptive receptive paths. In AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MMM: multi-source multi-net micro-video recommendation with clustered hidden item representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="240" to="253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural networks analyzed via convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2887" to="2938" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropedge</surname></persName>
		</author>
		<title level="m">Towards deep graph convolutional networks on node classification. In ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph augmented memory networks for recommending medication combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gamenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph Attention Networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improving graph attention networks with large margin-based constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11945</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<title level="m">Graph wavelet neural network. In ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ista-net: Interpretable optimization-inspired deep network for image compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1828" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In UAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
