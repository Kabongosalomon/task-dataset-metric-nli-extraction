<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coupled Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
							<email>mliu@merl.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Labs (MERL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Labs (MERL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Coupled Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The paper concerns the problem of learning a joint distribution of multi-domain images from data. A joint distribution of multi-domain images is a probability density function that gives a density value to each joint occurrence of images in different domains such as images of the same scene in different modalities (color and depth images) or images of the same face with different attributes (smiling and non-smiling). Once a joint distribution of multi-domain images is learned, it can be used to generate novel tuples of images. In addition to movie and game production, joint image distribution learning finds applications in image transformation and domain adaptation. When training data are given as tuples of corresponding images in different domains, several existing approaches <ref type="bibr">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> can be applied. However, building a dataset with tuples of corresponding images is often a challenging task. This correspondence dependency greatly limits the applicability of the existing approaches.</p><p>To overcome the limitation, we propose the coupled generative adversarial networks (CoGAN) framework. It can learn a joint distribution of multi-domain images without existence of corresponding images in different domains in the training set. Only a set of images drawn separately from the marginal distributions of the individual domains is required. CoGAN is based on the generative adversarial networks (GAN) framework <ref type="bibr" target="#b4">[5]</ref>, which has been established as a viable solution for image distribution learning tasks. CoGAN extends GAN for joint image distribution learning tasks.</p><p>CoGAN consists of a tuple of GANs, each for one image domain. When trained naively, the CoGAN learns a product of marginal distributions rather than a joint distribution. We show that by enforcing a weight-sharing constraint the CoGAN can learn a joint distribution without existence of corresponding images in different domains. The CoGAN framework is inspired by the idea that deep neural networks learn a hierarchical feature representation. By enforcing the layers that decode high-level semantics in the GANs to share the weights, it forces the GANs to decode the high-level semantics in the same way. The layers that decode low-level details then map the shared representation to images in individual domains for confusing the respective discriminative models. CoGAN is for multi-image domains but, for ease of presentation, we focused on the case of two image domains in the paper. However, the discussions and analyses can be easily generalized to multiple image domains.</p><p>We apply CoGAN to several joint image distribution learning tasks. Through convincing visualization results and quantitative evaluations, we verify its effectiveness. We also show its applications to unsupervised domain adaptation and image transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generative Adversarial Networks</head><p>A GAN consists of a generative model and a discriminative model. The objective of the generative model is to synthesize images resembling real images, while the objective of the discriminative model is to distinguish real images from synthesized ones. Both the generative and discriminative models are realized as multilayer perceptrons.</p><p>Let x be a natural image drawn from a distribution, p X , and z be a random vector in R d . Note that we only consider that z is from a uniform distribution with a support of [−1 1] d , but different distributions such as a multivariate normal distribution can be applied as well. Let g and f be the generative and discriminative models, respectively. The generative model takes z as input and outputs an image, g(z), that has the same support as x. Denote the distribution of g(z) as p G . The discriminative model estimates the probability that an input image is drawn from p X . Ideally,</p><formula xml:id="formula_0">f (x) = 1 if x ∼ p X and f (x) = 0 if x ∼ p G .</formula><p>The GAN framework corresponds to a minimax two-player game, and the generative and discriminative models can be trained jointly via solving</p><formula xml:id="formula_1">max g min f V (f, g) ≡ E x∼p X [− log f (x)] + E z∼p Z [− log(1 − f (g(z)))].<label>(1)</label></formula><p>In practice <ref type="formula" target="#formula_1">(1)</ref> is solved by alternating the following two gradient update steps:</p><p>Step</p><formula xml:id="formula_2">1: θ t+1 f = θ t f − λ t ∇ θ f V (f t , g t ),</formula><p>Step 2: θ t+1</p><formula xml:id="formula_3">g = θ t g + λ t ∇ θg V (f t+1 , g t )</formula><p>where θ f and θ g are the parameters of f and g, λ is the learning rate, and t is the iteration number.</p><p>Goodfellow et al. <ref type="bibr" target="#b4">[5]</ref> show that, given enough capacity to f and g and sufficient training iterations, the distribution, p G , converges to p X . In other words, from a random vector, z, the network g can synthesize an image, g(z), that resembles one that is drawn from the true distribution, p X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Coupled Generative Adversarial Networks</head><p>CoGAN as illustrated in <ref type="figure">Figure 1</ref> is designed for learning a joint distribution of images in two different domains. It consists of a pair of GANs-GAN 1 and GAN 2 ; each is responsible for synthesizing images in one domain. During training, we force them to share a subset of parameters. This results in that the GANs learn to synthesize pairs of corresponding images without correspondence supervision.</p><p>Generative Models: Let x 1 and x 2 be images drawn from the marginal distribution of the 1st domain, x 1 ∼ p X1 and the marginal distribution of the 2nd domain, x 2 ∼ p X2 , respectively. Let g 1 and g 2 be the generative models of GAN 1 and GAN 2 , which map a random vector input z to images that have the same support as x 1 and x 2 , respectively. Denote the distributions of g 1 (z) and g 1 (z) by p G1 and p G2 . Both g 1 and g 2 are realized as multilayer perceptrons:</p><formula xml:id="formula_4">g 1 (z) = g (m1) 1 g (m1−1) 1 . . . g (2) 1 g (1) 1 (z) , g 2 (z) = g (m2) 2 g (m2−1) 2 . . . g (2) 2 g (1) 2 (z) where g (i) 1 and g (i)</formula><p>2 are the ith layers of g 1 and g 2 and m 1 and m 2 are the numbers of layers in g 1 and g 2 . Note that m 1 need not equal m 2 . Also note that the support of x 1 need not equal to that of x 2 .</p><p>Through layers of perceptron operations, the generative models gradually decode information from more abstract concepts to more material details. The first layers decode high-level semantics and the last layers decode low-level details. Note that this information flow direction is opposite to that in a discriminative deep neural network <ref type="bibr" target="#b5">[6]</ref> where the first layers extract low-level features while the last layers extract high-level features.</p><p>Based on the idea that a pair of corresponding images in two domains share the same high-level concepts, we force the first layers of g 1 and g 2 to have identical structure and share the weights. That is θ g (i)</p><formula xml:id="formula_5">1 = θ g (i) 2</formula><p>, for i = 1, 2, ..., k where k is the number of shared layers, and θ g (i) 1 and θ g (i) 2 are the parameters of g (i) 1 and g (i) 2 , respectively. This constraint forces the high-level semantics to be decoded in the same way in g 1 and g 2 . No constraints are enforced to the last layers. They can materialize the shared high-level representation differently for fooling the respective discriminators.  <ref type="figure">Figure 1</ref>: CoGAN consists of a pair of GANs: GAN1 and GAN2. Each has a generative model for synthesizing realistic images in one domain and a discriminative model for classifying whether an image is real or synthesized. We tie the weights of the first few layers (responsible for decoding high-level semantics) of the generative models, g1 and g2. We also tie the weights of the last few layers (responsible for encoding high-level semantics) of the discriminative models, f1 and f2. This weight-sharing constraint allows CoGAN to learn a joint distribution of images without correspondence supervision. A trained CoGAN can be used to synthesize pairs of corresponding images-pairs of images sharing the same high-level abstraction but having different low-level realizations.</p><p>Discriminative Models: Let f 1 and f 2 be the discriminative models of GAN 1 and GAN 2 given by  We found that the performance was positively correlated with the number of weight-sharing layers in the generative models but was uncorrelated to the number of weight-sharing layers in the discriminative models. CoGAN learned the joint distribution without weight-sharing layers in the discriminative models.</p><formula xml:id="formula_6">f 1 (x 1 ) = f (n1) 1 f (n1−1) 1</formula><p>individually resembling to the images in the respective domains. With this more relaxed adversarial training setting, the weight-sharing constraint can then kick in for capturing correspondences between domains. With the weight-sharing constraint, the generative models must utilize the capacity more efficiently for fooling the discriminative models, and the most efficient way of utilizing the capacity for generating a pair of realistic images in two domains is to generate a pair of corresponding images since the neurons responsible for decoding high-level semantics can be shared.</p><p>CoGAN learning is based on existence of shared high-level representations in the domains. If such a representation does not exist for the set of domains of interest, it would fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the experiments, we emphasized there were no corresponding images in the different domains in the training sets. CoGAN learned the joint distributions without correspondence supervision. We were unaware of existing approaches with the same capability and hence did not compare CoGAN with prior works. Instead, we compared it to a conditional GAN to demonstrate its advantage. Recognizing that popular performance metrics for evaluating generative models all subject to issues <ref type="bibr" target="#b6">[7]</ref>, we adopted a pair image generation performance metric for comparison. Many details including the network architectures and additional experiment results are given in the supplementary materials. An implementation of CoGAN is available in https://github.com/mingyuliutw/cogan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Digits:</head><p>We used the MNIST training set to train CoGANs for the following two tasks. Task A is about learning a joint distribution of a digit and its edge image. Task B is about learning a joint distribution of a digit and its negative image. In Task A, the 1st domain consisted of the original handwritten digit images, while the 2nd domain consisted of their edge images. We used an edge detector to compute training edge images for the 2nd domain. In the supplementary materials, we also showed an experiment for learning a joint distribution of a digit and its 90-degree in-plane rotation.</p><p>We used deep convolutional networks to realized the CoGAN. The two generative models had an identical structure; both had 5 layers and were fully convolutional. The stride lengths of the convolutional layers were fractional. The models also employed the batch normalization processing <ref type="bibr" target="#b7">[8]</ref> and the parameterized rectified linear unit processing <ref type="bibr" target="#b8">[9]</ref>. We shared the parameters for all the layers except for the last convolutional layers. For the discriminative models, we used a variant of LeNet <ref type="bibr" target="#b9">[10]</ref>.</p><p>The inputs to the discriminative models were batches containing output images from the generative models and images from the two training subsets (each pixel value is linearly scaled to [0 1]).</p><p>We divided the training set into two equal-size non-overlapping subsets. One was used to train GAN 1 and the other was used to train GAN 2 . We used the ADAM algorithm <ref type="bibr" target="#b10">[11]</ref> for training and set the learning rate to 0.0002, the 1st momentum parameter to 0.5, and the 2nd momentum parameter to 0.999 as suggested in <ref type="bibr" target="#b11">[12]</ref>. The mini-batch size was 128. We trained the CoGAN for 25000 iterations. These hyperparameters were fixed for all the visualization experiments.</p><p>The CoGAN learning results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We found that although the CoGAN was trained without corresponding images, it learned to render corresponding ones for both Task A and B. This was due to the weight-sharing constraint imposed to the layers that were responsible for decoding high-level semantics. Exploiting the correspondence between the two domains allowed GAN 1 and GAN 2 to utilize more capacity in the networks to better fit the training data. Without the weight-sharing constraint, the two GANs just generated two unrelated images in the two domains.</p><p>Weight Sharing: We varied the numbers of weight-sharing layers in the generative and discriminative models to create different CoGANs for analyzing the weight-sharing effect for both tasks. Due to lack of proper validation methods, we did a grid search on the training iteration hyperparameter and reported the best performance achieved by each network. For quantifying the performance, we transformed the image generated by GAN 1 to the 2nd domain using the same method employed for generating the training images in the 2nd domain. We then compared the transformed image with the image generated by GAN 2 . A perfect joint distribution learning should render two identical images. Hence, we used the ratios of agreed pixels between 10K pairs of images generated by each network (10K randomly sampled z) as the performance metric. We trained each network 5 times with different initialization weights and reported the average pixel agreement ratios over the 5 trials for each network. The results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We observed that the performance was positively correlated with the number of weight-sharing layers in the generative models. With more sharing layers in the generative models, the rendered pairs of images resembled true pairs drawn from the joint distribution more. We also noted that the performance was uncorrelated to the number of weight-sharing layers in the discriminative models. However, we still preferred discriminator weight-sharing because this reduces the total number of network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Conditional GANs:</head><p>We compared the CoGAN with the conditional GANs <ref type="bibr" target="#b12">[13]</ref>.</p><p>We designed a conditional GAN with the generative and discriminative models identical to those in the CoGAN. The only difference was the conditional GAN took an additional binary variable as input, which controlled the domain of the output image. When the binary variable was 0, it generated an image resembling images in the 1st domain; otherwise, it generated an image resembling images in the 2nd domain. Similarly, no pairs of corresponding images were given during the conditional GAN training. We applied the conditional GAN to both Task A and B and hoped to empirically answer whether a conditional model can be used to learn to render corresponding images with correspondence supervision. The pixel agreement ratio was used as the performance metric. The experiment results showed that for Task A, CoGAN achieved an average ratio of 0.952, outperforming 0.909 achieved by the conditional GAN. For Task B, CoGAN achieved a score of 0.967, which was much better than 0.778 achieved by the conditional GAN. The conditional GAN just generated two different digits with the same random noise input but different binary variable values. These results showed that the conditional model failed to learn a joint distribution from samples drawn from the marginal distributions. We note that for the case that the supports of the two domains are different such as the color and depth image domains, the conditional model cannot even be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Faces:</head><p>We applied CoGAN to learn a joint distribution of face images with different. We trained several CoGANs, each for generating a face with an attribute and a corresponding face without the attribute. We used the CelebFaces Attributes dataset <ref type="bibr" target="#b13">[14]</ref> for the experiments. The dataset covered large pose variations and background clutters. Each face image had several attributes, including blond hair, smiling, and eyeglasses. The face images with an attribute constituted the 1st domain; and those without the attribute constituted the 2nd domain. No corresponding face images between the two domains was given. We resized the images to a resolution of 132 × 132 and randomly sampled 128 × 128 regions for training. The generative and discriminative models were both 7 layer deep convolutional neural networks.</p><p>The experiment results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. We randomly sampled two points in the 100dimensional input noise space and visualized the rendered face images as traveling from one pint to the other. We found CoGAN generated pairs of corresponding faces, resembling those from the same person with and without an attribute. As traveling in the space, the faces gradually change from one person to another. Such deformations were consistent for both domains. Note that it is difficult to create a dataset with corresponding images for some attribute such as blond hair since the subjects have to color their hair. It is more ideal to have an approach that does not require corresponding images like CoGAN. We also noted that the number of faces with an attribute was often several times smaller than that without the attribute in the dataset. However, CoGAN learning was not hindered by the mismatches.</p><p>Color and Depth Images: We used the RGBD dataset <ref type="bibr" target="#b14">[15]</ref> and the NYU dataset <ref type="bibr" target="#b15">[16]</ref> for learning joint distribution of color and depth images. The RGBD dataset contains registered color and depth images of 300 objects captured by the Kinect sensor from different view points. We partitioned the dataset into two equal-size non-overlapping subsets. The color images in the 1st subset were used for training GAN 1 , while the depth images in the 2nd subset were used for training GAN 2 . There were no corresponding depth and color images in the two subsets. The images in the RGBD dataset have different resolutions. We resized them to a fixed resolution of 64 × 64. The NYU dataset contains color and depth images captured from indoor scenes using the Kinect sensor. We used the 1449 processed depth images for the depth domain. The training images for the color domain were from all the color images in the raw dataset except for those registered with the processed depth images. We resized both the depth and color images to a resolution of 176 × 132 and randomly cropped 128 × 128 patches for training. <ref type="figure" target="#fig_4">Figure 5</ref> showed the generation results. We found the rendered color and depth images resembled corresponding RGB and depth image pairs despite of no registered images existed in the two domains in the training set. The CoGAN recovered the appearance-depth correspondence unsupervisedly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Applications</head><p>In addition to rendering novel pairs of corresponding images for movie and game production, the CoGAN finds applications in the unsupervised domain adaptation and image transformation tasks.</p><p>Unsupervised Domain Adaptation (UDA): UDA concerns adapting a classifier trained in one domain to classify samples in a new domain where there is no labeled example in the new domain for re-training the classifier. Early works have explored ideas from subspace learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> to deep discriminative network learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. We show that CoGAN can be applied to the UDA problem. We studied the problem of adapting a digit classifier from the MNIST dataset to the USPS dataset. Due to domain shift, a classifier trained using one dataset achieves poor performance in the other. We followed the experiment protocol in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>, which randomly samples 2000 images from the MNIST dataset, denoted as D 1 , and 1800 images from the USPS dataset, denoted as D 2 , to define an UDA problem. The USPS digits have a different resolution. We resized them to have the same resolution as the MNIST digits. We employed the CoGAN used for the digit generation task. For classifying digits, we attached a softmax layer to the last hidden layer of the discriminative models. We trained the CoGAN by jointly solving the digit classification problem in the MNIST domain which used the images and labels in D 1 and the CoGAN learning problem which used the images in both D 1 and D 2 . This produced two classifiers:</p><formula xml:id="formula_7">c 1 (x 1 ) ≡ c(f (3) 1 (f (2) 1 (f<label>(1)</label></formula><p>1 (x 1 )))) for MNIST and c 2 (</p><formula xml:id="formula_8">x 2 ) ≡ c(f (3) 2 (f (2) 2 (f<label>(1)</label></formula><p>2 (x 2 )))) for USPS. No label information in D 2 was used. Note that f  </p><p>2 due to weight sharing and c denotes the softmax layer. We then applied c 2 to classify digits in the USPS dataset. The classifier adaptation from USPS to MNIST can be achieved in the same way. The learning hyperparameters were determined via a validation set. We reported the average accuracy over 5 trails with different randomly selected D 1 and D 2 . <ref type="table">Table 1</ref> reports the performance of the proposed CoGAN approach with comparison to the stateof-the-art methods for the UDA task. The results for the other methods were duplicated from <ref type="bibr" target="#b19">[20]</ref>. We observed that CoGAN significantly outperformed the state-of-the-art methods. It improved the accuracy from 0.64 to 0.90, which translates to a 72% error reduction rate.  probability density, p(x 1 , x 2 ), is maximized. Let L be a loss function measuring difference between two images. Given g 1 and g 2 , the transformation can be achieved by first finding the random vector that generates the query image in the 1st domain z * = arg min z L(g 1 (z), x 1 ). After finding z * , one can apply g 2 to obtain the transformed image, x 2 = g 2 (z * ). In <ref type="figure" target="#fig_7">Figure 6</ref>, we show several CoGAN cross-domain transformation results, computed by using the Euclidean loss function and the L-BFGS optimization algorithm. We found the transformation was successful when the input image was covered by g 1 (The input image can be generated by g 1 .) but generated blurry images when it is not the case. To improve the coverage, we hypothesize that more training images and a better objective function are required, which are left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Neural generative models has recently received an increasing amount of attention. Several approaches, including generative adversarial networks <ref type="bibr" target="#b4">[5]</ref>, variational autoencoders (VAE) <ref type="bibr" target="#b21">[22]</ref>, attention models <ref type="bibr" target="#b22">[23]</ref>, moment matching <ref type="bibr" target="#b23">[24]</ref>, stochastic back-propagation <ref type="bibr" target="#b24">[25]</ref>, and diffusion processes <ref type="bibr" target="#b25">[26]</ref>, have shown that a deep network can learn an image distribution from samples. The learned networks can be used to generate novel images. Our work was built on <ref type="bibr" target="#b4">[5]</ref>. However, we studied a different problem, the problem of learning a joint distribution of multi-domain images. We were interested in whether a joint distribution of images in different domains can be learned from samples drawn separately from its marginal distributions of the individual domains. We showed its achievable via the proposed CoGAN framework. Note that our work is different to the Attribute2Image work <ref type="bibr" target="#b26">[27]</ref>, which is based on a conditional VAE model <ref type="bibr" target="#b27">[28]</ref>. The conditional model can be used to generate images of different styles, but they are unsuitable for generating images in two different domains such as color and depth image domains.</p><p>Following <ref type="bibr" target="#b4">[5]</ref>, several works improved the image generation quality of GAN, including a Laplacian pyramid implementation <ref type="bibr" target="#b28">[29]</ref>, a deeper architecture <ref type="bibr" target="#b11">[12]</ref>, and conditional models <ref type="bibr" target="#b12">[13]</ref>. Our work extended GAN to dealing with joint distributions of images.</p><p>Our work is related to the prior works in multi-modal learning, including joint embedding space learning <ref type="bibr" target="#b29">[30]</ref> and multi-modal Boltzmann machines <ref type="bibr">[1,</ref><ref type="bibr" target="#b2">3]</ref>. These approaches can be used for generating corresponding samples in different domains only when correspondence annotations are given during training. The same limitation is also applied to dictionary learning-based approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. Our work is also related to the prior works in cross-domain image generation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, which studied transforming an image in one style to the corresponding images in another style. However, we focus on learning the joint distribution in an unsupervised fashion, while <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> focus on learning a transformation function directly in a supervised fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented the CoGAN framework for learning a joint distribution of multi-domain images. We showed that via enforcing a simple weight-sharing constraint to the layers that are responsible for decoding abstract semantics, the CoGAN learned the joint distribution of images by just using samples drawn separately from the marginal distributions. In addition to convincing image generation results on faces and RGBD images, we also showed promising results of the CoGAN framework for the image transformation and unsupervised domain adaptation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Rotation</head><p>We applied CoGAN to a task of learning a joint distribution of images with different in-plane rotation angles. We note that this task is very different to the other tasks discussed in the paper. In the other tasks, the image contents in the same spatial region in the corresponding images are in direct correspondence. In this task, the content in one spatial region in one image domain is related to the content in a different spatial region in the other image domain. Through this experiment, we planed to verify whether CoGAN can learn a joint distribution of images related by a global transformation.</p><p>For this task, we partitioned the MNIST training set into two disjoint subsets. The first set consisted of the original digit images, which constitute the first domain. We applied a 90 degree rotation to all the digits in the second set to construct the second domain. There were no corresponding images in the two domains. The CoGAN architecture used for this task is shown in <ref type="table" target="#tab_1">Table 2</ref>. Different to the other tasks, the generative models in the CoGAN were based on fully connected layers, and the discriminative models only share the last layer. This design was due to lack of spatial correspondence between the two domains. We used the same hyperparameters to train the CoGAN. The results are shown in <ref type="figure">Figure 7</ref>. We found that the CoGAN was able to capture the in-plane rotation. For the same noise input, the digit generated by GAN 2 is a 90 degree rotated version of the digit generated by GAN 1 . CONV-(N20,K5x5,S1), POOL-(MAX,2) CONV-(N20,K5x5,S1), POOL-(MAX,2) No 2 CONV-(N50,K5x5,S1), POOL-(MAX,2) CONV-(N50,K5x5,S1), POOL-(MAX,2) No 3</p><p>FC-(N500), PReLU FC-(N500), PReLU No 4</p><p>FC-(N1), Sigmoid FC-(N1), Sigmoid Yes <ref type="figure">Figure 7</ref>: Generation of digit and 90-degree rotated digit images. We visualized the CoGAN results by rendering pairs of images, using the vectors that corresponded to paths connecting two pints in the input noise space. For each of the sub-figures, the top row was from GAN1 and the bottom row was from GAN2. Each of the top and bottom pairs was rendered using the same input noise vector. We observed that CoGAN learned to synthesized corresponding digits with different rotation angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Weight Sharing</head><p>We analyzed the effect of weight sharing in the CoGAN framework. We conducted an experiment where we varied the numbers of weight-sharing layers in the generative and discriminative models to create different CoGAN architectures and trained them with the same hyperparameters. Due to lack of proper validation methods, we did a grid search on the training iteration and reported the best performance achieved by each network configuration for both Task A and B 2 . For each network architecture, we run 5 trails with different random network initialization weights. We then rendered 10000 pairs of images for each learned network. A pair of images consisted of an image in the first domain (generated by GAN 1 ) and an image in the second domain (generated by GAN 2 ), which were rendered using the same z.</p><p>For quantifying the performance of each weight-sharing scheme, we transformed the images generated by GAN 1 to the second domain by using the same method employed for generating the training images in the second domain. We then compared the transformed images with the images generated by GAN 2 . The performance was measured by the average of the ratios of agreed pixels between the transformed image and the corresponding image in the other domain. Specifically, we rounded the transformed digit image to a binary image and we also rounded the rendered image in the second domain to a binary image. We then compared the pixel agreement ratio-the number of corresponding pixels that have the same value in the two images divided by the total image size. The performance of a trail was given by the pixel agreement ratio of the 10000 pairs of images. The performance of a network configuration was given by the average pixel agreement ratio over the 5 trails. We reported the performance results for Task A in <ref type="table" target="#tab_2">Table 3</ref> and the performance results for Task B in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>From the tables, we observed that the pair image generation performance was positively correlated with the number of weight-sharing layers in the generative models. With more shared layers in the generative models, the rendered pairs of images were resembling more to true pairs drawn from the joint distribution. We noted that the pair image generation performance was uncorrelated to the number of weight-sharing layers in the discriminative models. However, we still preferred applying discriminator weight sharing because this reduces the total number of parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Comparison with the Conditional Generative Adversarial Nets</head><p>We compared the CoGAN framework with the conditional generative adversarial networks (GAN) framework for joint image distribution learning. We designed a conditional GAN where the generative and discriminative models were identical to those used in the CoGAN in the digit experiments. The only difference was that the conditional GAN took an additional binary variable as input, which controlled the domain of the output image. The binary variable acted as a switch. When the value of the binary variable was zero, it generated images resembling images in the first domain. Otherwise, it generated images resembling those in the second domain. The output layer of the discriminative FCONV-(N1,K6x6,S1), Sigmoid Layer Discriminative models 1 CONV-(N20,K5x5,S1), POOL-(MAX,2) 2 CONV-(N50,K5x5,S1), POOL-(MAX,2) 3 FC-(N500), PReLU 4</p><p>FC-(N3), Softmax   <ref type="figure">figure,</ref> we observed that, although the conditional GAN learned to generate realistic digit images, it failed to learn the correspondence in the two domains. For the edge task, the conditional GAN rendered images of the same digits with a similar font. The edge style was not well-captured. For the negative image generation task, the conditional GAN simply failed to capture any correspondence. The rendered digits with the same input vector but different conditional variable values were not related.</p><p>model was a softmax layer with three neurons. If the first neuron was on, it meant the input to the discriminative model was a synthesized image from the generative model. If the second neuron was on, it meant the input was a real image from the first domain. If the third neuron was on, it meant the input was a real image from the second domain. The goal of the generative model was to render images resembling those from the first domain when the binary variable was zero and to render images resembling those from the second domain when the binary variable was one. The details of the conditional GAN network architecture is shown in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>Similarly to CoGAN learning, no correspondence was given during the conditional GAN learning. We applied the conditional GAN to the two digit generation tasks and hoped to answer whether a conditional model can be used to render corresponding images in two different domains without pairs of corresponding images in the training set. We used the same training data and hyperparameters as those used in the CoGAN learning. We trained the CoGAN for 25000 iterations <ref type="bibr" target="#b2">3</ref> and used the trained network to render 10000 pairs of images in the two domains. Specifically, each pair of images was rendered with the same z but with different conditional variable values. These images were used to compute the pair image generation performance of the conditional GAN measured by the average of the pixel agreement ratios. For each task, we trained the conditional GAN for 5 times, each with a different random initialization of the network weights. We reported the average scores and the standard deviations.</p><p>The performance results are reported in <ref type="table" target="#tab_5">Table 6</ref>. It can be seen that the conditional GAN achieved 0.909 for Task A and 0.778 for Task B, respectively. They were much lower than the scores of 0.952 and 0.967 achieved by the CoGAN. <ref type="figure" target="#fig_8">Figure 8</ref> visualized the conditional GAN's pair generation results, which suggested that the conditional GAN had difficulties in learning to render corresponding images in two different domains without pairs of corresponding images in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CoGAN Learning Algorithm</head><p>We present the learning algorithm for the coupled generative adversarial networks in Algorithm 1. The algorithm is an extension of the learning algorithm for the generative adversarial networks (GAN) to the case of training two GANs with weight sharing constraints. The convergence property follows the results shown in <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_10">1 N N j=1 − log 1 − f t+1 1 g t 1 (z j ) 11:</formula><p>Compute the gradients of the network parameters of the generative model, g 2 , ∆θ g (i)</p><formula xml:id="formula_11">2 ; ∇ θ g (i) 2 1 N N j=1 − log 1 − f t+1 2 g t 2 (z j ) 12:</formula><p>Average the gradients of the shared parameters of the generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Compute g t+1 1 and g t+1 2 according to the gradients. <ref type="bibr">14:</ref> end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Datasets</head><p>In <ref type="figure" target="#fig_9">Figure 9</ref>, <ref type="figure" target="#fig_10">Figure 10</ref>, <ref type="figure" target="#fig_11">Figure 11</ref>, and <ref type="figure" target="#fig_1">Figure 12</ref>, we show several example images of the training images used for the pair image generation tasks in the experiment section. <ref type="table" target="#tab_6">Table 7, Table 8</ref>, <ref type="table" target="#tab_8">Table 9</ref>, and <ref type="table" target="#tab_9">Table 10</ref> contain the statistics of the training datasets for the experiments.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Networks</head><p>In CoGAN, the generative models are based on the fractional length convolutional (FCONV) layers, while the discriminative models are based on the standard convolutional (CONV) layers with the exceptions that the last two layers are based on the fully-connected (FC) layers. The batch normalization (BN) layers <ref type="bibr" target="#b7">[8]</ref> are applied after each convolutional layer, which are followed by the parameterized rectified linear unit (PReLU) processing <ref type="bibr" target="#b8">[9]</ref>. The sigmoid units and the hyperbolic tangent units are applied to the output layers of the generative models for generating images with desired pixel range values. FCONV-(N1,K6x6,S1), Sigmoid FCONV-(N1,K6x6,S1), Sigmoid No Discriminative models Layer Domain 1 Domain 2 Shared? 1 CONV-(N20,K5x5,S1), POOL-(MAX,2) CONV-(N20,K5x5,S1), POOL-(MAX,2) No 2</p><p>CONV-(N50,K5x5,S1), POOL-(MAX,2) CONV-(N50,K5x5,S1), POOL-(MAX,2) Yes 3</p><p>FC-(N500), PReLU FC-(N500), PReLU Yes 4</p><p>FC-(N1), Sigmoid FC-(N1), Sigmoid Yes  FCONV-(N1024,K4x4,S1), BN, PReLU FCONV-(N1024,K4x4,S1), BN, PReLU Yes E Visualization <ref type="figure" target="#fig_2">Figure 13</ref>: Left: generation of digit and corresponding edge images. Right: generation of digit and corresponding negative images. We visualized the CoGAN results by rendering pairs of images, using the vectors that corresponded to paths connecting two pints in the input noise space. For each of the sub-figures, the top row was from GAN1 and the bottom row was from GAN2. Each of the top and bottom pairs was rendered using the same input noise vector. We observed that for both tasks the CoGAN learned to synthesized corresponding images in the two domains. This was interesting because there were no corresponding images in the training datasets. The correspondences were figured out during training in an unsupervised fashion.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left (Task A): generation of digit and corresponding edge images. Right (Task B): generation of digit and corresponding negative images. Each of the top and bottom pairs was generated using the same input noise. We visualized the results by traversing in the input space. # of weight-sharing layers in the discriminative models pair generation of digit and negative images Generative models share 1 layer. Generative models share 2 layers. Generative models share 3 layers. Generative models share 4 layers.# of weight-sharing layers in the discriminative models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The figures plot the average pixel agreement ratios of the CoGANs with different weight-sharing configurations for Task A and B. The larger the pixel agreement ratio the better the pair generation performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Generation of face images with different attributes using CoGAN. From top to bottom, the figure shows pair face generation results for the blond-hair, smiling, and eyeglasses attributes. For each pair, the 1st row contains faces with the attribute, while the 2nd row contains corresponding faces without the attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Generation of color and depth images using CoGAN. The top figure shows the results for the RGBD dataset: the 1st row contains the color images, the 2nd row contains the depth images, and the 3rd and 4th rows visualized the depth profile under different view points. The bottom figure shows the results for the NYU dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Cross-domain image transformation. For each pair, left is the input; right is the transformed image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Digit Generation with Conditional Generative Adversarial Nets. Left: generation of digit and corresponding edge images. Right: generation of digit and corresponding negative images. We visualized the conditional GAN results by rendering pairs of images, using the vectors that corresponded to paths connecting two pints in the input space. For each of the sub-figures, the top row was from the conditional GAN with the conditional variable set to 0, and the bottom row was from the conditional GAN with the conditional variable set to 1. That is each of the top and bottom pairs was rendered using the same input vector except for the conditional variable value. The conditional variable value was used to control the domain of the output images. From the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Training images for the digit experiments. Left (Task A): The images in the first row are from the original MNIST digit domain, while those in the second row are from the edge image domain. Right (Task B): The images in the first row are from the original MNIST digit domain, while those in the second row are from the negative image domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Training images from the Celeba dataset<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Training images from the RGBD dataset<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Training images from the NYU dataset<ref type="bibr" target="#b15">[16]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 23 :</head><label>23</label><figDesc>Generation of RGB and depth images of objects. The 1st row contains the color images. The 2nd row contains the depth images. The 3rd and 4th rows visualized the point clouds under different view points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 24 :</head><label>24</label><figDesc>Generation of RGB and depth images of objects. The 1st row contains the color images. The 2nd row contains the depth images. The 3rd and 4th rows visualized the point clouds under different view points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Image Transformation: Let x 1 be an image in the 1st domain. Cross-domain image transformation is about finding the corresponding image in the 2nd domain, x 2 , such that the joint</figDesc><table><row><cell>Method</cell><cell>[17]</cell><cell>[18]</cell><cell>[19]</cell><cell>[20]</cell><cell>CoGAN</cell></row><row><cell cols="6">From MNIST 0.408 0.467 0.478 0.607 0.912 ±0.008 to USPS</cell></row><row><cell>From USPS to MNIST</cell><cell cols="5">0.274 0.355 0.631 0.673 0.891 ±0.008</cell></row><row><cell>Average</cell><cell cols="4">0.341 0.411 0.554 0.640</cell><cell>0.902</cell></row><row><cell cols="6">Table 1: Unsupervised domain adaptation performance comparison. The</cell></row><row><cell cols="6">table reported classification accuracies achieved by competing algorithms.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>CoGAN for generating digits with different in-plane rotation angles</figDesc><table><row><cell></cell><cell></cell><cell>Generative models</cell><cell></cell></row><row><cell>Layer</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Shared?</cell></row><row><cell>1</cell><cell>FC-(N1024), BN, PReLU</cell><cell>FC-(N1024), BN, PReLU</cell><cell>Yes</cell></row><row><cell>2</cell><cell>FC-(N1024), BN, PReLU</cell><cell>FC-(N1024), BN, PReLU</cell><cell>Yes</cell></row><row><cell>3</cell><cell>FC-(N1024), BN, PReLU</cell><cell>FC-(N1024), BN, PReLU</cell><cell>Yes</cell></row><row><cell>4</cell><cell>FC-(N1024), BN, PReLU</cell><cell>FC-(N1024), BN, PReLU</cell><cell>Yes</cell></row><row><cell>5</cell><cell>FC-(N784), Sigmoid</cell><cell>FC-(N784), Sigmoid</cell><cell>No</cell></row><row><cell></cell><cell cols="2">Discriminative models</cell><cell></cell></row><row><cell>Layer</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Shared?</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The table shows the performance of pair generation of digits and corresponding edge images (Task A) with different CoGAN weight-sharing configurations. The results were the average pixel agreement ratios over 10000 images over 5 trials.</figDesc><table><row><cell cols="2">Avg. pixel agreement ratio</cell><cell>5</cell><cell>Weight-sharing layers in the generative models 5,4 5,4,3 5,4,3,2</cell></row><row><cell>Weight-sharing</cell><cell></cell><cell cols="2">0.894 ± 0.020 0.937 ± 0.004 0.943 ± 0.003 0.951 ± 0.004</cell></row><row><cell>layers in the</cell><cell>4</cell><cell cols="2">0.904 ± 0.018 0.939 ± 0.002 0.943 ± 0.005 0.950 ± 0.003</cell></row><row><cell>discriminative</cell><cell>4,3</cell><cell cols="2">0.888 ± 0.036 0.934 ± 0.005 0.946 ± 0.003 0.941 ± 0.024</cell></row><row><cell>models</cell><cell>4,3,2</cell><cell cols="2">0.903 ± 0.009 0.925 ± 0.021 0.944 ± 0.006 0.952 ± 0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The table shows the performance of pair generation of digits and corresponding negative images (Task B) with different CoGAN weight-sharing configurations. The results were the average pixel agreement ratios over 10000 images over 5 trials.</figDesc><table><row><cell cols="2">Avg. pixel agreement ratio</cell><cell>5</cell><cell>Weight-sharing layers in the generative models 5,4 5,4,3 5,4,3,2</cell></row><row><cell>Weight-sharing</cell><cell></cell><cell cols="2">0.932 ± 0.011 0.946 ± 0.013 0.970 ± 0.002 0.979 ± 0.001</cell></row><row><cell>layers in the</cell><cell>4</cell><cell cols="2">0.906 ± 0.066 0.953 ± 0.008 0.970 ± 0.003 0.978 ± 0.001</cell></row><row><cell>discriminative</cell><cell>4,3</cell><cell cols="2">0.908 ± 0.028 0.944 ± 0.012 0.965 ± 0.009 0.976 ± 0.001</cell></row><row><cell>models</cell><cell>4,3,2</cell><cell cols="2">0.917 ± 0.022 0.934 ± 0.011 0.955 ± 0.010 0.969 ± 0.008</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Network architecture of the conditional GAN</figDesc><table><row><cell>Layer</cell><cell>Generative models</cell></row><row><cell>input</cell><cell>z and conditional variable c ∈ {0, 1}</cell></row><row><cell>1</cell><cell>FCONV-(N1024,K4x4,S1), BN, PReLU</cell></row><row><cell>2</cell><cell>FCONV-(N512,K3x3,S2), BN, PReLU</cell></row><row><cell>3</cell><cell>FCONV-(N256,K3x3,S2), BN, PReLU</cell></row><row><cell>4</cell><cell>FCONV-(N128,K3x3,S2), BN, PReLU</cell></row><row><cell>5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance Comparison. For each task, we reported the average pixel agreement ratio scores and standard deviations over 5 trails, each trained with a different random initialization of the network connection weights.</figDesc><table><row><cell>Experiment</cell><cell cols="2">Task A: Digit and Edge Images Task B: Digit and Negative Images</cell></row><row><cell>Conditional GAN</cell><cell>0.909 ± 0.003</cell><cell>0.778 ± 0.021</cell></row><row><cell>CoGAN</cell><cell>0.952 ± 0.002</cell><cell>0.967 ± 0.008</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Numbers of training images in Domain 1 and Domain 2 in the MNIST experiments.</figDesc><table><row><cell></cell><cell>Task A</cell><cell>Task B</cell></row><row><cell></cell><cell>Pair generation of digits and</cell><cell>Pair generation of digits and</cell></row><row><cell></cell><cell cols="2">corresponding edge images corresponding negative images</cell></row><row><cell># of images in Domain 1</cell><cell>30,000</cell><cell>30,000</cell></row><row><cell># of images in Domain 2</cell><cell>30,000</cell><cell>30,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Numbers of training images of different attributes in the pair face generation experiments.</figDesc><table><row><cell>Attribute</cell><cell cols="3">Smiling Blond hair Glasses</cell></row><row><cell># of images with the attribute</cell><cell>97,669</cell><cell>29,983</cell><cell>13,193</cell></row><row><cell cols="2"># of images without the attribute 104,930</cell><cell cols="2">172,616 189,406</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Numbers of RGB and depth training images in the RGBD experiments.</figDesc><table><row><cell># of RGB images 93,564</cell></row><row><cell># of depth images 93,564</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Numbers of RGB and depth training images in the NYU experiments.</figDesc><table><row><cell cols="2"># of RGB images 514,192</cell></row><row><cell># of depth images</cell><cell>1,449</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>CoGAN for digit generation</figDesc><table><row><cell></cell><cell cols="2">Generative models</cell><cell></cell></row><row><cell>Layer</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Shared?</cell></row><row><cell>1</cell><cell>FCONV-(N1024,K4x4,S1), BN, PReLU</cell><cell>FCONV-(N1024,K4x4,S1), BN, PReLU</cell><cell>Yes</cell></row><row><cell>2</cell><cell>FCONV-(N512,K3x3,S2), BN, PReLU</cell><cell>FCONV-(N512,K3x3,S2), BN, PReLU</cell><cell>Yes</cell></row><row><cell>3</cell><cell>FCONV-(N256,K3x3,S2), BN, PReLU</cell><cell>FCONV-(N256,K3x3,S2), BN, PReLU</cell><cell>Yes</cell></row><row><cell>4</cell><cell>FCONV-(N128,K3x3,S2), BN, PReLU</cell><cell>FCONV-(N128,K3x3,S2), BN, PReLU</cell><cell>Yes</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>CoGAN for face generation</figDesc><table><row><cell></cell><cell>Generative models</cell><cell></cell><cell></cell></row><row><cell>Layer</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Shared?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>CoGAN for color and depth image generation for the RGBD object dataset</figDesc><table><row><cell></cell><cell>Generative models</cell><cell></cell><cell></cell></row><row><cell>Layer</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Shared?</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Remarks: CoGAN learning requires training samples drawn from the marginal distributions, p X1 and p X2 . It does not rely on samples drawn from the joint distribution, p X1,X2 , where corresponding supervision would be available. Our main contribution is in showing that with just samples drawn separately from the marginal distributions, CoGAN can learn a joint distribution of images in the two domains. Both weight-sharing constraint and adversarial training are essential for enabling this capability. Unlike autoencoder learning<ref type="bibr" target="#b2">[3]</ref>, which encourages a generated pair of images to be identical to the target pair of corresponding images in the two domains for minimizing the reconstruction loss 1 , the adversarial training only encourages the generated pair of images to be 1 This is why<ref type="bibr" target="#b2">[3]</ref> requires samples from the joint distribution for learning the joint distribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We noted that the performances were not sensitive to the number of training iterations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We note the generation performance of the conditional GAN did not change much after 5000 iterations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 14: Generation of faces with blond hair and without blond hair.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 15: Generation of faces with blond hair and without blond hair.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 16: Generation of faces with blond hair and without blond hair.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 17: Generation of smiling and non-smiling faces.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 18: Generation of smiling and non-smiling faces.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 19: Generation of smiling and non-smiling faces.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 20: Generation of faces with eyeglasses and without eyeglasses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 21: Generation of faces with eyeglasses and without eyeglasses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 22: Generation of faces with eyeglasses and without eyeglasses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 25: Generation of RGB and depth images of indoor scenes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 26: Generation of RGB and depth images of indoor scenes.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 Mini-batch stochastic gradient descent for training coupled generative adversarial nets.</p><p>1 's and θ g (i) 2 's with the shared network connection weights set to the same values. <ref type="bibr">2:</ref> for t = 0, 1, 2, ..., maximum number of iterations do 3:</p><p>Compute the gradients of the parameters of the discriminative model,</p><p>Compute the gradients of the parameters of the discriminative model,</p><p>Average the gradients of the shared parameters of the discriminative models.</p><p>9:</p><p>Compute f t+1 1 and f t+1 2 according to the gradients. <ref type="bibr">10:</ref> Compute the gradients of the parameters of the generative model, g t 1 , ∆θ g (i)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view rgb-d object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint cross-domain classification and subspace learning for unsupervised adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Beyond sharing weights for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06432</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Generative moment matching networks. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICML</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00570</idno>
		<title level="m">Conditional image generation from visual attributes</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkyu</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dusik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep visual analogy-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sigmoid Yes Table 14: CoGAN for color and depth image generation for the NYU indoor scene dataset Generative models Layer Domain 1 Domain 2 Shared?</title>
	</analytic>
	<monogr>
		<title level="m">BN, PReLU No 7 FCONV-(N3,K3x3,S1), TanH FCONV-(N3,K3x3,S1), TanH No Discriminative models Layer Domain 1 Domain 2 Shared? 1 CONV-(N32,K5x5,S2), BN, PReLU CONV-(N32,K5x5,S2), BN, PReLU No 2 CONV-(N64,K5x5,S2), BN, PReLU CONV-(N64,K5x5,S2), BN, PReLU No 3 CONV-(N128,K5x5,S2), BN, PReLU CONV-(N128,K5x5,S2), BN, PReLU Yes 4 CONV-(N256,K3x3,S2), BN, PReLU CONV-(N256,K3x3,S2), BN, PReLU Yes 5 CONV-(N512,K3x3,S2), BN, PReLU CONV-(N512,K3x3,S2), BN, PReLU Yes 6 CONV-(N1024,K3x3,S2), BN, PReLU CONV-(N1024,K3x3,S2), BN, PReLU Yes 7 FC-(N2048), BN, PReLU FC-(N2048), BN, PReLU Yes 8 FC-(N1)</title>
		<editor>FCONV-(N1024,K4x4,S1), BN, PReLU FCONV-(N1024,K4x4,S1), BN, PReLU Yes 2 FCONV-(N512,K4x4,S2), BN, PReLU FCONV-(N512,K4x4,S2), BN, PReLU Yes 3 FCONV-(N256,K4x4,S2), BN, PReLU FCONV-(N256,K4x4,S2), BN, PReLU Yes 4 FCONV-(N128,K4x4,S2), BN, PReLU FCONV-(N128,K4x4,S2), BN, PReLU Yes 5 FCONV-(N64,K4x4,S2), BN, PReLU FCONV-(N64,K4x4,S2), BN, PReLU Yes 6 FCONV-(N32,K4x4,S2), BN, PReLU FCONV-(N32,K4x4,S2</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Sigmoid Yes</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
