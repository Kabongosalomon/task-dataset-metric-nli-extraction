<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
							<email>jihoontack@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
							<email>swmo@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
							<email>jongheonj@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
							<email>jinwoos@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Graduate School of</roleName><forename type="first">‡</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Novelty detection, i.e., identifying whether a given sample is drawn from outside the training distribution, is essential for reliable machine learning. To this end, there have been many attempts at learning a representation well-suited for novelty detection and designing a score based on such representation. In this paper, we propose a simple, yet effective method named contrasting shifted instances (CSI), inspired by the recent success on contrastive learning of visual representations. Specifically, in addition to contrasting a given sample with other instances as in conventional contrastive learning methods, our training scheme contrasts the sample with distributionally-shifted augmentations of itself. Based on this, we propose a new detection score that is specific to the proposed training scheme. Our experiments demonstrate the superiority of our method under various novelty detection scenarios, including unlabeled one-class, unlabeled multi-class and labeled multi-class settings, with various image benchmark datasets. Code and pre-trained models are available at https://github.com/alinlab/CSI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Out-of-distribution (OOD) detection <ref type="bibr" target="#b25">[26]</ref>, also referred to as a novelty-or anomaly detection is a task of identifying whether a test input is drawn far from the training distribution (in-distribution) or not. In general, the OOD detection problem aims to detect OOD samples where a detector is allowed to access only to training data. The space of OOD samples is typically huge, i.e., an OOD sample can vary significantly and arbitrarily from the given training distribution. Hence, assuming specific prior knowledge, e.g., external data representing some specific OODs, may introduce a bias to the detector. The OOD detection is a classic yet essential problem in machine learning, with a broad range of applications, including medical diagnosis <ref type="bibr" target="#b3">[4]</ref>, fraud detection <ref type="bibr" target="#b52">[53]</ref>, and autonomous driving <ref type="bibr" target="#b11">[12]</ref>.</p><p>A long line of literature has thus been proposed, including density-based <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b16">17]</ref>, reconstruction-based <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b6">7]</ref>, one-class classifier <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b55">56]</ref>, and self-supervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref> approaches. Overall, a majority of recent literature is concerned with (a) modeling the representation to better encode normality <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, and (b) defining a new detection score <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b1">2]</ref>. In particular, recent studies have shown that inductive biases from self-supervised learning significantly help to learn discriminative features for OOD detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Meanwhile, recent progress on self-supervised learning has proven the effectiveness of contrastive learning in various domains, e.g., computer vision <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5]</ref>, audio processing <ref type="bibr" target="#b49">[50]</ref>, and reinforcement learning <ref type="bibr" target="#b62">[63]</ref>. Contrastive learning extracts a strong inductive bias from multiple (similar) views of a sample by let them attract each other, yet repelling them to other samples. Instance discrimination <ref type="bibr" target="#b69">[69]</ref> is a special type of contrastive learning where the views are restricted up to different augmentations, which have achieved state-of-the-art results on visual representation learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5]</ref>. Inspired by the recent success of instance discrimination, we aim to utilize its power of representation learning for OOD detection. To this end, we investigate the following questions: (a) how to learn a (more) discriminative representation for detecting OODs and (b) how to design a score function utilizing the representation from (a). We remark that the desired representation for OOD detection may differ from that for standard representation learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, as the former aims to discriminate in-distribution and OOD samples, while the latter aims to discriminate within in-distribution samples.</p><p>We first found that existing contrastive learning scheme is already reasonably effective for detecting OOD samples with a proper detection score. We further observe that one can improve its performance by utilizing "hard" augmentations, e.g., rotation, that were known to be harmful and unused for the standard contrastive learning <ref type="bibr" target="#b4">[5]</ref>. In particular, while the existing contrastive learning schemes act by pulling all augmented samples toward the original sample, we suggest to additionally push the samples with hard or distribution-shifting augmentations away from the original. We observe that contrasting shifted samples help OOD detection, as the model now learns a new task of discriminating between in-and out-of-distribution, in addition to the original task of discriminating within in-distribution.</p><p>Contribution. We propose a simple yet effective method for OOD detection, coined contrasting shifted instances (CSI). Built upon the existing contrastive learning scheme <ref type="bibr" target="#b4">[5]</ref>, we propose two novel additional components: (a) a new training method which contrasts distributionally-shifted augmentations (of the given sample) in addition to other instances, and (b) a score function which utilizes both the contrastively learned representation and our new training scheme in (a). Finally, we show that CSI enjoys broader usage by applying it to improve the confidence-calibration of the classifiers: it relaxes the overconfidence issue in their predictions for both in-and out-of-distribution samples while maintaining the classification accuracy.</p><p>We verify the effectiveness of CSI under various environments of detecting OOD, including unlabeled one-class, unlabeled multi-class, and labeled multi-class settings. To our best knowledge, we are the first to demonstrate all three settings under a single framework. Overall, CSI outperforms the baseline methods for all tested datasets. In particular, CSI achieves new state-of-the-art results 2 on one-class classification, e.g., it improves the mean area under the receiver operating characteristics (AUROC) from 90.1% to 94.3% (+4.2%) for CIFAR-10 <ref type="bibr" target="#b32">[33]</ref>, 79.8% to 89.6% (+9.8%) for CIFAR-100 <ref type="bibr" target="#b32">[33]</ref>, and 85.7% to 91.6% (+5.9%) for ImageNet-30 <ref type="bibr" target="#b24">[25]</ref> one-class datasets, respectively. We remark that CSI gives a larger improvement in harder (or near-distribution) OOD samples. To verify this, we also release new benchmark datasets: fixed version of the resized LSUN and ImageNet <ref type="bibr" target="#b38">[39]</ref>.</p><p>We remark that learning representation to discriminate in-vs. out-of-distributions is an important but under-explored problem. We believe that our work would guide new interesting directions in the future, for both representation learning and OOD detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CSI: Contrasting shifted instances</head><p>For a given dataset {x m } M m=1 sampled from a data distribution p data (x) on the data space X , the goal of out-of-distribution (OOD) detection is to model a detector from {x m } that identifies whether x is sampled from the data generating distribution (or in-distribution) p data (x) or not. As modeling p data (x) directly is prohibitive in most cases, many existing methods for OOD detection define a score function s(x) that a high value heuristically represents that x is from in-distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrastive learning</head><p>The idea of contrastive learning is to learn an encoder f θ to extract the necessary information to distinguish similar samples from the others. Let x be a query, {x + }, and {x − } be a set of positive and negative samples, respectively, and sim(z, z ) := z · z / z z be the cosine similarity. Then, the primitive form of the contrastive loss is defined as follows:</p><formula xml:id="formula_0">L con (x, {x + }, {x − }) := − 1 |{x + }| log x ∈{x+} exp(sim(z(x), z(x ))/τ ) x ∈{x+}∪{x−} exp(sim(z(x), z(x ))/τ ) ,<label>(1)</label></formula><p>where |{x + }| denotes the cardinality of the set {x + }, z(x) denotes the output feature of the contrastive layer, and τ denotes a temperature hyper-parameter. One can define the contrastive feature z(x)</p><p>directly from the encoder f θ , i.e., z(x) = f θ (x) <ref type="bibr" target="#b20">[21]</ref>, or apply an additional projection layer g φ , i.e., z(x) = g φ (f θ (x)) <ref type="bibr" target="#b4">[5]</ref>. We use the projection layer following the recent studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In this paper, we specifically consider the simple contrastive learning (SimCLR) <ref type="bibr" target="#b4">[5]</ref>, a simple and effective objective based on the task of instance discrimination <ref type="bibr" target="#b69">[69]</ref>. Letx</p><formula xml:id="formula_1">(1) i andx (2) i</formula><p>be two independent augmentations of x i from a pre-defined family T , namely,x <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_2">:= T 1 (x i ) andx (2) := T 2 (x i ), where T 1 , T 2 ∼ T .</formula><p>Then the SimCLR objective can be defined by the contrastive loss (1) where each (x <ref type="bibr" target="#b0">(1)</ref> i ,x <ref type="bibr" target="#b1">(2)</ref> i ) and (x <ref type="bibr" target="#b1">(2)</ref> i ,x <ref type="bibr" target="#b0">(1)</ref> i ) are considered as query-key pairs while others being negatives. Namely, for a given batch B :</p><formula xml:id="formula_3">= {x i } B i=1</formula><p>, the SimCLR objective is defined as follows:</p><formula xml:id="formula_4">L SimCLR (B; T ) := 1 2B B i=1 L con (x (1) i ,x (2) i ,B −i ) + L con (x (2) i ,x (1) i ,B −i ),<label>(2)</label></formula><p>whereB := {x</p><formula xml:id="formula_5">(1) i } B i=1 ∪ {x (2) i } B i=1 andB −i := {x (1) j } j =i ∪ {x (2) j } j =i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive learning for distribution-shifting transformations</head><p>Chen et al. <ref type="bibr" target="#b4">[5]</ref> has performed an extensive study on which family of augmentations T leads to a better representation when used in SimCLR, i.e., which transformations should f θ consider as positives. Overall, the authors report that some of the examined augmentations (e.g., rotation), sometimes degrades the discriminative performance of SimCLR. One of our key findings is that such augmentations can be useful for OOD detection by considering them as negatives -contrast from the original sample. In this paper, we explore which family of augmentations S, which we call distribution-shifting transformations, or simply shifting transformations, would lead to better representation in terms of OOD detection when used as negatives in SimCLR.</p><p>Contrasting shifted instances. We consider a set S consisting of K different (random or deterministic) transformations, including the identity I: namely, we denote S := {S 0 = I, S 1 , . . . , S K−1 }. In contrast to the vanilla SimCLR that considers augmented samples as positive to each other, we attempt to consider them as negative if the augmentation is from S. For a given batch of samples</p><formula xml:id="formula_6">B = {x i } B i=1</formula><p>, this can be done simply by augmenting B via S before putting it into the SimCLR loss defined in (2): namely, we define contrasting shifted instances (con-SI) loss as follows:</p><formula xml:id="formula_7">L con-SI := L SimCLR S∈S B S ; T , where B S := {S(x i )} B i=1 .<label>(3)</label></formula><p>Here, our intuition is to regard each distributionally-shifted sample (i.e., S = I) as an OOD with respect to the original. In this respect, con-SI attempts to discriminate an in-distribution (i.e., S = I) sample from other OOD (i.e., S ∈ {S 1 , . . . , S K−1 }) samples. We further verify the effectiveness of con-SI in our experimental results: although con-SI does not improve representation for standard classification, it does improve OOD detection significantly (see linear evaluation in Section 3.2).</p><p>Classifying shifted instances. In addition to contrasting shifted instances, we consider an auxiliary task that predicts which shifting transformation y S ∈ S is applied for a given input x, in order to facilitate f θ to discriminate each shifted instance. Specifically, we add a linear layer to f θ for modeling an auxiliary softmax classifier p cls-SI (y S |x), as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>. LetB S be the batch augmented from B S via SimCLR; then, we define classifying shifted instances (cls-SI) loss as follows:</p><formula xml:id="formula_8">L cls-SI := 1 2B 1 K S∈S x S ∈B S − log p cls-SI (y S = S |x S ).<label>(4)</label></formula><p>The final loss of our proposed method, CSI, is defined by combining the two objectives: L CSI = L con-SI + λ · L cls-SI (5) where λ &gt; 0 is a balancing hyper-parameter. We simply set λ = 1 for all our experiments.</p><p>OOD-ness: How to choose the shifting transformation? In principle, we choose the shifting transformation that generates the most OOD-like yet semantically meaningful samples. Intuitively, such samples can be most effective ('nearby' but 'not-too-nearby') OOD samples, as also discussed in Section 3.2. More specifically, we measure the OOD-ness of a transformation by the area under the receiver operating characteristics (AUROC) between in-distribution vs. transformed samples under vanilla SimCLR, using the detection score (6) defined in Section 2.3. The transformation with high OOD-ness values (i.e., OOD-like) indeed performs better (see <ref type="table" target="#tab_3">Table 4</ref> and <ref type="table" target="#tab_4">Table 5</ref> in Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Score functions for detecting out-of-distribution</head><p>Upon the representation z(·) learned by our proposed training objective, we define several score functions for detecting out-of-distribution; whether a given x is OOD or not. We first propose a detection score that is applicable to any contrastive representation. We then introduce how one could incorporate additional information learned by contrasting (and classifying) shifted instances as in <ref type="bibr" target="#b4">(5)</ref>.</p><p>Detection score for contrastive representation. Overall, we find that two features from SimCLR representations are surprisingly effective for detecting OOD samples: (a) the cosine similarity to the nearest training sample in {x m }, i.e., max m sim(z(x m ), z(x)), and (b) the norm of the representation, i.e., z(x) . Intuitively, the contrastive loss increases the norm of in-distribution samples, as it is an easy way to minimize the cosine similarity of identical samples by increasing the denominator of (1). We discuss further detailed analysis of both features in Appendix H. We simply combine these features to define a detection score s con for contrastive representation:</p><formula xml:id="formula_9">s con (x; {x m }) := max m sim(z(x m ), z(x)) · z(x) .<label>(6)</label></formula><p>We also discuss how one can reduce the computation and memory cost by choosing a proper subset (i.e., coreset) of training samples in Appendix E.</p><p>Utilizing shifting transformations. Given that our proposed L CSI is used for training, one can further improve the detection score s con significantly by incorporating shifting transformations S.</p><p>Here, we propose two additional scores, s con-SI and s cls-SI , where are corresponded to L con-SI (3) and L cls-SI (4), respectively.</p><p>Firstly, we define s con-SI by taking an expectation of s con over S ∈ S:</p><formula xml:id="formula_10">s con-SI (x; {x m }) := S∈S λ con S s con (S(x); {S(x m )}),<label>(7)</label></formula><p>where λ con S := M/ m s con (S(x m ); {S(x m )}) = M/ m z(S(x m )) for M training samples is a balancing term to scale the scores of each shifting transformation (See Appendix F for details).</p><p>Secondly, we define s cls-SI utilizing the auxiliary classifier p(y S |x) upon f θ as follows:</p><formula xml:id="formula_11">s cls-SI (x) := S∈S λ cls S W S f θ (S(x)),<label>(8)</label></formula><p>where λ cls S := M/ m [W S f θ (S(x m ))] are again balancing terms similarly to above, and W S is the weight vector in the linear layer of p(y S |x) per S ∈ S.</p><p>Finally, the combined score for CSI representation is defined as follows:</p><formula xml:id="formula_12">s CSI (x; {x m }) := s con-SI (x; {x m }) + s cls-SI (x).<label>(9)</label></formula><p>Ensembling over random augmentations. In addition, we find one can further improve each of the proposed scores by ensembling it over random augmentations T (x) where T ∼ T . Namely, for instance, the ensembled CSI score is defined by s CSI-ens (x) := E T ∼T [s CSI (T (x))]. Unless otherwise noted, we use these ensembled versions of (6) to <ref type="bibr" target="#b8">(9)</ref> in our experiments. See Appendix D for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Extension for training confidence-calibrated classifiers</head><p>Furthermore, we propose an extension of CSI for training confidence-calibrated classifiers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref> from a given labeled dataset {(x m , y m )} m ⊆ X × Y by adapting it to supervised contrastive learning (SupCLR) <ref type="bibr" target="#b29">[30]</ref>. Here, the goal is to model a classifier p(y|x) that is (a) accurate on predicting y when x is in-distribution, and (b) the confidence s sup (x) := max y p(y|x) <ref type="bibr" target="#b21">[22]</ref> of the classifier is well-calibrated, i.e., s sup (x) should be low if x is an OOD sample or arg max y p(y|x) = true label.</p><p>Supervised contrastive learning (SupCLR). SupCLR is a supervised extension of SimCLR that contrasts samples in class-wise, instead of in instance-wise: every samples of the same classes are considered as positives.</p><formula xml:id="formula_13">Let C = {(x i , y i )} B i=1</formula><p>be a training batch with class labels y i ∈ Y, andC be an augmented batch by random transformation T , i.e.,C := {(x j , y j ) |x j ∈B}. For a given label y, we divideC into two subsetsC =C y ∪C −y whereC y contains the samples of label y andC −y contains the remaining. Then, the SupCLR objective is defined by: After training the embedding network f θ (x) with the SupCLR objective (10), we train a linear classifier upon f θ (x) to model p SupCLR (y|x).</p><formula xml:id="formula_14">L SupCLR (C; T ) := 1 2B 2B j=1 L con (x j ,C yj \ {x j },C −yj ).<label>(10)</label></formula><p>Supervised extension of CSI. We extend CSI by incorporating the shifting transformations S into the SupCLR objective: here, we consider a joint label (y, y S ) ∈ Y × S of class label y and shifting transformation y S . Then, the supervised contrasting shifted instances (sup-CSI) loss is given by:</p><formula xml:id="formula_15">L sup-CSI := L SupCLR S∈S C S ; T , where C S := {(S(x i ), (y i , S))} B i=1 .<label>(11)</label></formula><p>Note that we do not use the auxiliary classification loss L cls-SI (4), since the objective already classifies the shifted instances under a self-label augmented <ref type="bibr" target="#b34">[35]</ref> space Y × S.</p><p>Upon the learned representation via (11), we additionally train two linear classifiers: p CSI (y|x) and p CSI-joint (y, y S |x) that predicts the class labels and joint labels, respectively. We directly apply s sup (x) for the former p CSI (y|x). For the latter, on the other hand, we marginalize the joint prediction over the shifting transformation in a similar manner of Section 2.3. Precisely, let l(x) ∈ R C×K be logit values of p CSI-joint (y, y S |x) for |Y| = C and |S| = K, and l(x) k ∈ R C be logit values correspond to p CSI-joint (y, y S = S k |x). Then, the ensembled probability is:</p><formula xml:id="formula_16">p CSI-ens (y|x) := σ 1 K k l(S k (x)) k ,<label>(12)</label></formula><p>where σ denotes the softmax activation. Here, we use p CSI-ens to compute the confidence s sup (x).</p><p>We denote the confidence computed by p CSI and p CSI-ens and "CSI" and "CSI-ens", respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In Section 3.1, we report OOD detection results on unlabeled one-class, unlabeled multi-class, and labeled multi-class datasets. In Section 3.2, we analyze the effects on various shifting transformations in the context of OOD detection, as well as an ablation study on each component we propose. Setup. We use ResNet-18 <ref type="bibr" target="#b19">[20]</ref> architecture for all the experiments. For data augmentations T , we adopt those used by Chen et al. <ref type="bibr" target="#b4">[5]</ref>: namely, we use the combination of Inception crop <ref type="bibr" target="#b63">[64]</ref>, horizontal flip, color jitter, and grayscale. For shifting transformations S, we use the random rotation 0°, 90°, 180°, 270°unless specified otherwise, as rotation has the highest OOD-ness (see Section 2.2) values for natural images, e.g., CIFAR-10 <ref type="bibr" target="#b32">[33]</ref>. However, we remark that the best shifting transformation can be different for other datasets, e.g., Gaussian noise performs better than rotation for texture datasets (see <ref type="table" target="#tab_5">Table 6</ref> in Section 3.2). By default, we train our models from scratch with the training objective in <ref type="bibr" target="#b4">(5)</ref> and detect OOD samples with the ensembled version of the score in <ref type="bibr" target="#b8">(9)</ref>.</p><p>We mainly report the area under the receiver operating characteristic curve (AUROC) as a thresholdfree evaluation metric for a detection score. In addition, we report the test accuracy and the expected calibration error (ECE) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19]</ref> for the experiments on labeled multi-class datasets. Here, ECE estimates whether a classifier can indicate when they are likely to be incorrect for test samples (from in-distribution) by measuring the difference between prediction confidence and accuracy. The formal description of the metrics and detailed experimental setups are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main results</head><p>Unlabeled one-class datasets. We start by considering the one-class setup: here, for a given multiclass dataset of C classes, we conduct C one-class classification tasks, where each task chooses one of the classes as in-distribution while the remaining classes being out-of-distribution. We run our experiments on three datasets, following the prior work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>: CIFAR-10 <ref type="bibr" target="#b32">[33]</ref>, CIFAR-100 labeled into 20 super-classes <ref type="bibr" target="#b32">[33]</ref>, and ImageNet-30 <ref type="bibr" target="#b24">[25]</ref> datasets. We compare CSI with various prior methods including one-class classifier <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b55">56]</ref>, reconstruction-based <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b51">52]</ref>, and self-supervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref> approaches. <ref type="table" target="#tab_0">Table 1</ref> summarizes the results, showing that CSI significantly outperforms the prior methods in all the tested cases. We provide the full, additional results, e.g., class-wise AUROC on CIFAR-100 (super-class) and ImageNet-30, in Appendix C.</p><p>Unlabeled multi-class datasets. In this setup, we assume that in-distribution samples are from a specific multi-class dataset without labels, testing on various external datasets as out-of-distribution. We compare CSI on two in-distribution datasets: CIFAR-10 <ref type="bibr" target="#b32">[33]</ref> and ImageNet-30 <ref type="bibr" target="#b24">[25]</ref>. We consider the following datasets as out-of-distribution: SVHN <ref type="bibr" target="#b47">[48]</ref>, resized LSUN and ImageNet <ref type="bibr" target="#b38">[39]</ref>, CIFAR-100 <ref type="bibr" target="#b32">[33]</ref>, and linearly-interpolated samples of CIFAR-10 (Interp.) <ref type="bibr" target="#b10">[11]</ref> for CIFAR-10 experiments, and CUB-200 <ref type="bibr" target="#b66">[67]</ref>, Dogs <ref type="bibr" target="#b28">[29]</ref>, Pets <ref type="bibr" target="#b50">[51]</ref>, Flowers <ref type="bibr" target="#b48">[49]</ref>, Food-101 <ref type="bibr" target="#b2">[3]</ref>, Places-365 <ref type="bibr" target="#b75">[75]</ref>, Caltech-256 <ref type="bibr" target="#b17">[18]</ref>, and DTD <ref type="bibr" target="#b7">[8]</ref> for ImageNet-30. We compare CSI with various prior methods, including density-based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61]</ref> and self-supervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref> approaches.  <ref type="table" target="#tab_1">Table 2</ref> shows the results. Overall, CSI significantly outperforms the prior methods in all benchmarks tested. We remark that CSI is particularly effective for detecting hard (i.e., near-distribution) OOD samples, e.g., CIFAR-100 and Interp. in <ref type="table" target="#tab_1">Table 2a</ref>. Also, CSI still shows a notable performance in the cases when prior methods often fail, e.g., AUROC of 50% (i.e., random guess) for Places-365 dataset in <ref type="table" target="#tab_1">Table 2b</ref>. Finally, we notice that the resized LSUN and ImageNet datasets officially released by Liang et al. <ref type="bibr" target="#b38">[39]</ref> might be misleading to evaluate detection performance for hard OODs: we find that those datasets contain some unintended artifacts, due to incorrect resizing procedure. Such an artifact makes those datasets easily-detectable, e.g., via input statistics. In this respect, we produce and test on their fixed versions, coined LSUN (FIX), and ImageNet (FIX). See Appendix I for details.</p><p>Labeled multi-class datasets. We also consider the labeled version of the above setting: namely, we now assume that every in-distribution sample also contains discriminative label information. We use the same datasets considered in the unlabeled multi-class setup for in-and out-of-distribution datasets. We train our model as proposed in Section 2.4, and compare it with those trained by other methods, the cross-entropy and supervised contrastive learning (SupCLR) <ref type="bibr" target="#b29">[30]</ref>. Since our goal is to calibrate the confidence, the maximum softmax probability is used to detect OOD samples (see <ref type="bibr" target="#b21">[22]</ref>). <ref type="table" target="#tab_2">Table 3</ref> shows the results. Overall, CSI consistently improves AUROC and ECE for all benchmarks tested. Interestingly, CSI also improves test accuracy; even our original purpose of CSI is to learn a representation for OOD detection. CSI can further improve the performance by ensembling over the transformations. We also remark that our results on unlabeled datasets (in <ref type="table" target="#tab_1">Table 2</ref>) already show comparable performance to the supervised baselines (in <ref type="table" target="#tab_2">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation study</head><p>We perform an ablation study on various shifting transformations, training objectives, and detection scores. Throughout this section, we report the mean AUROC values on one-class CIFAR-10.</p><p>Shifting transformation. We measure the OOD-ness (see Section 2.2) of transformations, i.e., the AUROC between in-distribution vs. transformed samples under vanilla SimCLR, and the effects of those transformations when used as a shifting transformation. In particular, we consider Cutout <ref type="bibr" target="#b9">[10]</ref>, Sobel filtering <ref type="bibr" target="#b27">[28]</ref>, Gaussian noise, Gaussian blur, and rotation <ref type="bibr" target="#b13">[14]</ref>. We remark that these transformations are reported to be ineffective in improving the class discriminative power of SimCLR <ref type="bibr" target="#b4">[5]</ref>. We also consider the transformation coined "Perm", which randomly permutes each part of the evenly partitioned image. Intuitively, such transformations commonly shift the input distribution, hence forcing them to be aligned can be harmful. <ref type="figure" target="#fig_0">Figure 1</ref> visualizes the considered transformations. <ref type="table" target="#tab_3">Table 4</ref> shows AUROC values of the vanilla SimCLR, where the in-distribution samples shifted by the chosen transformation are given as OOD samples. The shifted samples are easily detected: it validates our intuition that the considered transformations shift the input distribution. In particular, "Perm" and "Rotate" are the most distinguishable, which implies they shift the distribution the most.   Note that "Perm" and "Rotate" turns out to be the most effective shifting transformations; it implies that the transformations shift the distribution most indeed performs best for CSI. <ref type="bibr" target="#b2">3</ref> Besides, we apply the transformation upon the vanilla SimCLR: align the transformed samples to the original samples (i.e., use as T ) or consider them as the shifted samples (i.e., use as S). <ref type="table" target="#tab_4">Table  5a</ref> shows that aligning the transformations degrade (or on par) the detection performance, while shifting the transformations gives consistent improvements. We also remove or convert-to-shift the transformation from the vanilla SimCLR in <ref type="table" target="#tab_4">Table 5b</ref>, and see similar results. We remark that one can further improve the performance by combining multiple shifting transformations (see Appendix G).  <ref type="table" target="#tab_5">Table 6a</ref>). <ref type="table" target="#tab_5">Table 6b</ref> shows that CSI using Gaussian noise ("CSI(N)") indeed improves the vanilla SimCLR ("Base") while CSI using rotation ("CSI(R)") degrades instead. This results support our principles on selecting shifting transformations.</p><p>Linear evaluation. We also measure the linear evaluation <ref type="bibr" target="#b31">[32]</ref>, the accuracy of a linear classifier to discriminate classes of in-distribution samples. It is widely used for evaluating the quality of (unsupervised) learned representation. We report the linear evaluation of vanilla SimCLR and CSI (with shifting rotation), trained under unlabeled CIFAR-10. They show comparable results, 90.48% for SimCLR and 90.19% for CSI; CSI is more specialized to learn a representation for OOD detection.</p><p>Training objective. In <ref type="table" target="#tab_6">Table 7a</ref>, we assess the individual effects of each component that consists of our final training objective <ref type="formula">(5)</ref>: namely, we compare the vanilla SimCLR (2), contrasting shifted instances <ref type="formula" target="#formula_7">(3)</ref>, and classifying shifted instances (4) losses. For the evaluation of the models of different training objectives <ref type="formula" target="#formula_4">(2)</ref> to <ref type="formula">(5)</ref>, we use the detection scores defined in <ref type="bibr" target="#b5">(6)</ref> to <ref type="formula" target="#formula_12">(9)</ref>, respectively. We remark that both contrasting and classifying shows better results than the vanilla SimCLR; and combining them (i.e., the final CSI objective <ref type="formula">(5)</ref>) gives further improvements, i.e., two losses are complementary.</p><p>Detection score. Finally, <ref type="table" target="#tab_6">Table 7b</ref> shows the effect of each component in our detection score: the vanilla contrastive <ref type="formula" target="#formula_9">(6)</ref>, contrasting shifted instances <ref type="formula" target="#formula_10">(7)</ref>, and classifying shifted instances (8) scores. We ensemble the scores over both T and S for <ref type="formula" target="#formula_10">(7)</ref> to <ref type="formula" target="#formula_12">(9)</ref>, and use a single sample for <ref type="bibr" target="#b5">(6)</ref>. All the reported values are evaluated from the model trained by the final objective 5. Similar to above, both contrasting and classifying scores show better results than the vanilla contrastive score; and combining them (i.e., the final CSI score <ref type="formula" target="#formula_12">(9)</ref>) gives further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>OOD detection. Recent works on unsupervised OOD detection (i.e., no external OOD samples) <ref type="bibr" target="#b25">[26]</ref> can be categorized as: (a) density-based <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b16">17]</ref>, (b) reconstruction-based <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b6">7]</ref>, (c) one-class classifier <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b55">56]</ref>, and (d) self-supervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref> approaches. Our work falls into (c) the self-supervised approach, as it utilizes the representation learned from self-supervision <ref type="bibr" target="#b13">[14]</ref>. However, unlike prior works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref> focusing on the self-label classification tasks (e.g., predict the angle of the rotated image), we first incorporate contrastive learning <ref type="bibr" target="#b4">[5]</ref> for OOD detection. Concurrently, Winkens et al. <ref type="bibr" target="#b68">[68]</ref> and Liu and Abbeel <ref type="bibr" target="#b39">[40]</ref> report that contrastive learning also improves the OOD detection performance of classifiers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Confidence-calibrated classifiers. Confidence-calibrated classifiers aim to calibrate the prediction confidence (maximum softmax probability), which can be directly used as an uncertainty estimator for both within in-distribution <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19]</ref> and in-vs. out-of-distribution <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>. Prior works improved calibration through inference <ref type="bibr" target="#b18">[19]</ref> or training <ref type="bibr" target="#b36">[37]</ref> schemes, which are can be jointed applied to our method. Some works design a specific detection score upon the pre-trained classifiers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref>, but they only target OOD detection, while ours also consider the in-distribution calibration.</p><p>Self-supervised learning. Self-supervised learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>, particularly contrastive learning <ref type="bibr" target="#b12">[13]</ref> via instance discrimination <ref type="bibr" target="#b69">[69]</ref>, has shown remarkable success on visual representation learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5]</ref>. However, most prior works focus on the downstream tasks (e.g., classification), and other advantages (e.g., uncertainty or robustness) are rarely investigated <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>. Our work, concurrent with <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b68">68]</ref>, first verifies that contrastive learning is also effective for OOD detection. In particular, we find that the shifting transformations, which were known to be harmful and unused for the standard contrastive learning <ref type="bibr" target="#b4">[5]</ref>, can help OOD detection. This observation provides new considerations for selecting transformations, i.e., which transformation should be used for positive or negative <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b71">71]</ref>.</p><p>We further provide a more comprehensive survey and discussions with prior works in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a simple yet effective method named contrasting shifted instances (CSI), which extends the power of contrastive learning for out-of-distribution (OOD) detection problems. CSI demonstrates outstanding performance under various OOD detection scenarios. We believe our work would guide various future directions in OOD detection and self-supervised learning as an important baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This paper is focused on the subject of out-of-distribution (OOD) (or novelty, anomaly) detection, which is an essential ingredient for building safe and reliable intelligent systems <ref type="bibr" target="#b0">[1]</ref>. We expect our results to have two consequences for academia and broader society.</p><p>Rethinking representation for OOD detection. In this paper, we demonstrate that the representation for classification (or other related tasks, measured by linear evaluation <ref type="bibr" target="#b31">[32]</ref>) can be different from the representation for OOD detection. In particular, we verify that the "hard" augmentations, thought to be harmful for contrastive representation learning <ref type="bibr" target="#b4">[5]</ref>, can be helpful for OOD detection. Our observation raises new questions for both representation learning and OOD detection: (a) representation learning researches should also report the OOD detection results as an evaluation metric, (b) OOD detection researches should more investigate the specialized representation.</p><p>Towards reliable intelligent system. The intelligent system should be robust to the potential dangers of uncertain environments (e.g., financial crisis <ref type="bibr" target="#b64">[65]</ref>) or malicious adversaries (e.g., cybersecurity <ref type="bibr" target="#b33">[34]</ref>). Detecting outliers is also related to human safety (e.g., medical diagnosis <ref type="bibr" target="#b3">[4]</ref> or autonomous driving <ref type="bibr" target="#b11">[12]</ref>), and has a broad range of industrial applications (e.g., manufacturing inspection <ref type="bibr" target="#b41">[42]</ref>). However, the system can be stuck into confirmation bias, i.e., ignore new information with a myopic perspective. We hope the system to balance the exploration and exploitation of the knowledge.</p><p>Training details. We use ResNet-18 <ref type="bibr" target="#b19">[20]</ref> as the base encoder network f θ and 2-layer multi-layer perceptron with 128 embedding dimension as the projection head g φ . All models are trained by minimizing the final loss L CSI (5) with a temperature of τ = 0.5. We follow the same optimization step of SimCLR <ref type="bibr" target="#b4">[5]</ref>. For optimization, we train CSI with 1,000 epoch under LARS optimizer <ref type="bibr" target="#b72">[72]</ref> with weight decay of 1e−6 and momentum with 0.9. For the learning rate scheduling, we use linear warmup <ref type="bibr" target="#b15">[16]</ref> for early 10 epochs until learning rate of 1.0 and decay with cosine decay schedule without a restart <ref type="bibr" target="#b40">[41]</ref>. We use batch size of 512 for both vanilla SimCLR and ours: where the batch is given by B for vanilla SimCLR and the aggregated one S∈S B S for ours. Furthermore, we use global batch normalization (BN) <ref type="bibr" target="#b26">[27]</ref>, which shares the BN parameters (mean and variance) over the GPUs in distributed training.</p><p>For supervised contrastive learning (SupCLR) <ref type="bibr" target="#b29">[30]</ref> and supervised CSI, we select the best temperature from {0.07, 0.5}: SupCLR recommend 0.07 but 0.5 was better in our experiments. For training the encoder f θ , we use the same optimization scheme as above, except using 700 for the epoch. For training the linear classifier, we train the model for 100 epochs with batch size 128, using stochastic gradient descent with momentum 0.9. The learning rate starts at 0.1 and is dropped by a factor of 10 at 60%, 75%, and 90% of the training progress.</p><p>Data augmentation details. We use SimCLR augmentations: Inception crop <ref type="bibr" target="#b63">[64]</ref>, horizontal flip, color jitter, and grayscale for random augmentations T , and rotation as shifting transformation S. The detailed description of the augmentations are as follows:</p><p>• Inception crop. Randomly crops the area of the original image with uniform distribution 0.08 to 1.0. After the crop, cropped image are resized to the original image size. • Horizontal flip. Flips the image horizontally with 50% of probability. • Color jitter. Change the hue, brightness, and saturation of the image. We transform the RGB (red, green, blue) image into an HSV (hue, saturation, value) image format and add noise to the HSV channels. We apply color jitter with 80% of probability. • Grayscale. Convert into a gray image. Randomly apply a grayscale with 20% of probability. • Rotation. We use rotation as S, the shifting transformation, {0°, 90°, 180°, 270°}. For a given batch B, we apply each rotation degree to obtain the new batch for CSI: S∈S B S . Dataset details. For one-class datasets, we train one class of CIFAR-10 <ref type="bibr" target="#b32">[33]</ref>, CIFAR-100 (superclass) <ref type="bibr" target="#b32">[33]</ref>, and ImageNet-30 <ref type="bibr" target="#b24">[25]</ref>. CIFAR-10 and CIFAR-100 consist of 50  <ref type="bibr" target="#b48">[49]</ref>, Food-101 <ref type="bibr" target="#b2">[3]</ref> without the "hotdog" class to avoid overlap, Places-365 <ref type="bibr" target="#b75">[75]</ref> with small images (256 * 256) validation set, Caltech-256 <ref type="bibr" target="#b17">[18]</ref>, and Describable Textures Dataset (DTD) <ref type="bibr" target="#b7">[8]</ref>. Here, we randomly sample 3,000 images to balance with the in-distribution test set.</p><p>Evaluation metrics. For evaluation, we measure the two metrics that each measures (a) the effectiveness of the proposed score in distinguishing in-and out-of-distribution images, (b) the confidence calibration of softmax classifier.</p><p>• Area under the receiver operating characteristic curve (AUROC). Let TP, TN, FP, and FN denote true positive, true negative, false positive and false negative, respectively. The ROC curve is a graph plotting true positive rate = TP / (TP+FN) against the false positive rate = FP / (FP+TN) by varying a threshold. • Expected calibration error (ECE). For a given test data {(x n , y n )} N n=1 , we group the predictions into M interval bins (each of size 1/M ). Let B m be the set of indices of samples whose prediction confidence falls into the interval ( m−1 M , m M ]. Then, the expected calibration error (ECE) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19]</ref> is follows: </p><formula xml:id="formula_17">ECE = M m=1 |B m | N |acc(B m ) − conf(B m )|,<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed review on related work B.1 OOD detection</head><p>Out-of-distribution (OOD) detection is a classic and essential problem in machine learning, studied under different names, e.g., novelty or anomaly detection <ref type="bibr" target="#b25">[26]</ref>. In this paper, we primarily focus on unsupervised OOD detection, which is arguably the most traditional and popular setup in the field <ref type="bibr" target="#b58">[59]</ref>. In this setting, the detector can only access in-distribution samples while required to identify unseen OOD samples. There are other settings, e.g., semi-supervised setting -the detector can access a small subset of out-of-distribution samples <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b56">57]</ref>, or supervised setting -the detector knows the target out-of-distribution, but we do not consider those settings in this paper. We remark that the unsupervised setting is the most practical and challenging scenario since there are infinitely many cases for out-of-distribution, and it is often not possible to have such external data.</p><p>Most recent works can be categorized as: (a) density-based <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b16">17]</ref>, (b) reconstruction-based <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b6">7]</ref>, (c) one-class classifier <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, and (d) self-supervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref> approaches. We note that there are more extensive literature on this topic, but we mainly focus on the recent work based on deep learning. Brief description for each method are as follows:</p><p>• Density-based methods. Density-based methods are one of the most classic and principled approaches for OOD detection. Intuitively, they directly use the likelihood of the sample as the detection score. However, recent studies reveal that the likelihood is often not the best metric -especially for deep neural networks with complex datasets <ref type="bibr" target="#b45">[46]</ref>. Several work thus proposed modified scores, e.g., typicality <ref type="bibr" target="#b46">[47]</ref>, WAIC <ref type="bibr" target="#b5">[6]</ref>, likelihood ratio <ref type="bibr" target="#b54">[55]</ref>, input complexity <ref type="bibr" target="#b60">[61]</ref>, or unnormalized likelihood (i.e., energy) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>• Reconstruction-based methods. Reconstruction-based approach is another popular line of research for OOD detection. It trains an encoder-decoder network that reconstructs the training data in an unsupervised manner. Since the network would less generalize for unseen OOD samples, they use the reconstruction loss as a detection score. Some works utilize auto-encoders <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b53">54]</ref> or generative adversarial networks <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b51">52</ref>].</p><p>• One-class classifiers. One-class classifiers are also a classic and principled approach for OOD detection. They learn a decision boundary of in-vs. out-of-distribution samples by giving some margin covering the in-distribution samples <ref type="bibr" target="#b58">[59]</ref>. Recent works have shown that the one-class classifier is effective upon the deep representation <ref type="bibr" target="#b55">[56]</ref>.</p><p>• Self-supervised methods. Self-supervised approaches are a relatively new technique based on the rich representation learned from self-supervision <ref type="bibr" target="#b13">[14]</ref>. They train a network with a pre-defined task (e.g., predict the angle of the rotated image) on the training set, and use the generalization error to detect OOD samples. Recent self-supervised approaches show outstanding results on various OOD detection benchmark datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Our work falls into (c) the self-supervised approach <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>. However, unlike prior work focusing on the self-label classification tasks (e.g., rotation <ref type="bibr" target="#b13">[14]</ref>) which trains an auxiliary classifier to predict the transformation applied to the sample, we first incorporate contrastive learning <ref type="bibr" target="#b4">[5]</ref> for OOD detection. To that end, we design a novel detection score utilizing the unique characteristic of contrastive learning, e.g., the features in the projection layer learned by cosine similarity. We also propose a novel self-supervised training scheme that further improves the representation for OOD detection. Nevertheless, we acknowledge that the prior work largely inspired our work. For instance, the classifying shifted instances loss (4) follows the form of auxiliary classifiers <ref type="bibr" target="#b24">[25]</ref>, which gives further improvement upon our novel contrasting shifted instances loss (3).</p><p>Concurrently, Winkens et al. <ref type="bibr" target="#b68">[68]</ref> and Liu and Abbeel <ref type="bibr" target="#b39">[40]</ref> report the similar observations that contrastive learning also improves the OOD detection performance of classifiers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b24">25]</ref>. Winkens et al. <ref type="bibr" target="#b68">[68]</ref> jointly train a classifier with the SimCLR <ref type="bibr" target="#b4">[5]</ref> objective and use the Mahalanobis distance <ref type="bibr" target="#b37">[38]</ref> as a detection score. Liu and Abbeel <ref type="bibr" target="#b39">[40]</ref> approximates JEM <ref type="bibr" target="#b16">[17]</ref> (a joint model of classifier and energy-based model <ref type="bibr" target="#b10">[11]</ref>) by a combination of classification and contrastive loss and use densitybased detection scores <ref type="bibr" target="#b16">[17]</ref>. In contrast to both work, we mainly focus on the unlabeled OOD setting (although we also discuss the confident-calibrated classifiers). Here, we design a novel detection score, since how to utilize the contrastive representation (which is learned in an unsupervised manner) for OOD detection have not been explored before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Confidence-calibrated classifiers</head><p>Another line of research is on confidence-calibrated classifiers <ref type="bibr" target="#b21">[22]</ref>, which relaxes the overconfidence issue of the classifiers. There are two types of calibration: (a) in-distribution calibration <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19]</ref>, that aligns the uncertainty and the actual accuracy, measured by ECE, and (b) out-of-distribution detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>, that reduces the uncertainty of OOD samples, measured by AUROC. Note that the goal of confidence-calibrated classifiers is to regularize the prediction. Hence, the softmax probability is used for all three tasks: classification, in-distribution calibration, and out-of-distribution detection. Namely, the detection score is given by the prediction confidence (or maximum softmax probability) <ref type="bibr" target="#b21">[22]</ref>. Prior works improved calibration through inference (temperature scaling) <ref type="bibr" target="#b18">[19]</ref> or training (regularize predictions of OOD samples) <ref type="bibr" target="#b36">[37]</ref> schemes, which can be jointly applied to our method. Some works design a specific detection score upon the pre-trained classifiers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref>, but they only target OOD detection, while ours also consider the in-distribution calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Self-supervised learning</head><p>Self-supervised learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> has shown remarkable success in learning representations. In particular, contrastive learning <ref type="bibr" target="#b12">[13]</ref> via instance discrimination <ref type="bibr" target="#b69">[69]</ref> show the state-of-the-art results on visual representation learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5]</ref>. However, most prior works focus on improving the downstream task performance (e.g., classification), and other advantages of self-supervised learning (e.g., uncertainty or robustness) are rarely investigated <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>. Our work, concurrent with <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b68">68]</ref>, first verifies that contrastive learning is also effective for OOD detection.</p><p>Furthermore, we find that the shifting transformations, which were known to be harmful and unused for the standard contrastive learning <ref type="bibr" target="#b4">[5]</ref>, can help OOD detection. This observation provides new considerations for selecting transformations, i.e., which transformation should be used for positive or negative <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b71">71]</ref>. Specifically, Tian et al. <ref type="bibr" target="#b65">[66]</ref> claims the optimal views (or transformations) of the positive pairs should minimize the mutual information while keeping the task-relevant information. It suggests that the shifting transformation may not contain the information for classification, but may contain OOD detection information when used for the negative pairs. Xiao et al. <ref type="bibr" target="#b71">[71]</ref> suggests a framework that automatically learns whether the transformation should be positive or negative. One could consider incorporating our principle on shifting transformation (i.e., OOD-ness); OOD detection could be another evaluation metric for the learned representations. <ref type="table" target="#tab_9">Table 8</ref> presents the confusion matrix of AUROC values of our method on one-class CIFAR-10 datasets, where bold denotes the hard pairs. The results align with the human intuition that 'car' is confused to 'ship' and 'truck', and 'cat' is confused to 'dog'. <ref type="table" target="#tab_10">Table 9</ref> presents the OOD detection results of various methods on one-class CIFAR-100 (super-class) datasets, for all 20 super-classes. Our method outperforms the prior methods for all classes. <ref type="table" target="#tab_0">Table 10</ref> presents the OOD detection results of our method on one-class ImageNet-30 dataset, for all 30 classes. Our method consistently performs well for all classes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional one-class OOD detection results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation study on random augmentation</head><p>We verify that ensembling the scores over the random augmentations T improves OOD detection. However, naïve random sampling from the entire T is often sample inefficient. We find that choosing a proper subset T control ⊂ T improves the performance for given number of samples. Specifically, we choose T control as the set of the most common samples. For example, the size of the cropping area is sampled from U[0.08, 1] for uniform distribution U during training. Since the rare samples, e.g., area near 0.08 increases the noise, we only use the samples with size (0.08 + 1)/2 = 0.54 during inference. <ref type="table" target="#tab_0">Table 11</ref> shows random sampling from the controlled set often gives improvements.  <ref type="formula" target="#formula_8">(4)</ref>, we choose the coreset for each shifting transformation S. <ref type="table" target="#tab_0">Table 12</ref> shows the results for various coreset sizes, given by a ratio from the full training samples. Keeping only a few (e.g., 1%) samples is sufficient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Ablation study on the balancing terms</head><p>We study the effects of the balancing terms λ con S , λ cls S in Section 2.3. To this end, we compare of our final loss <ref type="bibr" target="#b4">(5)</ref>, without (w/o) and with (w/) the balancing terms λ con S and λ cls S . When not using the balancing terms, we set λ con S = λ cls S = 1 for all S. We follow the experimental setup of <ref type="table" target="#tab_0">Table 1</ref>, e.g., use rotation for the shifting transformation. We run our experiments on CIFAR-10, CIFAR-100 (super-class), and ImageNet-30 datasets. <ref type="table" target="#tab_0">Table 13</ref> shows that the balancing terms gives a consistent improvement. CIFAR-10 do not show much gain since all λ con S and λ cls S show similar values; in contrast, CIFAR-100 (super-class) and ImageNet-30 show large gain since they varies much. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Combining multiple shifting transformations</head><p>We find that combining multiple shifting transformations: given two transformations S 1 and S 2 , use S 1 × S 2 as the combined shifting transformation, can give further improvements. <ref type="table" target="#tab_0">Table 14</ref> shows that combining "Noise", "Blur", and "Perm" to "Rotate" gives additional gain. We remark that one can investigate the better combination; we choose rotation for our experiments due to its simplicity. In this section, we first demonstrate the properties of the two features under vanilla SimCLR. While we use the vanilla SimCLR to validate they are general properties of SimCLR, we remark that our training scheme (see Section 2.2) further improves the discrimination power of the features. Next, we verify that cosine similarity and feature norm are complementary, that combining both features (i.e., s con (6)) give additional gain. For the latter one, we use our final training loss to match the reported values in prior experiments, but we note that the trend is consistent among the models.</p><p>First, we demonstrate the effect of cosine similarity for OOD detection. To this end, we train vanilla SimCLR using CIFAR-10 and CIFAR-100 and in-and out-of-distribution datasets. Since SimCLR attracts the same image with different augmentations, it learns to cluster similar images; hence, it shows good discrimination performance measured by linear evaluation <ref type="bibr" target="#b4">[5]</ref>. <ref type="figure" target="#fig_3">Figure 3a</ref> presents the t-SNE <ref type="bibr" target="#b42">[43]</ref> plot of the normalized features that each color denote different class. Even though SimCLR is trained in an unsupervised manner, the samples of the same classes are gathered. <ref type="figure" target="#fig_3">Figure 3b</ref> and <ref type="figure" target="#fig_3">Figure 3c</ref> presents the histogram of the cosine similarities from the nearest training sample (i.e., max m sim(z(x m ), z(x))), for training and test datasets, respectively. For the training set, we choose the second nearest sample since the nearest one is itself. One can see that training samples are concentrated, even though contrastive learning pushes the different samples. It complements the results of <ref type="figure" target="#fig_3">Figure 3a</ref>. For test sets, the in-distribution samples show a similar trend with the training samples. However, the OOD samples are farther from the training samples, which implies that the cosine similarity is an effective feature to detect OOD samples. Second, we demonstrate that the feature norm is a discriminative feature for OOD detection. Following the prior setting, we use CIFAR-10 and CIFAR-100 for in-and out-of-distribution datasets, respectively. <ref type="figure" target="#fig_4">Figure 4a</ref> shows that the discriminative power of feature norm improves as the training epoch increases. We observe that this phenomenon consistently happens over models and settings; the contrastive loss makes the norm of in-distribution samples relatively larger than OOD samples. <ref type="figure" target="#fig_4">Figure 4b</ref> shows the norm of CIFAR-10 is indeed larger than CIFAR-100, under the final model. This is somewhat unintuitive since the SimCLR uses the normalized features to compute the loss (1). To understand this phenomenon, we visualize the t-SNE <ref type="bibr" target="#b42">[43]</ref> plot of the feature space in <ref type="figure" target="#fig_4">Figure 4c</ref>, randomly choosing 100 images from both datasets. We randomly augment each image for 100 times for better visualization. One can see that in-distribution samples tend to be spread out over the large sphere, while OOD samples are gathered near center. <ref type="bibr" target="#b3">4</ref> Also, note that the same image with different augmentations are highly clustered, while in-distribution samples are slightly more assembled. <ref type="bibr" target="#b4">5</ref> We suspect that increasing the norm may be an easier way to maximize cosine similarity between two vectors: instead of directly reducing the feature distance of two augmented samples, one can also increase the overall norm of the features to reduce the relative distance of two samples. Finally, we verify that cosine similarity (sim-only) and feature norm (norm-only) are complementary: combining them (sim+norm) gives additional improvements. Here, we use the model trained by our final objective <ref type="bibr" target="#b4">(5)</ref>, and follow the inference scheme of the main experiments (see <ref type="table" target="#tab_6">Table 7</ref>). <ref type="table" target="#tab_0">Table 15</ref> shows AUROC values under sim-only, norm-only, and sim+norm scores. Using only sim or norm already shows good results, but combining them shows the best results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Rethinking OOD detection benchmarks</head><p>We find that resized LSUN and ImageNet <ref type="bibr" target="#b38">[39]</ref>, one of the most popular benchmark datasets for OOD detection, are visually far from in-distribution datasets (commonly, CIFAR <ref type="bibr" target="#b32">[33]</ref>). <ref type="figure">Figure 5</ref> shows that resized LSUN and ImageNet contain artificial noises, produced by broken image operations. <ref type="bibr" target="#b5">6</ref> It is problematic since one can detect such datasets with simple data statistics, without understanding semantics from neural networks. To progress OOD detection research one step further, one needs more hard or semantic OOD samples that cannot be easily detected by data statistics.</p><p>To verify this, we propose a simple detection score that measures the input smoothness of an image. Intuitively, noisy images would have a higher variation in input space than natural images. Formally, let x (i,j) be the i-th value of the vectorized image x ∈ R HW K . Here, we define the neighborhood N as the set of spatially connected pairs of pixel indices. Then, the total variation distance is given by</p><formula xml:id="formula_18">TV(x) = i,j∈N x (i) − x (j) 2 2 .<label>(14)</label></formula><p>Then, we define the smoothness score as the difference of total variation from the training samples: <ref type="table" target="#tab_0">Table 16</ref> shows that this simple score detects current benchmark datasets surprisingly well.</p><formula xml:id="formula_19">s smooth (x) := |TV(x) − 1 M m TV(x m )|.<label>(15)</label></formula><p>To address this issue, we construct new benchmark datasets, using a fixed resize operation 7 , hence coined LSUN (FIX) and ImageNet (FIX). For LSUN (FIX), we randomly sample 1,000 images from every ten classes of the training set of LSUN. For ImageNet (FIX), we randomly sample 10,000 images from the entire training set of ImageNet-30, excluding "airliner", "ambulance", "parkingmeter", and "schooner" classes to avoid overlapping with CIFAR-10. <ref type="bibr" target="#b7">8</ref>  <ref type="figure">Figure 6</ref> shows that the new datasets are more visually realistic than the former ones ( <ref type="figure">Figure 5)</ref>. Also, <ref type="table" target="#tab_0">Table 16</ref> shows that the fixed datasets are not detected by the simple data statistics <ref type="bibr" target="#b14">(15)</ref>. We believe our newly produced datasets would be a stronger benchmark for hard or semantic OOD detection for future researches. <ref type="figure">Figure 5</ref>: Current benchmark datasets: resized LSUN (left two) and ImageNet (right two). <ref type="figure">Figure 6</ref>: Proposed datasets: LSUN (FIX) (left two) and ImageNet (FIX) (right two). <ref type="bibr" target="#b5">6</ref> It is also reported in https://twitter.com/jaakkolehtinen/status/1258102168176951299. <ref type="bibr" target="#b6">7</ref> We use PyTorch torchvision.transforms.Resize() operation. <ref type="bibr" target="#b7">8</ref> We provide the datasets and data generation code in https://github.com/alinlab/CSI. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Additional examples of rotation-invariant images</head><p>We provide additional examples of rotation-invariant images (see <ref type="table" target="#tab_5">Table 6</ref> in Section 3.2). Those image commonly appear in real-world scenarios since many practical applications deal with non-natural images, e.g., manufacturing -steel <ref type="bibr" target="#b61">[62]</ref> or textile <ref type="bibr" target="#b59">[60]</ref> for instance, or aerial <ref type="bibr" target="#b70">[70]</ref> images. <ref type="figure" target="#fig_5">Figure 7</ref> and <ref type="figure" target="#fig_6">Figure 8</ref> visualizes the samples of manufacturing and aerial images, respectively.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of the original image and the considered shifting transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of original image and SimCLR augmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 |Bm| i∈Bm 1</head><label>11</label><figDesc>where acc(B m ) is accuracy of B m : acc(B m ) = {yi=arg maxy p(y|xi)} where 1 is indicator function and conf(B m ) is confidence of B m : conf(B m ) = 1 |Bm| i∈Bm q(x i ) where q(x i ) is the confidence of data x i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) t-SNE visualization (b) Similarities (train) (c) Similarities (test) Plots for cosine similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Plots for feature norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Examples of steel (left two) and textile (right two) images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Examples of aerial images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>AUROC (%) of various OOD detection methods trained on one-class dataset of (a) CIFAR-10, (b) CIFAR-100 (super-class), and (c) ImageNet-30. For CIFAR-10, we report the means and standard deviations of per-class AUROC averaged over five trials, and the final column indicates the mean AUROC across all the classes. For CIFAR-100 and ImaegeNet-30, we only report the mean AUROC over a single trial. Bold denotes the best results, and * denotes the values from the reference. See Appendix C for additional results, e.g., per-class AUROC on CIFAR-100 and ImageNet-<ref type="bibr" target="#b29">30</ref>. 5±0.3 94.1±0.3 81.8±0.5 72.0±0.3 83.7±0.9 84.4±0.3 82.9±0.8 93.9±0.3 92.9±0.3 89.5±0.2 85.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(a) One-class CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Network</cell><cell cols="2">Plane Car</cell><cell>Bird</cell><cell>Cat</cell><cell>Deer</cell><cell>Dog</cell><cell>Frog</cell><cell cols="2">Horse Ship</cell><cell>Truck</cell><cell>Mean</cell></row><row><cell cols="2">OC-SVM  *  [59] -</cell><cell>65.6</cell><cell>40.9</cell><cell>65.3</cell><cell>50.1</cell><cell>75.2</cell><cell>51.2</cell><cell>71.8</cell><cell>51.2</cell><cell>67.9</cell><cell>48.5</cell><cell>58.8</cell></row><row><cell cols="2">DeepSVDD  *  [56] LeNet</cell><cell>61.7</cell><cell>65.9</cell><cell>50.8</cell><cell>59.1</cell><cell>60.9</cell><cell>65.7</cell><cell>67.7</cell><cell>67.3</cell><cell>75.9</cell><cell>73.1</cell><cell>64.8</cell></row><row><cell cols="2">AnoGAN  *  [58] DCGAN</cell><cell>67.1</cell><cell>54.7</cell><cell>52.9</cell><cell>54.5</cell><cell>65.1</cell><cell>60.3</cell><cell>58.5</cell><cell>62.5</cell><cell>75.8</cell><cell>66.5</cell><cell>61.8</cell></row><row><cell>OCGAN  *  [52]</cell><cell>OCGAN</cell><cell>75.7</cell><cell>53.1</cell><cell>64.0</cell><cell>62.0</cell><cell>72.3</cell><cell>62.0</cell><cell>72.3</cell><cell>57.5</cell><cell>82.0</cell><cell>55.4</cell><cell>65.7</cell></row><row><cell>Geom  *  [15]</cell><cell cols="2">WRN-16-8 74.7</cell><cell>95.7</cell><cell>78.1</cell><cell>72.4</cell><cell>87.8</cell><cell>87.8</cell><cell>83.4</cell><cell>95.5</cell><cell>93.3</cell><cell>91.3</cell><cell>86.0</cell></row><row><cell>Rot  *  [25]</cell><cell cols="2">WRN-16-4 71.9</cell><cell>94.5</cell><cell>78.4</cell><cell>70.0</cell><cell>77.2</cell><cell>86.6</cell><cell>81.6</cell><cell>93.7</cell><cell>90.7</cell><cell>88.8</cell><cell>83.3</cell></row><row><cell cols="3">Rot+Trans  *  [25] WRN-16-4 77.5</cell><cell>96.9</cell><cell>87.3</cell><cell>80.9</cell><cell>92.7</cell><cell>90.2</cell><cell>90.9</cell><cell>96.5</cell><cell>95.2</cell><cell>93.3</cell><cell>90.1</cell></row><row><cell>GOAD  *  [2]</cell><cell cols="2">WRN-10-4 77.2</cell><cell>96.7</cell><cell>83.3</cell><cell>77.7</cell><cell>87.8</cell><cell>87.8</cell><cell>90.0</cell><cell>96.1</cell><cell>93.8</cell><cell>92.0</cell><cell>88.2</cell></row><row><cell>Rot [25]</cell><cell cols="12">ResNet-18 78.3±0.2 94.3±0.3 86.2±0.4 80.8±0.6 89.4±0.5 89.0±0.4 88.9±0.4 95.1±0.2 92.3±0.3 89.7±0.3 88.4</cell></row><row><cell cols="13">Rot+Trans [25] ResNet-18 80.4±0.3 96.4±0.2 85.9±0.3 81.1±0.5 91.3±0.3 89.6±0.3 89.9±0.3 95.9±0.1 95.0±0.1 92.6±0.2 89.8</cell></row><row><cell cols="3">GOAD [2] ResNet-18 75.Method Network</cell><cell cols="2">AUROC</cell><cell cols="2">Method</cell><cell cols="4">(c) One-class ImageNet-30 Network</cell><cell cols="2">AUROC</cell></row><row><cell cols="2">OC-SVM  *  [59] -</cell><cell></cell><cell cols="2">63.1</cell><cell cols="2">Rot  *  [25]</cell><cell></cell><cell></cell><cell cols="2">ResNet-18</cell><cell cols="2">65.3</cell></row><row><cell>Geom  *  [15]</cell><cell cols="2">WRN-16-8</cell><cell cols="2">78.7</cell><cell cols="3">Rot+Trans  *  [25]</cell><cell></cell><cell cols="2">ResNet-18</cell><cell cols="2">77.9</cell></row><row><cell>Rot [25]</cell><cell cols="2">ResNet-18</cell><cell cols="2">77.7</cell><cell cols="3">Rot+Attn  *  [25]</cell><cell></cell><cell cols="2">ResNet-18</cell><cell cols="2">81.6</cell></row><row><cell cols="3">Rot+Trans [25] ResNet-18</cell><cell cols="2">79.8</cell><cell cols="3">Rot+Trans+Attn  *  [25]</cell><cell></cell><cell cols="2">ResNet-18</cell><cell cols="2">84.8</cell></row><row><cell>GOAD [2]</cell><cell cols="2">ResNet-18</cell><cell cols="2">74.5</cell><cell cols="6">Rot+Trans+Attn+Resize  *  [25] ResNet-18</cell><cell cols="2">85.7</cell></row><row><cell>CSI (ours)</cell><cell cols="2">ResNet-18</cell><cell cols="2">89.6</cell><cell cols="2">CSI (ours)</cell><cell></cell><cell></cell><cell cols="2">ResNet-18</cell><cell cols="2">91.6</cell></row></table><note>CSI (ours) ResNet-18 89.9±0.1 99.1±0.0 93.1±0.2 86.4±0.2 93.9±0.1 93.2±0.2 95.1±0.1 98.7±0.0 97.9±0.0 95.5±0.1 94.3 (b) One-class CIFAR-100 (super-class)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>AUROC (%) of various OOD detection methods trained on unlabeled (a) CIFAR-10 and (b) ImageNet-30. The reported results are averaged over five trials, subscripts denote standard deviation, and bold denote the best results. * denotes the values from the reference.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Unlabeled CIFAR-10</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR10 →</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Network</cell><cell cols="7">SVHN LSUN ImageNet LSUN (FIX) ImageNet (FIX) CIFAR-100 Interp.</cell></row><row><cell>Likelihood  *</cell><cell cols="3">PixelCNN++ 8.3</cell><cell>-</cell><cell>64.2</cell><cell>-</cell><cell>-</cell><cell>52.6</cell><cell>52.6</cell></row><row><cell>Likelihood  *</cell><cell>Glow</cell><cell></cell><cell>8.3</cell><cell>-</cell><cell>66.3</cell><cell>-</cell><cell>-</cell><cell>58.2</cell><cell>58.2</cell></row><row><cell>Likelihood  *</cell><cell>EBM</cell><cell></cell><cell>63.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.0</cell></row><row><cell cols="4">Likelihood Ratio  *  [55] PixelCNN++ 91.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Input Complexity  *  [61] PixelCNN++ 92.9</cell><cell>-</cell><cell>58.9</cell><cell>-</cell><cell>-</cell><cell>53.5</cell><cell>-</cell></row><row><cell cols="2">Input Complexity  *  [61] Glow</cell><cell></cell><cell>95.0</cell><cell>-</cell><cell>71.6</cell><cell>-</cell><cell>-</cell><cell>73.6</cell><cell>-</cell></row><row><cell>Rot [25]</cell><cell cols="2">ResNet-18</cell><cell cols="4">97.6±0.2 89.2±0.7 90.5±0.3 77.7±0.3</cell><cell>83.2±0.1</cell><cell>79.0±0.1</cell><cell>64.0±0.3</cell></row><row><cell>Rot+Trans [25]</cell><cell cols="2">ResNet-18</cell><cell cols="4">97.8±0.2 92.8±0.9 94.2±0.7 81.6±0.4</cell><cell>86.7±0.1</cell><cell>82.3±0.2</cell><cell>68.1±0.8</cell></row><row><cell>GOAD [2]</cell><cell cols="2">ResNet-18</cell><cell cols="4">96.3±0.2 89.3±1.5 91.8±1.2 78.8±0.3</cell><cell>83.3±0.1</cell><cell>77.2±0.3</cell><cell>59.4±1.1</cell></row><row><cell>CSI (ours)</cell><cell cols="2">ResNet-18</cell><cell cols="4">99.8±0.0 97.5±0.3 97.6±0.3 90.3±0.3</cell><cell>93.3±0.1</cell><cell>89.2±0.1</cell><cell>79.3±0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Unlabeled ImageNet-30</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ImageNet-30 →</cell><cell></cell></row><row><cell>Method</cell><cell>Network</cell><cell cols="2">CUB-200</cell><cell>Dogs</cell><cell>Pets</cell><cell cols="3">Flowers Food-101 Places-365 Caltech-256</cell><cell>DTD</cell></row><row><cell>Rot [25]</cell><cell cols="6">ResNet-18 76.5±0.7 77.2±0.5 70.0±0.5 87.2±0.2 72.7±1.5</cell><cell>52.6±1.4</cell><cell>70.9±0.1</cell><cell>89.9±0.5</cell></row><row><cell cols="7">Rot+Trans [25] ResNet-18 74.5±0.5 77.8±1.1 70.0±0.8 86.3±0.3 71.6±1.4</cell><cell>53.1±1.7</cell><cell>70.0±0.2</cell><cell>89.4±0.6</cell></row><row><cell>GOAD [2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>ResNet-18 71.5±1.4 74.3±1.6 65.5±1.3 82.8±1.4 68.7±0.7 51.0±1.1 67.4±0.8 87.5±0.8 CSI (ours) ResNet-18 90.5±0.1 97.1±0.1 85.2±0.2 94.7±0.4 89.2±0.3 78.3±0.3 87.1±0.1 96.9±0.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy (%), ECE (%), and AUROC (%) of confidence-calibrated classifiers trained on labeled (a) CIFAR-10 and (b) ImageNet-30. The reported results are averaged over five trials for CIFAR-10 and one trial for ImageNet-30. Subscripts denote standard deviation, and bold denote the best results. CSI-ens denotes the ensembled prediction, i.e., 4 times slower (as we use rotation).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Labeled CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR10 →</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Train method Test acc. ECE</cell><cell cols="7">SVHN LSUN ImageNet LSUN (FIX) ImageNet (FIX) CIFAR100 Interp.</cell></row><row><cell cols="5">Cross Entropy 93.0±0.2 6.44±0.2 88.6±0.9 90.7±0.5 88.3±0.6</cell><cell cols="2">87.5±0.3</cell><cell>87.4±0.3</cell><cell cols="2">85.8±0.3 75.4±0.7</cell></row><row><cell cols="5">SupCLR [30] 93.8±0.1 5.56±0.1 97.3±0.1 92.8±0.5 91.4±1.2</cell><cell cols="2">91.6±1.5</cell><cell>90.5±0.5</cell><cell cols="2">88.6±0.2 75.7±0.1</cell></row><row><cell>CSI (ours)</cell><cell cols="4">94.8±0.1 4.40±0.1 96.5±0.2 96.3±0.5 96.2±0.4</cell><cell cols="2">92.1±0.5</cell><cell>92.4±0.0</cell><cell cols="2">90.5±0.1 78.5±0.2</cell></row><row><cell cols="5">CSI-ens (ours) 96.1±0.1 3.50±0.1 97.9±0.1 97.7±0.4 97.6±0.3</cell><cell cols="2">93.5±0.4</cell><cell>94.0±0.1</cell><cell cols="2">92.2±0.1 80.1±0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Labeled ImageNet-30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ImageNet-30 →</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Train method</cell><cell cols="9">Test acc. ECE CUB-200 Dogs Pets Flowers Food-101 Places-365 Caltech-256 DTD</cell></row><row><cell>Cross Entropy</cell><cell>94.3</cell><cell>5.08</cell><cell>88.0</cell><cell>96.7 95.0</cell><cell>89.7</cell><cell>79.8</cell><cell>90.5</cell><cell>90.6</cell><cell>90.1</cell></row><row><cell>SupCLR [30]</cell><cell>96.9</cell><cell>3.12</cell><cell>86.3</cell><cell>95.6 94.2</cell><cell>92.2</cell><cell>81.2</cell><cell>89.7</cell><cell>90.2</cell><cell>92.1</cell></row><row><cell>CSI (ours)</cell><cell>97.0</cell><cell>2.61</cell><cell>93.4</cell><cell>97.7 96.9</cell><cell>96.0</cell><cell>87.0</cell><cell>92.5</cell><cell>91.9</cell><cell>93.7</cell></row><row><cell>CSI-ens (ours)</cell><cell>97.8</cell><cell>2.19</cell><cell>94.6</cell><cell>98.3 97.4</cell><cell>96.2</cell><cell>88.9</cell><cell>94.0</cell><cell>93.2</cell><cell>97.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>OOD-ness (%), i.e., the AUROC between in-distribution vs. transformed samples under the vanilla SimCLR (see Section 2.2), of various transformations. The vanilla SimCLR is trained on one-class CIFAR-10 under ResNet-18. Each column denotes the applied transformation.</figDesc><table><row><cell></cell><cell cols="6">Cutout Sobel Noise Blur Perm Rotate</cell></row><row><cell>OOD-ness</cell><cell>79.5</cell><cell>69.2</cell><cell>74.4</cell><cell>76.0</cell><cell>83.8</cell><cell>85.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on various transformations, added or removed from the vanilla SimCLR.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">a) Add transformations</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Remove transformations</cell></row><row><cell>Base</cell><cell></cell><cell cols="6">Cutout Sobel Noise Blur Perm Rotate</cell><cell cols="3">Crop Jitter Gray</cell></row><row><cell>87.9</cell><cell>+Align +Shift</cell><cell>84.3 88.5</cell><cell>85.0 88.3</cell><cell>85.5 89.3</cell><cell>88.0 89.2</cell><cell>73.1 90.7</cell><cell>76.5 94.3</cell><cell>-Align 55.7 +Shift -</cell><cell>78.8 -</cell><cell>78.4 88.3</cell></row></table><note>"Align" and "Shift" indicates that the transformation is used as T and S, respectively. (a) We add a new transformation as an aligned (up) or shifting (down) transformations. (b) We remove (up) or convert-to-shift (down) the transformation from the vanilla SimCLR. All reported values are the mean AUROC (%) over one-class CIFAR-10, and "Base" denotes the vanilla SimCLR.(</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>a) OOD-ness</cell><cell></cell><cell>(b) AUROC</cell><cell></cell></row><row><cell>Rot. Noise</cell><cell cols="3">Base CSI(R) CSI(N)</cell></row><row><cell>50.6 75.7</cell><cell>70.3</cell><cell>65.9</cell><cell>80.1</cell></row></table><note>OOD-ness (%) and AUROC (%) on DTD, where Textile is used for OOD.(Data-dependence of shifting transformations. We re- mark that the best shifting transformation depends on the dataset. For example, consider the rotation-invariant datasets: Describable Textures Dataset (DTD) [8] and Textile [60] are in-vs. out-of-distribution, respectively (see Appendix J for more visual examples). For such datasets, rotation (Rot.) does not shift the distribution, and Gaussian noise (Noise) is more suitable transfor- mation (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on each component of our proposed (a) training objective and (b) detection score. For (a), we use the corresponding detection score for each training loss; namely, (6) to (9) for (2) to<ref type="bibr" target="#b4">(5)</ref>, respectively. For (b), we use the model trained by the final training loss<ref type="bibr" target="#b4">(5)</ref>. We measure the mean AUROC (%) values, trained under CIFAR-10 with ResNet-18. Each row indicates the corresponding equation of the given checkmarks, and bold denotes the best results. "Con.", "Cls.", and "Ensem." denotes contrast, classify, and ensemble, respectively.</figDesc><table><row><cell></cell><cell cols="2">(a) Training objective</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Detection score</cell></row><row><cell></cell><cell cols="4">SimCLR Con. Cls. AUROC</cell><cell></cell><cell cols="3">Con. Cls. Ensem. AUROC</cell></row><row><cell>LSimCLR (2)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>87.9</cell><cell>scon (6)</cell><cell>-</cell><cell>-</cell><cell>91.3</cell></row><row><cell>Lcon-SI (3)</cell><cell></cell><cell></cell><cell>-</cell><cell>91.6</cell><cell>scon-SI (7)</cell><cell>-</cell><cell></cell><cell>93.3</cell></row><row><cell>Lcls-SI (4)</cell><cell>-</cell><cell>-</cell><cell></cell><cell>88.6</cell><cell>scls-SI (8)</cell><cell>-</cell><cell></cell><cell>93.8</cell></row><row><cell>LCSI (5)</cell><cell></cell><cell></cell><cell></cell><cell>94.3</cell><cell>sCSI (9)</cell><cell></cell><cell></cell><cell>94.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This work was supported by Institute of Information &amp; Communications TechnologyPlanning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST) and No.2017-0-01779, A machine learning and statistical inference framework for explainable artificial intelligence). We thank Sihyun Yu, Chaewon Kim, Hyuntak Cha, Hyunwoo Kang, and Seunghyun Lee for helpful feedback and suggestions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>,000 training and 10,000 test images with 10 and 20 (super-class) image classes, respectively. ImageNet-30 contains 39,000 training and 3,000 test images with 30 image classes.For unlabeled and labeled multi-class datasets, we train ResNet with CIFAR-10 and ImageNet-30. For CIFAR-10, out-of-distribution (OOD) samples are as follows: SVHN<ref type="bibr" target="#b47">[48]</ref> consists of 26,032 test images with 10 digits, resized LSUN<ref type="bibr" target="#b38">[39]</ref> consists of 10,000 test images of 10 different scenes, resized ImageNet<ref type="bibr" target="#b38">[39]</ref> consists of 10,000 test images with 200 images classes from a subset of full ImageNet dataset, Interp. consists of 10,000 test images of linear interpolation of CIFAR-10 test images, and LSUN (FIX), ImageNet (FIX) consists of 10,000 test images, respectively with following details in Appendix I. For multi-class ImageNet-30, OOD samples are as follows: CUB-200<ref type="bibr" target="#b66">[67]</ref>, Stanford Dogs<ref type="bibr" target="#b28">[29]</ref>, Oxford Pets<ref type="bibr" target="#b50">[51]</ref>, Oxford Flowers</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Confusion matrix of AUROC (%) values of our method on one-class CIFAR-10. The row and column indicates the in-distribution and OOD class, respectively, and the final column indicates the mean value. Bold denotes the values under 80%, which implies the hard pair.</figDesc><table><row><cell></cell><cell cols="3">Plane Car Bird</cell><cell>Cat</cell><cell cols="5">Deer Dog Frog Horse Ship Truck Mean</cell></row><row><cell>Plane</cell><cell>-</cell><cell cols="6">74.1 95.8 98.4 94.9 98.0 96.2</cell><cell>90.1</cell><cell>79.6</cell><cell>82.8</cell><cell>90.0</cell></row><row><cell>Car</cell><cell>99.3</cell><cell>-</cell><cell cols="5">99.9 99.9 99.8 99.9 99.8</cell><cell>99.7</cell><cell>98.7</cell><cell>95.0</cell><cell>99.1</cell></row><row><cell>Bird</cell><cell>91.1</cell><cell>97.5</cell><cell>-</cell><cell cols="4">97.3 87.0 92.5 96.1</cell><cell>83.2</cell><cell>96.4</cell><cell>98.0</cell><cell>93.2</cell></row><row><cell>Cat</cell><cell>91.9</cell><cell cols="2">91.5 90.3</cell><cell>-</cell><cell cols="3">83.3 67.0 89.6</cell><cell>79.0</cell><cell>92.8</cell><cell>91.9</cell><cell>86.4</cell></row><row><cell>Deer</cell><cell>95.7</cell><cell cols="3">98.4 94.9 96.6</cell><cell>-</cell><cell cols="2">94.7 98.7</cell><cell>69.0</cell><cell>97.4</cell><cell>98.8</cell><cell>93.8</cell></row><row><cell>Dog</cell><cell>97.9</cell><cell cols="4">98.5 95.5 90.3 88.1</cell><cell>-</cell><cell>96.8</cell><cell>76.6</cell><cell>98.6</cell><cell>98.3</cell><cell>93.4</cell></row><row><cell>Frog</cell><cell>93.6</cell><cell cols="5">92.3 94.6 96.1 96.8 96.3</cell><cell>-</cell><cell>95.2</cell><cell>94.4</cell><cell>97.3</cell><cell>95.2</cell></row><row><cell>Horse</cell><cell>99.3</cell><cell cols="6">99.5 99.0 99.3 94.2 97.4 99.8</cell><cell>-</cell><cell>99.7</cell><cell>99.4</cell><cell>98.6</cell></row><row><cell>Ship</cell><cell>96.6</cell><cell cols="6">91.2 99.5 99.7 99.4 99.7 99.5</cell><cell>99.3</cell><cell>-</cell><cell>96.6</cell><cell>97.9</cell></row><row><cell>Truck</cell><cell>96.2</cell><cell cols="6">72.3 99.4 99.5 99.1 99.4 98.7</cell><cell>98.3</cell><cell>96.2</cell><cell>-</cell><cell>95.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>AUROC (%) values of various OOD detection methods trained on one-class CIFAR-100 (super-class). Each row indicates the results of the selected super-class, and the final row indicates the mean value. * denotes the values from the reference, and bold denotes the best results.</figDesc><table><row><cell>Rot Rot+Trans GOAD CSI (ours)</cell></row></table><note>OC-SVM* DAGMM* DSEBM* ADGAN* Geom*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>AUROC (%) values of our method trained on one-class ImageNet-30. The first and third row indicates the selected class, and the second and firth row indicates the corresponding results.</figDesc><table><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell></row><row><cell cols="15">85.9 99.0 99.8 90.5 95.8 99.2 96.6 83.5 92.2 84.3 99.0 94.5 97.1 87.7 96.4</cell></row><row><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>25</cell><cell>26</cell><cell>27</cell><cell>28</cell><cell>29</cell></row><row><cell cols="15">84.7 99.7 75.6 95.2 73.8 94.7 95.2 99.2 98.5 82.5 89.7 82.1 97.2 82.1 97.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>AUROC (%) values of our method for different number of random augmentations, under one-class (OC-) CIFAR-10 and CIFAR-100 (super-class). The values are averaged over classes. Random augmentations over the controlled set show the best performance.One can reduce the computation and memory cost of the contrastive score (6) by selecting a proper subset, i.e., coreset, of the training samples. To this end, we run K-means clustering<ref type="bibr" target="#b43">[44]</ref> on the normalized features W</figDesc><table><row><cell cols="4"># of samples Controlled OC-CIFAR-10 OC-CIFAR-100</cell></row><row><cell>4</cell><cell>-</cell><cell>92.22</cell><cell>87.36</cell></row><row><cell>40</cell><cell>-</cell><cell>94.13</cell><cell>89.51</cell></row><row><cell>40</cell><cell></cell><cell>94.31</cell><cell>89.55</cell></row><row><cell cols="3">E Efficient computation of (6) via coreset</cell><cell></cell></row></table><note>m := z(x m )/ z(x m ) using cosine similarity as a metric. Then, we use the center of each cluster as the coreset. For contrasting shifted instances</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>AUROC (%) values of our method for various corset sizes (% of training samples), under one-class (OC-) CIFAR-10, CIFAR-100 (super-class), and ImageNet-30. The values are averaged over classes. Keeping only a few (e.g., 1%) samples shows sufficiently good results.</figDesc><table><row><cell cols="4">Coreset (%) OC-CIFAR-10 OC-CIFAR-100 OC-ImageNet-30</cell></row><row><cell>1%</cell><cell>94.22</cell><cell>89.27</cell><cell>91.06</cell></row><row><cell>10%</cell><cell>94.30</cell><cell>89.46</cell><cell>91.51</cell></row><row><cell>100%</cell><cell>94.31</cell><cell>89.55</cell><cell>91.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>AUROC (%) values of our method without (w/o) and with (w/) balancing terms, under one-class (OC-) CIFAR-10, CIFAR-100 (super-class), and ImageNet-30. The values are averaged over classes, and bold denotes the best results. Balancing terms give consistent improvements.</figDesc><table><row><cell></cell><cell cols="3">OC-CIFAR-10 OC-CIFAR-100 OC-ImageNet-30</cell></row><row><cell>CSI (w/o balancing)</cell><cell>94.28</cell><cell>89.00</cell><cell>91.04</cell></row><row><cell>CSI (w/ balancing)</cell><cell>94.31</cell><cell>89.55</cell><cell>91.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>AUROC (%) values of our method under various shifting transformations. Combining "Noise", "Blur", and "Perm" to "Rotate" gives additional gain. Discussion on the features of the contrastive score<ref type="bibr" target="#b5">(6)</ref> We find that the two features: a) the cosine similarity to the nearest training sample in {x m }, i.e., max m sim(z(x m ), z(x)), and (b) the feature norm of the representation, i.e., z(x) , are important features for detecting OOD samples under the SimCLR representation.</figDesc><table><row><cell>Base</cell><cell>Noise</cell><cell>Blur</cell><cell cols="3">Perm Rotate Rotate+Noise Rotate+Blur Rotate+Perm</cell></row><row><cell cols="4">AUROC 87.89 89.29 89.15 90.68</cell><cell>94.31</cell><cell>94.65</cell><cell>94.66</cell><cell>94.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 :</head><label>15</label><figDesc>AUROC (%) values for sim-only, norm-only, and sim+norm (i.e., contrastive (6)) scores, under one-class (OC-) CIFAR-10, CIFAR-100 (super-class), and ImageNet-30. The values are averaged over classes. Using both sim and norm features shows the best results.</figDesc><table><row><cell></cell><cell cols="3">OC-CIFAR-10 OC-CIFAR-100 OC-ImageNet-30</cell></row><row><cell>Sim-only</cell><cell>90.12</cell><cell>86.57</cell><cell>83.18</cell></row><row><cell>Norm-only</cell><cell>92.70</cell><cell>87.71</cell><cell>88.56</cell></row><row><cell>Sim+Norm</cell><cell>93.32</cell><cell>88.79</cell><cell>89.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 :</head><label>16</label><figDesc>AUROC (%) values using the smoothness score<ref type="bibr" target="#b14">(15)</ref>, under unlabeled CIFAR-10. Bold denotes the values over 80%, which implies the dataset is easily detected. CIFAR10 → SVHN LSUN ImageNet LSUN (FIX) ImageNet (FIX) CIFAR-100 Interp.</figDesc><table><row><cell>85.88</cell><cell>95.70</cell><cell>90.53</cell><cell>44.13</cell><cell>52.76</cell><cell>52.14</cell><cell>66.17</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We do not compare with methods using external OOD samples<ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b56">57]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also have tried contrasting external OOD samples similarly to<ref type="bibr" target="#b23">[24]</ref>; however, we find that naïvely using them in our framework degrade the performance. This is because the contrastive loss also discriminates within external OOD samples, which is unnecessary and an additional learning burden for our purpose.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">t-SNE plot does not tell the true behavior of the original feature space, but it may give some intuition.<ref type="bibr" target="#b4">5</ref> We also try the local variance of the norm as a detection score. It also works well, but the norm is better.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in ai safety</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<title level="m">Waic, but why? generative ensembles for robust anomaly detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Novelty detection via blurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A framework for contrastive self-supervised learning and designing a new approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00104</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey of outlier detection methodologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Design of an image edge detection filter using the sobel operator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasanthavada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of solid-state circuits</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial self-supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anomaly detection of web-based attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kruegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on Computer and communications security</title>
		<meeting>the 10th ACM conference on Computer and communications security</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised label augmentation via input transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Confident multiple choice learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09070</idno>
		<title level="m">Hybrid discriminative-generative training via contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Smart factory-a step towards the next generation of manufacturing. Manufacturing Systems and Technologies for the New Frontier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Westkämper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Detecting out-of-distribution inputs to deep generative models using a test for typicality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02994</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A comprehensive survey of data mining-based fraud detection research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gayler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1009.6119</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generative probabilistic novelty detection with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Almohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz-Mirbach</surname></persName>
		</author>
		<ptr target="https://lmb.informatik.uni-freiburg.de/resources/datasets/tilda.en.html" />
		<title level="m">Tilda-ein referenzdatensatz zur evaluierung von sichtprüfungsverfahren für textiloberflächen. Interner Bericht</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Input complexity and out-of-distribution detection with likelihood-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Severstal: Steel defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Severstal</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/severstal-steel-defect-detection" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Curl: Contrastive unsupervised representations for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A black swan in the money market</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Journal: Macroeconomics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD Birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Contrastive training for improved out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05659</idno>
		<title level="m">What should not be contrastive in contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<title level="m">Large batch training of convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Regularizing class-wise predictions via self-knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
