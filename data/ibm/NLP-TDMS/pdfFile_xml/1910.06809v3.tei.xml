<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
							<email>xihuiliu@ee.cuhk.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
							<email>shaojing@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, generative adversarial networks (GAN) <ref type="bibr" target="#b5">[6]</ref> have shown stunning results in generating photorealistic images of faces <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and simple objects <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref>. However, generating photorealistic images for complex scenes with different types of objects and stuff remains a challenging problem. We consider semantic image synthesis, which aims at generating photorealistic images conditioned on semantic layouts. It has wide applications on controllable image synthesis and interactive image manipulation. State-of-the-art methods are mostly based on Generative Adversarial Networks (GAN).</p><p>A fundamental question to semantic image synthesis is how to exploit the semantic layout information in the generator. Most previous GAN-based approaches feed the label maps as inputs, and generate images by an encoder-decoder network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref>. Nonetheless, since the semantic label maps are only fed into the network once at the input layer, the layout information cannot be well preserved in Semantic Label Map pix2pixHD <ref type="bibr" target="#b29">[30]</ref> SPADE <ref type="bibr" target="#b25">[26]</ref> Ours <ref type="figure">Figure 1</ref>: Semantic image synthesis results by previous approaches and our approach. Best viewed in color. Zoom in for details. Key differences are highlighted by red boxes.</p><p>the generator. To mitigate the problem, SPADE <ref type="bibr" target="#b24">[25]</ref> uses the label maps to predict spatially-adaptive affine transformations for modulating the activations in normalization layers. However, such feature modulation by simple affine transformations is limited in representational power and flexibility.</p><p>On the other hand, we rethink the functionality of convolutional layers for image synthesis. In a generation network, each convolutional layer learns "how to draw" by generating fine features at each location based on a local neighborhood of input features. The same translation-invariant convolutional kernels are applied to all samples and at all spatial locations, irrespective of different semantic labels at different locations, as well as the unique semantic layout of each sample. Our argument is that different convolutional kernels should be used for generating different objects or stuff.</p><p>Motivated by the two aforementioned aspects, we propose to predict spatially-varying conditional convolution kernels based on the input semantic layout, so that the layout information can more explicitly and effectively control the image generation process. However, naively predicting all convolutional kernels is infeasible, because it requires a large amount of learnable parameters, which causes overfitting and requires too much GPU memory. Inspired by recent works on lightweight convolutional neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>, we propose to predict the depthwise separable convolution, which factorizes a convolutional operation into a conditional depthwise convolution and a conventional pointwise convolution (i.e. 1Ã—1 convolution). The conditional kernel weights for each spatial location are predicted from the semantic layout by a global-context-aware weight prediction network. Our proposed conditional convolution enables the semantic layout to better control the generation process, without a heavy increase in network parameters and computational cost.</p><p>Most existing methods for semantic image synthesis adopt a multi-scale PatchGAN discriminator <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25]</ref>, but its limited representation power cannot match the increased capacity of the generator. We believe that a robust discriminator should focus on two indispensable and complementary aspects of the images: high-fidelity details, and semantic alignment with the input layout map. Motivated by the two principles, we propose to utilize multi-scale feature pyramids for promoting high-fidelity details such as texture and edges, and exploit patch-based semantic-embeddings to enhance the spatial semantic alignment between the generated images and the input semantic layout.</p><p>The contribution of this paper are summarized as follows. (1) We propose a novel approach for semantic image synthesis by learning to predict layout-to-image conditional convolution kernels based on the semantic layout. Such conditional convolution operations enable the semantic layout to adaptively control the generation process based on distinct semantic labels at different locations.</p><p>(2) We propose a feature pyramid semantics-embedding discriminator which is more effective in encouraging high-fidelity details and semantic alignment with the input layout map. (3) With the proposed approach CC-FPSE, we achieve state-of-the-art results on CityScapes, COCO-Stuff, and ADE20K datasets, demonstrating the effectiveness of our approach in generating images with complex scenes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generative adversarial networks (GAN) <ref type="bibr" target="#b5">[6]</ref> has made great success in image synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>. Conditional GANs synthesize images based on given conditions, which can be labels <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b0">1]</ref>, sentence descriptions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31]</ref>, or semantic layout in our task. Our work is also related to image-to-image translation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>, which translates a possible representation of an image into another representation.</p><p>Semantic image synthesis aims at synthesizing photorealistic images given the semantic layout. Pix2pix <ref type="bibr" target="#b12">[13]</ref> adopted an encoder-decoder generator which takes semantic label maps as inputs. Pix2pixHD <ref type="bibr" target="#b28">[29]</ref> proposed a coarse-to-fine generator and multi-scale discriminators to generate high-resolution images. SPADE <ref type="bibr" target="#b24">[25]</ref> used the semantic label maps to predict affine transformation parameters for modulating the activations in normalization layers. Besides GAN-based approaches, CRN <ref type="bibr" target="#b2">[3]</ref> used a cascaded refinement network with regression loss as training supervisions. SIMS <ref type="bibr" target="#b25">[26]</ref> developed a semi-parametric approach, by retrieving fragments and refining them with a refinement network. Our method differ from previous GAN-based approaches in how the semantic layout information controls the generation process. We propose to predict spatially-varying convolutional kernels conditioned on the semantic layout, so that it can explicitly control the generation process.</p><p>Dynamic filter networks <ref type="bibr" target="#b13">[14]</ref> was the first attempt to generate dynamic filters based on the inputs. Ha et al. <ref type="bibr" target="#b6">[7]</ref> proposed HyperNetworks, where a hyper-network is used to generate weights for another network. This idea has been applied to different applications such as neural style transfer <ref type="bibr" target="#b26">[27]</ref>, superresolution <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12]</ref>, image segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>, motion prediction <ref type="bibr" target="#b31">[32]</ref> and tracking <ref type="bibr" target="#b18">[19]</ref>. However, most of them only predicted a limited number of filters, and it would be computation and memory extensive if we use dynamically predicted filters in each layer. Su et al. <ref type="bibr" target="#b27">[28]</ref> proposed pixel-adaptive CNN which multiplies the conventional convolutional filter with a spatially-varying kernel to obtain convolutional kernels. Zhao et al. <ref type="bibr" target="#b34">[35]</ref> adopted a shared filter bank and predict adaptive weights to linearly combine the basis filters. Such operations are still based on the conventional convolutions.So the input information has limited capacity in controlling or influencing the adaptive convolutional kernels, and the behaviors of the generation networks were still dominated by the conventional convolutional kernels. Our approach differs from previous work in several aspects. Firstly, we predict the convolutional kernels conditioned on the layout information, so that the conditional information can explicitly control the generation process. Secondly, we reduce the computation and memory costs by introducing depthwise separable convolutions, while enable the conditional information to control the generation process by directly predicting the convolutional kernel weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose a novel approach for semantic image synthesis with conditional generative adversarial networks. The proposed framework, CC-FPSE, is composed of a novel generator G with conditional convolutions predicted by the weight prediction network, and a feature-pyramid semantics-embedding discriminator D as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (right). The proposed generator G is able to fully utilize the semantic layout information to control the image generation process by predicting the convolution kernels in multiple layers of the generation network with limited computational resources. The proposed discriminator D is able to supervise the generation of fine details and forces the spatial alignment between the generated image and the input semantic layout by embedding both images and label maps into a joint feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning to Predict Conditional Convolutions for Image Generator</head><p>Our proposed generator G takes a low-resolution noise map as input. It alternatively uses the proposed conditional convolution blocks <ref type="bibr" target="#b8">[9]</ref> and upsampling layers to gradually refine the intermediate feature maps and eventually generate the output image. In conventional convolution layers, the same convolution kernels are applied to all samples and at all spatial locations regardless of their distinct semantic layout. We argue that such convolution operation is not flexible and effective enough for semantic image synthesis. In semantic image synthesis, the convolution layers gradually generate refined features at each location given the coarse features in a local neighborhood. Since different objects or stuff should be generated differently, we would like the convolution layer to be aware of the unique semantic label at the target location.</p><p>In order to better incorporate the layout information into the image generation process, we propose to predict convolutional kernel weights based on the semantic layout. Given the input feature map X âˆˆ R CÃ—HÃ—W , we aim to produce the output feature map Y âˆˆ R DÃ—HÃ—W by a convolution layer with kernel size k Ã— k. We adopt a weight prediction network that takes the semantic label map as input and outputs the predicted convolutional kernel weights for each conditional convolution layer. However, naively predicting all the kernel weights causes excessive computational costs and GPU memory usage. To solve the problem, we factorize the convolutional layer into depthwise convolution and pointwise convolution, and only predict the weights of the lightweight depthwise convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Efficient Conditional Convolution Blocks for Image Generation</head><p>A conventional convolution kernel has D Ã— C Ã— k Ã— k weight parameters. A naive solution for generating the spatially-varying convolution kernel needs to predict D Ã— C Ã— k Ã— k Ã— H Ã— W weight parameters. This is impractical because the convolution operation is the basic building blocks of the generator G and would be stacked for multiple times in the generator. Such a network is not only computation and memory intensive, but also prone to overfit the training data.</p><p>To solve the problem, we introduce depthwise separable convolution <ref type="bibr" target="#b3">[4]</ref> and only predict the depthwise convolutional kernel weights, which substantially reduces the number of parameters to predict. In particular, we factorize the convolutional kernel into a conditional depthwise convolution and a conventional pointwise convolution (i.e., 1 Ã— 1 convolution). The conditional depthwise convolution performs spatial filtering over each input channel independently, and its spatially-varying kernel weights are dynamically predicted based on the semantic layout. The predicted weights for the conditional convolution layer are denoted as V âˆˆ R CÃ—kÃ—kÃ—HÃ—W , and the output feature maps are denoted as Y âˆˆ R CÃ—HÃ—W . The conditional depthwise convolution is formulated as,</p><formula xml:id="formula_0">Y c,i,j = kâˆ’1 m=0 kâˆ’1 n=0 X c,i+m,j+n V c,m,n,i,j ,<label>(1)</label></formula><p>where i, j denotes the spatial coordinates of the feature maps, k denotes the convolution kernel size, and c denotes the channel index. The C Ã— H Ã— W convolutional kernels in V with kernel size k Ã— k operates at each channel and each spatial location of X independently to generate output feature maps. Then we exploit a conventional pointwise convolution (1 Ã— 1 convolution) to map the C input channels to D output channels, and the output is denoted as Y âˆˆ R DÃ—HÃ—W .</p><p>In addition, we also propose a conditional attention operation to gate the information flow passed to the next layer. The conditional attention weights are predicted in the same way as the conditional convolution kernels, which will be detailed later. An element-wise product between the predicted attention weights A âˆˆ R CÃ—HÃ—W and the convolution output Y produces the gated feature maps,</p><formula xml:id="formula_1">Z c,i,j = Y c,i,j A c,i,j ,<label>(2)</label></formula><p>where c is the channel index and i, j denotes the spatial location in the feature maps.</p><p>The size of predicted parameters in the conditional convolution and the conditional attention are C Ã— k Ã— k Ã— H Ã— W (k = 3 in our implementation) and C Ã— H Ã— W , respectively. The parameter size is reduced by D times compared to directly predicting the whole convolutional kernel weights.</p><p>By predicting unique convolutional kernel weights for each spatial location, the image generation process becomes more flexible and adaptive to the semantic layout conditions. In the meantime, we keep an affordable parameter size and computational cost by introducing the depthwise separable convolutions. We define a ResBlock-like structure, named Conditional Convolution Block, with the operations introduced above. As shown in <ref type="figure" target="#fig_0">Figure 2</ref> (left), it includes a conventional batch normalization layer, a conditional depthwise convolution with k = 3, a conventional pointwise convolution, followed by a conditional attention layer, and finally the the non-linear activation layer. There are also identity additive skip connections for evert two such blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Conditional Weight Prediction and Overall Generator Structure</head><p>The conditional weight prediction network predicts the conditional weights V given the input semantic layout. A simple design of the weight prediction network would be simply stacking multiple convolutional layers. In SPADE <ref type="bibr" target="#b24">[25]</ref>, two convolutional layers of kernel size 3 Ã— 3 are applied to the downsampled semantic label map to generate the adaptive scale and bias for their proposed adaptive normalization layer. But downsampling a semantic label map to a very small size, e.g., 8 Ã— 8, by nearest neighbor interpolation will inevitably lose much useful information. Moreover, such a structure only has a receptive field of 5 Ã— 5, which restricts the weight prediction from incorporating long-range context information. If there is a large area of the same semantic label, pixels inside this area can only access a 5 Ã— 5 local neighborhood with identical semantic labels. So they will be processed by identical predicted weights, regardless of their relative positions inside the object or stuff.</p><p>Therefore, we design a global-context-aware weight prediction network with a feature pyramid structure <ref type="bibr" target="#b19">[20]</ref>. The architecture of our weight prediction network is shown in <ref type="figure" target="#fig_0">Figure 2</ref> (right). The label map is first downsampled through the layout encoder, and then upsampled by the decoder with lateral connections from the encoder. The features at different levels of the feature pyramid are concatenated with the original semantic map to obtain the global-context-aware semantic feature maps, which are used to predict the conditional convolution weights and conditional attention weights separately. We use two convolutional layers to predict the conditional convolution weights. To predict the conditional attention weights, we adopt two convolutional layers and a Sigmoid activation layer.</p><p>With the encoder-decoder structure of the weight prediction network, our predicted weights are aware of not only the local neighborhood, but also long-range context and relative locations.</p><p>The overall generator network G is built of a series of Conditional Convolution Blocks and upsampling layers, with conditional weights predicted by the weight prediction network .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Pyramid Semantics-Embedding Discriminator</head><p>We believe that a good discriminator should focus on two indispensable and complementary aspects: high-fidelity details such as texture and edges, and semantic alignment with the input semantic map.</p><p>Existing methods for semantic image synthesis apply a multi-scale PatchGAN discriminator <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25]</ref>, where images concatenated with the semantic label maps are scaled to multiple resolutions and fed into different discriminators with identical structure. But it still struggles to discriminate the fine details, and does not pose strong constraints on the spatial semantic alignment between the generated image and the input label map.</p><p>Motivated by the aforementioned two design principles of discriminators, we propose a more effective design for the discriminator D. We create multi-scale feature pyramids for promoting high-fidelity details such as texture and edges and exploit a semantics-embedding discriminator to force the spatial semantic alignment between the generated images and the input semantic layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Feature Pyramid Discriminator</head><p>Current image generation methods tend to generate images with blurry edges, textures and obvious artifacts. This problem suggests that we should cast more attention on low-level details when designing the discriminator architectures. On the other hand, the discriminator should also have a global view of the high-level semantics. The previously introduced multi-scale PatchGAN discriminator <ref type="bibr" target="#b28">[29]</ref> attempts to balance large receptive field and fine details by multiple discriminators at different scales. The same image at different scales are independently fed into different discriminators, leading to increased network parameters, memory footprint and computational cost.</p><p>Inspired by the evolution from image pyramids to feature pyramids <ref type="bibr" target="#b19">[20]</ref>, we propose a single feature pyramid discriminator to produce a multi-scale feature representation with both global semantics and low-level texture and edge information. As shown in <ref type="figure">Figure.</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Semantic Embeddings for Discriminator</head><p>In the conventional discriminators for semantic image synthesis, an image and its corresponding semantic label map is concatenated and fed into the discriminator as its inputs. However, there is no guarantee that the discriminator makes use of the label maps for distinguishing real/fake images. In other words, the discriminator could satisfy the training constraints by only discriminating whether an image is real or not, without considering whether it matches well with the label map. Inspired by projection discriminator <ref type="bibr" target="#b23">[24]</ref> which computes the dot product between the class label and image feature vector as part of the output discriminator score, we adapt this idea to our scenerio where the condition is the spatial label map. In order to encourage the semantic alignment between generated images and the conditional semantic layout, we propose a patch-based semantics embedding discriminator.</p><p>Our discriminator takes only the real or generated images as inputs, and produces a set of feature pyramids {F 1 , F 2 , F 3 } at different scales. F i âˆˆ R CÃ—NiÃ—Ni (i âˆˆ {1, 2, 3}) denotes feature maps at a spatial resolution of N i Ã— N i with C channels. The feature vector at each spatial location of F i represents a patch in the original image. The conventional PatchGAN discriminator tries to classify if each patch is real or not, by predicting a score for each spatial location in the feature map F i . While we force the discriminator to classify not only real or fake images, but also whether the patch features match with the semantic labels in that patch within a joint embedding space.</p><p>We downsample the label map to the same spatial resolution as F i , and embed the one-hot label at each spatial location into a C-dimensional vector. The embedded semantic layout is denoted as S i âˆˆ R CÃ—NiÃ—Ni . We calculate the inner product between each spatial location of F i and S i , to obtain a semantic matching score map, where each value represents the semantic alignment score of the corresponding patch in the original image. The semantic matching score is added with the conventional real/fake score as the final discriminator score. In this way, not only does the discriminator guide the generator to generate high-fidelity images, but also it drives the generated images to be better semantically aligned with the conditional semantic layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Functions and Training Scheme</head><p>The generator and the discriminator of our network are trained alternatively, where the discriminator adopts the hinge loss for distinguishing real/fake images while the generator is optimized with multiple losses, including the hinge-based adversarial loss, discriminator feature matching loss, and perceptual loss, following previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25]</ref>,</p><formula xml:id="formula_2">L D = âˆ’E (x,y) [min(0, âˆ’1 + D(x, y))] âˆ’ E z,y [min(0, âˆ’1 âˆ’ D(G(z, y), y))],<label>(3)</label></formula><formula xml:id="formula_3">L G = âˆ’E (z,y) D(G(z, y), y) + Î» P E (z,y) L P (G(z, y), x) + Î» F M E (z,y) L F M (G(z, y), x), (4)</formula><p>where x is a real image, y is the semantic label map, and z is the input noise map. L P <ref type="figure">(G(z, y)</ref>, x) denotes the perceptual loss, which matches the VGG extracted features between the generated images and the original images. L F M (G(z, y), x) denotes the discriminator feature matching loss, which matches the discriminator intermidiate features between the generated images and the original images. Î» P and Î» F M denote the weights for the perceptual loss and feature matching loss, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We experiment on Cityscapes <ref type="bibr" target="#b4">[5]</ref>, COCO-Stuff <ref type="bibr" target="#b1">[2]</ref>, and ADE20K <ref type="bibr" target="#b35">[36]</ref> datasets. The Cityscapes dataset has 3,000 training images and 500 validation images of urban street scenes. COCO-Stuff is the most challenging dataset, containing 118,000 training images and 5,000 validation images from complex scenes. ADE20K dataset provides 20,000 training images and 2,000 validation images from both outdoor and indoor scenes. All images are annotated with semantic segmentation masks.</p><p>We evaluate our approach from three aspects. We firstly compare synthesized images by our approach and previous approaches, and conduct a human perceptual evaluation to compare the visual quality of the generated images. We then evaluate the segmentation performance of the generated images using a segmentation model pretrained on the original datasets. We use the same segmentation models as those in <ref type="bibr" target="#b24">[25]</ref> for testing. The segmentation performance is measured by mean Intersection-over-Union (mIOU) and pixel accuracy. Finally, we calculate the distribution distances between the generated images and real images by the FrÃ©chet Inception Distance (FID) <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The training and generated image resolution is 256 Ã— 256 for COCO-Stuff and ADE20K datasets, and 256 Ã— 512 for Cityscapes dataset. For the generator, synchronized batch normalization between different GPUs is adopted for better estimating the batch statistics. For the discriminator, we utilize instance normalization. We use Leaky ReLU activations, to avoid sparse gradients caused by ReLU activation. We adopt ADAM <ref type="bibr" target="#b17">[18]</ref> optimizer with learning rate 0.0001 for the generator and 0.0004 for the discriminator. The weights for the perceptual loss Î» P is 10 and the weight discriminator feature matching loss Î» F M is 20. Following <ref type="bibr" target="#b24">[25]</ref>, to enable multi-modal synthesis and style-guided synthesis, we apply a style encoder and a KL-Divergence loss with loss weight 0.05. Our models are trained on 16 TITANX GPUs, with a batch size of 32. We train 200 epochs for Cityscapes and ADE20K datasets, and 100 epochs for COCO-Stuff dataset. Code is available at https://github.com/xh-liu/CC-FPSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Results and Human Perceptual Evaluation</head><p>We compare our results with previous approaches pix2pixHD <ref type="bibr" target="#b28">[29]</ref> and SPADE <ref type="bibr" target="#b24">[25]</ref>, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The images generated by our approach show significant improvement over previous approaches for challenging scenes. They have finer details such as edges and textures, and less artifacts, and matches better with the input semantic layout. <ref type="figure" target="#fig_3">Figure 4</ref> shows more images generated by our proposed approach. More results and comparisons are provided in the supplementary material.</p><p>We also conduct a human perception evaluation to compare the generated image quality between our method and the previous state-of-the-art method, SPADE <ref type="bibr" target="#b24">[25]</ref>. In particular, we randomly sample 500 semantic label maps from the validation set of each dataset. At each experiment, the worker is shown a semantic label map with two generated images by our approach and SPADE, respectively. The worker is required to choose an image with higher quality that matches better with the semantic layout. We found that in Cityscapes, COCO-Stuff, and ADE20K datasets respectively, 55%, 76%, and 61% images generated by our method is preferred compared to SPADE. The human perceptual evaluation validates that our approach is able to generate higher-fidelity images that are better spatially aligned with the semantic layout. <ref type="table" target="#tab_2">Table 1</ref> shows the segmentation performance and FID scores of results by our approach and those by previous approaches. CRN <ref type="bibr" target="#b2">[3]</ref> uses cascaded refinement networks with regression loss, without using GAN for training. SIMS is a semi-parametric approach which retrieves reference segments from a memory bank and refines the canvas by a refinement network. Both pix2pixHD <ref type="bibr" target="#b28">[29]</ref> and SPADE <ref type="bibr" target="#b24">[25]</ref> are GAN-based approaches. Pix2pixHD takes the semantic label map as the generator input, and uses a multi-scale generator and multi-scale discriminator to generate high-resolution images. SPADE takes a noise vector as input, and the semantic label map are used for modulating the activations in normalization layers by learned affine transformations. Our approach performs consistently better than previous approaches, which demonstrate the effectiveness of the propose approach. Note that  SIMS has better FID scores than GAN-based approaches, because it generates images by refining segments retrieved from the real data. However, it has poor segmentation performance, because it might retrieve semantically mismatched patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>We conduct controlled experiments to verify the effectiveness of each component in our approach. We use the SPADE <ref type="bibr" target="#b24">[25]</ref> model as our baseline, and gradually add or eliminate each component to the framework. Our model is denoted as CC-FPSE in the last column. The segmentation mIOU scores of the generated images by each experiment are shown in <ref type="table" target="#tab_3">Table 2</ref>   Conditional convolutions for generator. We firstly replace the SPADE layer with our conditional convolution layers to incorporate the semantic layout information in the experiments denoted as "CC". By comparing baseline with (1) (CC generator vs SPADE generator, both with MsPatch discriminator), Feature pyramid weight prediction network. Next, we replace the feature pyramid structure with a stack of two convolutional layers in the weight prediction network, and this experiment is denoted as "w/ FP" and "w/o FP". Comparing (1) with (2), and (3) with CC-FPSE (Ours), we found that removing the feature pyramid structure for the weight prediction network leads to inferior performance, indicating that the global and long-range information are necessary for predicting the convolutional weights.</p><p>FPSE Discriminator. We fix our proposed generator ("CC w/ FP" or "SPADE w/ FP") and test different designs of the discriminator, to demonstrate the effectiveness of our FPSE discriminator. We force the spatial semantic alignment with the semantic layout, by the introduced semantics-embedding constraint for the discriminator. Comparing (2) with (6) indicates the effectiveness of the semantics embedding discriminator. With the semantics-embedding constraint, the discriminator is driven to classify the correspondence between the image patches and the semantic layout. So the generator is encouraged to generate images that are better aligned with the semantic layout. Furthermore, we replace the multiscale discriminator with the feature pyramid structure, denoted as "FP+SE", which is our proposed discriminator design. The comparison between (6) and last column CC-FPSE (Ours) indicates that the feature pyramid discriminator structure, which combines the low-level and semantic features at different scales, leads to further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel approach (CC-FPSE) for image synthesis from a given semantic layout via better using the semantic layout information to generate images with high-quality details and well aligned semantic meanings. Our generator is able to better exploit the semantic layout to control the generation process, by predicting the spatially-varying weights for the conditional convolution layers. Our feature pyramid semantics-embedding discriminator guides the generator to generate images that contain high-fidelity details and aligns well with the conditional semantic layout. Our approach achieves state-of-the art performance and is able to generate photorealistic images on Cityscapes, COCO-Stuff, and ADE20K datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head><p>Examples of generated images by our approach are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Over proposed approach is able to synthesis images of diverse scenes. Moreover, we show the semantic image synthesis results compared to previous approaches pix2pixHD and SPADE in <ref type="figure" target="#fig_5">Figure 6</ref>. Some differences between the generated images of different approaches are highlighted in red boxes. Our proposed approach generates high-quality images with fine details. It can generate small objects based on the label map, while previous approaches are likely to ignore them. For example, in the first row of <ref type="figure" target="#fig_5">Figure 6</ref>, our approach generates a driver inside the bus based on the semantic layout, while other approaches fails to generate the driver.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(Left) The structure of a Conditional Convolution Block (CC Block). (Right) The overall framework of our proposed CC-FPSE. The weight prediction network predicts weights for CC Blocks in the generator. The conditional convolution generator is built up of Conditional Convolution (CC) Blocks shown on the left. The feature pyramid semantics-embedding (FPSE) discriminator predicts real/fake scores as well as semantic alignment scores. L-ReLU in the CC Block denotes Leaky ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2(right), our feature pyramid discriminator takes the input image at a single scale. The bottom-up pathway produces a feature hierarchy consisting of multi-scale feature maps and the top-down pathway gradually upsamples the spatially coarse but semantically rich feature maps. The lateral combines the high-level semantic feature maps from the top-down pathway with the low-level feature maps from the bottom-up pathway. As a result, the combined multi-scale features are semantically strong, as well as containing finer low-level details such as edges and textures. So the discriminator would pose stronger constraints on both the semantic information and the fine details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Results comparison with previous approaches. Better viewed in color. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Semantic image synthesis results on COCO and ADE20K. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Semantic image synthesis results by our proposed approach. Best viewed in color. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Semantic image synthesis results by previous approaches and our approach. Best viewed in color. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results by the proposed and previous approaches on multiple public datasets. Higher mIOU/accuracy and lower FID score indicate better performance. Accu â†‘ FID â†“ mIOU â†‘ Accu â†‘ FID â†“ mIOU â†‘ Accu â†‘ FID â†“</figDesc><table><row><cell></cell><cell cols="2">COCO-Stuff</cell><cell></cell><cell></cell><cell>Cityscapes</cell><cell></cell><cell></cell><cell>ADE20K</cell><cell></cell></row><row><cell cols="2">mIOU â†‘ CRN [3] 23.7</cell><cell>40.4</cell><cell>70.4</cell><cell>52.4</cell><cell>77.1</cell><cell>104.7</cell><cell>22.4</cell><cell>68.8</cell><cell>73.3</cell></row><row><cell>SIMS [26]</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>47.2</cell><cell>75.5</cell><cell>49.7</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>pix2pixHD [29]</cell><cell>14.6</cell><cell>45.7</cell><cell>111.5</cell><cell>58.3</cell><cell>81.4</cell><cell>95.0</cell><cell>20.3</cell><cell>69.2</cell><cell>81.8</cell></row><row><cell>SPADE [25]</cell><cell>37.4</cell><cell>67.9</cell><cell>22.6</cell><cell>62.3</cell><cell>81.9</cell><cell>71.8</cell><cell>38.5</cell><cell>79.9</cell><cell>33.9</cell></row><row><cell>Ours</cell><cell>41.6</cell><cell>70.7</cell><cell>19.2</cell><cell>65.5</cell><cell>82.3</cell><cell>54.3</cell><cell>43.7</cell><cell>82.9</cell><cell>31.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on COCO-Stuff dataset.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell><cell>(5)</cell><cell>(6)</cell><cell>CC-FPSE (Ours)</cell></row><row><cell>Generator</cell><cell>SPADE</cell><cell>CC w/o FP</cell><cell>CC w/ FP</cell><cell>CC w/o FP</cell><cell>SPADE w/ FP</cell><cell>SPADE w/ FP</cell><cell>CC w/ FP</cell><cell>CC w/ FP</cell></row><row><cell>Discriminator</cell><cell>MsPatch</cell><cell>MsPatch</cell><cell>MsPatch</cell><cell>FP+SE</cell><cell>MsPatch+SE</cell><cell>FP+SE</cell><cell>MsPatch+SE</cell><cell>FP+SE</cell></row><row><cell>mIOU</cell><cell>35.2</cell><cell>36.2</cell><cell>36.7</cell><cell>40.4</cell><cell>38.0</cell><cell>39.17</cell><cell>40.4</cell><cell>41.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To be comparable with ablation study results in<ref type="bibr" target="#b24">[25]</ref>, we report the model performance at 50 epochs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by SenseTime Group Limited, in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants CUHK14202217, CUHK14203118, CUHK14207319, CUHK14208417, CUHK14239816, and in part by CUHK Direct Grant. We thank Lu Sheng for proofreading and helpful suggestions on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FranÃ§ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation-aware convolutional networks using local attention masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Meta-sr: A magnificationarbitrary network for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00875</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6495" to="6503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">High-fidelity image generation with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02271</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">cgans with projection discriminator. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07291</idno>
		<title level="m">Semantic image synthesis with spatiallyadaptive normalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-parametric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8808" to="8816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural style transfer via meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8061" to="8069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05373</idno>
		<title level="m">Erik Learned-Miller, and Jan Kautz. Pixel-adaptive convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic filtering with large sampling field for convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrajit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic conditional networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
