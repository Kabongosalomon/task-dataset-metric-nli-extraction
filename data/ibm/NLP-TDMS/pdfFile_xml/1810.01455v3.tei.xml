<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Flow for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Flow for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the 'flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other CNN model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning 'flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance. The code is publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Activity recognition is an important problem in computer vision with many societal applications including surveillance, robot perception, smart environment/city, and more. Use of video convolutional neural networks (CNNs) have become the standard method for this task, as they can learn more optimal representations for the problem. Two-stream networks <ref type="bibr" target="#b19">[20]</ref>, taking both RGB frames and optical flow as input, provide state-of-the-art results and have been extremely popular. 3-D spatio-temporal CNN models, e.g., I3D <ref type="bibr" target="#b2">[3]</ref>, with XYT convolutions also found that such two-stream design (RGB + optical flow) increases their accuracy. Abstracting both appearance information and explicit motion flow benefits the recognition.</p><p>However, optical flow is expensive to compute. It often requires hundreds of optimization iterations every frame, and causes learning of two separate CNN streams (i.e., RGBstream and flow-stream). This requires significant computation cost and a great increase in the number of model parameters to learn. Further, this means that the model needs to compute optical flow every frame even during inference and <ref type="bibr" target="#b0">1</ref> Code/models available here: https://piergiaj.github.io/rep-flow-site/ (a) (b) (c) <ref type="figure">Figure 1</ref>: Comparing the results of (b) TVL-1 and (c) our learned flow when applied to RGB images. Our layer is able to capture similar motion information to TVL-1. However, compared to TVL-1, our layer is faster, is learnable, and can be applied directly to any intermediate CNN feature maps. With the representation flow layer, optical flow preextraction is no longer needed and a single-stream CNN design becomes possible.</p><p>run two parallel CNNs, limiting its real-time applications. There were previous works to learn representations capturing motion information without using optical flow as input, such as motion feature networks <ref type="bibr" target="#b14">[15]</ref> and ActionFlowNet <ref type="bibr" target="#b15">[16]</ref>. However, although they were more advantageous in terms of the number of model parameters and computation speed, they suffered from inferior performance compared to two-stream models on public datasets such as Kinetics <ref type="bibr" target="#b12">[13]</ref> and HMDB <ref type="bibr" target="#b13">[14]</ref>. We hypothesize that the iterative optimization performed by optical flow methods produces an important feature that other methods fail to capture.</p><p>In this paper, we propose a CNN layer inspired by optical flow algorithms to learn motion representations for action recognition without having to compute optical flow. Our representation flow layer is a fully-differentiable layer designed to capture 'flow' of any representation channels within the model. Its parameters for iterative flow optimization are learned together with other model parameters, maximizing the action recognition performance. This is also done without having/training multiple network streams, reducing the number of parameters in the model. Further, we newly introduce the concept of learning 'flow of flow' representations by stacking multiple representation flow layers. We conduct extensive action classification experimental evaluation of where to compute optical flow and various hyperparameters, learning parameters, and fusion techniques.</p><p>Our contribution is the introduction of a new differentiable CNN layer that unrolls the iterations of the TV-L1 optical flow method. This allows for learning of the optical flow parameters, application to any CNN feature maps (i.e., intermediate representations), and lower computational cost while maintaining performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Capturing motion and temporal information has been studied for activity recognition. Early, hand-crafted approaches such as dense trajectories <ref type="bibr" target="#b23">[24]</ref> captured motion information by tracking points through time. Many algorithms have been developed to compute optical flow as a way to capture motion in video <ref type="bibr" target="#b7">[8]</ref>. Other works have explored learning the ordering of frames to summarize a video in a single 'dynamic image' used for activity recognition <ref type="bibr" target="#b0">[1]</ref>.</p><p>Convolutional neural networks (CNNs) have been applied to activity recognition. Initial approaches explored methods to combine temporal information based on pooling or temporal convolution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. Other works have explored using attention to capture sub-events of activities <ref type="bibr" target="#b17">[18]</ref>. Two-stream networks have been very popular: they take input of a single RGB frame (captures appearance information) and a stack of optical flow frames (captures motion information). Often, the two network streams of the model are separately trained and the final predictions are averaged together <ref type="bibr" target="#b19">[20]</ref>. There were other two-stream CNN works exploring different ways to 'fuse' or combine the motion CNN with the appearance CNN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>. There were also large 3D XYT CNNs learning spatio-temporal patterns <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b2">3]</ref>, enabled by large video datasets such as Kinetics <ref type="bibr" target="#b12">[13]</ref>. However, these approaches still rely on optical flow input to maximize their accuracies.</p><p>While optical flow is known to be an important feature, flows optimized for activity recognition are often different from the true optical flow <ref type="bibr" target="#b18">[19]</ref>, suggesting that end-to-end learning of motion representations is beneficial. Recently, there have been works on learning such motion representations using convolutional models. Fan et al. <ref type="bibr" target="#b4">[5]</ref> implemented the TV-L1 method using deep learning libraries to increase its computational speed and allow for learning some parameters. The result was fed to a two-stream CNN for the recognition. Several works explored learning a CNN to predict optical flow, which also can be used for action recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>. Lee et al. <ref type="bibr" target="#b14">[15]</ref> shifted features from sequential frames to capture motion in a non-iterative fashion. Sun et al. <ref type="bibr" target="#b20">[21]</ref> proposed an optical flow guided feature (OFF) by computing the gradients of representations and temporal differences, but it lacked the iterative optimization necessary for accurate flow computation. Further, it requires a three-stream model taking RGB, optical flow, and RGB differences to achieve state-of-the-art performance.</p><p>Unlike prior works, our proposed model with representation flow layers relies only on RGB input, learning far fewer parameters while correctly representing motion with the iterative optimization. It is significantly faster than the video CNNs requiring optical flow input, while still performing as good as or even better than the two-stream models. It clearly outperforms existing motion representation methods including TVNet <ref type="bibr" target="#b4">[5]</ref> and OFF <ref type="bibr" target="#b20">[21]</ref> in both speed and accuracy, which we experimentally confirm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our method is a fully-differentiable convolutional layer inspired by optical flow algorithms. Unlike traditional optical flow methods, all the parameters of our method can be learned end-to-end, maximizing action recognition performance. Furthermore, our layer is designed to compute the 'flow' of any representation channels, instead of limiting its input to be traditional RGB frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of Optical Flow Methods</head><p>Before describing our layer, we briefly review how optical flow is computed. Optical flow methods are based on the brightness consistency assumption. That is, given sequential images I 1 , I 2 , a point x, y in I 1 is located at x + ∆x, y + ∆y in I 2 , or I 1 (x, y) = I 2 (x + ∆x, y + ∆y). These methods assume small movements between frames, so this can be approximated with a Taylor series: I 2 = I 1 + δI δx ∆x+ δI δy ∆y, where u = [∆x, ∆y]. These equations are solved for u to obtain the flow, but can only be approximated due to the two unknowns.</p><p>The standard, variational methods for approximating optical flow (e.g., Brox <ref type="bibr" target="#b1">[2]</ref> and TV-L1 <ref type="bibr" target="#b26">[27]</ref> methods) take sequential images I 1 , I 2 as input. Variational optical flow methods estimate the flow field, u, using an iterative optimization method. The tensor u ∈ R 2×W ×H is the x and y directional flow for every location in the image. Taking two sequential images as input, I 1 , I 2 , the methods first compute the gradient in both x and y directions: ∇I 2 . The initial flow is set to 0, u = 0. Then ρ, which captures the motion residual between two frames, based on the current flow estimate u, can be computed. For efficiency, the constant part of ρ, ρ c is pre-computed:</p><formula xml:id="formula_0">ρ c = I 2 − ∇ x I 2 · u x − ∇ y I 2 · u y − I 1<label>(1)</label></formula><p>The iterative optimization is then performed, each updat-ing u:</p><formula xml:id="formula_1">ρ = ρ c + ∇ x I 2 · u x + ∇ y I 2 · u y (2) v =      u + λθ∇I 2 ρ &lt; −λθ|∇I 2 | 2 u − λθ∇I 2 ρ &gt; λθ|∇I 2 | 2 u − ρ ∇I2 |I2| 2 otherwise (3) u = v + θ · divergence(p) (4) p = p + τ θ ∇u 1 + τ θ |∇u|<label>(5)</label></formula><p>Here θ controls the weight of the TV-L1 regularization term, λ controls the smoothness of the output and τ controls the time-step. These hyperparameters are manually set. p is the dual vector fields, which are used to minimize the energy. The divergence of p, or backward difference, is computed as:</p><formula xml:id="formula_2">divergence(p) = p x,i,j − p x,i−1,j + p y,i,j − p y,i,j−1 (6)</formula><p>where p x is the x direction and p y is the y direction, and p contains all the spatial locations in the image.</p><p>The goal is to minimize the total variational energy:</p><formula xml:id="formula_3">E = |∇u| + λ|∇I 1 * u + I 1 − I 2 |<label>(7)</label></formula><p>Approaches run this iterative optimization for multiple input scales, from small to large, and use the previous flow estimate u to warp I 2 at the larger scale, providing a coarseto-fine optical flow estimation. These standard approaches require multiple scales and warpings to obtain a good flow estimate, taking thousands of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Representation Flow Layer</head><p>Inspired by the optical flow algorithm, we design a fullydifferentiable, learnable, convolutional representation flow layer by extending the general algorithm outlined above. The main differences are that (i) we allow the layer to capture flow of any CNN feature map, and that (ii) we learn its parameters including θ, λ, and τ as well as the divergence weights. We also make several key changes to reduce computation time: (1) we only use a single scale, (2) we do not perform any warping, and (3) we compute the flow on a CNN tensor with a smaller spatial size. Multiple scale and warping are computationally expensive, each requiring many iterations. By learning the flow parameters, we can eliminate the need for these additional steps. Our method is applied on lower resolution CNN feature maps, instead of the RGB input, and is trained in an end-to-end fashion. This not only benefits its speed, but also allows the model to learn a motion representation optimized for activity recognition.</p><p>We note that the brightness consistency assumption can similarly be applied to CNN feature maps. Instead of capturing pixel brightness, we capture feature value consistency. Eq. 4</p><p>Eq. 5</p><formula xml:id="formula_4">u 1 u 0 p 1 p 0 v 0 ⍴ 0 i=1 u 2 p 2 v 1 ⍴ 1 Eq. 2 Eq. 3 Eq. 4 Eq. 5 F 1 F 2 ... i=n-1 u n p n v i ⍴ i Eq. 2 Eq. 3 Eq. 4</formula><p>Eq. 5 This same assumption holds as CNNs are designed to be spatially invariant; i.e., they produce roughly the same feature value for the same object as it moves.</p><p>Given the input F 1 , F 2 , a single channel from sequential CNN feature maps (or input image), we compute the featuremap-gradient by convolving the input feature maps with the Sobel filter:</p><formula xml:id="formula_5">∇F 2x =   1 0 −1 2 0 −2 1 0 −1   * F 2 , ∇F 2y =   1 2 1 0 0 0 −1 −2 −1   * F 2 (8)</formula><p>We set u = 0, p = 0 initially, each having width and height matching the input, then we can compute ρ c = F 2 − F 1 . Next, following Algorithm 1, we repeatedly apply the operations in Eqs. 2-5 for a fixed number of iterations to enable the iterative optimization. To compute the divergence, we zero-pad p on the first column (x-direction) or row (ydirection) then convolve it with weights, w x , w y to compute Eq. 6:</p><formula xml:id="formula_6">divergence(p) = p x * w x + p y * w y<label>(9)</label></formula><p>where initially w x = −1 1 and w y = −1 1 . Note that these parameters are also differentiable and can be learned with backpropagation. We compute ∇u as</p><formula xml:id="formula_7">∇u x =   1 0 −1 2 0 −2 1 0 −1   * u x , ∇u y =   1 2 1 0 0 0 −1 −2 −1   * u y<label>(10)</label></formula><p>Representation Flow within a CNN Algorithm 1 and <ref type="figure" target="#fig_1">Fig. 2</ref> describe the process of our representation flow layer. Our flow layer with multiple iterations could also be interpreted as having a sequence of convolutional layers sharing parameters (i.e., each blue box in <ref type="figure" target="#fig_1">Fig. 2)</ref>, with each layer's behavior dependent on its previous layer. As a result of this formulation, the layer becomes fully differentiable and allows for the learning of all parameters, including (τ, λ, θ) and the divergence weights (w x , w y ). This enables our learned representation flow layer to be optimized for its task (i.e., action recognition). </p><formula xml:id="formula_8">function REPRESENTATIONFLOW(F 1 , F 2 ) u = 0, p = 0 Compute image/feature map gradients (Eq. 8) ρ c = F 2 − F 1 for n iterations do ρ = ρ c + ∇ x F 2 · u x + ∇ y F 2 · u y v =      u + λθ∇F 2 ρ &lt; −λθ|∇F 2 | 2 u − λθ∇F 2 ρ &gt; λθ|∇F 2 | 2 u − ρ ∇F2 |F2| 2 otherwise u = v + θ · divergence(p) p = p+ τ θ ∇u 1+ τ θ |∇u| end for return u end function</formula><p>Computing Flow-of-Flow Standard optical flow algorithms compute the flow for two sequential images. An optical flow image contains information about the direction and magnitude of the motion. Applying the flow algorithm directly on two flow images means that we are tracking pixels/locations showing similar motion in two consecutive frames. In practice, this typically leads to a worse performance due to inconsistent optical flow results and non-rigid motion. On the other hand, our representation flow layer is 'learned' from the data, and is able to suppress such inconsistency and better abstract/represent motion by having multiple regular convolutional layers between the flow layers. <ref type="figure" target="#fig_5">Fig. 6</ref> illustrates such design, which we confirm its benefits in the experiment section. By stacking multiple representation flow layers, our model is able to capture longer temporal intervals and consider locations with motion consistency.</p><p>CNN feature maps may have hundreds or thousands of channels and our representation flow layer computes the flow for each channel, which can take significant time and memory. To address this, we apply a convolutional layer to reduce the number of channels from C to C before the flow layer (note that C is still significantly more than traditional optical flow algorithms, which were only applied to single-channel, greyscale images). For numerical stability, we normalize this feature map to be in [0, 255], matching standard image values. We found that the CNN features were quite small on average (&lt; 0.5) and the TVL-1 algorithm default hyperparameters are designed for standard images values in [0, 255], thus we found this normalization step important. Using the normalized feature, we compute the flow and stack the x and y flows, resulting in 2C channels. Finally, we apply another convolutional layer to convert from 2C channels to C channels. This is passed to the remaining CNN layers for the prediction. We average predictions from many frames to classify each video, as shown in <ref type="figure">Fig. 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Activity Recognition Model</head><p>We place the representation flow layer inside a standard activity recognition model taking a T × C × W × H tensor as input to a CNN. Here, C is 3 as our model uses direct RGB frames as an input. T is the number of frames the model processes, and W and H are the spatial dimensions. The CNN outputs a prediction per-timestep and these are temporally averaged to produce a probability for each class. The model is trained to minimize cross-entropy:</p><formula xml:id="formula_9">L(v, c) = − K i (c == i) log(p i )<label>(11)</label></formula><p>where p = M (v), v is the video, the function M is the classification CNN and c represents which of the K classes v belongs. That is, the parameters in our flow layers are trained together with the other layers, so that it maximizes the final classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Implementation details We implemented our representation flow layer in PyTorch and our code and models are available. As training CNNs on videos is computationally expensive, we used a subset of the Kinetics dataset <ref type="bibr" target="#b12">[13]</ref> with 100k videos from 150 classes: Tiny-Kinetics. This allowed testing many models more quickly, while still having sufficient data to train large CNNs. For most experiments, we used ResNet-34 <ref type="bibr" target="#b9">[10]</ref> with input of size 16 × 112 × 112 (i.e., 16 frames with spatial size of 112). To further reduce the computation time for many studies, we used this smaller input, which reduces performance, but allowed us to use larger batch sizes and run many experiments more quickly. Our final models are trained on standard 224 × 224 images. Check Appendix for specific training details.</p><p>Where to compute flow? To determine where in the network to compute the flow, we compare applying our flow layer on the RGB input, after the first conv. layer, and after the each of the 5 residual blocks. The results are shown in Table 1. We find that computing the flow on the input provides poor performance, similar to the performance of the flowonly networks, but there is a significant jump after even 1 layer, suggesting that computing the flow of a feature is beneficial, capturing both the appearance and motion information. However, after 4 layers, the performance begins to decline as the spatial information is too abstracted/compressed (due to pooling and large spatial receptive field size), and sequential features become very similar, containing less motion information. Note that our HMDB performance in this table is quite low compared to state-of-the-art methods due to being trained from scratch using few frames and low spatial resolution (112 × 112). For the following experiments, unless otherwise noted, we apply the layer after the 3rd residual block. In <ref type="figure" target="#fig_6">Fig. 7</ref>, we visualize the learned motion representations computer after block 3.</p><p>What to learn? As our method is fully differentiable, we can learn any of the parameters, such as the kernels used  to compute image gradients, the kernels for the divergence computation and even τ, λ, θ. In <ref type="table" target="#tab_1">Table 2</ref>, we compare the effects of learning different parameters. We find that learning the Sobel kernel values reduces performance due to noisy gradients particularly when the batch size is limited, but learning the divergence and τ, λ, θ is beneficial.</p><p>How many iterations for flow? To confirm that the iterations are important and determine how many we need, we experiment with various numbers of iterations. We compare the number of iterations needed for both learning (divergence+τ, λ, θ) and not learning parameters. The flow is computed after 3 residual blocks. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. We find that learning provides better performance with fewer iterations (similar to the finding in <ref type="bibr" target="#b4">[5]</ref>), and that iteratively computing the feature is important. We use 10 or 20 iterations in the remaining experiments as they provide good performance and are fast.</p><p>Two-stream fusion? Two-stream CNNs fusing both RGB and optical flow features has been heavily studied <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7]</ref>. Based on these works, we compare various ways of fusing RGB and our flow representation, shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. We compare no fusion, late fusion (i.e., separate RGB and flow  We experimentally find that no fusion performs comparably, when applied to after 3rd residual block.</p><p>CNNs) and addition/multiplication/concatenation fusion. In <ref type="table" target="#tab_3">Table 4</ref>, we compare different fusion methods for different locations in the network. We find that fusing RGB information is very important "when computing flow directly from RGB input". However, it is not as beneficial when computing the flow of representations as the CNN has already abstracted much appearance information away. We found that concatenation of the RGB and flow features perform poorly compared to the others. We do not use two-stream fusion in any other experiments, as we found that computing the representation flow after the 3rd residual block provides sufficient performance even without any fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flow-of-flow</head><p>We can stack our layer multiple times, computing the flow-of-flow (FoF). This has the advantage of combining more temporal information into a single feature. Our results are shown in <ref type="table" target="#tab_4">Table 5</ref>. Applying the TV-L1 algorithm twice gives quite poor performance, as optical flow features do not really satisfy the brightness consistency assumption, as they capture magnitude and direction of motion (shown in <ref type="figure" target="#fig_4">Fig. 5</ref>). Applying our representation flow layer twice performs significantly better than TV-L1 twice, but still   worse than our baseline of not doing so. However, we can add a convolutional layer between the first and second flow layer, flow-conv-flow (FcF), <ref type="figure" target="#fig_5">(Fig. 6</ref>), allowing the model to better learn longer-term flow representations. We find this performs best, as this intermediate layer is able to smooth the flow and produce a better input for the representation flow layer. However, we find adding a third flow layer reduces performance as the motion representation becomes unreliable, due to the large spatial receptive field size. In <ref type="figure" target="#fig_6">Fig.  7</ref>, we visualize the learned flow-of-flow, which is a smoother, acceleration-like feature with abstract motion patterns.</p><p>Flow of 3D CNN Feature Since 3D convolutions capture some temporal information, we test computing our flow rep-  resentation on features from a 3D CNN. As 3D CNNs are expensive to train, we follow the method of I3D <ref type="bibr" target="#b2">[3]</ref> to inflate a ResNet-18 pretrained on ImageNet to a 3D CNN for videos. We also compare to the (2+1)D method of spatial conv. followed by temporal conv from <ref type="bibr" target="#b25">[26]</ref>, which produces a similar feature combining spatial and temporal information. We find our flow layer increases performance even with 3D and (2+1)D CNNs already capturing some temporal information: <ref type="table" target="#tab_5">Tables 6 and 7</ref>. These experiments used 10 iterations and learning the flow parameters. In these experiments, FcF was not used.</p><p>We also compared to the OFF <ref type="bibr" target="#b20">[21]</ref> using (2+1)D and 3D CNNs. We observe that this method does not result in meaningful performance increases using CNNs that capture temporal information, while our approach does.</p><p>Comparison to other motion representations We compare to existing CNN-based motion representation methods to confirm the usefulness of our representation flow. For these experiments, when available, we used code provided by the authors and otherwise implemented the methods ourselves. To better compare to existing works, we used (16×) 224 × 224 images. <ref type="table" target="#tab_7">Table 8</ref> shows the results. MFNet <ref type="bibr" target="#b14">[15]</ref> captures motion by spatially shifting CNN feature maps, then summing the results, TVNet <ref type="bibr" target="#b4">[5]</ref> applies a convolutional optical flow method to RGB inputs, and ActionFlowNet    <ref type="bibr" target="#b14">[15]</ref> 52.5 56.8 TVNet <ref type="bibr" target="#b4">[5]</ref> 39.4 57.5 RGB-OFF <ref type="bibr" target="#b20">[21]</ref> 55.6 56.9 Ours 61.1 65.4 <ref type="bibr" target="#b15">[16]</ref> trains a CNN to jointly predict optical flow and activity classes. We also compare to OFF <ref type="bibr" target="#b20">[21]</ref> using only RGB inputs. Note that the HMDB performance in <ref type="bibr" target="#b20">[21]</ref> was reported using their three-stream model (i.e., RGB + RGB-diff + optical flow inputs), and here we compare to the version only using RGB. Our method, which applies the iterative flow computation on CNN feature maps, performs the best.</p><p>Computation time We compare our representation flow to state-of-the-art two-stream approaches in terms of runtime and number of parameters. All timings were measured using a single Pascal Titan X GPU, for a batch of videos with size 32 × 224 × 224. The flow/two-stream CNNs include the time to run the TV-L1 algorithm (OpenCV GPU version) to compute the optical flow. All CNNs were based on the ResNet-34 architecture. As also shown in <ref type="table" target="#tab_8">Table 9</ref>, our method is significantly faster than two-stream models relying on TV-L1 or other optical flow methods, while performing similarly or better. The number of parameters our model has is half of its two-stream competitors (e.g., 21M vs. 42M, in the case of 2D CNNs).</p><p>Comparison to state-of-the-arts We also compared our action recognition accuracies with the state-of-the-arts on Kinetics and HMDB. For this, we train our models using 32 × 224 × 224 inputs with the full kinetics dataset, using 8 V100s. We used the 2D ResNet-50 as the architecture. Based on our experiments, we applied our representation flow layer after the 3rd residual block, learned the hyperparameters and divergence kernels, and used 20 iterations. We also compare our flow-of-flow model. Following <ref type="bibr" target="#b21">[22]</ref>, the evaluation is performed using a running average of the parameters over time. Our results, shown in <ref type="table" target="#tab_8">Table 9</ref>, confirm that this approach clearly outperforms existing models using RGB only inputs, and is competitive against expensive twostream networks. Our model performs the best among those not using optical flow inputs (i.e., among the models only taking ∼600ms per video). The models requiring optical flow were more than 10 times slower, including two-stream versions of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced a learnable representation flow layer inspired by optical flow algorithms. We experimentally compared various forms of our layer to confirm that the iterative optimization and learnable parameters are important. Our model clearly outperformed existing methods in both speed and accuracy on standard datasets. We also introduced the concept of 'flow of flow' to compute longer-term motion representations and showed it benefits performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training and Implementation Details</head><p>Implementation Details When applying the representation flow layer within a CNN, we first applied a 1x1 convolutional layer to reduce the number of channels from C to 32. CNN feature maps often have hundreds of channels, but computing the representation flow for hundreds of channels is computationally expensive. We found 32 channels to be a good trade-off between performance and speed. The flow layer produces output with 64 channels, x and y flows for the 32 input channels, which are concatenated together. We apply a 3x3 convolutional layer to this representation to produce C output channels. This allows us to apply the rest of the standard CNN to the representation flow feature.</p><p>Two-stream networks stack 10 optical flow frames to capture temporal information <ref type="bibr" target="#b19">[20]</ref>. However, we found that stacking representation flows did not perform well. Instead, we computed the flow for sequential images and averaged the predictions from a sequence of 16 frames. We found this outperformed stacking flow representations.</p><p>Training Details We trained the network using stochastic gradient descent with momentum set to 0.9. For Kinetics and Tiny-Kinetics, the initial learning rate was 0.1 and decayed by a factor of 10 every 50 epochs. The model was trained for 200 epochs. The 2D CNNs were trained using a batch size of 32 on 4 Titan X GPUs. The 3D and (2+1)D CNNs were trained with a batch size of 24 using 8 V100 GPUs. When fine-tuning on HMDB, the learning rate started at 0.005 and decayed by a factor of 10 every 20 epochs. The network was fine-tuned for 50 epochs. When learning the optical flow parameters, the learning rate for the parameters (i.e., λ, τ, θ, divergence kernels and Sobel filters) was set of 0.01 · lr, otherwise the model produced poor predictions. This is likely due to the accumulation of gradients from the many iterations of the algorithm. For Kinetics and Tiny-Kinetics, we used dropout at 0.5 and for HMDB it was set to 0.8.</p><p>Testing Details For the results reported in <ref type="table" target="#tab_8">Table 9</ref>, we classified actions by applying our model to 25 different random croppings of each video. As found in many previous works, this helps increase the performance slightly. In all the other experiments (i.e., Tables 1-8), random cropping was not used. Also notice that only the results in <ref type="table" target="#tab_8">Table 9</ref> uses our full model with 32 × 224 × 224 input resolution. The other experiments uses spatially and/or temporally smaller models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of our flow layer. It unrolls the iterations of the TV-L1 algorithm as a sequence of tensor operations, while sharing parameters across the iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Algorithm 1</head><label>31</label><figDesc>Illustration of a video-CNN with our representation flow layer. The CNN computes intermediate feature maps, and sequential feature maps are used as input to the flow layer. The outputs of the flow layer are used for prediction. Method for the representation flow layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Different approaches to fusing RGB and flow information. (a) No fusion (b) Late fusion (c) The circle represents elementwise addition/multiplication or concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example (a) RGB image, (b) TVL-1 flow image and (c) TVL-1 applied twice (i.e., Flow-of-Flow). Directly computing flow-of-flow results in poor input, as the inputs of magnitude and directions do not follow the brightness consistency assumption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of how our model computes the FoF. Adding the intermediate conv layer allows for the smoothing of flow and conversion from magnitude+direction to feature values. This allows a second flow layer to further refine the motion feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of learned representation flows. Note these are after the 3rd residual block and are low-resolution (28x28). (a) Examples of rep. flow for various activities. (b) Example of different channels capturing different motions: (left) hands (right) other motion. (c) Flow-of-flow is an acceleration-like feature with smoother, more abstract motion patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Computing the optical flow representation after various number of CNN layers. Results are video classification accuracy on our Tiny-Kinetics and LowRes-HMDB51 datasets using 100 iterations to compute the flow representation.</figDesc><table><row><cell></cell><cell cols="2">Tiny-Kinetics LowRes-HMDB</cell></row><row><cell>RGB CNN</cell><cell>55.2</cell><cell>35.5</cell></row><row><cell>Flow CNN</cell><cell>35.4</cell><cell>37.5</cell></row><row><cell>Two-Stream CNN</cell><cell>57.6</cell><cell>41.5</cell></row><row><cell>Flow Layer on RGB Input</cell><cell>37.4</cell><cell>40.5</cell></row><row><cell>After Block 1</cell><cell>52.4</cell><cell>42.6</cell></row><row><cell>After Block 2</cell><cell>57.4</cell><cell>44.5</cell></row><row><cell>After Block 3</cell><cell>59.4</cell><cell>45.4</cell></row><row><cell>After Block 4</cell><cell>52.1</cell><cell>43.5</cell></row><row><cell>After Block 5</cell><cell>50.3</cell><cell>42.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Tiny-Kinetics LowRes-HMDB</cell></row><row><cell>None (all fixed)</cell><cell>59.4</cell><cell>45.4</cell></row><row><cell>Sobel kernels</cell><cell>58.5</cell><cell>43.5</cell></row><row><cell>Divergence (w x , w y )</cell><cell>60.2</cell><cell>46.4</cell></row><row><cell>τ, λ, θ</cell><cell>59.9</cell><cell>46.2</cell></row><row><cell>All</cell><cell>59.2</cell><cell>46.2</cell></row><row><cell>Divergence + τ, λ, θ</cell><cell>60.7</cell><cell>46.8</cell></row></table><note>Comparison of learning different parameters. The flow was computed after Block 3 using 100 iterations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of the number of iterations on our Tiny-Kinetics dataset for learning and not learning.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Not learned Learned</cell><cell></cell></row><row><cell cols="2">1 iteration</cell><cell></cell><cell>46.7</cell><cell>49.5</cell><cell></cell></row><row><cell cols="2">5 iterations</cell><cell></cell><cell>51.3</cell><cell>55.4</cell><cell></cell></row><row><cell cols="3">10 iterations</cell><cell>52.4</cell><cell>59.4</cell><cell></cell></row><row><cell cols="3">20 iterations</cell><cell>53.6</cell><cell>60.7</cell><cell></cell></row><row><cell cols="3">50 iterations</cell><cell>59.2</cell><cell>60.9</cell><cell></cell></row><row><cell cols="3">100 iterations</cell><cell>59.4</cell><cell>60.7</cell><cell></cell></row><row><cell>F 1</cell><cell>F 2</cell><cell>F 1</cell><cell>F 2</cell><cell>F 1</cell><cell>F 2</cell></row><row><cell>Flow</cell><cell></cell><cell>Flow</cell><cell></cell><cell>Flow</cell><cell></cell></row><row><cell>CNN</cell><cell></cell><cell>CNN</cell><cell>CNN</cell><cell>CNN</cell><cell></cell></row><row><cell cols="2">Prediction</cell><cell cols="2">Avg Prediction</cell><cell cols="2">Prediction</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell>(c)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Different fusion methods for flow computed at different locations in the network on our Tiny-Kinetics dataset using 10 iterations with flow parameter learning.</figDesc><table><row><cell></cell><cell cols="3">RGB 1 Block 3 Blocks</cell></row><row><cell>None</cell><cell>37.4</cell><cell>52.4</cell><cell>59.4</cell></row><row><cell>Late</cell><cell>61.3</cell><cell>60.4</cell><cell>61.5</cell></row><row><cell>Add</cell><cell>59.7</cell><cell>57.2</cell><cell>56.5</cell></row><row><cell>Multiply</cell><cell>58.3</cell><cell>58.1</cell><cell>57.8</cell></row><row><cell cols="2">Layer + Multiply 60.1</cell><cell>61.7</cell><cell>61.7</cell></row><row><cell>Concat</cell><cell>42.4</cell><cell>48.5</cell><cell>47.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Computing the FoF representation. TV-L1 twice provides poor performance, using two flow layers with a conv. in between provides the best performance. Experiments used 10 iterations and learning flow parameters.</figDesc><table><row><cell></cell><cell></cell><cell>Tiny-Kinetics</cell></row><row><cell>TVL-1 twice</cell><cell></cell><cell>12.2</cell></row><row><cell>Single Flow Layer</cell><cell></cell><cell>59.4</cell></row><row><cell>Flow-of-Flow</cell><cell></cell><cell>47.2</cell></row><row><cell>Flow-Conv-Flow (FcF)</cell><cell></cell><cell>62.3</cell></row><row><cell cols="2">Flow-Conv-Flow-Conv-Flow</cell><cell>56.5</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Computing representation flow using 3D ResNet-18. We find that even though 3D CNNs capture some temporal information, the use of the iterative representation flow further improves performance.</figDesc><table><row><cell></cell><cell>Tiny-Kinetics</cell></row><row><cell>RGB 3D ResNet-18</cell><cell>54.6</cell></row><row><cell>TVL-1 3D ResNet-18</cell><cell>37.6</cell></row><row><cell>Two-Stream 3D ResNet</cell><cell>57.5</cell></row><row><cell>RGB-Only OFF [21]</cell><cell>54.8</cell></row><row><cell>Input (RGB)</cell><cell>38.5</cell></row><row><cell>After Block 1</cell><cell>58.4</cell></row><row><cell>After Block 3</cell><cell>59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">: Computing representation flow using (2+1)D</cell></row><row><cell cols="2">ResNet-18. We find that the representation flow layer is</cell></row><row><cell cols="2">beneficial with this base network, confirming it captures</cell></row><row><cell cols="2">features standard spatio-temporal convolution does not.</cell></row><row><cell></cell><cell>Tiny-Kinetics</cell></row><row><cell>RGB (2+1)D ResNet-18</cell><cell>53.4</cell></row><row><cell>TVL-1 (2+1)D ResNet-18</cell><cell>36.3</cell></row><row><cell>Two-Stream (2+1)D ResNet</cell><cell>55.6</cell></row><row><cell>RGB-Only OFF [21]</cell><cell>53.7</cell></row><row><cell>Input (RGB)</cell><cell>39.2</cell></row><row><cell>After Block 1</cell><cell>57.3</cell></row><row><cell>After Block 3</cell><cell>60.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell cols="3">Comparisons to other CNN-based motion repre-</cell></row><row><cell cols="3">sentations, using 10 iterations and learning flow parameters.</cell></row><row><cell cols="2">This is without FcF and two-stream fusion.</cell><cell></cell></row><row><cell></cell><cell cols="2">Tiny-Kinetics HMDB</cell></row><row><cell>ActionFlownet [16]</cell><cell>51.8</cell><cell>56.2</cell></row><row><cell>MFNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparison to the state-of-the-art action classifications. 'HMDB(+Kin)' means that the model was pre-trained on Kinetics before training/testing with HMDB. Missing results are due to those papers not reporting that setting. We marked the best performances (per dataset) with bold texts. Note that all our models have a single-stream design.</figDesc><table><row><cell>Kinetics HMDB HMDB(+Kin) Run-time (ms)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was supported in part by the National Science Foundation (IIS-1812943 and CNS-1814985).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optical flow modeling and computation: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fortun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kervrann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Im2flow: Motion hallucination from static images for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Eui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning latent sub-events in activity videos using temporal attention filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Association for Artificial Intelligence (AAAI)</title>
		<meeting>the American Association for Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
