<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>July 25-30, 2020. 2020. July 25-30, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Deng</surname></persName>
							<email>dengkuan@mail.ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<email>xiangwang@u.nus.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
							<email>liyan@kuaishou.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Beijing Kuaishou Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;20)</title>
						<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;20) <address><addrLine>New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<date type="published">July 25-30, 2020. 2020. July 25-30, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3397271.3401063</idno>
					<note>* Meng Wang is the corresponding author. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 • Information systems → Recommender systems. KEYWORDS Collaborative Filtering, Recommendation, Embedding Propagation, Graph Neural Network ACM Reference Format: 10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolution Network (GCN) has become new state-ofthe-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -feature transformation and nonlinear activation -contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.</p><p>In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -neighborhood aggregation -for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -a state-of-the-art GCN-based recommender model -under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives. Our implementations are available in both TensorFlow 1 and PyTorch 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>To alleviate information overload on the web, recommender system has been widely deployed to perform personalized information filtering <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. The core of recommender system is to predict whether a user will interact with an item, e.g., click, rate, purchase, among other forms of interactions. As such, collaborative filtering (CF), which focuses on exploiting the past user-item interactions to achieve the prediction, remains to be a fundamental task towards effective personalized recommendation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>The most common paradigm for CF is to learn latent features (a.k.a. embedding) to represent a user and an item, and perform prediction based on the embedding vectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>. Matrix factorization is an early such model, which directly projects the single ID of a user to her embedding <ref type="bibr" target="#b25">[26]</ref>. Later on, several research find that augmenting user ID with the her interaction history as the input can improve the quality of embedding. For example, SVD++ <ref type="bibr" target="#b24">[25]</ref> demonstrates the benefits of user interaction history in predicting user numerical ratings, and Neural Attentive Item Similarity (NAIS) <ref type="bibr" target="#b17">[18]</ref> differentiates the importance of items in the interaction history and shows improvements in predicting item ranking. In view of user-item interaction graph, these improvements can be seen as coming from using the subgraph structure of a user -more specifically, her one-hop neighbors -to improve the embedding learning.</p><p>To deepen the use of subgraph structure with high-hop neighbors, Wang et al. <ref type="bibr" target="#b38">[39]</ref> recently proposes NGCF and achieves state-of-the-art performance for CF. It takes inspiration from the Graph Convolution Network (GCN) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>, following the same propagation rule to refine embeddings: feature transformation, neighborhood aggregation, and nonlinear activation. Although NGCF has shown promising results, we argue that its designs are rather heavy and burdensome -many operations are directly inherited from GCN without justification. As a result, they are not necessarily useful for the CF task. To be specific, GCN is originally proposed for node classification on attributed graph, where each node has rich attributes as input features; whereas in user-item interaction graph for CF, each node (user or item) is only described by a one-hot ID, which has no concrete semantics besides being an identifier. In such a case, given the ID embedding as the input, performing multiple layers of nonlinear feature transformationwhich is the key to the success of modern neural networks <ref type="bibr" target="#b15">[16]</ref> -will bring no benefits, but negatively increases the difficulty for model training.</p><p>To validate our thoughts, we perform extensive ablation studies on NGCF. With rigorous controlled experiments (on the same data splits and evaluation protocol), we draw the conclusion that the two operations inherited from GCN -feature transformation and nonlinear activation -has no contribution on NGCF's effectiveness. Even more surprising, removing them leads to significant accuracy improvements. This reflects the issues of adding operations that are useless for the target task in graph neural network, which not only brings no benefits, but rather degrades model effectiveness. Motivated by these empirical findings, we present a new model named LightGCN, including the most essential component of GCN -neighborhood aggregation -for collaborative filtering. Specifically, after associating each user (item) with an ID embedding, we propagate the embeddings on the user-item interaction graph to refine them. We then combine the embeddings learned at different propagation layers with a weighted sum to obtain the final embedding for prediction. The whole model is simple and elegant, which not only is easier to train, but also achieves better empirical performance than NGCF and other state-of-the-art methods like Mult-VAE <ref type="bibr" target="#b27">[28]</ref>.</p><p>To summarize, this work makes the following main contributions:</p><p>• We empirically show that two common designs in GCN, feature transformation and nonlinear activation, have no positive effect on the effectiveness of collaborative filtering. • We propose LightGCN, which largely simplifies the model design by including only the most essential components in GCN for recommendation. • We empirically compare LightGCN with NGCF by following the same setting and demonstrate substantial improvements.</p><p>In-depth analyses are provided towards the rationality of LightGCN from both technical and empirical perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We first introduce NGCF <ref type="bibr" target="#b38">[39]</ref>, a representative and state-of-the-art GCN model for recommendation. We then perform ablation studies on NGCF to judge the usefulness of each operation in NGCF. The novel contribution of this section is to show that the two common designs in GCNs, feature transformation and nonlinear activation, have no positive effect on collaborative filtering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NGCF Brief</head><p>In the initial step, each user and item is associated with an ID embedding. Let e (0) u denote the ID embedding of user u and e (0) i denote the ID embedding of item i. Then NGCF leverages the useritem interaction graph to propagate embeddings as: i respectively denote the refined embedding of user u and item i after k layers propagation, σ is the nonlinear activation function, N u denotes the set of items that are interacted by user u, N i denotes the set of users that interact with item i, and W 1 and W 2 are trainable weight matrix to perform feature transformation in each layer. By propagating L layers, NGCF obtains L + 1 embeddings to describe a user (e (0)</p><formula xml:id="formula_0">e (k +1) u = σ W 1 e (k ) u + i ∈N u 1 |N u ||N i | (W 1 e (k ) i + W 2 (e (k) i ⊙ e (k ) u )) , e (k +1) i = σ W 1 e (k ) i + u ∈N i 1 |N u ||N i | (W 1 e (k ) u + W 2 (e (k ) u ⊙ e (k ) i )) ,<label>(1)</label></formula><formula xml:id="formula_1">u , e (1) u , ..., e (L) u ) and an item (e (0) i , e (1) i , ..., e (L)</formula><p>i ). It then concatenates these L + 1 embeddings to obtain the final user embedding and item embedding, using inner product to generate the prediction score.</p><p>NGCF largely follows the standard GCN <ref type="bibr" target="#b22">[23]</ref>, including the use of nonlinear activation function σ (·) and feature transformation matrices W 1 and W 2 . However, we argue that the two operations are not as useful for collaborative filtering. In semi-supervised node classification, each node has rich semantic features as input, such as the title and abstract words of a paper. Thus performing multiple layers of nonlinear transformation is beneficial to feature learning. Nevertheless, in collaborative filtering, each node of useritem interaction graph only has an ID as input which has no concrete semantics. In this case, performing multiple nonlinear transformations will not contribute to learn better features; even worse, it may add the difficulties to train well. In the next subsection, we provide empirical evidence on this argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Empirical Explorations on NGCF</head><p>We conduct ablation studies on NGCF to explore the effect of nonlinear activation and feature transformation. We use the codes released by the authors of NGCF 3 , running experiments on the same data splits and evaluation protocol to keep the comparison as fair as possible. Since the core of GCN is to refine embeddings by propagation, we are more interested in the embedding quality under the same embedding size. Thus, we change the way of obtaining final embedding from concatenation (i.e., e * u = e  on NGCF's performance, but makes the following ablation studies more indicative of the embedding quality refined by GCN. We implement three simplified variants of NGCF:</p><p>• NGCF-f, which removes the feature transformation matrices W 1 and W 2 . • NGCF-n, which removes the non-linear activation function σ .</p><p>• NGCF-fn, which removes both the feature transformation matrices and non-linear activation function.</p><p>For the three variants, we keep all hyper-parameters (e.g., learning rate, regularization coefficient, dropout ratio, etc.) same as the optimal settings of NGCF. We report the results of the 2-layer setting on the Gowalla and Amazon-Book datasets in <ref type="table" target="#tab_0">Table 1</ref>. As can be seen, removing feature transformation (i.e., NGCF-f) leads to consistent improvements over NGCF on all three datasets. In contrast, removing nonlinear activation does not affect the accuracy that much. However, if we remove nonlinear activation on the basis of removing feature transformation (i.e., NGCF-fn), the performance is improved significantly. From these observations, we conclude the findings that:</p><p>(1) Adding feature transformation imposes negative effect on NGCF, since removing it in both models of NGCF and NGCF-n improves the performance significantly;</p><p>(2) Adding nonlinear activation affects slightly when feature transformation is included, but it imposes negative effect when feature transformation is disabled.</p><p>(3) As a whole, feature transformation and nonlinear activation impose rather negative effect on NGCF, since by removing them simultaneously, NGCF-fn demonstrates large improvements over NGCF (9.57% relative improvement on recall).</p><p>To gain more insights into the scores obtained in <ref type="table" target="#tab_0">Table 1</ref> and understand why NGCF deteriorates with the two operations, we plot the curves of model status recorded by training loss and testing recall in <ref type="figure" target="#fig_1">Figure 1</ref>. As can be seen, NGCF-fn achieves a much lower training loss than NGCF, NGCF-f, and NGCF-n along the whole training process. Aligning with the curves of testing recall, we find that such lower training loss successfully transfers to better recommendation accuracy. The comparison between NGCF and NGCF-f shows the similar trend, except that the improvement margin is smaller.</p><p>From these evidences, we can draw the conclusion that the deterioration of NGCF stems from the training difficulty, rather than overfitting. Theoretically speaking, NGCF has higher representation power than NGCF-f, since setting the weight matrix W 1 and W 2 to identity matrix I can fully recover the NGCF-f model. However, in practice, NGCF demonstrates higher training loss and worse generalization performance than NGCF-f. And the incorporation of nonlinear activation further aggravates the discrepancy between representation power and generalization performance. To round out this section, we claim that when designing model for recommendation, it is important to perform rigorous ablation studies to be clear about the impact of each operation. Otherwise, including less useful operations will complicate the model unnecessarily, increase the training difficulty, and even degrade model effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>The former section demonstrates that NGCF is a heavy and burdensome GCN model for collaborative filtering. Driven by these findings, we set the goal of developing a light yet effective model by including the most essential ingredients of GCN for recommendation. The advantages of being simple are severalfold -more interpretable, practically easy to train and maintain, technically easy to analyze the model behavior and revise it towards more effective directions, and so on.</p><p>In this section, we first present our designed Light Graph Convolution Network (LightGCN) model, as illustrated in <ref type="figure">Figure 2</ref>. We then provide an in-depth analysis of LightGCN to show the rationality behind its simple design. Lastly, we describe how to do model training for recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LightGCN</head><p>The basic idea of GCN is to learning representation for nodes by smoothing features over the graph <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40]</ref>. To achieve this, it performs graph convolution iteratively, i.e., aggregating the features of neighbors as the new representation of a target node. Such neighborhood aggregation can be abstracted as:</p><formula xml:id="formula_2">e (k +1) u = AGG(e (k ) u , {e (k ) i : i ∈ N u }).<label>(2)</label></formula><p>The AGG is an aggregation function -the core of graph convolution -that considers the k-th layer's representation of the target node and its neighbor nodes. Many work have specified the AGG, such as the weighted sum aggregator in GIN <ref type="bibr" target="#b41">[42]</ref>, LSTM aggregator in GraphSAGE <ref type="bibr" target="#b13">[14]</ref>, and bilinear interaction aggregator in BGNN <ref type="bibr" target="#b47">[48]</ref> etc. However, most of the work ties feature transformation or nonlinear activation with the AGG function. Although they perform well on node or graph classification tasks that have semantic input features, they could be burdensome for collaborative filtering (see preliminary results in Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Light Graph Convolution (LGC)</head><p>Normalized Sum</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Combination (weighted sum)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized Sum</head><p>Prediction neighbors of u 1 neighbors of i 4 <ref type="figure">Figure 2</ref>: An illustration of LightGCN model architecture.</p><p>In LGC, only the normalized sum of neighbor embeddings is performed towards next layer; other operations like self-connection, feature transformation, and nonlinear activation are all removed, which largely simplifies GCNs.</p><p>In Layer Combination, we sum over the embeddings at each layer to obtain the final representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Light Graph Convolution (LGC).</head><p>In LightGCN, we adopt the simple weighted sum aggregator and abandon the use of feature transformation and nonlinear activation. The graph convolution operation (a.k.a., propagation rule <ref type="bibr" target="#b38">[39]</ref>) in LightGCN is defined as:</p><formula xml:id="formula_3">e (k +1) u = i ∈N u 1 |N u | |N i | e (k ) i , e (k +1) i = u ∈N i 1 |N i | |N u | e (k ) u .<label>(3)</label></formula><p>The symmetric normalization term</p><formula xml:id="formula_4">1 √ | N u | √ | N i |</formula><p>follows the design of standard GCN <ref type="bibr" target="#b22">[23]</ref>, which can avoid the scale of embeddings increasing with graph convolution operations; other choices can also be applied here, such as the L 1 norm, while empirically we find this symmetric normalization has good performance (see experiment results in Section 4.4.2). It is worth noting that in LGC, we aggregate only the connected neighbors and do not integrate the target node itself (i.e., selfconnection). This is different from most existing graph convolution operations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> that typically aggregate extended neighbors and need to handle the self-connection specially. The layer combination operation, to be introduced in the next subsection, essentially captures the same effect as self-connections. Thus, there is no need in LGC to include self-connections. i for all items. When they are given, the embeddings at higher layers can be computed via LGC defined in Equation <ref type="formula" target="#formula_3">(3)</ref>. After K layers LGC, we further combine the embeddings obtained at each layer to form the final representation of a user (an item):</p><formula xml:id="formula_5">e u = K k =0 α k e (k) u ; e i = K k =0 α k e (k ) i ,<label>(4)</label></formula><p>where α k ≥ 0 denotes the importance of the k-th layer embedding in constituting the final embedding. It can be treated as a hyperparameter to be tuned manually, or as a model parameter (e.g., output of an attention network <ref type="bibr" target="#b2">[3]</ref>) to be optimized automatically.</p><p>In our experiments, we find that setting α k uniformly as 1/(K + 1) leads to good performance in general. Thus we do not design special component to optimize α k , to avoid complicating LightGCN unnecessarily and to keep its simplicity. The reasons that we perform layer combination to get final representations are threefold. <ref type="formula" target="#formula_0">(1)</ref> With the increasing of the number of layers, the embeddings will be over-smoothed <ref type="bibr" target="#b26">[27]</ref>. Thus simply using the last layer is problematic. <ref type="formula" target="#formula_2">(2)</ref> The embeddings at different layers capture different semantics. E.g., the first layer enforces smoothness on users and items that have interactions, the second layer smooths users (items) that have overlap on interacted items (users), and higher-layers capture higher-order proximity <ref type="bibr" target="#b38">[39]</ref>. Thus combining them will make the representation more comprehensive. <ref type="formula" target="#formula_3">(3)</ref> Combining embeddings at different layers with weighted sum captures the effect of graph convolution with self-connections, an important trick in GCNs (proof sees Section 3.2.1).</p><p>The model prediction is defined as the inner product of user and item final representations:ŷ</p><formula xml:id="formula_6">ui = e T u e i ,<label>(5)</label></formula><p>which is used as the ranking score for recommendation generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Matrix Form.</head><p>We provide the matrix form of LightGCN to facilitate implementation and discussion with existing models. Let the user-item interaction matrix be R ∈ R M ×N where M and N denote the number of users and items, respectively, and each entry R ui is 1 if u has interacted with item i otherwise 0. We then obtain the adjacency matrix of the user-item graph as</p><formula xml:id="formula_7">A = 0 R R T 0 ,<label>(6)</label></formula><p>Let the 0-th layer embedding matrix be E (0) ∈ R (M +N )×T , where T is the embedding size. Then we can obtain the matrix equivalent form of LGC as:</p><formula xml:id="formula_8">E (k+1) = (D − 1 2 AD − 1 2 )E (k ) ,<label>(7)</label></formula><p>where D is a (M + N ) × (M + N ) diagonal matrix, in which each entry D ii denotes the number of nonzero entries in the i-th row vector of the adjacency matrix A (also named as degree matrix). Lastly, we get the final embedding matrix used for model prediction as:</p><formula xml:id="formula_9">E = α 0 E (0) + α 1 E (1) + α 2 E (2) + ... + α K E (K ) = α 0 E (0) + α 1Ã E (0) + α 2Ã 2 E (0) + ... + α KÃ K E (0) ,<label>(8)</label></formula><formula xml:id="formula_10">whereÃ = D − 1 2 AD − 1 2</formula><p>is the symmetrically normalized matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Analysis</head><p>We conduct model analysis to demonstrate the rationality behind the simple design of LightGCN. First we discuss the connection with the Simplified GCN (SGCN) <ref type="bibr" target="#b39">[40]</ref>, which is a recent linear GCN model that integrates self-connection into graph convolution; this analysis shows that by doing layer combination, LightGCN subsumes the effect of self-connection thus there is no need for LightGCN to add self-connection in adjacency matrix. Then we discuss the relation with the Approximate Personalized Propagation of Neural Predictions (APPNP) <ref type="bibr" target="#b23">[24]</ref>, which is recent GCN variant that addresses oversmoothing by inspiring from Personalized PageRank <ref type="bibr" target="#b14">[15]</ref>; this analysis shows the underlying equivalence between LightGCN and APPNP, thus our LightGCN enjoys the sames benefits in propagating long-range with controllable oversmoothing. Lastly we analyze the second-layer LGC to show how it smooths a user with her second-order neighbors, providing more insights into the working mechanism of LightGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Relation with SGCN.</head><p>In <ref type="bibr" target="#b39">[40]</ref>, the authors argue the unnecessary complexity of GCN for node classfication and propose SGCN, which simplifies GCN by removing nonlinearities and collapsing the weight matrices to one weight matrix. The graph convolution in SGCN is defined as <ref type="bibr" target="#b3">4</ref> :</p><formula xml:id="formula_11">E (k +1) = (D + I) − 1 2 (A + I)(D + I) − 1 2 E (k ) ,<label>(9)</label></formula><p>where I ∈ R (M +N )×(M +N ) is an identity matrix, which is added on A to include self-connections. In the following analysis, we omit the (D + I) − 1 2 terms for simplicity, since they only re-scale embeddings. In SGCN, the embeddings obtained at the last layer are used for downstream prediction task, which can be expressed as:</p><formula xml:id="formula_12">E (K ) = (A + I)E (K −1) = (A + I) K E (0) = K 0 E (0) + K 1 AE (0) + K 2 A 2 E (0) + ... + K K A K E (0) .<label>(10)</label></formula><p>The above derivation shows that, inserting self-connection into A and propagating embeddings on it, is essentially equivalent to a weighted sum of the embeddings propagated at each LGC layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Relation with APPNP.</head><p>In a recent work <ref type="bibr" target="#b23">[24]</ref>, the authors connect GCN with Personalized PageRank <ref type="bibr" target="#b14">[15]</ref>, inspiring from which they propose a GCN variant named APPNP that can propagate long range without the risk of oversmoothing. Inspired by the teleport design in Personalized PageRank, APPNP complements each propagation layer with the starting features (i.e., the 0-th layer embeddings), which can balance the need of preserving locality (i.e., staying close to the root node to alleviate oversmoothing) and leveraging the information from a large neighborhood. The propagation layer in APPNP is defined as:</p><formula xml:id="formula_13">E (k +1) = βE (0) + (1 − β)ÃE (k ) ,<label>(11)</label></formula><p>where β is the teleport probability to control the retaining of starting features in the propagation, andÃ denotes the normalized adjacency matrix. In APPNP, the last layer is used for final prediction, i.e.,</p><formula xml:id="formula_14">E (K ) = βE (0) + (1 − β)ÃE (K −1) , = βE (0) + β(1 − β)ÃE (0) + (1 − β) 2Ã 2 E (K −2) = βE (0) + β(1 − β)ÃE (0) + β(1 − β) 2Ã 2 E (0) + ... + (1 − β) KÃ K E (0) .<label>(12)</label></formula><p>Aligning with Equation <ref type="formula" target="#formula_9">(8)</ref>, we can see that by setting α k accordingly, LightGCN can fully recover the prediction embedding used by APPNP. As such, LightGCN shares the strength of APPNP in combating oversmoothing -by setting the α properly, we allow using a large K for long-range modeling with controllable oversmoothing.</p><p>Another minor difference is that APPNP adds self-connection into the adjacency matrix. However, as we have shown before, this is redundant due to the weighted sum of different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Second-Order Embedding Smoothness.</head><p>Owing to the linearity and simplicity of LightGCN, we can draw more insights into how does it smooth embeddings. Here we analyze a 2-layer LightGCN to demonstrate its rationality. Taking the user side as an example, intuitively, the second layer smooths users that have overlap on the interacted items. More concretely, we have:</p><formula xml:id="formula_15">e (2) u = i ∈N u 1 |N u | |N i | e (1) i = i ∈N u 1 |N i | v ∈N i 1 |N u | |N v | e (0) v .<label>(13)</label></formula><p>We can see that, if another user v has co-interacted with the target user u, the smoothness strength of v on u is measured by the coefficient (otherwise 0):</p><formula xml:id="formula_16">c v−&gt;u = 1 |N u | |N v | i ∈N u ∩N v 1 |N i | .<label>(14)</label></formula><p>This coefficient is rather interpretable: the influence of a secondorder neighbor v on u is determined by 1) the number of cointeracted items, the more the larger; 2) the popularity of the co-interacted items, the less popularity (i.e., more indicative of user personalized preference) the larger; and 3) the activity of v, the less active the larger. Such interpretability well caters for the assumption of CF in measuring user similarity <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref> and evidences the reasonability of LightGCN. Due to the symmetric formulation of LightGCN, we can get similar analysis on the item side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>The trainable parameters of LightGCN are only the embeddings of the 0-th layer, i.e., Θ = {E (0) }; in other words, the model complexity is same as the standard matrix factorization (MF). We employ the Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b31">[32]</ref>, which is a pairwise loss that encourages the prediction of an observed entry to be higher than its unobserved counterparts:</p><formula xml:id="formula_17">L BP R = − M u=1 i ∈N u j / ∈N u ln σ (ŷ ui −ŷ u j ) + λ||E (0) || 2<label>(15)</label></formula><p>where λ controls the L 2 regularization strength. We employ the Adam <ref type="bibr" target="#b21">[22]</ref> optimizer and use it in a mini-batch manner. We are aware of other advanced negative sampling strategies which might improve the LightGCN training, such as the hard negative sampling <ref type="bibr" target="#b30">[31]</ref> and adversarial sampling <ref type="bibr" target="#b8">[9]</ref>. We leave this extension in the future since it is not the focus of this work. Note that we do not introduce dropout mechanisms, which are commonly used in GCNs and NGCF. The reason is that we do not have feature transformation weight matrices in LightGCN, thus enforcing L 2 regularization on the embedding layer is sufficient to prevent overfitting. This showcases LightGCN's advantages of being simple -it is easier to train and tune than NGCF which additionally requires to tune two dropout ratios (node dropout and message dropout) and normalize the embedding of each layer to unit length. Moreover, it is technically viable to also learn the layer combination coefficients {α k } K k =0 , or parameterize them with an attention network. However, we find that learning α on training data does not lead improvement. This is probably because the training data does not contain sufficient signal to learn good α that can generalize to unknown data. We have also tried to learn α from validation data, as inspired by <ref type="bibr" target="#b4">[5]</ref> that learns hyper-parameters on validation data. The performance is slightly improved (less than 1%). We leave the exploration of optimal settings of α (e.g., personalizing it for different users and items) as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We first describe experimental settings, and then conduct detailed comparison with NGCF <ref type="bibr" target="#b38">[39]</ref>, the method that is most relevant with LightGCN but more complicated (Section 4.2). We next compare with other state-of-the-art methods in Section 4.3. To justify the designs in LightGCN and reveal the reasons of its effectiveness, we perform ablation studies and embedding analyses in Section 4.4. The hyper-parameter study is finally presented in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>To reduce the experiment workload and keep the comparison fair, we closely follow the settings of the NGCF work <ref type="bibr" target="#b38">[39]</ref>. We request the experimented datasets (including train/test splits) from the authors, for which the statistics are shown in <ref type="table" target="#tab_2">Table 2</ref>. The Gowalla and Amazon-Book are exactly the same as the NGCF paper used, so we directly use the results in the NGCF paper. The only exception is the Yelp2018 data, which is a revised version. According to the authors, the previous version did not filter out cold-start items in the testing set, and they shared us the revised version only. Thus we re-run NGCF on the Yelp2018 data. The evaluation metrics are recall@20 and ndcg@20 computed by the all-ranking protocolall items that are not interacted by a user are the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Compared Methods.</head><p>The main competing method is NGCF, which has shown to outperform several methods including GCNbased models GC-MC <ref type="bibr" target="#b34">[35]</ref> and PinSage <ref type="bibr" target="#b44">[45]</ref>, neural network-based models NeuMF <ref type="bibr" target="#b18">[19]</ref> and CMN <ref type="bibr" target="#b9">[10]</ref>, and factorization-based models MF <ref type="bibr" target="#b31">[32]</ref> and HOP-Rec <ref type="bibr" target="#b42">[43]</ref>. As the comparison is done on the same datasets under the same evaluation protocol, we do not further compare with these methods. In addition to NGCF, we further compare with two relevant and competitive CF methods:</p><p>• Mult-VAE <ref type="bibr" target="#b27">[28]</ref>. This is an item-based CF method based on the variational autoencoder (VAE). It assumes the data is generated from a multinomial distribution and using variational inference for parameter estimation. We run the codes released by the authors <ref type="bibr" target="#b4">5</ref> , tuning the dropout ratio in [0, 0.2, 0.5], and the β in [0.2, 0.4, 0.6, 0.8]. The model architecture is the suggested one in the paper: 600 → 200 → 600. • GRMF <ref type="bibr" target="#b29">[30]</ref>. This method smooths matrix factorization by adding the graph Laplacian regularizer. For fair comparison on item recommendation, we change the rating prediction loss to BPR loss. The objective function of GRMF is:  <ref type="bibr" target="#b11">[12]</ref>. We optimize LightGCN with Adam <ref type="bibr" target="#b21">[22]</ref> and use the default learning rate of 0.001 and default mini-batch size of 1024 (on Amazon-Book, we increase the minibatch size to 2048 for speed). The L 2 regularization coefficient λ is searched in the range of {1e −6 , 1e −5 , ..., 1e −2 }, and in most cases the optimal value is 1e −4 . The layer combination coefficient α k is uniformly set to <ref type="bibr">1 1+K</ref> where K is the number of layers. We test K in the range of 1 to 4, and satisfactory performance can be achieved when K equals to 3. The early stopping and validation strategies are the same as NGCF. Typically, 1000 epochs are sufficient for LightGCN to converge. Our implementations are available in both TensorFlow 6 and PyTorch 7 .</p><formula xml:id="formula_18">L = − M u=1 i ∈N u j / ∈N u ln σ (e T u e i − e T u e j ) + λ д ||e u − e i || 2 + λ||E|| 2 ,<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison with NGCF</head><p>We perform detailed comparison with NGCF, recording the performance at different layers (1 to 4) in <ref type="table" target="#tab_5">Table 4</ref>, which also shows the percentage of relative improvement on each metric. We further plot the training curves of training loss and testing recall in <ref type="figure" target="#fig_3">Figure 3</ref> to reveal the advantages of LightGCN and to be clear of the training process. The main observations are as follows:</p><p>• In all cases, LightGCN outperforms NGCF by a large margin. For example, on Gowalla the highest recall reported in the NGCF paper is 0.1570, while our LightGCN can reach 0.1830 under the 4-layer setting, which is 16.56% higher. On average, the recall improvement on the three datasets is 16.52% and the ndcg improvement is 16.87%, which are rather significant. • Aligning <ref type="table" target="#tab_5">Table 4</ref> with <ref type="table" target="#tab_0">Table 1</ref> in Section 2, we can see that LightGCN performs better than NGCF-fn, the variant of NGCF that removes feature transformation and nonlinear activation. As NGCF-fn still contains more operations than LightGCN (e.g., selfconnection, the interaction between user embedding and item  embedding in graph convolution, and dropout), this suggests that these operations might also be useless for NGCF-fn. • Increasing the number of layers can improve the performance, but the benefits diminish. The general observation is that increasing the layer number from 0 (i.e., the matrix factorization model, results see <ref type="bibr" target="#b38">[39]</ref>) to 1 leads to the largest performance gain, and using a layer number of 3 leads to satisfactory performance in most cases. This observation is consistent with NGCF's finding. • Along the training process, LightGCN consistently obtains lower training loss, which indicates that LightGCN fits the training data better than NGCF. Moreover, the lower training loss successfully transfers to better testing accuracy, indicating the strong generalization power of LightGCN. In contrast, the higher training loss and lower testing accuracy of NGCF reflect the practical difficulty to train such a heavy model it well. Note that in the figures we show the training process under the optimal hyper-parameter setting for both methods. Although increasing the learning rate of NGCF can decrease its training loss (even lower than that of LightGCN), the testing recall could not be improved, as lowering training loss in this way only finds trivial solution for NGCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison with</head><p>State-of-the-Arts <ref type="table" target="#tab_5">Table 4</ref> shows the performance comparison with competing methods. We show the best score we can obtain for each method.</p><p>We can see that LightGCN consistently outperforms other methods on all three datasets, demonstrating its high effectiveness with simple yet reasonable designs. Note that LightGCN can be further improved by tuning the α k (see <ref type="figure" target="#fig_4">Figure 4</ref> for an evidence), while here we only use a uniform setting of 1 K +1 to avoid over-tuning it. Among the baselines, Mult-VAE exhibits the strongest performance, which is better than GRMF and NGCF. The performance of GRMF is on a par with NGCF, being better than MF, which admits the utility of enforcing embedding smoothness with Laplacian regularizer. By adding normalization into the Laplacian regularizer, GRMFnorm betters than GRMF on Gowalla, while brings no benefits on Yelp2018 and Amazon-Book. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation and Effectiveness Analyses</head><p>We perform ablation studies on LightGCN by showing how layer combination and symmetric sqrt normalization affect its performance. To justify the rationality of LightGCN as analyzed in Section 3.2.3, we further investigate the effect of embedding smoothness -the key reason of LightGCN's effectiveness. <ref type="figure" target="#fig_4">Figure 4</ref> shows the results of LightGCN and its variant LightGCN-single that does not use layer combination (i.e., E (K ) is used for final prediction for a K-layer LightGCN). We omit the results on Yelp2018 due to space limitation, which show similar trend with Amazon-Book. We have three main observations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Impact of Layer Combination.</head><p>• Focusing on LightGCN-single, we find that its performance first improves and then drops when the layer number increases from   1 to 4. The peak point is on layer 2 in most cases, while after that it drops quickly to the worst point of layer 4. This indicates that smoothing a node's embedding with its first-order and secondorder neighbors is very useful for CF, but will suffer from oversmoothing issues when higher-order neighbors are used. • Focusing on LightGCN, we find that its performance gradually improves with the increasing of layers. Even using 4 layers, LightGCN's performance is not degraded. This justifies the effectiveness of layer combination for addressing over-smoothing, as we have technically analyzed in Section 3.2.2 (relation with APPNP). • Comparing the two methods, we find that LightGCN consistently outperforms LightGCN-single on Gowalla, but not on Amazon-Book and Yelp2018 (where the 2-layer LightGCN-single performs the best). Regarding this phenomenon, two points need to be noted before we draw conclusion: 1) LightGCN-single is special case of LightGCN that sets α K to 1 and other α k to 0; 2) we do not tune the α k and simply set it as 1 K +1 uniformly for LightGCN. As such, we can see the potential of further enhancing the performance of LightGCN by tuning α k .  <ref type="formula" target="#formula_3">(3)</ref>). To study its rationality, we explore different choices here. We test the use of normalization only at the left side (i.e., the target node's coefficient) and the right side (i.e., the neighbor node's coefficient). We also test L 1 normalization, i.e., removing the square root. Note that if removing normalization, the training becomes numerically unstable and suffers from nota-value (NAN) issues, so we do not show this setting. <ref type="table" target="#tab_6">Table 5</ref>  shows the results of the 3-layer LightGCN. We have the following observations:</p><p>• The best setting in general is using sqrt normalization at both sides (i.e., the current design of LightGCN). Removing either side will drop the performance largely. • The second best setting is using L 1 normalization at the left side only (i.e., LightGCN-L 1 -L). This is equivalent to normalize the adjacency matrix as a stochastic matrix by the in-degree. • Normalizing symmetrically on two sides is helpful for the sqrt normalization, but will degrade the performance of L 1 normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Analysis of Embedding Smoothness.</head><p>As we have analyzed in Section 3.2.3, a 2-layer LightGCN smooths a user's embedding based on the users that have overlap on her interacted items, and the smoothing strength between two users c v→u is measured in Equation <ref type="bibr" target="#b13">(14)</ref>. We speculate that such smoothing of embeddings is the key reason of LightGCN's effectiveness. To verify this, we first define the smoothness of user embeddings as:</p><formula xml:id="formula_19">S U = M u=1 M v=1 c v→u ( e u ||e u || 2 − e v ||e v || 2 ) 2 ,<label>(17)</label></formula><p>where the L 2 norm on embeddings is used to eliminate the impact of the embedding's scale. Similarly we can obtained the definition for item embeddings. <ref type="table" target="#tab_7">Table 6</ref> shows the smoothness of two models, matrix factorization (i.e., using the E (0) for model prediction) and the 2-layer LightGCN-single (i.e., using the E (2) for prediction). Note that the 2-layer LightGCN-single outperforms MF in recommendation accuracy by a large margin. As can be seen, the smoothness loss of LightGCN-single is much lower than that of MF. This indicates that by conducting light graph convolution, the embeddings become smoother and more suitable for recommendation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-parameter Studies</head><p>When applying LightGCN to a new dataset, besides the standard hyper-parameter learning rate, the most important hyper-parameter to tune is the L 2 regularization coefficient λ. Here we investigate the performance change of LightGCN w.r.t. λ. As shown in <ref type="figure" target="#fig_7">Figure 5</ref>, LightGCN is relatively insensitive to λ -even when λ sets to 0, LightGCN is better than NGCF, which additionally uses dropout to prevent overfitting <ref type="bibr" target="#b7">8</ref> . This shows that LightGCN is less prone to overfitting -since the only trainable parameters in LightGCN are ID embeddings of the 0-th layer, the whole model is easy to train and to regularize. The optimal value for Yelp2018, Amazon-Book, and Gowalla is 1e −3 , 1e −4 , and 1e −4 , respectively. When λ is larger than 1e −3 , the performance drops quickly, which indicates that too strong regularization will negatively affect model normal training and is not encouraged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK 5.1 Collaborative Filtering</head><p>Collaborative Filtering (CF) is a prevalent technique in modern recommender systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45]</ref>. One common paradigm of CF model is to parameterize users and items as embeddings, and learn the embedding parameters by reconstructing historical useritem interactions. For example, earlier CF models like matrix factorization (MF) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref> project the ID of a user (or an item) into an embedding vector. The recent neural recommender models like NCF <ref type="bibr" target="#b18">[19]</ref> and LRML <ref type="bibr" target="#b33">[34]</ref> use the same embedding component, while enhance the interaction modeling with neural networks.</p><p>Beyond merely using ID information, another type of CF methods considers historical items as the pre-existing features of a user, towards better user representations. For example, FISM <ref type="bibr" target="#b20">[21]</ref> and SVD++ <ref type="bibr" target="#b24">[25]</ref> use the weighted average of the ID embeddings of historical items as the target user's embedding. Recently, researchers realize that historical items have different contributions to shape personal interest. Towards this end, attention mechanisms are introduced to capture the varying contributions, such as ACF <ref type="bibr" target="#b2">[3]</ref> and NAIS <ref type="bibr" target="#b17">[18]</ref>, to automatically learn the importance of each historical item. When revisiting historical interactions as a user-item bipartite graph, the performance improvements can be attributed to the encoding of local neighborhood -one-hop neighbors -that improves the embedding learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Methods for Recommendation</head><p>Another relevant research line is exploiting the user-item graph structure for recommendation. Prior efforts like ItemRank <ref type="bibr" target="#b12">[13]</ref>, use the label propagation mechanism to directly propagate user preference scores over the graph, i.e., encouraging connected nodes to have similar labels. Recently emerged graph neural networks (GNNs) shine a light on modeling graph structure, especially highhop neighbors, to guide the embedding learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>. Early studies define graph convolution on the spectral domain, such as Laplacian eigen-decomposition <ref type="bibr" target="#b0">[1]</ref> and Chebyshev polynomials <ref type="bibr" target="#b7">[8]</ref>, which are computationally expensive. Later on, GraphSage <ref type="bibr" target="#b13">[14]</ref> and GCN <ref type="bibr" target="#b22">[23]</ref> re-define graph convolution in the spatial domain, i.e., aggregating the embeddings of neighbors to refine the target node's embedding. Owing to its interpretability and efficiency, it quickly becomes a prevalent formulation of GNNs and is being widely used <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref>. Motivated by the strength of graph convolution, recent efforts like NGCF <ref type="bibr" target="#b38">[39]</ref>, GC-MC <ref type="bibr" target="#b34">[35]</ref>, and PinSage <ref type="bibr" target="#b44">[45]</ref> adapt GCN to the user-item interaction graph, capturing CF signals in high-hop neighbors for recommendation.</p><p>It is worth mentioning that several recent efforts provide deep insights into GNNs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40]</ref>, which inspire us developing LightGCN. Particularly, Wu et al. <ref type="bibr" target="#b39">[40]</ref> argues the unnecessary complexity of GCN, developing a simplified GCN (SGCN) model by removing nonlinearities and collapsing multiple weight matrices into one. One main difference is that LightGCN and SGCN are developed for different tasks, thus the rationality of model simplification is different. Specifically, SGCN is for node classification, performing simplification for model interpretability and efficiency. In contrast, LightGCN is on collaborative filtering (CF), where each node has an ID feature only. Thus, we do simplification for a stronger reason: nonlinearity and weight matrices are useless for CF, and even hurt model training. For node classification accuracy, SGCN is on par with (sometimes weaker than) GCN. While for CF accuracy, LightGCN outperforms GCN by a large margin (over 15% improvement over NGCF). Lastly, another work conducted in the same time <ref type="bibr" target="#b3">[4]</ref> also finds that the nonlinearity is unnecessary in NGCF and develops linear GCN model for CF. In contrast, our LightGCN makes one step further -we remove all redundant parameters and retain only the ID embeddings, making the model as simple as MF.</p><p>We believe the insights of LightGCN are inspirational to future developments of recommender models. With the prevalence of linked graph data in real applications, graph-based models are becoming increasingly important in recommendation; by explicitly exploiting the relations among entities in the predictive model, they are advantageous to traditional supervised learning scheme like factorization machines <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref> that model the relations implicitly. For example, a recent trend is to exploit auxiliary information such as item knowledge graph <ref type="bibr" target="#b37">[38]</ref>, social network <ref type="bibr" target="#b40">[41]</ref> and multimedia content <ref type="bibr" target="#b43">[44]</ref> for recommendation, where GCNs have set up the new state-of-the-art. However, these models may also suffer from the similar issues of NGCF since the user-item interaction graph is also modeled by same neural operations that may be unnecessary. We plan to explore the idea of LightGCN in these models. Another future direction is to personalize the layer combination weights α k , so as to enable adaptive-order smoothing for different users (e.g., sparse users may require more signal from higher-order neighbors while active users require less). Lastly, we will explore further the strengths of LightGCN's simplicity, studying whether fast solution exists for non-sampling regression loss <ref type="bibr" target="#b19">[20]</ref> and streaming it for online industrial scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>u</head><label></label><figDesc>∥· · · ∥e (L) u ) to sum (i.e., e * u = e (0) u + · · · + e (L) u ). Note that this change has little effect 3 https://github.com/xiangwang1223/neural_graph_collaborative_filtering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Training curves (training loss and testing recall) of NGCF and its three simplified variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3. 1 . 2</head><label>12</label><figDesc>Layer Combination and Model Prediction. In LightGCN, the only trainable model parameters are the embeddings at the 0-th layer, i.e., e (0) u for all users and e (0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Training curves of LightGCN and NGCF, which are evaluated by training loss and testing recall per 20 epochs on Gowalla and Amazon-Book (results on Yelp2018 show exactly the same trend which are omitted for space).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Results of LightGCN and the variant that does not use layer combination (i.e., LightGCN-single) at different layers on Gowalla and Amazon-Book (results on Yelp2018 shows the same trend with Amazon-Book which are omitted for space).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>LightGCN-L 1 -</head><label>1</label><figDesc>L 0.1724 0.1414 0.0630 0.0511 0.0419 0.0320 LightGCN-L 1 -R 0.1578 0.1348 0.0587 0.0477 0.0334 0.0259 LightGCN-L 1 0.159 0.1319 0.0573 0.0465 0.0361 0.0275 LightGCN-L 0.1589 0.1317 0.0619 0.0509 0.0383 0.0299 LightGCN-R 0.1420 0.1156 0.0521 0.0401 0.0252 0.0196 LightGCN 0.1830 0.1554 0.0649 0.0530 0.0411 0.0315 Method notation: -L means only the left-side norm is used, -R means only the right-side norm is used, and -L 1 means the L 1 norm is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 4 . 2 1 √</head><label>421</label><figDesc>Impact of Symmetric Sqrt Normalization. In LightGCN, we employ symmetric sqrt normalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Performance of 2-layer LightGCN w.r.t. different regularization coefficient λ on Yelp and Amazon-Book.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of NGCF and its three variants.</figDesc><table><row><cell></cell><cell>Gowalla</cell><cell>Amazon-Book</cell></row><row><cell></cell><cell cols="2">recall ndcg recall ndcg</cell></row><row><cell>NGCF</cell><cell cols="2">0.1547 0.1307 0.0330 0.0254</cell></row><row><cell>NGCF-f</cell><cell cols="2">0.1686 0.1439 0.0368 0.0283</cell></row><row><cell cols="3">NGCF-n 0.1536 0.1295 0.0336 0.0258</cell></row><row><cell cols="3">NGCF-fn 0.1742 0.1476 0.0399 0.0303</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the experimented data.</figDesc><table><row><cell>Dataset</cell><cell cols="3">User # Item # Interaction # Density</cell></row><row><cell>Gowalla</cell><cell>29, 858 40, 981</cell><cell>1, 027, 370</cell><cell>0.00084</cell></row><row><cell>Yelp2018</cell><cell>31, 668 38, 048</cell><cell>1, 561, 406</cell><cell>0.00130</cell></row><row><cell cols="2">Amazon-Book 52, 643 91, 599</cell><cell>2, 984, 108</cell><cell>0.00062</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>where λ д is searched in the range of [1e −5 , 1e −4 , ..., 1e −1 ]. Moreover, we compare with a variant that adds normalization to graph Laplacian: λ д || e u √</figDesc><table><row><cell>|N u |</cell><cell>− e i √ |N i |</cell><cell>|| 2 , which is termed</cell></row><row><cell cols="3">as GRMF-norm. Other hyper-parameter settings are same as</cell></row><row><cell cols="3">LightGCN. The two GRMF methods benchmark the performance</cell></row><row><cell cols="3">of smoothing embeddings via Laplacian regularizer, while our</cell></row><row><cell cols="3">LightGCN achieves embedding smoothing in the predictive</cell></row><row><cell>model.</cell><cell></cell><cell></cell></row></table><note>4.1.2 Hyper-parameter Settings. Same as NGCF, the embedding size is fixed to 64 for all models and the embedding parameters are initialized with the Xavier method</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison between NGCF and LightGCN at different layers. +12.79%) 0.1492(+13.46%) 0.0631(+16.20%) 0.0515(+16.51%) 0.0384(+22.68%) 0.0298(+23.65%) The scores of NGCF on Gowalla and Amazon-Book are directly copied fromTable 3of the NGCF paper (https://arxiv.org/abs/1905.08108)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell></cell><cell>Gowalla</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Yelp2018</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Amazon-Book</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Layer #</cell><cell cols="2">Method</cell><cell>recall</cell><cell></cell><cell cols="2">ndcg</cell><cell></cell><cell>recall</cell><cell></cell><cell></cell><cell>ndcg</cell><cell></cell><cell></cell><cell>recall</cell><cell></cell><cell></cell><cell>ndcg</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="10">NGCF NGCF LightGCN 0.1755(2 Layers 0.1556 1 Layer 0.1547 LightGCN 0.1777(+14.84%) 0.1524(+16.60%) 0.0622(+9.89%) 0.1315 0.0543 0.1307 0.0566</cell><cell></cell><cell cols="3">0.0442 0.0465 0.0504(+8.38%)</cell><cell cols="6">0.0313 0.0330 0.0411(+24.54%) 0.0315(+24.02%) 0.0241 0.0254</cell></row><row><cell></cell><cell></cell><cell>3 Layers</cell><cell cols="19">NGCF LightGCN 0.1823(+16.19%) 0.1555(+17.18%) 0.0639(+10.38%) 0.0525(+10.06%) 0.0410(+21.66%) 0.0318(+21.84%) 0.1569 0.1327 0.0579 0.0477 0.0337 0.0261</cell></row><row><cell></cell><cell></cell><cell>4 Layers</cell><cell cols="19">NGCF LightGCN 0.1830(+16.56%) 0.1550(+16.80%) 0.0649(+14.58%) 0.0530(+15.02%) 0.0406(+17.92%) 0.0313(+18.92%) 0.1570 0.1327 0.0566 0.0461 0.0344 0.0263</cell></row><row><cell></cell><cell>0.06</cell><cell></cell><cell>Gowalla</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Gowalla</cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell cols="2">Amazon-Book</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Amazon-Book</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.040</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training-Loss</cell><cell>0.02 0.03 0.04</cell><cell></cell><cell></cell><cell></cell><cell>recall@20</cell><cell>0.12 0.14 0.16</cell><cell></cell><cell></cell><cell></cell><cell>Training-Loss</cell><cell>0.02 0.03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>recall@20</cell><cell>0.035 0.020 0.025 0.030</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.015</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.010</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epoch</cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The comparison of overall performance among LightGCN and competing methods.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Gowalla</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell>Method</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row><row><cell>NGCF</cell><cell cols="6">0.1570 0.1327 0.0579 0.0477 0.0344 0.0263</cell></row><row><cell>Mult-VAE</cell><cell cols="6">0.1641 0.1335 0.0584 0.0450 0.0407 0.0315</cell></row><row><cell>GRMF</cell><cell cols="6">0.1477 0.1205 0.0571 0.0462 0.0354 0.0270</cell></row><row><cell cols="7">GRMF-norm 0.1557 0.1261 0.0561 0.0454 0.0352 0.0269</cell></row><row><cell>LightGCN</cell><cell cols="6">0.1830 0.1554 0.0649 0.0530 0.0411 0.0315</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance of the 3-layer LightGCN with different choices of normalization schemes in graph convolution.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Gowalla</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell></row><row><cell>Method</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Smoothness loss of the embeddings learned by LightGCN and MF (the lower the smoother).</figDesc><table><row><cell>Dataset</cell><cell cols="3">Gowalla Yelp2018 Amazon-book</cell></row><row><cell></cell><cell cols="3">Smoothness of User Embeddings</cell></row><row><cell>MF</cell><cell>15449.3</cell><cell>16258.2</cell><cell>38034.2</cell></row><row><cell>LightGCN-single</cell><cell>12872.7</cell><cell>10091.7</cell><cell>32191.1</cell></row><row><cell></cell><cell cols="3">Smoothness of Item Embeddings</cell></row><row><cell>MF</cell><cell>12106.7</cell><cell>16632.1</cell><cell>28307.9</cell></row><row><cell>LightGCN-single</cell><cell>5829.0</cell><cell>6459.8</cell><cell>16866.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The weight matrix in SGCN can be absorbed into the 0-th layer embedding parameters, thus it is omitted in the analysis.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/dawenl/vae_cf 6 https://github.com/kuandeng/LightGCN 7 https://github.com/gusye1234/pytorch-light-gcn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Note that Gowalla shows the same trend with Amazon-Book, so its curves are not shown to better highlight the trend of Yelp2018 and Amazon-Book.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">CONCLUSION AND FUTURE WORKIn this work, we argued the unnecessarily complicated design of GCNs for collaborative filtering, and performed empirical studies to justify this argument. We proposed LightGCN which consists of two essential components -light graph convolution and layer combination. In light graph convolution, we discard feature transformation and nonlinear activation -two standard operations in GCNs but inevitably increase the training difficulty. In layer combination, we construct a node's final embedding as the weighted sum of its embeddings on all layers, which is proved to subsume the effect of self-connections and is helpful to control oversmoothing.We conduct experiments to demonstrate the strengths of LightGCN in being simple: easier to be trained, better generalization ability, and more effective.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collaborative Similarity Embedding for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2637" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attentive Collaborative Filtering: Multimedia Recommendation with Item-and Component-Level Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting Graph based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">λOpt: Learn to Regularize Recommender Models in Finer Levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="978" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reinforced Negative Sampling for Recommendation with Exposure Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2230" to="2236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative Memory Network for Recommendation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Ebesu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal Relational Ranking for Stock Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ItemRank: A Random-Walk Based Scoring Algorithm for Recommender Engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Pucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2766" to="2771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topic-sensitive pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAIS: Neural Attentive Item Similarity Model for Recommendation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">Neural Collaborative Filtering. In WWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast Matrix Factorization with Nonuniform Weights on Missing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FISM: factored item similarity models for top-N recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="659" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper Insights Into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<title level="m">Collaborative filtering with graph information: Consistency and scalable methods. In NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving pairwise learning for item recommendation from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast context-aware recommendations with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Latent relational metric learning via memory-based attention for collaborative ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop on Deep Learning Day</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unifying User-based and Item-based Collaborative Filtering Approaches by Similarity Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arjen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">J T</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">KGAT: Knowledge Graph Attention Network for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Neural Influence Diffusion Model for Social Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HOP-rec: high-order proximity for implicit recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MMGCN: Multimodal Graph Convolution Network for Personalized Recommendation of Micro-video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Xiangnan He, Richang Hong, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD (Data Science track</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liguang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-Domain Recommendation via Preference Propagation GraphNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2165" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bilinear Graph Neural Network with Neighbor Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
