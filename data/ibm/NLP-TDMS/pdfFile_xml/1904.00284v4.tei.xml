<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COCO-GAN: Generation by Parts via Conditional Coordinating</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><forename type="middle">Hubert</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Wei</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
							<email>htchen@cs.nthu.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COCO-GAN: Generation by Parts via Conditional Coordinating</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose COnditional COordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never generated during training, we show that COCO-GAN can produce state-of-the-art-quality full images during inference. We further demonstrate a variety of novel applications enabled by teaching the network to be aware of coordinates. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides highparallelism, and can generate parts of images on-demand.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The human perception has only partial access to the surrounding environment due to biological restrictions (such as the limited acuity area of the fovea), and therefore humans infer the whole environment by "assembling" few local views obtained from their eyesight. This recognition can be done partially because humans are able to associate the spatial coordination of these local views with the environment (where they are situated in), then correctly assemble these local views, and recognize the whole environment. Cur-enerator iscriminator <ref type="figure">Figure 1</ref>: COCO-GAN generates and discriminates only parts of the full image via conditional coordinating. Despite the full images are never generated during training, the generator can still produce full images that are visually indistinguishable to standard GAN samples during inference.</p><p>rently, most of the computational vision models assume to have access to full images as inputs for down-streaming tasks, which sometimes may become a computational bottleneck of modern vision models when dealing with large field-of-view images. This limitation piques our interest and raises an intriguing question: "is it possible to train generative models to be aware of coordinate system for generating local views (i.e. parts of the image) that can be assembled into a globally coherent image?"</p><p>Conventional GANs <ref type="bibr" target="#b13">[11]</ref> target at learning a generator that models a mapping from a prior latent distribution (normally a unit Gaussian) to the real data distribution. To achieve generating high-quality images by parts, we introduce coordinate systems within an image and divide image generation into separated parallel sub-procedures. Our framework, named COnditional COordinate GAN (COCO-GAN), aims at learning a coordinate manifold that is orthogonal to the latent distribution manifold. After a latent vector is sampled, the generator conditions on each spatial coordinate and generate patches at each corresponding spatial position. On the  <ref type="figure">Figure 2</ref>: An overview of COCO-GAN training. The latent vectors are duplicated multiple times, concatenated with micro coordinates, and feed to the generator to generate micro patches. Then we concatenate multiple micro patches to form a larger macro patch. The discriminator learns to discriminate between real and fake macro patches and an auxiliary task predicting the coordinate of the macro patch. Note that the full images are only generated in the testing phase (Appendix 12).</p><p>other hand, the discriminator learns to judge whether adjacent patches are structurally sound, visually homogeneous, and continuous across the edges between multiple patches. <ref type="figure">Figure 1</ref> depicts the high-level idea.</p><p>We perform a series of experiments that set the generator to generate patches under different configurations. The results show that COCO-GAN can achieve state-of-the-art generation quality in multiple setups with "Frchet Inception Distance" (FID) <ref type="bibr" target="#b15">[13]</ref> score measurement. Furthermore, to our surprise, even if the generated patch sizes are set to as small as 4 × 4 pixels, the full images that are composed by 1024 separately generated patches can still consistently form complete and plausible human faces. To further demonstrate the generator indeed learns the coordinate manifold, we perform an extrapolation experiment on the coordinate condition. Interestingly, the generator is able to generate novel contents that are never explicitly presented in the real data. We show that COCO-GAN can produce 384 × 384 images that are larger than the 256 × 256 real training samples. We call such a procedure "beyond-boundary generation"; all the samples created through this procedure are guaranteed to be novel samples, which is a powerful example of artificial creativity.</p><p>We then investigate another series of novel applications and merits brought about by teaching the network to be aware of the coordinates. The first is panorama generation. To preserve the native horizontally-cyclic topology of panoramic images, we apply cylindrical coordinate to COCO-GAN training process and show that the generated samples are indeed horizontally cyclic. Next, we demonstrate that the "image generation by parts" schema is highly parallelable and saves a significant amount of memory for both training and infer-ence. Furthermore, as the generation procedures of patches are disjoint, COCO-GAN inherently supports generation ondemand, which particularly fits applications for computationrestricted environments, such as mobile and virtual reality. Last but not the least, we show that by adding an extra prediction branch that reconstructs latent vectors, COCO-GAN can generate an entire image with respect to a patch of real image as guidance, which we call "patch-guided generation".</p><p>COCO-GAN unveils the potential of generating high-quality images with conditional coordinating. This property enables a wide range of new applications, and can further be used by other tasks with encoding-decoding schema. With the "generation by parts" property, COCO-GAN is highly parallelable and intrinsically inherits the classic divide-and-conquer design paradigm, which facilitates future research toward large field-of-view data generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">COCO-GAN</head><p>Overview. COCO-GAN consists of two networks (a generator G and a discriminator D), two coordinate systems (a finer-grained micro coordinate system for G and a coarsergrained macro coordinate system for D), and images of three sizes: full images (real: x, generated: s), macro patches (real: x , generated: s ) and micro patches (generated: s ).</p><p>The generator of COCO-GAN is a conditional model that generates micro patches with s = G(z, c ), where z is a latent vector and c is a micro coordinate condition designating the spatial location of s to be generated. The final goal of G is to generate realistic and seamless full images We list all the used symbols in Appendix B.</p><p>by assembling a set of s altogether with a merging function ϕ. In practice, we find that setting ϕ as a concatenation function without overlapping is sufficient for COCO-GAN to synthesize high-quality images. Note that the size of the micro patches and ϕ also imply a cropping transformation ψ, cropping out a macro patch x from a real image x, which is used to sample real macro patches for training D.</p><p>In the above setting, the seams between consecutive patches become the major obstacle of full image realism. To mitigate this issue, we train the discriminator with larger macro patches that are assembled with multiple micro patches. Such a design aims to introduce the continuity and coherence of multiple consecutive or nearby micro patches into the consideration of adversarial loss. In order to fool the discriminator, the generator has to close the gap at the boundaries between the generated patches.</p><p>COCO-GAN is trained with three loss terms: patch Wasserstein loss L W , patch gradient penalty loss L GP , and spatial consistency loss L S . For L W and L GP , compared with conventional GANs that use full images x for both G and D training, COCO-GAN only cooperates with macro patches and micro patches. Meanwhile, the spatial consistency loss L S is an ACGAN-like <ref type="bibr" target="#b26">[24]</ref> loss function. Depending on the design of ϕ, we can calculate macro coordinate c for the macro patches x . L S aims at minimizing the distance loss between the real macro coordinate c and the discriminator-estimated macro coordinateĉ . The loss functions of COCO-GAN are</p><formula xml:id="formula_0">L W + λL GP + αL S , for the discriminator D, −L W + αL S ,</formula><p>for the generator G.</p><p>(1)</p><p>Spatial coordinate system. We start with designing the two spatial coordinate systems, a micro coordinate system for the generator G and a macro coordinate system for the discriminator D. Depending on the design of the aforementioned merging function ϕ, each macro coordinate c (i,j) is associated with a matrix of micro coordinates: C (i,j) = c (i:i+N,j:j+M ) , whose complete form is</p><formula xml:id="formula_1">C (i,j) =    c (i,j) c (i,j+1) . . . c (i,j+M −1) c (i+1,j) c (i+1,j+1) . . . c (i+1,j+M −1) . . . . . . . . . . . . c (i+N −1,j) c (i+N −1,j+1) . . . c (i+N −1,j+M −1)    .</formula><p>During COCO-GAN training, we uniformly sample all combinations of C (i,j) . The generator G conditions on each micro coordinate c (i,j) , and learns to accordingly produce micro patches s (i,j) by G(z, c (i,j) ). The matrix of generated micro patches S (i,j) = G(z, C (i,j) ) are produced independently while sharing the same latent vector z across the micro coordinate matrix.</p><p>The design principle of the C (i,j) construction is that, the accordingly generated micro patches S (i,j) should be spatially close to each other. Then the micro patches are merged by the merging function ϕ to form a complete macro patch s (i,j) = ϕ(S (i,j) ) as a coarser partial-view of the imagery full-scene. Meanwhile, we assign s (i,j) with a new macro coordinate c (i,j) under the macro coordinate system with respect to C (i,j) . On the real data side, we directly sample macro coordinates c (i,j) , then produce real macro patches x (i,j) = ψ(x, c (i,j) ) with the cropping function ψ. Note that the design choice of the micro coordinates C (i,j) is also correlated with the topological characteristic of the micro/macro coordinate systems (for instance, the cylindrical coordinate system for panoramas used in Section 3.4).</p><p>In <ref type="figure">Figure 2</ref>, we illustrate one of the most straightforward designs for the above heuristic functions that we have adopted throughout our experiments. The micro patches are always a neighbor of each other and can be directly combined into a square-shaped macro patch using ϕ. We observe that setting ϕ to be a concatenation function is sufficient for G to learn smoothly, and eventually to produce seamless and highquality images.</p><p>During the testing phase, depending on the design of the micro coordinate system, we can infer a corresponding spatial coordinate matrix C full . Such a matrix is used to independently produce all the micro patches required for constituting the full image.</p><p>Loss functions. The patch Wasserstein loss L W is a macropatch-level Wasserstein distance loss similar to Wasserstein-GAN <ref type="bibr" target="#b3">[1]</ref> loss. It forces the discriminator to distinguish between the real macro patches x and fake macro patches s , and on the other hand, encourages the generator to confuse the discriminator with seemingly realistic micro patches s . Its complete form is</p><formula xml:id="formula_2">L W = E x,c [ D(ψ(x, c )) ] − E z,C [ D(ϕ(G(z, C )) ] . (2)</formula><p>Again, note that G(z, C ) represents that the micro patches are generated through independent processes. We also apply Gradient Penalty <ref type="bibr" target="#b14">[12]</ref> to the macro patches discrimination:</p><formula xml:id="formula_3">L GP = Ê s ( ∇ŝ D(ŝ ) 2 − 1) 2 ,<label>(3)</label></formula><p>whereŝ = s + (1 − ) x is calculated between randomly paired s and x with a random number ∈ 0, 1 .</p><p>Finally, the spatial consistency loss L S is similar to ACGAN loss <ref type="bibr" target="#b26">[24]</ref>. The discriminator is equipped with an auxiliary prediction head A, which aims to estimate the macro coordinate of a given macro patch with A(x ). A slight difference is that both c and c have relatively more continuous values than the discrete setting of ACGAN. As a result, we apply a distance measurement loss for L S , which is an L 2 -loss. It aims to train G to generate corresponding micro patches by G(z, c ) with respect to the given spatial condition c . The spatial consistency loss is  For the first five columns, each column uses the same latent vector, e.g., the leftmost full image (first row), the leftmost micro patch (second row), and the leftmost micro patch (third row) share the same latent vector. Note that the columns are not aligned due to different sizes. More results can be found in the Appendix F.</p><formula xml:id="formula_4">L S = E c [ c − A(x ) 2 ] .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quality of Generation by Parts</head><p>We start with validating COCO-GAN on two common GANs testbeds: CelebA <ref type="bibr" target="#b21">[19]</ref> and LSUN <ref type="bibr" target="#b37">[35]</ref> (bedroom). To verify that COCO-GAN can learn to generate the full image without the access to the full image, we first conduct a basic setting for both datasets in which the macro patch edge length (CelebA: 64 × 64, LSUN: 128 × 128) is 1/2 of the full image and the micro patch edge length (CelebA: 32 × 32, LSUN: 64 × 64) is 1/2 of the macro patch. We denote the above cases as CelebA (N2,M2,S32) and LSUN (N2,M2,S32), where N2 and M2 represent that a macro patch is composed of 2 × 2 micro patches, and S32 means each of the micro patches is 32 × 32 pixels. Our results in <ref type="figure" target="#fig_2">Figure 3</ref> show that COCO-GAN generates high-quality images in the settings that the micro patch size is 1/16 of the full image.</p><p>To further show that COCO-GAN can learn more fine-grained and tiny micro patches under the same macro patch size setting, we sweep through the resolution of micro patch from 32 × 32, 16 × 16, 8 × 8, 4 × 4, labelled as (N2,M2,S32), (N4,M4,S16), (N8,M8,S8) and (N16,M16,S4), respectively. The results shown in <ref type="figure" target="#fig_5">Figure 4</ref> suggest that COCO-GAN can learn coordinate information and generate images by parts even with extremely tiny 4 × 4 pixels micro patch.</p><p>We report Frchet Inception Distance (FID) <ref type="bibr" target="#b15">[13]</ref> in <ref type="table">Table 1</ref> comparing with state-of-the-art GANs. Without additional hyper-parameter tuning, the quantitative results show that COCO-GAN is competitive with other state-of-the-art GANs.</p><p>In Appendix L, we also provide Wasserstein distance and FID score through time as training indicators. The curves suggest that COCO-GAN is stable during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Space Continuity</head><p>To demonstrate the space continuity more precisely, we perform the interpolation experiment in two directions: "fullimages interpolation" and "coordinates interpolation". We describe the model details in Appendix C.</p><p>(a) CelebA (N4,M4,S16) (full image: 128×128, FID: 9.99).    <ref type="figure" target="#fig_2">Figure 3</ref>). Better to view in high-resolution since the micro patches are very small. More generation results are available in the Appendix F.</p><p>Full-Images Interpolation. Intuitively, the inter-fullimage interpolation is challenging for COCO-GAN, since all micro patches generated with different spatial coordinates must all change synchronously to make the full-image interpolation smooth. Nonetheless, as shown in <ref type="figure" target="#fig_6">Figure 5</ref>, we  <ref type="table">Table 1</ref>: The FID score suggests that COCO-GAN is competitive with other state-of-the-art generative models. FID scores are measured between 50,000 real and generated samples based on the original implementation provided at https://github.com/bioinf-jku/TTUR. Note that all the FID scores (except proj. D) are officially reported numbers. The real samples for evaluation are held-out from training.</p><p>empirically find COCO-GAN can interpolate smoothly and synchronously without producing unnatural artifacts. We randomly sample two latent vectors z 1 and z 2 . With any given interpolation point z in the slerp-path <ref type="bibr" target="#b34">[32]</ref> between z 1 and z 2 , the generator uses the full spatial coordinate sequence C full to generate all corresponding patches. Then we assemble all the generated micro patches together and form a generated full image s.</p><p>Coordinates Interpolation. Another dimension of the interpolation experiment is inter-class (e.g. between spatial coordinate condition) interpolation with a fixed latent vector. We linearly-interpolate spatial coordinates between [−1, 1] with a fixed latent vector z. The results in <ref type="figure" target="#fig_7">Figure 6</ref> show that, although we only uniformly sample spatial coordinates within a discrete spatial coordinate set, the spatial coordinates interpolation is still overall continuous.</p><p>An interesting observation is about the interpolation at the position between the eyebrows. In <ref type="figure" target="#fig_7">Figure 6</ref>, COCO-GAN does not know the existence of the glabella between two eyes due to the discrete and sparse spatial coordinates sampling strategy. Instead, it learns to directly deform the shape of the eye to switch from one eye to another. This phenomenon raises an interesting discussion, even though the model learns to produce high-quality face images, it still may learn wrong relationships of objects behind the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Beyond-Boundary Generation</head><p>COCO-GAN enables a new type of image generation that has never been achieved by GANs before: generate full images that are larger than any training sample from scratch. In this context, all the generated images are guaranteed to be novel  and original, since these generated images do not even exist in the training distribution. A supportive evidence is that the generated images have higher resolution than any sample in the training data. In comparison, existing GANs mostly have their output shape fixed after its creation and prove the generator can produce novel samples instead of memorizing real data via interpolating between generated samples.</p><p>A shared and interesting behavior of learned manifold of GANs is that, in most cases, the generator can still produce plausible samples with latent vectors slightly out of the training distribution, which we called extrapolation. We empirically observe that with a fixed z, extrapolation can be done on the coordinate condition beyond the training coordinates distribution. However, as the continuity among patches at these positions is not considered during training, the generated images might show a slight discontinuity at the border. As a solution, we apply a straightforward post-training process (described in Appendix E) for improving the continuity among patches.</p><p>In <ref type="figure">Figure 7</ref>, we perform the post-training process on check- * The model is not fully converged due to computational resource constraints. One can obtain even lower FID with more GPU-days. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Panorama Generation &amp; Partial Generation</head><p>Generating panoramas using GANs is an interesting problem but has never been carefully investigated. Different from normal image generation, panoramas are expected to be cylindrical and cyclic in the horizontal direction. However, normal GANs do not have built-in ability to handle such cyclic characteristic if without special types of padding mechanism support <ref type="bibr" target="#b6">[4]</ref>. In contrast, COCO-GAN is a coordinate-systemaware learning framework. We can easily adopt a cylindrical coordinate system, and generate panoramas that are having "cyclic topology" in the horizontal direction as shown in <ref type="figure">Figure 8</ref>.</p><p>To train COCO-GAN with a panorama dataset under a cylindrical coordinate system, the spatial coordinate sampling strategy needs to be slightly modified. In the horizontal direction, the sampled value within the normalized range [−1, 1] is treated as an angular value θ, and then is projected with cos(θ) and sin(θ) individually to form a unit-circle on a 2D surface. Along with the original sampling strategy on the vertical axis, a cylindrical coordinate system is formed.</p><p>We conduct our experiment on Matterport3D <ref type="bibr" target="#b4">[2]</ref> dataset. We first take the sky-box format of the dataset, which consists of six faces of a 3D cube. We preprocess and project the sky-box to a cylinder using Mercator projection, then resize to 768 × 512 resolution. Since the Mercator projection creates extreme sparsity near the northern and southern poles, which lacks information, we directly remove the upper and lower 1/4 areas. Eventually, the size of panorama we use for training is 768 × 256 pixels.</p><p>We also find COCO-GAN has an interesting connection with virtual reality (VR). VR is known to have a tight computational budget due to high frame-rate requirement and highresolution demand. It is hard to generate full-scene for VR in real time using standard generative models. Some recent VR studies on omnidirectional view rendering and streaming <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b7">5]</ref> focus on reducing computational cost or network bandwidth by adapting to the user's viewport. COCO-GAN, with the generation-by-parts feature, can easily inherit the same strategy and achieve computation on-demand with respect to the user's viewpoint. Such a strategy can largely reduce unnecessary computational cost outside the region of interest, thus making image generation in VR more applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Patch-Guided Image Generation</head><p>We further explore an interesting application of COCO-GAN named "Patch-Guided Image Generation". By training an extra auxiliary network Q within D that predicts the latent vector of each generated macro patch s , the discriminator is able to find a latent vector z est = Q(x ) that generates a macro patch similar to a provided real macro patch x . Moreover, the estimated latent vector z est can be applied to the full-image generation process, and eventually generates an image that is partially similar to the original real macro patch, while globally coherent.</p><p>This application shares similar context to some bijection methods <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b5">3]</ref>, despite COCO-GAN estimates the latent vector with a single macro patch instead of the full image. In addition, the application is also similar to image restoration <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b35">33,</ref><ref type="bibr" target="#b36">34]</ref> or image out-painting <ref type="bibr" target="#b29">[27]</ref>. However, these related applications heavily rely on the information from the surrounding environment, which is not fully accessible from a single macro patch. In <ref type="figure">Figure 9</ref>, we show that our method is robust to extremely damaged images. More samples and analyses are described in Appendix K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Computation-Friendly Generation</head><p>Recent studies in high-resolution image generation <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b22">20,</ref><ref type="bibr" target="#b16">14]</ref> have gained lots of success; however, a shared conundrum among these existing approaches is the computation being memory hungry. Therefore, these approaches make 0°360°720°F igure 8: The generated panorama is cyclic in the horizontal direction since COCO-GAN is trained with a cylindrical coordinate system. Here, we paste the same generated panorama twice (from 360 • to 720 • ) to better illustrate the cyclic property of the generated panorama. More generation results are provided in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Macro Patch</head><p>Partial Conv.</p><p>Ours <ref type="figure">Figure 9</ref>: Patch-guided image generation loosely retains the local structures from the original image and make the full image still globally coherent. The quality outperforms the partial convolution <ref type="bibr" target="#b19">[17]</ref>. The blue boxes visualize the predicted spatial coordinates A(x ), while the red boxes indicate the ground truth coordinates c . Note that the generated images are not expected to be identical to the original real images. More examples are provided in Appendix K. some compromises to reduce memory usage <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b22">20]</ref>. Moreover, this memory bottleneck cannot be easily resolved without specific hardware support, which makes the generation of over 1024 × 1024 resolution images difficult to achieve. These types of high-resolution images are commonly seen in panoramas, street views, and medical images.</p><p>In contrast, COCO-GAN only requires partial views of the full image for both training and inference. Note that the memory consumption for training (and making inference) GANs grows approximately linearly with respect to the image size. Due to using only partial views, COCO-GAN changes the growth in memory consumption to be associated with the size of a macro patch, not the full image. For instance, on the CelebA 128 × 128 dataset, the (N2,M2,S16) setup of COCO-GAN reduces memory requirement from 17,184 MB (our projection discriminator backbone) to 8,992 MB (i.e., 47.7% reduction), with a batch size 128. However, if the size of a macro patch is too small, COCO-GAN will be misled to learn incorrect spatial relation; in <ref type="figure">Figure 10</ref>, we show an experiment with a macro patch of size 32 × 32 and a micro patch size of 16 × 16. Notice the low quality (i.e., duplicated faces). Empirically, the minimum requirement of macro patch size varies for different datasets; for instance, COCO-GAN does not show similar poor quality in panorama generation in Section 3.4, where the macro patch size is 1/48 of the full panorama. Future research on a) how to mitigate such effects (for instance, increase the receptive field of D without harming performance) and b) how to evaluate a proper macro patch size, may further advance the generation-by-parts property particularly in generating large field-of-view data. <ref type="figure">Figure 10</ref>: Examples to show that with macro patches smaller than 1/16 of the full image causes COCO-GAN to learn incorrect spatial relation. Note that this value may vary due to the nature (local structure, texture, etc) of each dataset being different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Ablation Study</head><p>In <ref type="table">Table 2</ref>, the ablation study aims to analyze the trade-offs of each component of COCO-GAN. We perform experiments in CelebA 64 × 64 with four ablation configurations: "continuous sampling" demonstrates that using continuous uniform sampling strategy for spatial coordinates during training will result in moderate generation quality drop; "optimal D" lets the discriminator directly discriminate the full image while the generator still generates micro patches; "optimal G" lets Model best FID (150 epochs) COCO-GAN (cont. sampling) 6.13 COCO-GAN + optimal D 4.05 COCO-GAN + optimal G 6.12 Multiple G 7.26</p><p>COCO-GAN (N2,M2,S16) 4.87 <ref type="table">Table 2</ref>: The ablation study shows that COCO-GAN (N2,M2,S16) can converge well with little trade-off in convergence speed on CelebA 64 × 64 dataset. the generator directly generate the full image while the discriminator still discriminates macro patches; "multiple G" trains an individual generator for each spatial coordinate.</p><p>We observe that, surprisingly, despite the convergence speed is different, "optimal discriminator", COCO-GAN, and "optimal generator" (ordered by convergence speed from fast to slow) can all achieve similar FID scores if with sufficient training time. The difference in convergence speed is expected since "optimal discriminator" provides the generator with more accurate and global adversarial loss. In contrast, the "optimal generator" has relatively more parameters and layers to optimize, which causes the convergence speed slower than COCO-GAN. Lastly, the "multiple generators" setting cannot converge well. Although it can also concatenate micro patches without obvious seams as COCO-GAN does, the full-image results often cannot agree and are not coherent. More experimental details and generated samples are shown in Appendix J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Non-Aligned Dataset</head><p>It is easy to get confused that the coordinate system would restrain COCO-GAN from learning on less aligned datasets. In fact, this is completely not true. For instance, the bedroom category of LSUN, the location, size and orientation of the bed are very dynamic and non-aligned. On the other hand, the Matterport3D panoramas are completely non-aligned in the horizontal direction.</p><p>To further resolve all the potential concerns, we propose CelebA-syn, which applies a random displacement on the raw data (different from data augmentation, this preprocessing directly affects the dataset) to mess up the face alignment. We first trim the raw images to 128×128. The position of the upper-left corner is sampled by (x, y) = (25 + dx, 50 + dy), where dx ∼ U(−25, 25) and dy ∼ U(−25, 25). Then we resize the trimmed images to 64×64 for training. As shown in <ref type="figure" target="#fig_8">Figure 11</ref>, COCO-GAN can stably create reasonable samples of high diversity (also notice the high diversity at the eye positions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Generative Adversarial Network (GAN) <ref type="bibr" target="#b13">[11]</ref> and its conditional variant <ref type="bibr" target="#b23">[21]</ref> have shown their potential and flexibility to many different tasks. Recent studies on GANs are focusing on generating high-resolution and high-quality synthetic images in different settings. For instance, generating images with 1024 × 1024 resolution <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b22">20]</ref>, generating images with low-quality synthetic images as condition <ref type="bibr" target="#b30">[28]</ref>, and by applying segmentation maps as conditions <ref type="bibr" target="#b33">[31]</ref>. However, these prior works share similar assumptions: the model must process and generate the full image in a single shot. This assumption consumes an unavoidable and significant amount of memory when the size of the targeting image is relatively large, and therefore makes it difficult to satisfy memory requirements for both training and inference. Searching for a solution to this problem is one of the initial motivations of this work.</p><p>COCO-GAN shares some similarities to Pixel-RNN <ref type="bibr" target="#b32">[30]</ref>, which is a pixel-level generation framework while COCO-GAN is a patch-level generation framework. Pixel-RNN transforms the image generation task into a sequence generation task and maximizes the log-likelihood directly. In contrast, COCO-GAN aims at decomposing the computation dependencies between micro patches across the spatial dimensions, and then uses the adversarial loss to ensure smoothness between adjacent micro patches.</p><p>CoordConv <ref type="bibr" target="#b20">[18]</ref> is another similar method but with fundamental differences. CoordConv provides spatial positioning information directly to the convolutional kernels in order to solve the coordinate transform problem and shows multiple improvements in different tasks. In contrast, COCO-GAN uses spatial coordinates as an auxiliary task for the GANs training, which enforces both the generator and the discriminator to learn coordinating and correlations between the generated micro patches. We have also considered incorporating CoordConv into COCO-GAN. However, empirical results show little visual improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>In this paper, we propose COCO-GAN, a novel GAN incorporating the conditional coordination mechanism. COCO-GAN enables "generation by parts" and demonstrates the generation quality being competitive to state-of-the-arts. COCO-GAN also enables several new applications such as "Beyond-Boundary Generation" and "Panorama Generation", which serve as intriguing directions for future research on leveraging the learned coordinate manifold for (a) tackling with large field-of-view generation and (b) reducing computational requisition.</p><p>Particularly, given a random latent vector, Beyond-Boundary Generation generates images larger than any training sample by extrapolating the learned coordinate manifold, which is enabled exclusively by COCO-GAN. Future research on extending this property to other tasks or applications may further take advantage of such an out-of-distribution generation paradigm.</p><p>We show that COCO-GAN produces 128 × 128 images with micro patches as small as 4 × 4 pixels. The overall FID score slightly degrades due to the small micro patch size. Further studies on the relationship between the patch size and generation stability are left as a straight-line future work.</p><p>Although COCO-GAN has achieved a high generation quality comparable to state-of-the-art GANs, for several generated samples we still observe that the local structures may be discontinued or mottled. This suggests further studies on additional refinements or blending approaches that could be applied on COCO-GAN for generating more stable and reliable samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>We sincerely thank David Berthelot and Mong-li Shih for the insightful suggestions and advice. We are grateful to the National Center for High-performance Computing for computer time and facilities. Hwann-Tzong Chen was supported in part by MOST grants 107-2634-F-001-002 and 107-2218-E-007-047. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter α</head><p>Weight of LS Controls the strength of LS (we use 100).</p><formula xml:id="formula_5">L W + λL GP + αL S , for D, −L W + αL S , for G. λ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight of</head><p>LGP Controls the strength of LGP (we use 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing Only</head><p>s Generated full image Composed by s generated with C F ull . s = ϕ(G(z, C F ull )) C F ull Matrix of c for testing</p><p>The matrix of c used during testing. s = ϕ(G(z, C F ull ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments Setup and Model Architecture Details</head><p>Architecture. Our G and D design uses projection discriminator <ref type="bibr" target="#b24">[22]</ref> as the backbone and adding class-projection to the discriminator. All convolutional and feed-forward layers of generator and discriminator are added with the spectralnormalization <ref type="bibr" target="#b25">[23]</ref> as suggested in <ref type="bibr" target="#b38">[36]</ref>. Detailed architecture diagram is illustrated in <ref type="figure" target="#fig_2">Figure 13</ref> and <ref type="figure" target="#fig_5">Figure 14</ref>. Specifically, we directly duplicate/remove the last residual block if we need to enlarge/reduce the size of output patch. However, for (N8,M8,S8) and (N16,M16,S4) settings, since the model becomes too shallow, we keep using (N4,M4,S16) architecture, but without strides in the last one and two layer(s), respectively.</p><p>Conditional Batch Normalization (CBN). We follow the projection discriminator that employs CBN <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b9">7]</ref> in the generator. The concept of CBN is to normalize, then modulate the features by conditionally produce γ and β that used in conventional batch normalization, which computes o K = ((i K − µ K )/σ K ) * γ + β for the K-th input feature i K , output feature o K , feature mean µ K and feature variance σ K . However, in the COCO-GAN setup, we provide both spatial coordinate and latent vector as conditional inputs, which both having real values instead of common discrete classes. As a result, we create two MLPs, MLP γ (z, c) and MLP β (z, c), for each CBN layer, that conditionally produces γ and β.</p><p>Hyperparameters. For all the experiments, we set the gradient penalty weight λ = 10 and auxiliary loss weight α = 100. We use Adam <ref type="bibr" target="#b18">[16]</ref> optimizer with β 1 = 0 and β 2 = 0.999 for both the generator and the discriminator. The learning rates are based on the Two Time-scale Update Rule (TTUR) <ref type="bibr" target="#b15">[13]</ref>, setting 0.0001 for the generator and 0.0004 for the discriminator as suggested in <ref type="bibr" target="#b38">[36]</ref>. We do not specifically balance the generator and the discriminator by manually setting how many iterations to update the generator once as described in the WGAN paper <ref type="bibr" target="#b3">[1]</ref>.</p><p>Coordinate Setup. For the micro coordinate matrix C (i,j) sampling, although COCO-GAN framework supports real-valued coordinate as input, however, with sampling only the discrete coordinate points that is used in the testing phase will result in better overall visual quality. As a result, all our experiments select to adopt such discrete sampling strategy. We show the quantitative degradation in the ablation study section. To ensure that the latent vectors z, macro coordinate conditions c , and micro coordinate conditions c share the similar scale, which z and c are concatenated before feeding to G. We normalize c and c values into range [−1, 1], respectively. For the latent vectors z sampling, we adopts uniform sampling between [−1, 1], which is numerically more compatible with the normalized spatial condition space.   <ref type="figure" target="#fig_5">Figure 14</ref>: The detailed discriminator architecture of COCO-GAN for discriminate macro patches with a size of 64 × 64 pixels. Both the content vector prediction head (Q) and the spatial condition prediction head use the same structure shown in (c).   For instance, 3D cubic data may choose to use each of its face as a macro patch. Also, a recent work <ref type="bibr" target="#b31">[29]</ref> shows that horizontal tiles are naturally suitable for indoor layout task on panoramas, which points out that using horizontal tiles as macro patch in panorama generation may be an interesting future direction. Directly train with coordinates out of the [−1, 1] range (restricted by the real full images) is infeasible, since there is no real data at the coordinates outside of the boundary, thus the generator can exploit the discriminator easily. However, interestingly, we find extrapolating the coordinates of a manually trained COCO-GAN can already produce contents that seemingly extended from the edge of the generated full images (e.g., <ref type="figure" target="#fig_7">Figure 16</ref>). We empirically observe that by only training the first-two layers of the generator (while the whole discriminator at the same time) can largely stabilize the post-training process. Otherwise, the generator will start to produce weird and mottled artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Example of Coordinate Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Beyond-Boundary Generation: More Examples and Details of Post-Training</head><p>As the local textures are handled by later layers of the generator, we decide to freeze all the later layers and only train the first-two layers, which controls the most high-level representations. We flag the more detailed investigation on the root-cause of such an effect and other possible solutions as interesting future research direction. F. More Full Image Generation Examples <ref type="bibr">(a)</ref> Selected generation samples. <ref type="bibr">(b)</ref> Random generation without calibration.</p><p>(c) Generated micro patches. <ref type="figure">Figure 18</ref>: Full images generated by COCO-GAN on CelebA 128 × 128 with (N2,M2,S32) setting.</p><p>Due to file size limit, all images are compressed, please access the full resolution pdf from: https://goo.gl/5HLynv (a) Selected generation samples. <ref type="bibr">(b)</ref> Random generation without calibration.</p><p>(c) Generated micro patches. <ref type="figure">Figure 19</ref>: Full images generated by COCO-GAN on CelebA 128 × 128 with (N4,M4,S16) setting.</p><p>Due to file size limit, all images are compressed, please access the full resolution pdf from: https://goo.gl/5HLynv (a) Selected generation samples. <ref type="bibr">(b)</ref> Random generation without calibration.      H. More Panorama Generation Samples <ref type="figure" target="#fig_5">Figure 24</ref>: More examples of generated panoramas. All samples possess the cyclic property along the horizontal direction. Each sample is generated with a resolution of 768 × 256 pixels, and micro patch size 64 × 64 pixels.  (d) COCO-GAN (optimal G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Micro Patches Interpolation Full-Images Interpolation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Spatial Coordinates Interpolation</head><p>(e) Multiple generators. <ref type="figure">Figure 27</ref>: Some samples generated by different variants of COCO-GAN. Note that each set of samples is extracted at the epoch when each model variant reaches its lowest FID score. We also provide more samples for each of the variants at different epochs via following : https://goo.gl/Wnrppf. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Due to file size limit, all images are compressed, please access the full resolution pdf from: http://bit.ly/COCO-GAN-full arXiv:1904.00284v4 [cs.LG] 5 Jan 2020 COCO-GAN: Generation by Parts via Conditional Coordinating</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )</head><label>a</label><figDesc>CelebA (N2,M2,S32) (full image: 128×128). (b) LSUN bedroom (N2,M2,S64) (full image: 256×256).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>COCO-GAN generates visually smooth and globally coherent full images without any post-processing. The three rows from top to bottom show: (a) the generated full images, (b) macro patches, and (c) micro patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( b )</head><label>b</label><figDesc>CelebA (N8,M8,S8) (full image: 128×128, FID: 15.99).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( c )</head><label>c</label><figDesc>CelebA (N16,M16,S4) (full image: 128×128, FID: 23.90).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Various sizes of micro patches (from 16 × 16 to 4 × 4, even smaller than any human face organs) consistently generate visually smooth and globally coherent full images. Each sub-figure consists of three rows, from top to bottom: full images, macro patches, and micro patches. For the first five columns, each column uses the same latent vector (similar with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The results of full-images interpolation between two latent vectors show that all micro patches are changed synchronously in response to the change of the latent vector. More interpolation results are available in Appendix G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>An example of spatial coordinates interpolation showing the spatial continuity of the micro patches. The spatial coordinates are interpolated between range [−1, 1] of the micro coordinate with a fixed latent vector. More examples are shown in Appendix I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>COCO-GAN can learn and synthesis samples with diverse position on the non-aligned Celeba-syn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>(Figure 12 :</head><label>12</label><figDesc>Appendix) COCO-GAN: Generation by Parts via Conditional Coordinating A. COCO-GAN during Testing Phase Micro Coordinate ′′ Latent Vector An overview of COCO-GAN during testing phase. The micro patches generated by G are directly combined into a full image as the final output. . s = G(z, c ) D Discriminator Discriminates macro patches. D(ϕ(G(z, C ))) A Spatial prediction head Predicts coordinate of a given macro patch.ĉ = A(x ) Q † Content prediction head Predicts latent vector of a given macro patch. zest = Q(s ) Heuristic Function ϕ Merging function Merges multiple s to form a s or s. s = ϕ(G(z, C )) ψ Cropping function Crops x from x. Corresponding to ϕ. x = ψ(x, c ) Variable z Latent vector Latent variable shared among s generation. s = G(z, c ) zest † Predicted z Predicted z of a given macro patch. LQ = E [ z − zest 1 ] c Macro coordinate Coordinate for macro patches on D side. LS = E [ c −ĉ 2 ] c Micro coordinate Coordinate for micro patches on G side. s = G(z, c ) c Predicted c Coordinate predicted by A with a given x . LS = E [ c −ĉ 2 ] C Matrix of c The matrix of c used to generate S . s = ϕ(G(z, C )) Data x Real full image Full resolution data, never directly used. x = ψ(x, c ) x Real macro patch A macro patch of x which D trains on. adv x = D(ψ(x, c )) s Generated macro patch Composed by s generated with C . adv s = D(s ) s Generated micro patch Smallest data unit generated by G. s = G(z, c ) S Matrix of s Matrix of s generated by C . S = G(z, C ) s Interpolated macro patch Interpolation between random x and s .ŝ = s + (1 − ) x , which ∼ [0, 1] Loss LW WGAN loss The patch-level WGAN loss. LW = E [D(x )] − E [D(s )] LGP Gradient penalty loss The gradient penalty loss to stabilize training. LGP = E ( ∇ŝ D(ŝ ) 2 − 1) 2 LS Spatial consistency loss Consistency loss of coordinates. LS = E [ c − A(x ) 2] LQ † Content consistency loss Consistency loss of latent vectors. LQ = E [ z − Q(s ) 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Output shape: (B, 2, 2, 1024) (a) Generator Overall Architecture ReLU Output shape: (B, H, W, C) Input shape: (B, H, W, C) shape: (B, Hx2, Wx2, C) Element-wise Add Output shape: (B, Hx2, Wx2, D) Output shape: (B, Hx2, Wx2, D) Generator Residual Block (b) Generator Residual Block The detailed generator architecture of COCO-GAN for generating micro patches with a size of 32 × 32 pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Input shape: (B, 64, 64, 3) D_Residual_Block (w/o 1 st relu) Output shape: (B, 32, 32, 64) Overall Architecture ReLU Output shape: (B, Hx2, Wx2, C) Input shape: (B, Hx2, Wx2, C) Conv2D Output shape: (B, Hx2, Wx2, D) ReLU Output shape : (B, Hx2, Wx2, D) Conv2D Output shape: (B, Hx2, Wx2, D) Conv2D Output shape: (B, H, W, D) Average Pooling Output shape: (B, H, W, C) Element-wise Add Output shape: (B, H, W, D) Output shape: (B, H, W, D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Implementations used in this paper with (Left) P4x4, (Middle) P8x8 and (Right) P16x16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Other possible implementations (not used in this paper).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>We showcase some of the coordinate systems: (a) implementations we used in our experiments, and (b) some of the other possible implementations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Without any extra training, original COCO-GAN can already perform slight extrapolations (i.e. the edge of the bed extends out of the normal generation area annotated with the red box), however, expectedly discontinuous on the edges. We show more examples of "Beyond-Boundary Generation" in Figure 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 :</head><label>17</label><figDesc>"Beyond-Boundary Generation" generates additional contents by extrapolating the learned coordinate manifold. Note that the generated samples are 384 × 384 pixels, whereas all of the training samples are of a smaller 256 × 256 resolution. The red box annotates the 256 × 256 region for regular generation without extrapolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>( c )</head><label>c</label><figDesc>Generated micro patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 20 :</head><label>20</label><figDesc>Full images generated by COCO-GAN on CelebA 128 × 128 with (N8,M8,S8) setting. Due to file size limit, all images are compressed, please access the full resolution pdf from: https://goo.gl/5HLynv (a) Selected generation samples.(b) Random generation without calibration.(c) Generated micro patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 :</head><label>21</label><figDesc>Full images generated by COCO-GAN on CelebA 128 × 128 with (N16,M16,S4) setting.Due to file size limit, all images are compressed, please access the full resolution pdf from: https://goo.gl/5HLynv (a) Selected generation samples.(b)  Random generation without calibration.(c)  Generated micro patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 22 :</head><label>22</label><figDesc>Full images generated by COCO-GAN on LSUN 256 × 256 with (N4,M4,S64) setting. Due to file size limit, all images are compressed, please access the full resolution pdf from: https://goo.gl/5HLynv G. More Interpolation Examples Micro Patches Interpolation Full-Images Interpolation (a) CelebA (128 × 128).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>( b )</head><label>b</label><figDesc>LSUN (bedroom category) (256 × 256).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 23 :</head><label>23</label><figDesc>More interpolation examples. Given two latent vectors, COCO-GAN generates the micro patches and full images that correspond to the interpolated latent vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 25 :</head><label>25</label><figDesc>Spatial interpolation shows the spatial continuity of the micro patches. The spatial conditions are interpolated between range [−1, 1] of the micro coordinate with a fixed latent vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 26 :</head><label>26</label><figDesc>FID score curves of different variants of COCO-GAN in CelebA 64 × 64 setting. Combined withFigure 27, the results do not show significant differences in quality between COCO-GAN variants. Therefore, COCO-GAN does not pay significant trade-off for the conditional coordinate property.(a) COCO-GAN (ours). (b) COCO-GAN (cont sampling). (c) COCO-GAN (optimal D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 28 :Figure 29 :</head><label>2829</label><figDesc>Patch-guided image generation can loosely retain some local structure or global characteristic of the original image.(b)  shows the patch-guided generated images based on z est estimated from(a). The blue boxes visualize the predicted spatial coordinates A(x ), while the red boxes indicates the ground truth coordinates c . Since the information loss of cropping the macro patches from real images is critical, we do not expect(b)  to be identical to the original real image. Instead, the area within blue boxes of (b) should be visually similar to(a), in the meanwhile, (b) should be globally coherent. Both Wasserstein distance and FID through time show that the training of COCO-GAN is stable. Both two figures are logged while training on CelebA with 128 × 128 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 7: "Beyond-Boundary Generation" generates additional contents by extrapolating the learned coordinate manifold. Note that the generated samples are 384 × 384 pixels, whereas all of the training samples are of a smaller 256 × 256 resolution. The red box annotates the 256 × 256 region for regular generation without extrapolation. point of (N4,M4,S64) variant of COCO-GAN that trained on LSUN dataset. Then, we show that COCO-GAN generates high-quality 384 × 384 images: the original size is 256, with each direction being extended by one micro patch (64 pixels), resulting a size of 384 × 384. Note that the model is in fact trained on 256 × 256 images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>With such an observation, we select to perform additional post-training on the checkpoint(s) of manually trained COCO-GAN (e.g., (N2,M2,S64) variant of COCO-GAN that trained on LSUN dataset for 1 million steps with resolution 256 × 256 and a batch size 128). Aside from the original Adam optimizer that trains COCO-GAN with coordinates ∈ [−1, 1], we create another Adam optimizer with the default learning rate setup (i.e., 0.0004 for D and 0.0001 for G). The additional optimizer trains COCO-GAN with additional coordinates along with the original coordinates. For instance, in our experiments, we extend an extra micro patch out of the image boundary, as a result, we train the model with c ∈ −1.66, 1.66 (the distance between two consecutive micro patches is 2/(4 − 1) = 0.66) and c ∈ [−2, 2] (the distance between two consecutive macro patches is 2/((4 − 1) − 1) = 1). We only use the new optimizer to train COCO-GAN until the discontinuity becomes patches becomes invisible. Note that we do not train the spatial prediction head A with coordinates out of [−1, 1], since our original model has a tanh activation function on the output of A, which is impossible to produce predictions out of the range of [−1, 1].</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">† Only used in "Patch-Guided Image Generation" application.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CelebA 128×128) Real full images</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">CelebA 128×128) Real macro patches</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">CelebA 128×128) Patch-guided full image generation. (d) (CelebA 128×128) Patch-guided macro patch generation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Matter-port3d: Learning from RGB-D data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision</title>
		<meeting><address><addrLine>Qingdao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-10" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Escaping from collapsing modes in a constrained space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cube padding for weakly-supervised saliency prediction in 360 videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal set of 360-degree videos for viewport-adaptive streaming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Corbillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devlic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chakareski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference<address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-23" />
			<biblScope unit="page" from="943" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Viewport-adaptive navigable 360-degree video delivery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Corbillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devlic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chakareski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Communications</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05-21" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6597" to="6607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJtNZAFgg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno>abs/1610.07629</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1ElR4cgg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introvae: Introspective variational autoencoders for photographic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>abs/1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>abs/1804.07723</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosinski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="9628" to="9639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="page" from="3478" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">cgans with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno>abs/1802.05637</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno>abs/1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Viewport-aware adaptive 360 • video streaming using tiles for virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ozcinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Abreu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-17" />
			<biblScope unit="page" from="2174" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Painting outside the box: Image outpainting with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rusak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08483</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horizonnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03861</idno>
		<title level="m">Learning room layout with 1d representation and pano stretch data augmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>abs/1711.11585</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sampling generative networks: Notes on a few effective techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<idno>abs/1609.04468</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4076" to="4084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6882" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LSUN: construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>abs/1506.03365</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno>abs/1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
