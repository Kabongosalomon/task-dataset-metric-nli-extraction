<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Motion Estimation via Motion Compression and Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-05">5 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Human Motion Estimation via Motion Compression and Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-05">5 Oct 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zhengyi Luo [0000−0002−1842−7622] , S. Alireza Golestaneh [0000−0002−3230−2797]  , and Kris M. Kitani [0000−0002−9389−4060] <ref type="figure">Fig. 1</ref>. Given an in-the-wild video, state-of-the-art methods (e.g., VIBE [1]) can achieve high per joint accuracy, but also suffers from high acceleration error. To tackle this, we develop a two-stage motion estimation method, MEVA, that is able to produce both accurate and smooth human motion.</p><p>Abstract. We develop a technique for generating smooth and accurate 3D human pose and motion estimates from RGB video sequences. Our method, which we call Motion Estimation via Variational Autoencoder (MEVA), decomposes a temporal sequence of human motion into a smooth motion representation using auto-encoder-based motion compression and a residual representation learned through motion refinement. This twostep encoding of human motion captures human motion in two stages: a general human motion estimation step that captures the coarse overall motion, and a residual estimation that adds back person-specific motion details. Experiments show that our method produces both smooth and accurate 3D human pose and motion estimates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating the 3D pose sequence of a person from a single video requires a computational model that can extract the underlying kinematics of human motion while also preserving motions that are unique to the person being captured. Since people share a similar body structure (e.g., same number of joints) and similar physical constraints (e.g., joint limitations), it is possible to learn a generalized kinematic model that can be matched against the image to infer the general motion of a person. However, since generalized models of motion can also fail to model person-specific motions, it may also be necessary to 'add back in' or refine the general motion estimates using image evidence. In this work, we present a two-stage 3D motion estimation method that first extracts coarse kinematic sequences of a person in a video and then refines that sequence to produce a more accurate 3D motion estimate. We show that by decomposing the inference process into (1) a general model of motion and (2) a person-specific model of motion, we are able to obtain more accurate and smooth estimates.</p><p>Over the past years, significant progress has been made on improving the accuracy of 3D human pose estimation. Impressive results have been obtained through human mesh recovery from single images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> and videos <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. The main metric used to evaluate these methods is the Mean Per Joint Position Error (MPJPE), which measures the performance in terms of the relative joint positions computed for each frame of a video. However, less emphasis has been given to the temporal smoothness of the estimated motion. Optimizing for this metric, the tendency is to generate pose estimates that 'jitter' near the true pose. This is expected as the MPJPE only penalizes for spatial errors and is not designed to account for temporal consistency. As the methods for 3D pose estimation have improved in recent years, the 'jitter' has become less pronounced, especially when applied to dynamic scenes with vibrant moving backgrounds and camera motion. However, by rendering the estimated 3D pose of state-of-the-art methods on a plain background, the 'jitter' can still be observed, resulting in an overall unnatural motion estimation.</p><p>The issue of temporal smoothness is a known problem and has been addressed in part by prior work. Large-scale motion datasets such as Archive of Motion Capture as Surface Shapes (AMAAS) <ref type="bibr" target="#b9">[10]</ref>) and adversarial loss have enabled methods to improve both pose accuracy and temporal smoothness <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. Other methods have been developed to enforce temporal smoothness <ref type="bibr" target="#b1">[2]</ref> by letting the model predict frame ordering. However, using prior knowledge only in the loss function, it is hard to find the balance between smoothness and accuracy.</p><p>In this work, we argue that striking such balance between smoothness and accuracy can be done through an explicit breakdown of coarse and fine motion. First, we learn a coarse motion model by observing a large dataset of human motion-since human motion is typically smooth (e.g., we usually do not shake as we walk), if one were to fit a model to a large set of human motions, most of the data would lie in a sub-space in which motions are smooth. This implies that if we were to compress human motion data, it should learn a latent subspace in which the motions are inherently smooth and coarse. Using this latent space as the regression target, we can directly infer coarse human motion from the input videos. The problem of using such a human motion subspace, of course, is that rare motions (e.g., sudden motions) are removed from the motion data. To retain such motion, we also argue that producing the final 3D motion estimate can be treated as a refinement step to "add back" the fine details to the coarse motion sequence.</p><p>To validate our arguments, we propose a two-stage 3D human motion estimation method that first estimates a coarse human pose sequence through data compression using a Variational Autoencoder (VAE), which we call the Variational Motion Estimator (VME). Then we take the output of the VME and refine the pose estimate using the image evidence with a pose regressor, which we call the Motion Refinement Regressor (MRR).</p><p>In summary, we propose a video-based 3D human motion estimation method that focuses on producing smooth and accurate human motion sequences. Our main contributions are as follows: (1) we propose a two-stage motion estimation method for ensuring temporal smoothness and accurate pose estimates; (2) we describe a procedure to learn a robust Variational Autoencoder that can serve as a latent human motion subspace for estimating coarse 3D human motion from videos; (3) we demonstrate state-of-the-art pose/motion estimation performance on challenging in-the-wild dataset such as 3DPW <ref type="bibr" target="#b10">[11]</ref>, reducing acceleration error by 54.3% while achieving state-of-the-art MPJPE results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>In this section, we will first review the relevant work in human shape and pose recovery from a single image and from videos-human motion recovery can be treated as a subset of human pose estimation, as human motion is a sequence of human poses. Then we will review how existing methods use human motion as a prior and how popular methods map motion sequences to a low-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recovering 3D human pose and shape from a single image</head><p>Here we focus on model-based methods that jointly recover human shape and pose. We choose to use a parametric 3D human body model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> since it can be easily turned into a 3D human mesh that is usable for downstream tasks such as animation. Directly fitting a parametric 3D human body to image input has gained substantial traction over the years, morphing from methods that require silhouette or human input <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, to ones that can directly fit model parameters to 2D joint positions <ref type="bibr" target="#b19">[20]</ref>, and to ones that can directly estimate shape and pose from images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref>. Due to the lack of ground truth 3D labels, these methods use a weakly supervised approach to fit the 3D human body to 2D joint positions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, body part segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, or dense pixel correspondence <ref type="bibr" target="#b5">[6]</ref>. Although these methods achieve amazing results, their extracted motion tends to be unstable due to the lack of temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recovering 3D human pose and shape from video</head><p>Using temporal information to aid 3D human pose estimation is a natural extension to single frame methods. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> focus on "lifting" predicted joint positions from 2D to 3D, and uses LSTM <ref type="bibr" target="#b8">[9]</ref>, Temporal Convolution <ref type="bibr" target="#b25">[26]</ref>, and fully connected layers <ref type="bibr" target="#b26">[27]</ref> to exploit temporal information. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, on the other hand, predict 3D joint positions directly from images and use a temporal filter to postprocess the motion sequence. For methods that jointly recover shape and pose. HMMR <ref type="bibr" target="#b6">[7]</ref>, Sun et al. <ref type="bibr" target="#b1">[2]</ref> and VIBE <ref type="bibr" target="#b0">[1]</ref> are the best performing models that exploit temporal information. HMMR <ref type="bibr" target="#b6">[7]</ref> proposes to enforce temporal consistency by letting the model predict future and past frames of motion. Sun et al. <ref type="bibr" target="#b1">[2]</ref> learns temporal information by predicting the ordering of shuffled frames. VIBE <ref type="bibr" target="#b0">[1]</ref> utilizes temporal information by employing a Gated Recurrent Unit (GRU) to convert the input frames of features into a sequence of temporally correlated latent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Human Pose and Motion Prior</head><p>Using prerecorded human motion sequences as a prior has also been explored in various tasks related to human motion. Earlier work like <ref type="bibr" target="#b30">[31]</ref> tries to quantify unnaturalness in animated human motion by statistically analyzing existing motion capture (MoCap) sequences, and <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> propose to use learned MoCap motions to aid 3D motion tracking. More recently, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> use an adversarial discriminator at a per-frame level to ensure that the recovered pose is a valid human pose. <ref type="bibr" target="#b11">[12]</ref> uses a pretrained pose VAE's latent space for a similar purpose. <ref type="bibr" target="#b0">[1]</ref> proposes to use the discriminator at a temporal level, discriminating against a whole motion sequence. All above methods use a pose or motion prior in an adversarial way, utilizing the prior knowledge in the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Human Motion Representation</head><p>Compressing human motion into a compact latent representation plays an important role in tasks such as human motion generation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, human motion generation across different modalities <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, and trajectory forecasting <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. Existing methods leverage different generative models such as VAE <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, generative flow <ref type="bibr" target="#b44">[45]</ref>, or generative adversarial networks <ref type="bibr" target="#b33">[34]</ref> to achieve a compact motion representation in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>As discussed in Sections 1 and 2, existing human motion estimation methods often find it difficult to achieve a balance between temporal smoothness and accuracy. To tackle this, we propose MEVA, Motion Estimation via Variational Autoencoder, a framework that learns the overall coarse motion first then adds back fine detailed motion as a residual. MEVA processes the inputs in three steps:</p><p>it first extracts correlated temporal features using a spatio-temporal feature extractor (STE), and then captures the overall coarse motion through a variational motion estimator (VME), and finally uses a motion residual regressor (MRR) to add back the fine motion details. Our overall framework can be shown in <ref type="figure" target="#fig_0">Fig.  2</ref>. In this section, we will first setup the overall problem and then present the details of our framework. Finally, we will discuss the training procedures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given an input video V T = {I t } T t=1 , where I t denotes the t th frame, our task is to recover coherent human motion sequences M T = {θ t } T t=1 where each θ t represents the human pose for the t th frame. To represent the human motion, we utilize the SMPL 3D human mesh model <ref type="bibr" target="#b12">[13]</ref>. We choose SMPL parametrization over 3D joint positions or other human models due to its versatility: SMPL parameters can be easily converted to 3D joint positions and human mesh. Specifically, a human body is represented by its shape β and pose θ, denoted by Θ = {β, θ}. Given θ and β, let S denote the pretrained SMPL function, where S(Θ) : β, θ → R 6890×3 ( 6890 is the number of vertices of the resulting triangular human mesh). The pose parameter θ ∈ R 24×N stands for the joint angles for the 23 joints plus the root orientation. N is the dimension of the chosen rotation representation (N = 3 for axis/euler angle, N = 4 for quaternions, N = 6 for a 6 degrees-of-freedom rotation representation <ref type="bibr" target="#b45">[46]</ref>). The shape parameters β ∈ R 10 represent the linear coefficient for the principal component of the parametric human shape space. Given a set of β and θ, S can recover the 3D joint positions through a pretrained mesh vertex regressor P : jp 3d = P {S(β, θ)} ∈ R N ×3 . To project the 3D joint positions back to 2D images, a weak perspective camera π = {s, t x , t y } needs to be estimated:</p><formula xml:id="formula_0">jp 2d = Π(P {S(β, θ)}) ∈ R N ×2 .</formula><p>Intuitively, recovering human motion from video frames does not require recovering the human shape: one can directly learn a mapping from input video frames to the estimated human motion if sufficient paired ground truth data exist. However, videos paired with ground truth motion annotation (SMPL sequences) require professional capture equipment such as a motion capture (Mo-Cap) rig, which is still rather rare compared to annotated 2D pose datasets. In the absence of 3D data samples, it is critical for our model to learn motion sequences in a semisupervised fashion (i.e., from videos with 3D or 2D labeled joint positions following the approach utilized in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>).</p><p>Overall, our motion estimation objective is to learn a function MEVA(V ) :</p><formula xml:id="formula_1">V T → M T where V T = {I t } T t=1 and M T = {θ t } T t=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatio-Temporal Feature Extractor (STE)</head><p>Human motion is inherently temporal and correlated, and past movement can give cues about future motion. Thus, instead of extracting per-frame visual features using a feed-forward convolutional network independently, we can produce temporally correlated features that lead to better motion sequence modeling. Similar to <ref type="bibr" target="#b0">[1]</ref>, we use a GRU-based temporal feature extractor (STE) that encodes the input video frames I 1 ,</p><formula xml:id="formula_2">I 2 , I 3 , ...I T into a sequence of temporally corre- lated features f 1 , f 2 , f 3 , ...f T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variational Motion Estimator (VME)</head><p>Human Motion VAE To learn a human motion subspace that can encapsulate a broad spectrum of human motion, we choose to use a Variational Autoencoder (VAE). VAEs can effectively capture a large number of possible data modes by explicitly mapping each data point to a latent code, and by imposing a Gaussian prior on the learned latent space, similar motions' latent codes will be near each other. Thus, the VAE's latent space allows for more overlap between codes and therefore enforces smoothness in the latent space. Having a smooth latent code is essential in improving the generalizability of the model since the space of possible human motion is highly correlated and limited. Formally, following the previous work on VAEs <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b47">48]</ref>, the objective is to maximize the evidence lower bound of the log-likelihood p λ (x) (λ and φ denotes the function parametrization):</p><formula xml:id="formula_3">L V AE = E q φ [log p(x|z)] − KLD(q φ (z|x)||p φ (z)),<label>(1)</label></formula><p>where x is the input and the latent code z ∼ N (0, I).</p><p>In the context of encoding human motion via VAE, the encoder E vae takes in a sequence of W frames of human motion represented in terms of SMPL pose parameters:</p><formula xml:id="formula_4">x = M W = [θ w1 , θ w2 , θ w3 , .</formula><p>..] ∈ R W ×144 and outputs the latent code z. A single frame of SMPL pose is represented in joint rotations, resulting by a 24 × 6 = 144 dimensional input (a 6 degrees-of-freedom rotation representation <ref type="bibr" target="#b45">[46]</ref> is used for continuity purpose). The decoder D vae takes in the latent code z and reconstructs the motionM W . Both the encoder E vae and decoder D vae are implemented as GRUs, and the detailed architectures are given in <ref type="figure">Fig.3</ref>. Based on the Gaussian parameterization of the VAE, the objective function of Eq.(1) can be written Eq.(2)</p><formula xml:id="formula_5">Fig. 3. Motion VAE Architecture. L vae (x; θ, φ) = − 1 S S s=1 x − x 2 + β · 1 S z Sz j=1 1 + 2 log σ j − µ 2 j − σ 2 j ,<label>(2)</label></formula><p>where S is the number of samples for the current batch, S z is the dimension of the current latent variable, and β is the weighing parameter. Once the VAE is trained and converged to a desirable reconstruction accuracy, the decoder D vae is frozen for later use. During inference, given a latent code z ∈ R 1×Sz , D vae can decode it back into a sequence of human motion:</p><formula xml:id="formula_6">M W ∈ R w×144 .</formula><p>Human Motion Data augmentation Our learned VAE should be able to generalize to unseen human motion sequences and achieve high reconstruction accuracy to ensure that the learned latent space can indeed serve as a comprehensive human motion subspace. Using an already large-scale human motion dataset AMASS <ref type="bibr" target="#b9">[10]</ref> (13k motion samples with varying length), our trained VAE still has poor generalizability on unseen sequences (for details refer to Sec.4.3). Thus, we devise an elaborate data augmentation scheme that can augment the existing motion and produce viable yet distinct human motion sequences. While data augmentation has been studied extensively in image processing, to the best of our knowledge, few attempts have been done in augmenting a human motion dataset. When used in trajectory forecasting <ref type="bibr" target="#b42">[43]</ref> and human motion generation <ref type="bibr" target="#b33">[34]</ref>, the generalizability of the VAE latent space has not been discussed extensively since the models only need to generate new motion sequences and do not emphasize on the ability to encode unseen motion sequences.</p><p>Given a T frame human motion sequence in SMPL parameters M T ∈ R T ×144 with a frame-rate F amass , we employ the following data augmentation scheme:</p><p>-Speeding up and slowing down: based on F amass , we can uniformly upsample or downsample the frames and produce novel sequences that are still plausible and natural human motion. -Flipping left and right: The same action, performed using either the left or right hand, will remain a valid human motion. Thus, we can follow the kinematic tree of the SMPL model and mirror the motion across the left and right and generate a new motion sequence. -Random root rotation: We randomly sample a root rotation from a unit sphere to capture different root orientations for possible human motion. Different pose estimators may assume different ground planes and coordinate systems, so SMPL parameters usually come in different root orientations. Sampling random root rotation helps the model cope with different possible coordinate frame choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Smooth Motion from videos</head><p>After learning a comprehensive human motion subspace using the VAE, we learn an additional encoder E motion that can directly extract coarse motion sequences from video features, mapping to the same latent space as E vae . Given an input sequence of video features f W = {f w } W w=1 , the encoder E motion 's task is to compress the input features into a latent code z that best summarizes the current observation as a coarse human motion sequence. We use the pretrained decoder D vae from the motion VAE to force E motion to sample from our pretrained motion subspace. Constraining the latent space of the E motion to a pretrained human motion subspace provides a strong human motion prior that greatly aids the learning process of E motion . Combining E motion and D vae , we form our Variational Motion Estimator (VME).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Motion Residual Regressor (MRR)</head><p>As noted in the previous section, the learned motion sequences using the VAE's latent space as the target are inherently smooth and coarse, capturing the overall motion signature of the current video frames through information compression. To capture the details, we utilize a SMPL regressor from <ref type="bibr" target="#b7">[8]</ref> that can iteratively refine the estimated poses. The regressor takes in an initial pose and shape estimation Θ t and the visual feature f t for a single frame to calculate its estimation Θ t for k iterations. Notice that though <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref> all utilize the same regressor, MEVA uses it in a fundamentally different way-in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>, the regressor is initialized with mean SMPL pose Θ mean . At a sequence level, a regressor initialized uniformly with the mean pose Θ mean is trying to capture the overall motion in one pass, while in MEVA, the regressor is initialized with the computed poses from VME. Thus, the regressor is only tasked to do small cosmetic changes to the coarse estimation, adding back the fine details of the motion lost during our compression step. Similar to <ref type="bibr" target="#b0">[1]</ref>, the input visual features f t to the regressor are encoded using a temporal visual encoder, so even though each frame's estimation is calculated separately at this stage, the visual feature is already temporally correlated. The VME computes the overall coarse motion from videos by using a general model of motion, and the regressor jointly refines motion and human shape estimates, which amounts to adding back the person-specific motion details at a per frame level. We call this regressor the Motion Residual Regressor (MRR). MRR completes the overall framework of our proposed method, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Losses</head><p>Our framework is trained in two stages. At first, the motion VAE is pretrained. Then STE, VME, and MRR are trained jointly end-to-end. Using videos with various levels of annotation (2D joint positions, 3D joint positions, SMPL parameters), similar to in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>, the networks are trained with losses consisting of L 2D , L 3D , L SM P L , as long as respective data is available. Specifically:</p><formula xml:id="formula_7">L meva = L 3D + L 2D + L SM P L ,<label>(3)</label></formula><formula xml:id="formula_8">L 3D = Σ T t=1 ||jp 3d t −ĵp 3d t || 2 ,<label>(4)</label></formula><formula xml:id="formula_9">L 2D = Σ T t=1 ||jp 2d t −ĵp 2d t || 2 ,<label>(5)</label></formula><formula xml:id="formula_10">L SM P L = ||β −β|| 2 + Σ T t=1 ||θ t −θ t || 2 .<label>(6)</label></formula><p>For implementation details, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate the effectiveness of our proposed method, we evaluate our method in terms of the overall accuracy and smoothness of the estimated motion on MPI-INF-3DPH <ref type="bibr" target="#b48">[49]</ref>, 3DPW <ref type="bibr" target="#b10">[11]</ref>, and human 3.6M <ref type="bibr" target="#b49">[50]</ref>. In the following sections, we will first describe the main datasets used to train and evaluate MEVA. Then in Section 4.2 we provide extensive evaluation results. Finally, in Section 4.3, we provide the ablation studies for our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For the motion VAE, we use motion sequences from AMASS <ref type="bibr" target="#b9">[10]</ref> for training and sequences from 3DPW <ref type="bibr" target="#b10">[11]</ref> for evaluation. For training with videos, in addition to the train split of MPI-INF-3DPH <ref type="bibr" target="#b48">[49]</ref>, 3DPW <ref type="bibr" target="#b10">[11]</ref>, and human 3.6M <ref type="bibr" target="#b49">[50]</ref>, which have 3D joint annotation, we also use InstaVariety <ref type="bibr" target="#b6">[7]</ref> and PennAction <ref type="bibr" target="#b50">[51]</ref> which contain 2D joint annotation. For training our motion VAE, we use AMASS <ref type="bibr" target="#b9">[10]</ref>. It is a recent dataset that contains a large sample of human motion sequences in SMPL parameters. These sequences are fitted from MoCap sequences using Mosh++ <ref type="bibr" target="#b9">[10]</ref>. There are in total 13k motion sequences with varying length. We use this dataset only for training our motion VAE.</p><p>For training with videos, 3DPW is the only dataset that contains paired SMPL parameters and video sequences (which provide direct supervision to MEVA). The videos from this dataset are mostly outdoors and in-the-wild. It uses paired IMU sensors and video input to compute the near ground truth SMPL pose and shape parameters. This is a relatively small dataset and we use the official split in <ref type="bibr" target="#b10">[11]</ref> for train, validation, and test. There are in total 60 videos with varying length (24 train, 24 test, 12 val). MPI-INF-3DHP is a dataset that contains 3D joint position annotation. It is captured using a multiview camera setup, and the 3D joint annotation is calculated through multiview methods <ref type="bibr" target="#b48">[49]</ref>. There are 8 subjects and 16 videos per subject, in total 128 videos with varying length. We use the official test and train split. H3.6M is a popular pose estimation dataset that contains 3D joint position annotations, captured indoors with MoCap markers. Notice that a number of previous works [8,7,3,1,2] had access to a near-ground truth SMPL pose and shape parameters calculated by the Mosh <ref type="bibr" target="#b51">[52]</ref> method. However, this annotation has since been removed from public access due to legal issues. SMPL parameters provide the best supervision, as noted in <ref type="bibr" target="#b52">[53]</ref>, so for a fair comparison we retrain some of the state-of-the-art methods without such supervision. There are 840 videos in total across 7 subjects in H3.6M and we use the official train/test subject split ([S1, S5, S6, S7, S8] vs [S9, S11]). During preprocessing, we subsample every 5 frames from the dataset. The PennAction dataset contains human annotated ground truth 2D keypoints paired with video sequences. There are in total 2326 videos with varying length. InstaVariety dataset contains human annotated pseudo ground truth 2D keypoints paired with video sequences. The 2D keypionts are estimated using openpose <ref type="bibr" target="#b53">[54]</ref>. There are in total 28,272 videos with varying length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Results and Analysis</head><p>Metrics To best capture human motion, we utilize three popular metrics that measure the overall accuracy and smoothness of the motion. Mean per joint position error (MPJPE) and MPJPE after Procrustes Alignment (PA-MPJPE) measure the 3D joint positional discrepancy between the predicted and ground truth 3D joint positions in millimeters (mm), and are calculated after aligning the root position (human pelvis) of the 3D joint positions. Both MPJPE and PA-MPJPE serve as the accuracy indicator of the motion estimator. Acceleration error (ACC-ERR), proposed in <ref type="bibr" target="#b6">[7]</ref>, measures the difference between the predicted and ground truth 3D acceleration for each keypoint in mm/s 2 . ACC-ERR serves as the major smoothness indicator for the estimated motion sequences. Acceleration is calculated using the finite difference between individual frames. It is imperative to view these metrics jointly: a low position error indicates overall correctness in motion capture and a better acceleration error marks a smooth and natural estimation of human motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization of Motion VAE</head><p>In this section, we study the genealizability of our learned motion VAE. We report the reconstruction error of the VAE on unseen motion sequences from the 3DPW dataset. <ref type="table" target="#tab_0">Table 1</ref> shows VAE motion reconstruction error over the different splits of 3DPW. The Motion VAE model is the best performing model that is trained with all three forms of data augmentation techniques. Detailed analysis about the effects of data augmentation can be found in 4.3. The result shows that our VAE generalizes well to unseen sequences and the learned subspace can represent the human motion space with reasonable quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>The result in <ref type="table" target="#tab_5">Table 2</ref> shows that our method obtains state-of-the-art results on video motion estimation across all three test datasets. Overall, our method achieves comparable results in terms of position error (MPJPE and PA-MPJPE) while significantly improving the smoothness (acceleration error), signifying a smoother and more natural motion estimation without sacrificing accuracy. Notice that all methods in italics have access to SMPL parameter annotation to the H3.6M dataset, which has since been removed from the web due to legal reasons. The SMPL parameters provide the most direct supervision for the task, so the performance gain is significant especially on the H3.6M test set. For a more direct comparison, we retrain the previous state-of-the-art method, VIBE <ref type="bibr" target="#b0">[1]</ref>, using the official implementation with the exact same datasets as ours. On 3DPW, under the same training condition, MEVA outperforms VIBE on almost all three metrics while reducing the acceleration error by 54.3%, 59.3%, and 41.3 %, respectively. Even compared to VIBE trained with extra data, our model achieves comparable results in accuracy while sporting a great reduction in acceleration error, except for the H3.6M dataset. Note that the H3.6M dataset contains mainly indoor scenes with limited background variation and models trained with direct SMPL supervision tend to perform well on this dataset. Compared to HMMR <ref type="bibr" target="#b6">[7]</ref>, which is the state-of-theart on smoothness, our model still achieves a smoother result (23.7% reduction in acceleration error) while improving greatly in MPJPE by 25.4%.</p><p>Qualitatively Results As motions are best seen in videos, please refer to the supplementary video for qualitative results. Overall, our model achieves better <ref type="table" target="#tab_5">Table 2</ref>. Testing error of state-of-the-art models on 3DPW, MPI-INF-3DHP, and H3.6M. Here we compare with state-of-the-art methods on video pose estimation, and report metrics on positional accuracy (MPJPE and PA-MPJPE) as well as acceleration error. Notice that since an important annotation of H3.6M dataset has since been made unavailable, we put all methods that are trained with such supervision in italics. The most fair comparison is between our method and VIBE <ref type="bibr" target="#b0">[1]</ref>, the previous best-performing model (trained using the same datasets). acceleration error while preserving high joint position accuracy, resulting in an overall smooth and natural motion.</p><formula xml:id="formula_11">3DPW MPI-INF-3DHP H3.6M MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓ MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓ MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓ HMR (w/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Experiments</head><p>Effect of data augmentation for training the motion VAE As mentioned in Sec.3.3, data augmentation performed on the AMASS dataset significantly improves the generalizability of our motion VAE. <ref type="table" target="#tab_2">Table 3</ref> demonstrates the VAE reconstruction error on the unseen sequences from the 3DPW dataset (train/test/val), with various levels of data augmentation techniques. Overall, RR (random root) rotation is essential since the motion sequences in AMASS dataset, captured mostly in MoCap studio, have a single fixed initial root rotation. Model trained only on AMASS would suffer greatly when it encounters variation in root rotation. Changing the sampling frame rate (FR) and flip left and right (LR) also provide a significant boost to generalizability. Using all three augmentation techniques results in our best performing motion VAE. Coarse motion vs fine motion retrieval MEVA benefits from an explicit breakdown of coarse and fine motion retrieval, using a temporal compressive step that captures the overall motion in a given human motion sequence. Just how much coarse/smooth motion information is retrieved in the framework? <ref type="table" target="#tab_3">Table 4</ref> shows the result of MEVA if trained only using the VME (capturing only coarse motion). Notice that MEVA with only VME achieves result similar to HMMR <ref type="bibr" target="#b6">[7]</ref>, the previous state-of-the-art in producing low acceleration error motion estimation. An illustrative visualization of coarse and fine motion decomposition can be found in <ref type="figure" target="#fig_1">Fig.4</ref>.  Effects of the pretrained Motion VAE MEVA benefits from using a pretrained motion VAE's latent space. As argued in Sec.3.3, using a pretrained VAE provides a human motion subspace that assists in constraining the estimated motion sequences to be natural and plausible human motion. <ref type="table" target="#tab_3">Table 4</ref> shows the result of MEVA trained without using a pretrained VAE (not using the pretrained D vae ). In this case, the whole framework is trained end-to-end from scratch. Here we observe that the model performed relatively well in both accuracy and smoothness, demonstrating the power of our two-stage estimation framework. However, upon visual inspection, as shown in <ref type="figure" target="#fig_2">Fig.5</ref>, a few kinematically invalid human poses are estimated during the sequence, resulting in an overall accurate but flawed estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have shown that to achieve temporally smooth and accurate 3D human pose estimates, it is important to learn a compressive model that encodes the smoothness of general human motion while also learning an image-based regression model that can capture person-specific motion. We propose a two-stage model that first trains a Variational Autoencoder to model the general statistics of coarse/smooth human motion and then learns a person-specific motion refinement regression module to retain motions not captured by the general motion model. Through comprehensive experiments, we demonstrate that our method produces both smooth and accurate motion.</p><p>This supplementary material is organized as follows. In Section 1, we provide qualitative results for our proposed method (MEVA). In Section 2, we provide complementary ablation studies. In Section 3, we will discuss the failure modes of our method. In the last section, we include the details of our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Qualitative Results</head><p>To best view our motion estimation and compare it with state-of-the-art, please refer to the supplementary video.</p><p>Specifically, in the supplementary video, we first show a visual demonstration of our two-stage decomposition of coarse and fine motion from a given video sequence. Next, we demonstrate the qualitative comparison between our algorithm and the prior state-of-the-art (VIBE <ref type="bibr" target="#b0">[1]</ref>) and show that our method achieves smoother, more natural, and accurate motion estimation. Finally, we will discuss the implementation details of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Additional Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Comparison with Average Filtering</head><p>While our method has significantly reduced the acceleration error and achieves state-of-the-art accuracy, one can still apply postprocessing to existing sequences to further improve the prediction. To best study its effects, here we implement a simple average filter using spherical linear interpolation (slerp) in quaternion. Specifically, for each joint rotation in SMPL q i at timestep t, we apply slerp with a ratio of 0.5: q i t = slerp(q i t , q i t+1 , 0.5). <ref type="table" target="#tab_0">Table 1</ref> shows the result of applying averaging filtering as postprocessing on both VIBE <ref type="bibr" target="#b0">[1]</ref> and MEVA. From the results, it is clear that average filtering can help reduce the acceleration error of both VIBE and MEVA while slightly affecting accuracy. It is also conceivable that more sophisticated methods such as solving a constrained optimization problem <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b28">29]</ref> can further improve results. Nonetheless, in the paper we only compare with feed-forward methods without any postprocessing, since postprocessing approaches are complementary to feed-forward methods and would be beneficial to all of them.  <ref type="table" target="#tab_5">Table 2</ref>, using the same size temporal window, MEVA produces better results on all three metrics and maintains a significant advantage in acceleration error. Notice that VIBE trained with a longer temporal window shows a slight improvement against the ones that use a shorter window, validating our intuition that a longer temporal window provides a more substantial context for motion estimation. Nonetheless, our two-stage decomposition method is more effective in utilizing a longer temporal window due to its separate motion compression and refinement stages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Effects of STE and VME on MEVA</head><p>Here we take a further look into the effects of different components (STE, VME, and MRR) of our proposed method. Notice that without the Variational Motion Estimator (VME), our method will collapse into a single-stage estimator that only relies on the SMPL regressor, which has been studied extensively in prior art. Thus, here we only study the effects of Spatial Temporal Feature Extractor (STE) and Motion Refinement Regressor (MRR). <ref type="table" target="#tab_2">Table 3</ref> shows the results of our framework trained without STE or MRR. Without the STE, MEVA obtains high accuracy but suffers from high acceleration error. This indicates that STE produces correlated features that impart the necessary temporal consistency information to MRR. We reason that without STE, even although initialized with coarse estimation from VME, MRR will be biased by the input visual features and produce a temporally inconsistent refinement pose that negatively affects the overall estimation. On the other hand, without MRR, our method is reduced to one stage and only estimates the coarse motion. As shown in the result, using only VME will lead to an overly smoothed motion estimation and result in a higher acceleration error (underestimating movement also leads to high acceleration error). 3 Failure Modes</p><p>Although MEVA shows promising results in producing smooth and accurate human motion, there is still room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sliding window processing</head><p>MEVA processes videos using a sliding window: input video sequences are splited into chunks of 90 frames for processing. Due to the natural of this sliding window approach, inconsistency can sometimes be observed at a 3 second interval (videos are assumed to be at 30 fps). The explanation is as follows: the coarse motion estimated by VME can be quite different between each temporal window and MRR sometimes is unable to make enough adjustments to account for a smooth transition. Each temporal window also has their own STE, so the features from each window are longer correlated. <ref type="figure">Fig. 1</ref> shows an instance of this behavior. For visual inspection, please refer to our supplement video. <ref type="figure">Fig. 1</ref>. Sliding window failure mode. This plot shows that at the intersection of temporal windows, MEVA can result in a inaccurate transition and bring a large acceleration error. Each green line in the plot marks a temporal window, and there are large spikes of acceleration error at the first two intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Occluded body parts</head><p>Occluded body parts can still be challenging for MEVA. During occlusion, the lack of visual indicators will compel MEVA to rely on coarse motion estimation over the whole sequence and leads to a misscapture of detailed motion. Please refer to the supplementary video for an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Missing hands and face movement</head><p>Since the original SMPL[13] model does not contain joints for the hand and face, all methods using SMPL do not capture hand movements and facial expressions. Moreover, there is not enough high quality 3D data that provides hand and face annotations. A recent work <ref type="bibr" target="#b11">[12]</ref> develops an enhanced SMPL model that jointly models body pose, hands, and face, but this model has not gained significant traction in the pose estimation community. We believe that capturing hands and face movement in motion estimation is an essential direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human Motion VAE</head><p>The motion VAE's encoder, E vae , is a bidirectional Gated Recurrent Unit (bi-GRU) with average pooling to obtain the temporal encoding h of the overall input motion sequence M W ∈ R W ×144 . We pass the temporal encoding h into a multilayer perceptron (MLP) with two hidden layers (1024, 512) and two heads to obtain the mean µ and variance σ for the latent code z. For the decoder D vae , a forward GRU is used to decode the output motion sequence. At each time step, the GRU takes in the previous step estimation θ t−1 and the current latent code z ∈ R 1×Sz to output a 512 latent feature. The feature is then passed through an MLP with two hidden layers (1024, 512) to generate the reconstructed pose θ ∈ R 1×144</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2</head><p>Spatio-Temporal Feature Extractor For a video input, we first preprocess the video frames using a pretrained ResNet-59 network <ref type="bibr" target="#b7">[8]</ref>. The feature extractor outputs f i ∈ R 2048 for each frame. The extracted features within the same temporal window W (we choose W = 90) are stacked together [f t ] 90 t=1 ∈ R 90×2048 and are encoded by STE into a sequence of temporally correlated features [f t ] 90 t=1 ∈ R 90×2048 . STE is a 2 layer bi-GRU with hidden size 1024 that outputs a feature encoding at each timestep. E motion that shares the same architecture as E f eat except for the final average pooling step to come up with a latent code z ∈ R 1×512 that represents the whole motion sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Motion Residual Regressor</head><p>The MRR consists of 2 fully connected layers, each with 1024 neurons. It takes in per-frame features and a set of initializing parameters (pose, shape, and camera) and iterative refines its predictions (pose, shape, and camera) for k iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overall Architecture. MEVA estimates motion from videos by first extracting temporal features using Spatio-Temporal Feature Extractor (STE) and then estimates the overall coarse motion inside the video with Variational Motion Estimator (VME). Finally, a Motion Residual Regressor (MRR) is used to refine the motion estimates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Breakdown of coarse and fine human motion. The first row of estimated human is the coarse part of the motion (output of VME), while the second row adds back the fine details (output of MRR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>MEVA results without using pretrained VAE. Here we show an ablative study where we do not use a pretrained VAE. The second row shows MEVA result without using a pretrained VAE and the first row shows our full model estimation. Notice that the model estimates unnatural human poses in the first few frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>VAE Reconstruction Test Error on 3DPW dataset Here, the VAE is tasked to encode and decode unseen motion sequences from the 3DPW dataset, and we calculate our metrics between the ground truth and reconstructed sequences. Motion sequences from different splits of 3DPW have varying difficulties, but are all unseen by our VAE.PA-MPJPE ↓ ACC-ERR ↓ MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓ MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓</figDesc><table><row><cell cols="2">3DPW Train Split</cell><cell></cell><cell></cell><cell>3DPW Val Split</cell><cell></cell><cell></cell><cell>3DPW Test Split</cell><cell></cell></row><row><cell>MPJPE ↓ Motion VAE 72.7</cell><cell>52.3</cell><cell>8.9</cell><cell>72.1</cell><cell>52.9</cell><cell>9.3</cell><cell>58.7</cell><cell>43.9</cell><cell>8.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation of Data augmentation for VAE. Here we show the effects of data augmentation on the VAE. RR: random root rotation, FR: different sampling framerate, LR: left and right flip PA-MPJPE ↓ ACC-ERR ↓ MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓ MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓</figDesc><table><row><cell></cell><cell></cell><cell>Train</cell><cell></cell><cell></cell><cell>Val</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell cols="2">MPJPE ↓ No Aug 200.2</cell><cell>86.0</cell><cell>15.9</cell><cell>181.7</cell><cell>90.5</cell><cell>15.8</cell><cell>172.8</cell><cell>75.2</cell><cell>14.1</cell></row><row><cell>RR</cell><cell>146.3</cell><cell>78.0</cell><cell>12.1</cell><cell>133.9</cell><cell>81.2</cell><cell>13.0</cell><cell>115.0</cell><cell>70.4</cell><cell>11.0</cell></row><row><cell>FR + LR</cell><cell>291.0</cell><cell>63.03</cell><cell>14.7</cell><cell>308.8</cell><cell>66.1</cell><cell>16.1</cell><cell>261.9</cell><cell>58.9</cell><cell>13.3</cell></row><row><cell cols="2">FR + LR + RR 72.7</cell><cell>52.3</cell><cell>8.9</cell><cell>72.1</cell><cell>52.9</cell><cell>9.3</cell><cell>58.7</cell><cell>43.9</cell><cell>8.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation of MEVA. Here we show the MEVA model trained with only Variational Motion Estimator (without Motion Residual Regressor) or without using pretrained VAE.</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell></row><row><cell></cell><cell cols="3">MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓</cell></row><row><cell>MEVA-VME only</cell><cell>118.1</cell><cell>73.7</cell><cell>15.4</cell></row><row><cell>MEVA-without using pretrained VAE</cell><cell>95.9</cell><cell>59.7</cell><cell>14.1</cell></row><row><cell>MEVA</cell><cell>86.9</cell><cell>54.7</cell><cell>11.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Ablation study on average filtering. Here we show the result of applying the average filter on the output from VIBE<ref type="bibr" target="#b0">[1]</ref> and MEVA.</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell></row><row><cell cols="4">MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓</cell></row><row><cell>VIBE (w/o H3.6M SMPL) [1]</cell><cell>91.9</cell><cell>57.6</cell><cell>25.4</cell></row><row><cell>VIBE (w/o H3.6M SMPL) [1] + Average Filtering</cell><cell>91.6</cell><cell>57.8</cell><cell>13.5</cell></row><row><cell>MEVA (w/o H3.6M SMPL) (ours)</cell><cell>86.9</cell><cell>54.7</cell><cell>11.6</cell></row><row><cell>MEVA (w/o H3.6M SMPL) + Average Filtering (ours)</cell><cell>87.6</cell><cell>55.5</cell><cell>8.2</cell></row><row><cell>2.2 Effects of a long temporal window</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MEVA uses a significantly longer temporal window (90 frames) than prior art</cell></row><row><cell cols="4">(HMMR [7]: 20 frames, VIBE[1]: 16 frames). To show that our MEVA frame-</cell></row><row><cell cols="4">work benefits more from this setting, we retrain VIBE with a 90 frames temporal</cell></row><row><cell>window. As shown in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on temporal window size. Here we show the results of using different temporal windows in VIBE<ref type="bibr" target="#b0">[1]</ref> and MEVA</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell></row><row><cell></cell><cell cols="3">MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓</cell></row><row><cell>VIBE (w/o H3.6M SMPL) + 16 frames [1]</cell><cell>91.9</cell><cell>57.6</cell><cell>25.4</cell></row><row><cell>VIBE (w/o H3.6M SMPL) + 90 frames [1]</cell><cell>88.1</cell><cell>56.6</cell><cell>21.2</cell></row><row><cell cols="2">MEVA (w/o H3.6M SMPL) + 90 frames (ours) 86.9</cell><cell>54.7</cell><cell>11.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Ablation of MEVA components. Here we show MEVA trained without STE (with both VME and MRR) and without MRR (with both STE and VME).</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell></row><row><cell></cell><cell cols="3">MPJPE ↓ PA-MPJPE ↓ ACC-ERR ↓</cell></row><row><cell>MEVA w/o STE</cell><cell>89.7</cell><cell>55.4</cell><cell>29.0</cell></row><row><cell cols="2">MEVA w/o MRR 118.1</cell><cell>73.7</cell><cell>15.4</cell></row><row><cell>MEVA</cell><cell>86.9</cell><cell>54.7</cell><cell>11.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This project was sponsored in part by IARPA (D17PC00340), and JST AIP Acceleration Research Grant (JPMJCR20U1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7759" to="7769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno>ArXiv abs/2003.04232</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7759" to="7769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5607" to="5616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5441" to="5450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="614" to="631" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10967" to="10977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Smpl: a skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SCAPE: Shape Completion and Animation of People</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring 3D structure with a statistical image-based shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recovering 3D human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 -Proceedings of the 2007 Conference</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parametric reshaping of human bodies in images. ACM SIGGRAPH 2010 Papers</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Bãlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="page" from="561" to="578" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="10876" to="10886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="5237" to="5247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="7745" to="7754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning 3D human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 11213 LNCS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="679" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">VNect Real-time 3D Human Pose Estimation with a Single RGB Camera ACM Reference format VNect Real-time 3D Human Po.pdf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M H M M H M M H</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M H M M H M M H</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tog</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Xnect: Real-time multi-person 3d motion capture with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A data-driven approach to quantifying natural human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2005</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D people tracking with Gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning and tracking cyclic human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ormoneit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning diverse stochastic human-action generators by learning smooth latent transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv abs/1912.10150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep video generation, prediction and completion of human action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mandery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="13" to="26" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<title level="m">Language2pose: Natural language grounded pose forecasting. 2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Matsunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3441" to="3448" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dancing to music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision 2015 Inter</title>
		<meeting>the IEEE International Conference on Computer Vision 2015 Inter</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 11209 LNCS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="276" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Diverse trajectory forecasting with determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno>ArXiv abs/1907.04967</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Butepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dlow: Diversifying latent flows for diverse human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<idno>abs/2003.08386</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5738" to="5746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Pose Knows: Video Forecasting by Generating Pose Futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3352" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">From actemes to action: A stronglysupervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmoody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Blackz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning diverse stochastic human-action generators by learning smooth latent transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv abs/1912.10150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SFV: Reinforcement learning of physical skills from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical Papers</title>
		<imprint>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SIGGRAPH Asia</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">3D Human Motion Estimation via Motion Compression and Refinement Supplementary Material</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Alireza</forename><surname>Zhengyi Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<pubPlace>Pittsburgh, PA 15213, USA {zluo2</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>sgolesta,kkitani}@cs.cmu.edu</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
