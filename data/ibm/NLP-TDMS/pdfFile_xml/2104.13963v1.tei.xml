<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Assran</surname></persName>
							<email>massran@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Mila -Quebec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
							<email>mathilde@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria Univ. Grenoble Alpes</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<email>imisra@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
							<email>bojanowski@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<email>ajoulin@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
							<email>ballasn@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
							<email>mikerabbat@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Mila -Quebec AI Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel method of learning by predicting view assignments with support samples (PAWS). The method trains a model to minimize a consistency loss, which ensures that different views of the same unlabeled instance are assigned similar pseudo-labels. The pseudo-labels are generated non-parametrically, by comparing the representations of the image views to those of a set of randomly sampled labeled images. The distance between the view representations and labeled representations is used to provide a weighting over class labels, which we interpret as a soft pseudo-label. By non-parametrically incorporating labeled samples in this way, PAWS extends the distance-metric loss used in self-supervised methods such as BYOL and SwAV to the semi-supervised setting. Despite the simplicity of the approach, PAWS outperforms other semi-supervised methods across architectures, setting a new state-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of the labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4× to 12× less training than the previous best methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning with less labeled data has been a longstanding challenge of computer vision and machine learning research. One popular approach for learning with few labels is to first perform unsupervised pre-training on a large dataset followed by supervised fine-tuning on the small set of available labels. Self-supervised methods generally adhere to this paradigm (e.g., see <ref type="bibr" target="#b0">[1]</ref> for an analysis in the context of semisupervised learning), and they have demonstrated competitive performance on semi-supervised learning benchmarks across a wide range of self-supervised pre-training strategies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. However, the self-supervised paradigm also requires substantially more computational effort than other approaches and does not make use of labeled data when it is available.</p><p>An alternative line of work suggests to use available la- * Co-last author Code: github.com/facebookresearch/suncet  beled data to generate pseudo-labels for the unlabeled data, and then train a model using the labeled and pseudo-labeled data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. This begs the question, can we get the best of both worlds, leveraging labeled data throughout training while also building on advances in self-supervised learning?</p><p>This paper proposes a novel method of learning by predicting view assignments with support samples (PAWS). The method trains a model to minimize a consistency loss, which ensures that different views of the same unlabeled instance are assigned similar pseudo-labels. The pseudo-labels are generated non-parametrically, by comparing the representations of the image views to those of a set of randomly sampled labeled images. The distance between the view representations and labeled representations is used to provide a weighting over class labels, which we interpret as a soft pseudo-label. By non-parametrically incorporating labeled samples in this way, PAWS extends the distance-metric loss in self-supervised methods such as BYOL <ref type="bibr" target="#b3">[4]</ref> and SwAV <ref type="bibr" target="#b2">[3]</ref> to the semi-supervised setting.</p><p>Despite the simplicity of the approach, PAWS outperforms other semi-supervised methods across architectures, setting a new state-of-the-art for a ResNet-50 trained on ImageNet with either 10% or 1% of the training instances labeled, achieving 75% and 66% top-1 respectively. Moreover, this is achieved with only 200 epochs of training, which is 4× less than that of the previous best method. The same conclusion holds when training with wider ResNet architectures as well (i.e., ResNet-50 2× or 4×).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semi-supervised learning. One procedure to simultaneously learn with both labeled and unlabeled data is to combine a supervised loss on the labeled samples with an unsupervised loss on the unlabeled samples. For example, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> train a model by adding an unsupervised regularization term to a supervised cross-entropy loss. Similarly, UDA <ref type="bibr" target="#b14">[15]</ref> adds a supervised cross-entropy loss to an appropriately weighted unsupervised regularization term. Likewise, S4L <ref type="bibr" target="#b15">[16]</ref> adds a supervised cross-entropy loss to a weighted mixture of self-supervised pretext loss terms.</p><p>There is also a family of semi-supervised methods related to self-training <ref type="bibr" target="#b4">[5]</ref> that explicitly generate pseudo-labels for the unlabeled samples and that optimize prediction accuracy on both the ground truth labels (for the labeled samples) and the pseudo-labels (for the unlabeled samples). For example Pseudo-Label <ref type="bibr" target="#b16">[17]</ref> and earlier related methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> first train a model on the labeled samples, use this model to assign pseudo-labels to unlabeled samples, and then retrain the model using both the labeled and unlabeled samples. The MixMatch trilogy of work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref> operates similarly, but generates the pseudo-labels in an online fashion. Specifically, FixMatch <ref type="bibr" target="#b10">[11]</ref> trains with a supervised cross-entropy loss while simultaneously making predictions on weakly augmented unlabeled images. When the unsupervised predictions are confident enough, they are used as pseudo-labels for strongly augmented views of those same unlabeled images. Another recent method building on the self-training framework is MPL <ref type="bibr" target="#b5">[6]</ref>. MPL uses a teacher network to pseudolabel unlabeled images for a student network. The student then performs an update by minimizing its prediction error with respect to the teacher's pseudo-label. Subsequently, the student is evaluated on a mini-batch of labeled samples, and the teacher network is updated using a meta-learning loss based on the student's evaluation performance. In MPL, the overall teacher update consists of the combination of the student's meta-learning loss plus a separate UDA loss. After self-training, the MPL student model is subsequently finetuned on the labeled samples using a standard cross-entropy loss.</p><p>Few-shot learning. In few-shot classification, a network must be adapted to learn to recognize new classes when given only a few labeled examples of these classes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>One common approach, which is adopted by Matching Networks <ref type="bibr" target="#b20">[21]</ref> and Prototypical Networks <ref type="bibr" target="#b21">[22]</ref>, is to learn a metric space to embed the data. A differentiable nearestneighbour classifier is then used in this space to predict the class of a query point given some labeled data-points in the support set <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Although there are few-shot approaches that learn entirely from unsupervised data <ref type="bibr" target="#b24">[25]</ref>, the majority train using labeled data, which is in contrast to the selfsupervised approaches discussed next.</p><p>Self-supervised learning. Major advances have been made in learning useful image representations from unlabeled data. Some methods take the approach of incorporating domainspecific knowledge in the form of specific pre-training tasks, such as solving jigsaws <ref type="bibr" target="#b25">[26]</ref>. More recent success has been achieved by contrasting multiple views of an image <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, where the views come from different random augmentations. Such methods aim to learn a mapping from images to a representation space such that different views of the same image have similar representations. Various approaches have been proposed to avoid the trivial solution of collapsing all images to the same point, including contrasting negative samples <ref type="bibr" target="#b1">[2]</ref> and using Sinkhorn-Knopp normalization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>It has been demonstrated that self-supervised pre-training produces image representations that can be leveraged effectively for semi-supervised learning <ref type="bibr" target="#b0">[1]</ref>. Contrastive selfsupervised pre-training generally benefits from training with very large batch sizes, containing sufficiently many positive and negative examples, and consequently is very computationally expensive, e.g., requiring between 800-1000 epochs of pre-training to learn state-of-the-art representations on ImageNet. Some recent works have demonstrated that the batch-size requirements can be reduced at the expense of maintaining an additional memory bank <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>. Further performance benefits have been obtained by distilling very large pre-trained teacher models to smaller student models <ref type="bibr" target="#b0">[1]</ref>. In contrast, PAWS only trains with positive examples, and leverages available annotated data during pre-training to significantly reduce the amount of pre-training required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We consider a large dataset of unlabeled images D = (x i ) i∈ <ref type="bibr">[1,N ]</ref> and a small support dataset of annotated images S = (x si , y i ) i∈ <ref type="bibr">[1,M ]</ref> , with M N . <ref type="bibr" target="#b0">1</ref> Our goal is to learn image representations by leveraging both D and S during pretraining. After pre-training with D and S, we fine-tune the learned representations using only the labeled set S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">High-level Description</head><p>A schematic of the high-level pre-training approach is shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Given an image x i from D, we use a random  The method assigns soft pseudo-labels to an anchor view of an image and an associated positive view, and subsequently minimizes the cross-entropy H between them. The soft pseudo-labels are generated using a differentiable similarity classifier π d that measures the similarity to a mini-batch of labeled support samples, and outputs a soft class distribution. Positive views are created using data-augmentations of the anchor view. Since the trivial collapse of all representations to a single vector would lead to high-entropy predictions by the similarity classifier, sharpening the target pseudo-labels is sufficient to eliminate all trivial solutions.</p><p>set of data augmentations to generate two views, an anchor viewx i , and an associated positive viewx + i . Learning proceeds by non-parametrically assigning soft pseudo-labels to the anchor and positive view and subsequently minimizing the cross-entropy H(·, ·) between them.</p><p>The soft pseudo-labels are generated using a differentiable similarity-based classifier π d that measures the similarity of a given representation to those of a mini-batch of labeled samples from the support set S, and outputs a (soft) class label. We use a simple Soft Nearest Neighbours strategy <ref type="bibr" target="#b30">[31]</ref> for the similarity classifier π d .</p><p>Connection to few-shot learning. The mini-batch of labeled samples is obtained by first sampling a subset of classes and then sampling a few instances of each class. This, along with the use of a soft nearest-neighbours strategy is similar to approaches previously used for few-shot classification <ref type="bibr" target="#b20">[21]</ref>. However, unlike <ref type="bibr" target="#b20">[21]</ref>, we do not use LSTMs or other mechanisms for encoding or accessing elements of the support set, and furthermore, we never seek to directly predict the labels of elements of the support set. Rather, the support set is only used to assign pseudo-labels to unlabeled image views, and the loss is only evaluated with respect to the pseudo-labels assigned to the unlabeled image views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Detailed Methodology</head><p>Let x ∈ R n×(3×H×W ) denote a mini-batch of n anchor image views, and let x + ∈ R n×(3×H×W ) denote the associated n positive image views. Similarly, let x S ∈ R m×(3×H×W ) denote a mini-batch of m support images drawn from S with one-hot class labels y S ∈ R m×K , where K is the number of classes.</p><p>Encoder. Given a parameterized encoder, denoted by f θ : R 3×H×W → R d , let z ∈ R n×d and z + ∈ R n×d denote the representations computed from x and x + , respectively, and let z S ∈ R m×d denote the m support representations computed from x S . In our experiments below, the encoder will be the trunk of a deep residual network <ref type="bibr" target="#b31">[32]</ref>. The i th representation in the mini-batch z is written as a row-vector z i ∈ R 1×d , and its associated positive view in the mini-batch is denoted</p><formula xml:id="formula_0">z + i ; i.e., z i = f θ (x i ) and z + i = f θ (x + i )</formula><p>. For a scalar-valued similarity function d(·, ·) ≥ 0, the similarity classifier π d (·, ·) is given by</p><formula xml:id="formula_1">π d (z i , z S ) = (zs j ,yj )∈z S d(z i , z sj ) zs k ∈z S d(z i , z sk ) y j</formula><p>where y j is the one-hot ground truth label vector associated with the j th row vector z sj from z S .</p><p>Similarity metric and predictions. In this work, we take the similarity metric d(a, b) to be exp( a T b / a b τ ), the exponential temperature-scaled cosine. For L2-normalized representations, the similarity classifier π d (·, ·) can be concisely written as</p><formula xml:id="formula_2">p i := π d (z i , z S ) = σ τ (z i z S )y S ,</formula><p>where σ τ (·) is the softmax with temperature τ &gt; 0, and p i ∈ [0, 1] K is the prediction for representation z i . <ref type="bibr" target="#b1">2</ref> The positive view predictions p + i are calculated similarly from representations z + i . To avoid representation collapse, rather than contrast negative samples or incorporate Sinkhorn-Knopp normalization, we compare the prediction of one view with the sharpened prediction of the other view. We define the sharpening function ρ(·) with temperature T &gt; 0 as</p><formula xml:id="formula_3">[ρ(p i )] k := [p i ] k 1 /T K j=1 [p i ] j 1 /T , k = 1, . . . , K.</formula><p>Sharpening the targets encourages the network to produce confident predictions. As will be clear in Section 4, sharpening the targets is provably sufficient to eliminate collapsing solutions in the PAWS framework. Empirically, we have observed that training without sharpening can result in collapsing solutions.</p><p>Note that in the case where the support set contains only one instance per sampled class, sharpening the target predictions is equivalent to using a lower temperature in the cosine similarity between the unlabeled representation and support representations. However, when the sampled support set contains more than one instance per sampled class, then sharpening the target predictions is actually different from adjusting the cosine temperature. In this case, it is preferable to sharpen the target predictions rather than use a different temperature in the cosine similarity, since changing the cosine temperature can significantly affect the accuracy of the similarity classifier π d .</p><p>Training objective. To train the encoder, we penalize when the predictions p i and p + i of two views of the same image are different. As mentioned above, we compare the prediction of one view with the sharpened prediction of the other view; i.e., H(ρ(p i ), p + i ) + H(ρ(p + i ), p i ). We also incorporate a regularization term to encourage the image view representations to utilize the full set of classes represented in the support set. Let p := 1 2n n i=1 ρ(p i ) + ρ(p + i ) denote the average of the sharpened predictions across all unlabeled representations. The regularization term, which we refer to as mean entropy maximization (ME-MAX), seeks to maximize the entropy of p, denoted H(p). That is, while the individual predictions are encouraged to be confident, the average prediction is encouraged to be close to the uniform distribution. The ME-MAX regularizer has previously been used in the discriminative unsupervised clustering community for balancing learned cluster sizes (see, e.g., <ref type="bibr" target="#b32">[33]</ref>).</p><p>Thus, the overall objective to be minimized when training the parameters θ of the encoder f θ is</p><formula xml:id="formula_4">1 2n n i=1 H(ρ(p + i ), p i ) + H(ρ(p i ), p + i ) − H(p).<label>(1)</label></formula><p>Note that we only differentiate the cross-entropy loss terms with respect to the predictions p i and p + i , and not the sharpened targets ρ(p i ) and ρ(p + i ). The discussion so far has assumed that we only generate two views for each unlabeled image. One could generate more than two views, in which case we sum the loss over all views and take the target to be the average prediction across the other views of the same image.</p><p>The proposed approach seeks to improve on existing selfsupervised approaches for semi-supervised learning by: (i) efficiently using available task information, and (ii) addressing representation collapse. On the first issue, since the similarity classifier is differentiable, we evaluate gradients with respect to the labeled samples, but do not directly optimize prediction accuracy on the ground truth labels to avoid overfitting. On the second issue, since the trivial collapse of all representations to a single vector would lead to high-entropy predictions by the similarity classifier, sharpening the target pseudo-labels is sufficient to eliminate all trivial solutions as we will demonstrate in Section 4.</p><p>Neural architectures with external memory. PAWS can be interpreted as a neural network architecture with an external memory. Typically, in those architectures, a differentiable neural attention mechanism is used to read and access a memory space which contains a set of elements that are relevant to the task at hand. In PAWS, the support representations z S of labeled images characterize the external memory of the network, while the non-parameteric classifier π d corresponds to the soft-attention operation that retrieves memory elements given a query z i . From this perspective, PAWS optimizes an encoder network such that two views of the same image activate the same elements in the memory. Moreover, by randomly sampling a subset of labeled images to use as the support set at each iteration, PAWS avoids developing a strong dependence on any particular elements in the memory.</p><p>Assimilation &amp; Accommodation. PAWS also has connections to Piaget's Constructivist learning theory of assimilation &amp; accommodation <ref type="bibr" target="#b33">[34]</ref>, which provided grounding for work in cybernetics <ref type="bibr">[35,</ref> Chapter VII]. <ref type="bibr" target="#b2">3</ref> At the heart of Constructivism is the idea that every individual possesses representations relating to distinct semantic concepts that are updated through the process of assimilation and accommodation. During assimilation, the mind adapts its representations of new observations to fit its past observations, while during accommodation, the representations of past observations are updated to account for the new observations (cf. Appendix F). In the PAWS procedure, backpropagating with respect to the image views can be seen as a process of assimilation, ensuring that new observations (the image views) are consistent with the current schemata (the support representations). Similarly, backpropagating with respect to the support samples can be seen as a process of accommodation, ensuring that the current schemata (the support representations) are effective at describing the new observations (the image views).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Guarantees</head><p>Next we show that PAWS is guaranteed to avoid the trivial collapse of representations under the following assumptions.</p><p>Assumption 1 (Class Balanced Sampling). Each mini-batch of labeled support samples contains an equal number of instances from each of the sampled classes.</p><p>Assumption 2 (Target Sharpening). The target p + is sharpened, such that it is not equal to the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1 (Non-Collapsing Representations). Suppose</head><p>Assumptions 1 and 2 hold. If f θ is such that the representations collapse, i.e., z i = z for all z i ∈ S, then ∇ θ H(p + , p) &gt; 0.</p><p>Proof. Since z = z i for all z i ∈ S, it holds that d(z, z i ) = d(z, z j ) for all z i , z j ∈ S. Therefore p := π d (z, S) = 1 /n (zi,yi) y i , where y i is the one-hot class label for the representation z i . Let K denote the number of classes represented in the mini-batch of support samples. By Assumption 1, since the mini-batch of support samples contains an equal number of instances from each sampled class, it follows that there are n /K instances for each of the K represented classes. Therefore, the prediction p further simplifies to 1 n 1 K n K = 1 K 1 K , the uniform distribution over the K classes. However, by Assumption 2, the targets p + are sharpened such that they are not equal to the uniform distribution. Therefore, p = p + , from which it follows that ∇H(p + , p) &gt; 0.</p><p>Proposition 1 provides a theoretical guarantee that the proposed method is immune to the trivial collapse of representations. It is also straightforward to extend Proposition 1 to accommodate certain popular transformations of the labels y i , such as label smoothing. In short, the underlying principle is that collapsing representations result in high entropy predictions under the non-parametric similarity classifier, but the targets are always low-entropy (because we sharpen them), and so collapsing all representations to a single vector is not a stationary point of the training dynamics.</p><p>There are also alternative strategies to guarantee the non-collapse of representations without making the targetsharpening assumption, such as by directly using the available class labels for prediction or adding an entropyminimization term; see Appendix D for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation Details</head><p>We first pre-train a network using PAWS, and then fine-tune the learned representations for the classification task using only the labeled samples. We also report results using the pretrained representations directly in a nearest-neighbour classifier.</p><p>We adopt similar hyper-parameter settings that have previously been reported in the self-supervised literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Specifically, for pre-training, we use the LARS optimizer <ref type="bibr" target="#b35">[36]</ref> with a momentum value of 0.9, weight decay 10 −6 , cosine-similarity temperature of τ = 0.1, and batchsize of 4096. We linearly warm-up the learning-rate from 0.3 to 6.4 during the first 10 epochs of pre-training, and decay it following a cosine schedule <ref type="bibr" target="#b36">[37]</ref> thereafter.</p><p>To construct the different image views, we use the multicrop strategy from SwAV <ref type="bibr" target="#b2">[3]</ref>, generating two large crops (224 × 224), and six small crops (96 × 96) of each unlabeled image. Each small crop has two positive views (the two large crops), while each large crop has only one positive view (the other large crop). <ref type="bibr" target="#b3">4</ref> To construct the support mini-batch at each iteration, we also randomly sample 6720 images, comprising 960 classes and 7 images per class, from the labeled set. For all sampled images (both unlabeled images and support images), we apply the SimCLR dataaugmentations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>, specifically random crop, horizontal flip, color distortion, and Gaussian blur. For the sampled support images, we also apply label smoothing with a smoothing factor of 0.1. Lastly, for the target sharpening, we use a temperature of T = 0.25.</p><p>Following previous self-supervised methods, the encoder f θ in our experiments is a ResNet trunk with a 3-layer MLP projection head <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. To facilitate comparison with BYOL <ref type="bibr" target="#b3">[4]</ref>, we also include a 2-layer MLP prediction head, g ζ , after f θ , before computing the anchor predictions. Specifically, the representations z and z S are fed into g ζ before computing their cosine similarity. While this prediction head is included in our default setup for consistency with previous work, the ablation experiments below (see <ref type="table" target="#tab_6">Table 5</ref>), show that PAWS also works well without it. Similar to previous self-supervised methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>, we also use global batch normalization during pre-training, and exclude the bias and batch-norm parameters from weight decay and LARS adaptation.</p><p>After pre-training, we fine-tune a linear classifier from the first layer of the projection head in the encoder f θ , and follow the evaluation protocol of BYOL <ref type="bibr" target="#b3">[4]</ref>. Specifically, we simultaneously fine-tune the encoder/classifier weights using the available labeled samples and a standard supervised crossentropy loss. See Appendix A for more details, and Section 7 for ablation experiments.</p><p>We also report the results of using the pre-trained representations directly in a nearest-neighbour classifier (i.e., without fine-tuning). Specifically, the nearest-neighbour classifier compares the representations of new query images to those of the available labeled data. We refer to this approach as PAWS-NN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Main Results</head><p>In this section we analyze the features learned by PAWS on ImageNet <ref type="bibr" target="#b38">[39]</ref>. The standard procedure for evaluating semisupervised methods on ImageNet is to assume that some percentage of the data is labeled, and treat the rest of the data as unlabeled. For reproducibility, we use the same 1% and 10% data splits used in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Baselines. We focus on comparing PAWS to other methods in the literature that train using the same architectures to make a fair comparison. We do not include comparisons with results that first train a larger teacher model and then distill it to a smaller student <ref type="bibr" target="#b0">[1]</ref>. For reference, the best reported result in the literature for a ResNet-50 and 1% or 10% labeled data are 73.9% and 77.5% top-1, achieved by distilling from a ResNet-152 with 3× wider channels and selective kernels <ref type="bibr" target="#b0">[1]</ref>. We impose this constraint on the baselines to provide a fair comparison and better isolate what factors contribute to performance improvements. It is know that using distillation in conjunction with larger architectures can result in improvements for any method, and we leave further investigation of distilling larger models pre-trained with PAWS for future work. Comparison to self-supervised pre-training. We compare PAWS to other self-supervised pre-training approaches, namely SimCLRv2 <ref type="bibr" target="#b0">[1]</ref>, BYOL <ref type="bibr" target="#b3">[4]</ref>, SwAV <ref type="bibr" target="#b2">[3]</ref>, and SwAV+CT <ref type="bibr" target="#b37">[38]</ref>, which simply adds a supervised contrastivetask loss to SwAV pre-training. Results are reported in <ref type="table">Table 1</ref>    the self-supervised SwAV method trained on identical hardware <ref type="bibr" target="#b2">[3]</ref>. Pre-training with SwAV for 800 epochs requires 49.6 hours, while pre-trianing with PAWS for 100 epochs only requires 8.2 hours, and results in a +9.9% improvement in top-1 accuracy in the 1% label setting, and a +3.7% improvement in top-1 accuracy in the 10% label setting. In contrast to SimCLRv2 and BYOL, the PAWS method does not use an additional momentum encoder or a memory buffer, and thereby avoids this added computational and memory overhead, but may also benefit (in terms of final model accuracy) by incorporating such innovations.</p><p>Comparison to semi-supervised methods. We also compare PAWS to other semi-supervised learning methods, namely UDA <ref type="bibr" target="#b14">[15]</ref>, FixMatch <ref type="bibr" target="#b10">[11]</ref> and MPL <ref type="bibr" target="#b5">[6]</ref>. Results are reported in <ref type="table">Table 1</ref> for a ResNet-50 encoder network in the 10% label setting. MPL holds the current state-of-art in semisupervised learning, and simultaneously trains a student and teacher network for 800 epochs by adding a meta-learning loss and a teacher network to the UDA objective. PAWS out-performs MPL, the state-of-art semi-supervised learning approach, while requiring significantly fewer training epochs.</p><p>Impact of larger architectures. We examine the impact of training larger encoder networks with PAWS pre-training. Specifically, we pre-train ResNet-50 encoders with width multipliers of 2× and 4× in <ref type="table" target="#tab_2">Table 2</ref>. As expected, increasing the model capacity improves semi-supervised performance. Specifically, pre-training a Resnet-50 (4×) for 200 epochs with PAWS achieves 69.9% top-1 accuracy in the 1% label setting and 79.0% top-1 accuracy in the 10% label setting. We expect increasing the model capacity further to yield additional performance improvements. In general, results with the larger models are consistent with previous observations; PAWS pre-training outperforms other methods using similar architectures, while requiring significantly fewer pre-training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ablation Study</head><p>Longer training. The results reported in Section 6 illustrate the performance of PAWS after 100 and 200 pre-training epochs. We have not observed substantial benefits to training for longer than this. Results after pre-training longer are shown in <ref type="table" target="#tab_3">Table 3</ref> for ResNet-50 1× and 2× architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-1 Architecture</head><p>Epochs 1% 10% While PAWS does not seem to benefit from longer training, it is interesting to observae that, by contrast, PAWS-NN, which performs nearest neighbours classification (no fine-tuning), may benefit from longer training, as suggested by <ref type="table">Table 1</ref>.</p><p>Learning during pre-training. To further examine the behaviour of PAWS, we examine some metrics related to model quality during pre-training in <ref type="figure">Figure 4</ref>. <ref type="figure">Figure 4a</ref> shows the training cross-entropy loss when pre-training for 100 epochs. As expected, this loss decreases during training, indicating that the model is learning to assign similar pseudo-labels to different views of the same image. <ref type="figure">Figure 4b</ref> shows two additional losses computed using the sampled mini-batch and support set during training. Here, the instance discrimination loss is the normalized temperaturescaled cross-entropy loss <ref type="bibr" target="#b1">[2]</ref> computed using only unlabeled samples in the minibatch, and the classification loss is supervised noise-contrastive estimation loss <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> computed using only labeled samples in the support set. Note that these losses are only computed and reported to better understand PAWS pre-training, and they are not used to train the model. The decreasing instance discrimination loss (top) indicates that the model is learning representations that are invariant to the data augmentations used to construct different views.</p><p>The decreasing classification loss (bottom) also indicates that the model is learning to correctly classify labeled examples in the support set, despite not directly using labeled examples as targets.</p><p>Support set construction. PAWS pre-training requires specifying how to sample a support set. At each iteration, a support set is sampled by first sampling a subset of the K classes, and then sampling a certain number of images per class. We ablate the effect of these two parameters in <ref type="table">Table 4</ref>. Since we experiment with ImageNet, we can sample up to 1000 classes. Overall, we observe that using a larger support set consistently improves performance. Sampling more classes and fewer samples per class is better than the contrary (cf. bottom two rows). Note that no result is reported for 1000 classes and 16 images per class for the case of 1% labeled data, since in that case there are only 12811 labeled images in total.  <ref type="table">Table 4</ref>: Support Set. Ablating the composition of the sampled support mini-batches when training a ResNet-50 on ImageNet for 100 epochs. Our default setup is shaded in green. Increasing the size of the support set improves performance. However, when sampling a fixed number of instances, it is preferable to sample many classes with a few images per class, rather than few classes with many images per class.</p><p>Prediction head. As noted in Section 5, we include a prediction head to facilitate comparison to previous work <ref type="bibr" target="#b3">[4]</ref>, where it was suggested as a mechanism to prevent representation collapse. <ref type="table" target="#tab_6">Table 5</ref> illustrates that this is not needed when pre-training with PAWS, and in fact the performance of PAWS is marginally better when the prediction head is omitted during pre-training.</p><p>ME-Max regularization. Finally, recall that PAWS pretraining uses a cross-entropy loss with sharpened targets to encourage representations of different views of the same image to be consistent (reducing cross-entropy), and it also uses the mean-entropy maximization regularizer to maximize the entropy of the average prediction, computed across the unlabeled samples in the mini-batch. <ref type="table">Table 6</ref> illustrates the   <ref type="figure">Figure 4</ref>: Reporting various metric during training of a ResNet-50 on ImageNet, when 10% of the data is labeled. <ref type="figure">Fig.4a</ref> Cross-entropy loss between anchor view and (target) positive view during training. As expected, this loss decreases during training, indicating that the model is learning to assign similar pseudo-labels to different views of the same image. <ref type="figure">Fig.4b</ref> Additional losses computed with the sampled mini-batch and support-set during training for reporting purposes only. Specifically, no gradient is computed with respect to these losses. The decrease in the instance discrimination loss during training suggests that the model is learning representations that are invariant to the data-augmentations used for training. The decrease in the classification loss indicates that the model is learning to correctly classify the labeled support samples. <ref type="figure">Fig.4c</ref> The average confidence of the argmax target prediction during training. As training progresses, the model's target predictions become increasingly confident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top 1 100 epochs 200 epochs</head><p>With Prediction Head 73.9 75.0 Without Prediction Head 74.2 75.2 Our default setup is shaded in green. Unlike self-supervised methods that collapse without a prediction head <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>, PAWS still converges without a prediction head, as predicted by the theoretical result Proposition 1.</p><p>effect of training with only the cross-entropy term and disabling the ME-MAX regularization. While the impact is more pronounced in the setting with only 1% labeled data, using ME-MAX regularization improves performance in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top 1 1% 10%</head><p>With ME-MAX 63.8 73.9 Without ME-MAX 52.9 73.6 <ref type="table">Table 6</ref>: ME-Max Regularization. Examining the effect of the ME-MAX regularizer when training a ResNet-50 on ImageNet for 100 epochs. Our default setup is shaded in green. The ME-MAX regularizer is especially helpful in the 1% label setting, but only provides a marginal improvement in the 10% label setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>By leveraging a small labeled support set during pre-training, PAWS achieves competitive classification accuracy for semisupervised problems and requires significantly less training than previous works. PAWS also provably avoids collapsing solutions, a common challenge in self-supervised approaches. <ref type="bibr">PAWS</ref> can be interpreted as a neural network architecture with an external memory that is trained using the assimilation &amp; accommodation principle <ref type="bibr" target="#b33">[34]</ref>. During assimilation, PAWS updates the representations of new observations so that they are easily described by its external memory (or schemata), while during accommodation, PAWS updates its external memory to account for the new observations.</p><p>The use of a supervised support set has some practical advantages as well, since it enables the model to learn efficiently. However, it remains an interesting question to see if one can learn competitive representations in this framework using only instance supervision and more flexible memory representations. We plan to investigate those directions in future work.</p><p>Sampling the support mini-batches. In each iteration, PAWS randomly samples a small support mini-batch from the set of available labeled samples to compute the unsupervised consistency loss. Specifically, these support samples are used to determine the soft pseudo-labels for the unlabeled image views. To construct the support mini-batch in each iteration, we first sample a subset of classes, and then sample an equal number of images from each sampled class. Notably, we sample images with replacement. Therefore, while images in the same support mini-batch in a given iteration are always unique, some of the images may be resampled in the subsequent iteration's support mini-batch. This decision was made to simplify the implementation, although it is possible that epoch-based sampling of the support mini-batches (i.e., iterating through labeled samples with random reshuffling) could lead to improved performance.</p><p>Projection &amp; prediction heads. The projection head is a 3-layer MLP with ReLU activations, consisting of three fullyconnected layers of dimension 2048, and Batch Normalization applied to the hidden layers. The prediction head is a 2-layer MLP with ReLU activations, consisting of two fully-connected layers. The hidden layer has dimension 512, and the output layer has dimension 2048. Batch Normalization is applied to the input of the prediction head as well as to the hidden layer. The architectures of these projection and prediction heads are similar to those used in previous works on self-supervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Fine-tuning details. Following <ref type="bibr" target="#b0">[1]</ref>, we fine-tune a linear classifier from the first layer of the projection head in the pretrained encoder f θ , and initialize the weights of the linear classifier to zero. Specifically, we simultaneously fine-tune the encoder/classifier weights by optimizing a supervised cross-entropy loss on the small set of available labeled samples. We do not employ weight-decay during fine-tuning, and only make use of basic data augmentations (random cropping and random horizontal flipping). Following the experimental protocol of BYOL <ref type="bibr" target="#b3">[4]</ref>, we sweep the learning rate {0.01, 0.02, 0.05, 0.1, 0.2} and the number of epochs {30, 50}. Similarly to BYOL, to avoid performing parameter selection on the ImageNet validation set (used for reporting), we use a local validation set (12000 images from the ImageNet train set). Optimization is conducted using SGD with Nesterov momentum. We use a momentum value of 0.9 and a batch size of 1024. All results are reported using a single center-crop.</p><p>Nearest neighbours classifier. We also report additional results without fine-tuning the encoder. Specifically, the PAWS-NN results in <ref type="table">Table 1</ref> are reported by directly applying a soft nearest neighbours classifier to the pre-trained representations. To determine a class prediction for an image x, we compare its representation, z = f θ (x), to the representations of the available labeled training samples, z S ∈ R M ×d , and subsequently choose the class label with the highest probability under the similarity classifier; i.e., argmax k∈[1000] [π d (z, z S )] k . All results are reported using a single center-crop. <ref type="figure">Figure 5</ref> provides a schematic of the nearest neighbours classifier in an illustrative example with only only three labeled training images. <ref type="figure">Figure 5</ref>: Soft Nearest Neighbours classifier π d . For a K-way classification problem, and a scalar-valued similarity function d(·, ·) ≥ 0, the similarity classifier assigns a soft pseudo-label y ∈ [0, 1] K to a representation z by measuring its similarity to a set of labeled representations {z i } i with class labels {y i ∈ [0, 1] K } i . The soft pseudo-label y is a weighted average of the labels {y i } i , with labels corresponding to more similar representations assigned larger weights.</p><formula xml:id="formula_5">z p(y|z) ← ∑ i d(z,z i ) ∑k d(z,z k ) y i z 1 y 1 d ( z , z1 ) z 2 y 2 d(z, z 2 ) z 3 y 3 d ( z , z 3 )</formula><p>Momentum. When using momentum in our experiments, unless otherwise specified, we implicitly refer to classical momentum, commonly referred to as heavy-ball or Polyak momentum, given by</p><formula xml:id="formula_6">v t+1 = βv t + η t 1 |B| x∈B ∇ θ (x, θ t ) θ t+1 = θ t − v t+1 ,<label>(2)</label></formula><p>where β ≥ 0 is the momentum parameter and η t ≥ 0 is the learning rate. The model parameters are denoted by θ and the velocity buffer is denoted by v. Note that in some deep learning frameworks, such as PyTorch, the update is instead written</p><formula xml:id="formula_7">v t+1 = βv t + 1 |B| x∈B ∇ θ (x, θ t ) θ t+1 = θ t − η t v t+1 .<label>(3)</label></formula><p>Specifically, in the PyTorch version, the learning rate is not incorporated into the velocity buffer update. Thus, under a trivial re-parameterization, the PyTorch implementation can be interpreted as classical momentum with a time-varying momentum schedule, given by {βη t } t≥0 . In practice, when using a diminishing learning-rate schedule, this is equivalent to simultaneously taking the momentum parameter to 0. However, classically speaking, from an convex optimization perspective, momentum is most helpful in the later stages of training, where it accelerates convergence along low-curvature directions, which provably converge much slower than high curvature directions. In fact, classical acceleration guarantees typically advocate for either increasing the momentum parameter during training (e.g., in the convex setting), or simply using a constant momentum parameter (e.g., in the strongly convex setting). Additionally, note that training using the PyTorch implementation of momentum SGD in equation <ref type="formula" target="#formula_7">(3)</ref> with an adaptive learning rate, e.g., as prescribed by the LARS optimizer, leads to the adaptive scaling of the momentum parameter as well, which could possibly result in unexpectedly large momentum values, &gt; 1, causing training to diverge. However, it is worth pointing out that the LARS optimizer provided in the popular NVIDIA APEX package wraps around the optimizer, and applies learning-rate adaptation by directly scaling the gradient before the optimization step. Therefore, using the NVIDIA APEX implementation of LARS with PyTorch momentum SGD, without accounting for the subtle implementation differences of PyTorch momentum, produces an odd hybrid of equations (2) and (3). In our experiments, we use the original version of classical momentum with a constant momentum parameter (i.e., equation <ref type="bibr" target="#b1">(2)</ref>, which is different from the PyTorch implementation), and observe a non-trivial improvement in performance, especially when coupled with LARS adaptation.</p><p>Multi-Crop. <ref type="figure" target="#fig_3">Figure 2</ref> illustrates the PAWS method when generating two views of each unlabeled image, however, as mentioned in Section 5, we use the multi-crop data-augmentation of SwAV <ref type="bibr" target="#b2">[3]</ref> to generate more than two views of each image in all of our experiments. Given an unlabeled image, we generate several views of that image by taking two large crops (224 × 224) and six small crops (96 × 96). We use the RandomResizedCrop method from the torchvision.transforms module in PyTorch. The two large-crops (global views) are generated with scale (0.14, 1.0), and the six small-crops (local views) are generated with scale (0.05, 0.14), following the original implementation in <ref type="bibr" target="#b2">[3]</ref>.</p><p>When computing the PAWS loss, each small crop has two positive views (the two global views), and each large crop has one positive view (the other global view). Specifically, let x ∈ R n×(3×H×W ) denote a mini-batch of n unlabeled images. For each image x i in the mini-batch, we generate two large crop views, x <ref type="bibr" target="#b0">(1)</ref> i , x (2) i ∈ R 3×224×224 , and six small crop views, i ) denote the average of the sharpened predictions (recall ρ(·) is the sharpening function defined in Section 3). The overall PAWS objective to be minimized is</p><formula xml:id="formula_8">x (3) i , . . . , x (8) i ∈ R 3×96×96 . Let z (1) i , . . . , z (8) i ∈ R d denote the representations computed from x (1) i , . . . , x<label>(8)</label></formula><formula xml:id="formula_9">1 8n n i=1 H(ρ(p (1) i ), p (2) i ) + H(ρ(p (2) i ), p (1) i ) + 8 k=3 H ρ(p (1) i ) + ρ(p (2) i ) 2 , p (k) i − H(p).<label>(4)</label></formula><p>While the multi-crop augmentation makes the notation in equation (4) a little cumbersome, note that this objective is nearly identical to the objective in equation <ref type="formula" target="#formula_4">(1)</ref>, except that (4) also includes a sum over the small crop-views, 8 k=3 (· · · ). The Multi-Crop augmentation is in fact an essential component of the PAWS algorithm. As will be shown in Appendix C, the multi-crop augmentation strategy in PAWS is not only important when training on large internet images, containing possibly obfuscated objects at various scales, such as ImageNet <ref type="bibr" target="#b38">[39]</ref>, but is also important for small object-centric images, such as CI-FAR10 <ref type="bibr" target="#b40">[41]</ref>. This observation suggests that the benefit of "local-to-global" matching induced by the multi-crop augmentation strategy in PAWS goes beyond simply inducing obfuscation or scale invariant image representations.    <ref type="figure" target="#fig_7">Figure 6</ref> reports results for the best supervised model found by <ref type="bibr" target="#b1">[2]</ref>, which corresponds to 90 epochs of training with random crop/flip for the ResNet-50, and 90 epochs of training with random crop/flip+color distortion for the wider ResNets. When training with ResNet-50 (2×) and ResNet-50 (4×) architectures, PAWS matches the performance of fully supervised learning. Specifically, PAWS is the first method to, with only 10% of training instances labeled, match fully supervised learning on ImageNet with 100% of training instances labeled, using the same architecture, and without distilling from a larger teacher model. Notably, this result is achieved with only 200 epochs of training. However, as a word of caution, this experiment should only be interpreted as a type of ablation, since the performance of supervised learning models can likely be improved by incorporating additional advanced supervised augmentation strategies like Mixup <ref type="bibr" target="#b41">[42]</ref>, CutMix <ref type="bibr" target="#b42">[43]</ref>, and AutoAugment <ref type="bibr" target="#b43">[44]</ref>, which simultaneously learns a data-augmentation policy during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiments -CIFAR10</head><p>We also evaluate the PAWS pre-training scheme on the CIFAR10 <ref type="bibr" target="#b40">[41]</ref> dataset using a single NVIDIA V100 GPU. We first pre-train a network using PAWS on CIFAR10 with access to 4000 labels, and then report the nearest-neighbour classification accuracy on the test set using the 4000 labeled training images as support. On CIFAR10 we only report PAWS-NN, and do not fine-tune a linear classifier on top of the network. For details on the Nearest Neighbours classifier, see Appendix A. Implementation details. We adopt similar hyper-parameters to the ImageNet experiments. Specifically, for pre-training, we use the LARS optimizer with a momentum value of 0.9, weight decay 10 −6 , cosine-similarity temperature of τ = 0.1, and target sharpening temperature of T = 0.25. To construct the different image views, we use the multi-crop strategy, generating two large crops <ref type="bibr">(32 × 32)</ref>, and six small crops (18 × 18) of each unlabeled image. We use the RandomResizedCrop method from the torchvision.transforms module in PyTorch. The two large-crops (global views) are generated with scale (0.75, 1.0), and the six small-crops (local views) are generated with scale (0.3, 0.75). We use a batch-size of 256 and linearly warm-up the learning rate from 0.8 to 3.2 during the first 10 epochs of pre-training, and decay it following a cosine schedule thereafter. To construct the support mini-batch at each iteration, we also randomly sample 640 images, comprising 10 classes and 64 images per class, from the labeled set, and apply label smoothing with a smoothing factor of 0.1. For all sampled images (both unlabeled images and support images) we apply the basic set of SimCLR data-augmentations, specifically, random crop, horizontal flip, and color distortion (but no Gaussian blur). However, in contrast to the ImageNet setup, we also generate two views of each sampled support image. On CIFAR10 we find it much easier for the network to learn to classify the images than to perform instance discrimination. Generating two views of each sampled support image helps the network improve its instance discrimination ability and produce representations that are invariant to the data-augmentations used for training.</p><p>The encoder f θ in our experiments is a WideResNet-28-2 <ref type="bibr" target="#b44">[45]</ref> trunk without dropout, containing a 3-layer MLP projection head, consisting of three fully-connected layers of dimension 128, and Batch Normalization applied to the hidden layers. To simplify the implementation, we do not include a prediction head after the projection head. As shown in <ref type="table" target="#tab_6">Table 5</ref> on ImageNet, PAWS pre-training works well without a prediction head, and we find this to be true on CIFAR10 as well.</p><p>Following pre-training, we freeze the batch-norm layers, and fine-tune the trunk of the network for 180 optimization steps on the available labeled samples using the supervised contrastive loss of <ref type="bibr" target="#b37">[38]</ref>, and do not apply any data-augmentations during this phase. The point of these few optimization steps is to tighten the representation clusters of the labelled training samples before using them as support to classify the test images. For this phase, we use momentum SGD with a batch-size of 640 (comprising 64 images from 10 classes), and sample the mini-batches with replacement; i.e., while images in the same mini-batch in a given iteration are always unique, some of the images may be re-sampled in the subsequent iteration's mini-batch. We use a cosinetemperature of τ = 0.1, momentum parameter 0.9, a learning rate of 0.1 with cosine-decay, and no weight-decay.  <ref type="figure">Figure 7</ref>: *For label propagation methods, the number of epochs is counted with respect to the unsupervised mini-batches. *For Meta Pseudo-Labels (MPL), the number of epochs only includes the student-network updates, and does not count the additional 1,000,000 teacher-network updates (computationally equivalent to roughly an additional 2564 epochs) that must happen sequentially (not in parallel) with the student updates. PAWS-NN refers to performing nearest-neighbour classification directly using the PAWS-pretrained representations, with the 4000 labeled training samples as support. We report the mean top-1 accuracy and standard deviation across 5 seeds for the 4000 label split.</p><p>Results. <ref type="table">Table 7a</ref> compares PAWS-NN to other semi-supervised learning methods trained using identical networks (WideResNet-28-2) on CIFAR10 with access to 4000 labels. Although the intention here is to simply validate PAWS on another dataset, the observations are similar to ImageNet. By using the pre-trained representations directly in a nearest neighbour classifier, PAWS can match the state-of-the-art on CIFAR10 with significantly less training. It is possible that carefully fine-tuning a linear classifier on top of the trunk and incorporating more advanced data-augmentations would further improve performance. <ref type="table">Table 7b</ref> compares PAWS-NN to the self-supervised SimCLRv2 <ref type="bibr" target="#b0">[1]</ref> method trained (and fine-tuned) with larger architectures. The PAWS method achieves superior performance in fewer pre-training epochs, using a residual network containing over 60× fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Alternative Strategies for Non-Collapse</head><p>Proposition 1 provides a theoretical guarantee that the proposed method is immune to the trivial collapse of representations. The underlying principle is that collapsing representations result in high entropy predictions under the non-parametric similarity classifier, but the targets are always low-entropy (because we sharpen them), and so collapsing all representations to a single vector is not a stationary point of the training dynamics. In this section we demonstrate two simple alternative strategies to guarantee non-collapse of representations without making the target-sharpening assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Semi-Supervised Prediction</head><p>If an image in the sampled mini-batch of image views has a class label, then we can directly use that class label as the target for its prediction p, rather than using the positive view prediction, p + , as the target. Under such a scenario, Proposition 2 provides the theoretical guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 3 (Semi-Supervised Image Views). Each mini-batch of image views contains at least one labeled sample.</head><p>Proposition 2 (Non-Collapsing Representations -Semi-Supervised). Suppose Assumptions 1 and 3 hold. If the representations collapse, i.e., z = z i for all z i ∈ S, then ∇H(p + , p) &gt; 0, and the solution is non-stationary.</p><p>Proof. The proof is identical to that of Proposition 1, up to the last step. At which point, letting z correspond to the labeled instance in the mini-batch of images views, we have that the target p + is not equal to the uniform distribution because it corresponds to the corresponding ground truth class label. From which it follows that p = p + and ∇H(p + , p) &gt; 0.</p><p>Note that Proposition 2 is only presented as a theoretical alternative strategy to prevent collapse, but is not used in our experiments; instead, we always use the sharpened positive view prediction p + as the target for the anchor view prediction p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Entropy Minimization</head><p>A third possible strategy to guarantee non-collapsing representations without using the target sharpening assumption is to add an entropy minimization term <ref type="bibr" target="#b11">[12]</ref> to the loss. As shown in the proofs for Propositions 1 and 2, collapsing representations always result in high-entropy predictions p. These high-entropy predictions result in large non-zero gradients due to the entropy minimization term (which as the name implies is minimized when the entropy is low), and so, just as before, collapsing representations are not stationary points of the training dynamics. While adding an entropy minimization term to the loss is a conceptually simple strategy, target sharpening is arguably even simpler, and, by Proposition 1, suffices to guarantee non-collapsing representations, so we do not use entropy minimization in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ethical Considerations</head><p>Increasing model and dataset sizes is a proven approach to improving the performance of image recognition models. Depending on the intended application, more accurate image recognition models may yield substantial social benefits for society; e.g., improving the quality and safety of systems relying on image recognition. However, as with any engineering problem, there is no free lunch, and one must not stop grappling with the ethical concerns of more computationally expensive training pipelines, such as potentially larger environmental footprints (depending on the compute cluster used for training) and exclusionary ramifications. Computationally intensive training pipelines may exclude participation from researchers without access to the computational resources needed to conducted such experiments, which in-turn may lead to slower progress in the field.</p><p>The proposed method in this work matches the current state-of-the-art in data-efficient image recognition using considerably smaller models and fewer training epochs. While our method still benefits from wider and deeper architectures, we demonstrate that the performance of smaller models is not yet saturated, and that research targeting improvements on these smaller models may very well translate to larger-scale settings.</p><p>However, generally speaking, we caution against conflating increased computational effort with larger models, since we observe that this relationship is not always linear. For example, when training a ResNet-50 (2×) for 12 hours (100 epochs) on 64 V100 GPUs, we obtain 68% top-1 accuracy in the 1% label setting and 77% in the 10% label setting. Conversely, when training a smaller ResNet-50 for 17 hours (200 epochs) on 64 V100 GPUs, we obtain 66% top-1 accuracy in the 1% label setting and 75% in the 10% label setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Historical Perspective</head><p>Constructivist learning theory-developed a near half-century ago by Jean Piaget and built on notions of schemata put forth by Immannuel Kant-has (surprisingly) withstood the test of time. Constructivism not only revolutionized school curricula in the 20 th century, but remains to this day a crucial element of many teaching philosophies-placing greater emphasis on spontaneous learning through self-regulation and concrete activities, often under the pseudonym of Project-Based Learning in primary and secondary schools, and Lab-Based Instruction in post-secondary institutions. At the heart of Constructivism is the idea that every individual possesses mental schemata-representations relating to distinct semantic concepts-and that learning occurs through the process of assimilation and accommodation. <ref type="bibr" target="#b4">5</ref> During assimilation, the mind adapts its representation of new experiences to fit its existing schemata, while during accommodation, the existing schemata are updated to make sense of new experiences. In short, Constructivism purports that knowledge is "constructed" through self-guided exploration, and that mental representations of semantic concepts in sensorimotor observations are learned by conforming new observations to past experiences and vice versa.</p><p>It is of particular interest to us to note that one of Piaget's tenets was that sensorimotor development came about the process of optimizing a non-purposive mental objective using assimilation and accommodation. Non-purposive learning generally refers to the process of learning without working towards any particular purpose or goal. As such, non-purposive learning is generally concerned with deriving mental models, or schemata, of sensorimotor observations, under which all new observations can be readily explained in terms of past observations. Clearly, non-purposive learning is closely related to the idea embodied nowadays by task-agnostic self-supervised pre-training, but differs slightly. Whereas current task-agnostic self-supervised learning approaches predict inputs from inputs in a fully unsupervised manner, non-purposive learning approaches do not preclude the use of semantic information. To the contrary, semantic information can be used to aid in the construction of sensorimotor schemata; i.e., non-purposive learning can be unsupervised, semi-supervised, weakly-supervised, or fully supervised. This paper proposes a non-purposive method for semi-supervised learning.</p><p>Criticisms of Constructivist Learning Theory. Despite the widespread success of Constructivisim, one of the weaknesses of Piagetian theory is its lack of specificity in describing the mechanisms by which assimilation and accommodation occur to produce mental representations of semantic concepts in sensorimotor observations <ref type="bibr" target="#b48">[49]</ref>. It is perhaps for this reason that Piaget was especially interested in the emerging field of cybernetics (a precursor to artificial intelligence developed in the 40's by Norbert Wiener) and has gone so far as to say that "Life is essentially auto-regulation," and "cybernetic models are, so far, the only ones throwing any light on the nature of auto-regulatory mechanisms" <ref type="bibr" target="#b49">[50]</ref>. Piaget advocated for cybernetic models with great aplomb, "I wish to urge that we make an attempt to use it" <ref type="bibr" target="#b50">[51]</ref>, and may have attempted to use them himself had it not been for his advanced age. Unfortunately, despite the clear links to cybernetics, the connection to Constructivism did not readily carry over to artificial intelligence (AI) in the 70's due to the largely symbolic nature of AI approaches at the time; e.g., it was not obvious how to represent the near infinite variations of a hand-drawn curve in a single concise representation (i.e., a schema); an issue which is now largely resolved by gradient-based learning and modern neural network architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Training a ResNet-50 on ImageNet when only 10% of the training set is labeled. The figure shows top-1 validation accuracy as a function of the number of training epochs. The proposed method, PAWS, achieves higher accuracy than previous work while requiring significantly fewer training epochs. Concretely, 100 epochs of PAWS training takes less than 8.5 hours using 64 NVIDIA V100-16G GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>PAWS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Training a ResNet-50 on ImageNet when only 1% of the training set is labeled. The figure shows top-1 validation accuracy as a function of the number of training epochs. The proposed method, PAWS, achieves higher accuracy than previous work while requiring significantly fewer training epochs. Concretely, 100 epochs of PAWS training takes less than 8.5 hours using 64 NVIDIA V100-16G GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Comparing ResNet architectures trained using PAWS on ImageNet, with only a small fraction of the training instances labeled, to the same ResNet architectures trained in a fully supervised manner on ImageNet with all training instances labeled. Supervised models are reported from SimCLR [2, Appendix B.3], and ablated over the same data-augmentations used to train PAWS. We report results for the best supervised model found by<ref type="bibr" target="#b1">[2]</ref> over the data-augmentation sweep. When training with ResNet-50 (2×) and ResNet-50 (4×) architectures, PAWS matches the performance of fully supervised learning. Specifically, PAWS is the first method to, with only 10% of training instances labeled, match fully supervised learning on ImageNet with 100% of training instances labeled, using the same architecture, and without distilling from a larger teacher model. Notably, this result is achieved with only 200 epochs of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6</head><label>6</label><figDesc>compares PAWS semi-supervised training to supervised learning with the same architecture using a standard crossentropy loss. The supervised baseline is trained on the full set of ImageNet labels, whereas the PAWS result is obtained by pre-training (and fine-tuning) with access to only a small fraction of the ImageNet labels. The supervised models are reported from SimCLR [2, Appendix B.3], where they are swept over the number of training epochs {90, 500, 1000}, and ablated over the data-augmentations used in PAWS pre-training {crop/flip, crop/flip+color distortion, crop/flip+color distortion+Gaussian blur}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for a ResNet-50 encoder network and in Figures 1,3. PAWS outperforms all other self-supervised representation learning approaches while using roughly 10× fewer pre-training epochs. Specifically, with just 100 epochs of pre-training, PAWS surpasses the state-of-the-art in selfsupervised representation learning. With 200 epochs of pretraining, PAWS further improves upon this result and achieves 75% top-1 accuracy in the 10% label setting and 66% top-1 in the 1% label setting, setting a new state-of-the-art for a ResNet-50.Using the pre-trained representations directly in a nearest-neighbour classifier (PAWS-NN) also performs surprisingly well-surpassing all other self-supervised representation learning methods-although fine-tuning increases top-1 accuracy by 1-3%. Because PAWS with fine-tuning consistently achieves superior results compared to PAWS-NN, we only report results for PAWS for the remainder of the paper.</figDesc><table><row><cell></cell><cell cols="2">Additional ResNet Architectures</cell></row><row><cell></cell><cell></cell><cell>Top 1</cell></row><row><cell>Method</cell><cell>Architecture</cell><cell>Epochs 1% 10%</cell></row><row><cell>BYOL [4]</cell><cell>ResNet-50 (2×)</cell><cell>1000 62.2 73.5</cell></row><row><cell cols="2">SimCLRv2 [1] ResNet-50 (2×)</cell><cell>800 66.3 73.9</cell></row><row><cell>PAWS</cell><cell>ResNet-50 (2×)</cell><cell>100 68.2 77.0</cell></row><row><cell>PAWS</cell><cell>ResNet-50 (2×)</cell><cell>200 69.6 77.8</cell></row><row><cell>SimCLR [2]</cell><cell>ResNet-50 (4×)</cell><cell>1000 63.0 74.4</cell></row><row><cell>BYOL [4]</cell><cell>ResNet-50 (4×)</cell><cell>1000 69.1 75.7</cell></row><row><cell>PAWS</cell><cell>ResNet-50 (4×)</cell><cell>100 69.8 78.5</cell></row><row><cell>PAWS</cell><cell>ResNet-50 (4×)</cell><cell>200 69.9 79.0</cell></row></table><note>By reducing the number of pre-training epochs, PAWS can obtain significant computational savings compared to other approaches. We illustrate this observation by compar- ing PAWS training time on 64 NVIDIA V100-16G GPUs to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semi-supervised classification results on ImageNet when training with larger ResNet architectures.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>(ImageNet, ResNet-50, 1% labels)</cell></row><row><cell></cell><cell>66</cell><cell></cell><cell>PAWS</cell></row><row><cell></cell><cell>64</cell><cell></cell><cell></cell></row><row><cell>Top 1 (%)</cell><cell>58 60 62</cell><cell></cell><cell>SimCLRv2</cell><cell>(+Self.Dist.) SimCLRv2</cell></row><row><cell></cell><cell>56</cell><cell></cell><cell></cell></row><row><cell></cell><cell>52 54</cell><cell>0</cell><cell cols="2">200 400 600 800 1,000 1,200 SwAV BYOL</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pre-training Epochs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Longer Training. Examining the impact of longer training for var- ious ResNet architectures on ImageNet. In both 1% and 10% label settings, and across both ResNet-50 and ResNet-50 (2×) architectures, training for more than 200 epochs is generally not necessary and only yields marginal improvements.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Prediction Head. Examining the effect of the prediction-head when training a ResNet-50 on ImageNet and 10% of the training set is labeled.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Specifically, given a vector a ∈ R K , the softmax στ (a) ∈ [0, 1] K is defined as [στ (a)] k := exp(a k /τ ) K j=1 exp(a j /τ ) for k = 1, . . . , K.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This connection did not readily carry-over to Artificial Intelligence (AI) in the 70's due to the largely symbolic nature of AI approaches at the time; e.g., it was not obvious how to represent the near infinite variations of a hand-drawn curve in a single concise representation; an issue which is now largely resolved by gradient-based learning and modern neural network architectures.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The target for the small crops is the average of the large crop predictions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The term schema may be familiar to researchers working with relational database systems, where it has become standard jargon referring to the logical structure of a database (in close relation to its original meaning in psychology).</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Implementation Details</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Meta pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Entropy regularization</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">S4l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive patternrecognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatically generating extraction patterns from untagged text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04080</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2007. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="412" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A convex relaxation for weakly supervised classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6413</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cognitive development in children: Piaget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piaget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of research in science teaching</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Viking Adult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Piaget</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Recovering petaflops in contrastive semi-supervised learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10803</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Semi-supervised learning by label gradient alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02336</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Enaet: Self-trained ensemble autoencoding transformations for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09265</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Artificial intelligence and piagetian theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Boden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Biology and knowledge: An essay on the relations between organic regulations and cognitive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piaget</surname></persName>
		</author>
		<idno>1971. 16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Reply to individual and collective problems in the study of thinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bruner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
