<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attribute Mix: Semantic Data Augmentation for Fine-grained Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attribute Mix: Semantic Data Augmentation for Fine-grained Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fine-grained Recognition</term>
					<term>Attribute Augmentation</term>
					<term>Semi- supervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collecting fine-grained labels usually requires expert-level domain knowledge and is prohibitive to scale up. In this paper, we propose Attribute Mix, a data augmentation strategy at attribute level to expand the fine-grained samples. The principle lies in that attribute features are shared among fine-grained sub-categories, and can be seamlessly transferred among images. Toward this goal, we propose an automatic attribute mining approach to discover attributes that belong to the same super-category, and Attribute Mix is operated by mixing semantically meaningful attribute features from two images. Attribute Mix is a simple but effective data augmentation strategy that can significantly improve the recognition performance without increasing the inference budgets. Furthermore, since attributes can be shared among images from the same super-category, we further enrich the training samples with attribute level labels using images from the generic domain. Experiments on widely used fine-grained benchmarks demonstrate the effectiveness of our proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained recognition aims at discriminating sub-categories that belong to the same general category, i.e., recognizing different kinds of birds <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b1">2]</ref>, dogs <ref type="bibr" target="#b17">[18]</ref>, and cars <ref type="bibr" target="#b20">[21]</ref> etc.. Different from general category recognition, fine-grained subcategories often share the same parts (e.g., all birds should have wings, legs, heads, etc.), and usually can only be distinguished by the subtle differences in texture and color properties of these parts (e.g., only the breast color counts when discriminating some similar birds). Although the advances of Convolutional Neural Networks (CNNs) have fueled remarkable progress for general image recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>, fine-grained recognition still remains to be challenging where discriminative details are too subtle to discern.</p><p>Existing deep learning based fine-grained recognition approaches usually focus on developing better models for part localization and representation. Typical strategies include: 1) part based methods that first localize parts, crop This work was done when the first author was an intern at Huawei Noah's Ark Lab. and amplify the attended parts, and concatenate part features for recognition <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b40">41]</ref>; 2) attention based methods that use visual attention mechanism to find the most discriminative regions of the fine-grained images <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b43">44]</ref>; 3) feature based methods such as bilinear pooling <ref type="bibr" target="#b23">[24]</ref> or trilinear pooling <ref type="bibr" target="#b45">[46]</ref> for better representation. However, we argue that for fine-grained recognition, the most critical challenge arises from the limited training samples, since collecting labels for fine-grained samples often requires expert-level domain knowledge, which is difficult to extend to large scale. As a result, existing deep models are easy to overfit to the small scale training data, and this is especially true when deploying with more complex modules. In this paper, we promote fine-grained recognition via enriching the training samples at low cost, and propose a simple but effective data augmentation method to alleviate the overfitting and improve model generalization. Our method, termed as Attribute Mix, aims at enlarging the training data substantially via mixing semantically meaningful attribute features from two images. The motivation is that attribute level features are the key success factors to discriminate different sub-categories, and are transferable since all sub-categories share the same attributes. Toward this goal, we propose an automatic attribute learning approach to discover attribute features. This is achieved by training a multi-hot attribute level deep classification network through iteratively masking out the most discriminative parts, hoping that the network can focus on diverse parts of an object. The new generated images, accompanied with attribute labels mixed proportionally to the extent that two attributes fuse, can greatly enrich the training samples while maintaining the discriminative semantic meanings, and thus greatly alleviate the overfitting issue and are beneficial to improve model generalization.</p><p>Attribute Mix shares similarity with MixUp <ref type="bibr" target="#b37">[38]</ref> and CutMix <ref type="bibr" target="#b36">[37]</ref>, which all mix two samples by interpolating both images and labels. The difference is that both MixUp and CutMix randomly mix images or patches from two images, without considering their semantic meanings. As a result, it is usual for these two mixing operations to fuse images with non-discriminative regions, which in turn introduces noise and makes the model unstable for training. An example is shown in <ref type="figure" target="#fig_0">Fig.1</ref>. In comparison, Attribute Mix intentionally fuses two images at the attribute level, which results in more semantically meaningful images, and is helpful for improving model generalization.</p><p>Benefiting from the discovered attributes, we are able to introduce more training samples at attribute level with only generic labels. Here we denote the generic labels as single bit supervision indicating whether an object is from a general category, e.g., whether an object is a bird or not. We claim that the attribute features can be seamlessly transferred from the generic domain to the fine-grained domain without knowing the object's fine-grained sub-category labels. This is achieved by a standard semi-supervised learning strategy that mines samples at the attribute levels. The mined samples, intrinsically with mixed attribute labels, and we term this proposed method which mines attribute from the general domain as Attribute Mix+. Note that although fine-grained labels are difficult to obtain, it is much easier to obtain a general label of an object. In this way, we are able to conveniently scale the fine-grained training samples via mining attributes from the generic domain for better performance investigation.</p><p>Our proposed data augmentation strategy is a general framework at low cost, and can be combined with most state-of-the-art fine-grained recognition methods to further improve the performance. Experiments conducted on several widely used fine-grained benchmarks have demonstrated the effectiveness of our proposed method. We hope that our research on attribute-based data augmentation could offer useful guidelines for fine-grained recognition.</p><p>To sum up, this paper makes the following contributions:</p><p>• We propose Attribute Mix, a data augmentation strategy to alleviate the overfitting for fine-grained recognition.</p><p>• We propose Attribute Mix+, which mines fine-grained samples at attribute level, and does not need to know the specific sub-category labels of the mined samples. Attribute Mix+ is able to scale up the fine-grained training for better performance investigation conveniently.</p><p>• We evaluate our methods on three challenging datasets (CUB-200-2011, FGVC-Aircraft, Standford Cars), and achieve superior performance over the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We briefly review some works for fine-grained recognition, as well as some recent technologies in data augmentation, which are most related with our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fine-grained Recognition</head><p>Fine-grained recognition has been studied for several years. Early works on finegrained recognition focus on leveraging extra parts and bounding box annotations to localize the discriminative regions of an object <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23]</ref>. Later, some weakly supervised localization methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref> are proposed to localize objects with only image level annotations. In <ref type="bibr" target="#b46">[47]</ref>, Zhou et al. proposed to localize the objects by picking out the class-specific feature maps. In <ref type="bibr" target="#b41">[42]</ref>, Zhang et al. proposed to use adversarial training to locate the integral object and achieved superior localization results.</p><p>On the other hand, powerful features have been provided by better CNN networks. Lin et al. <ref type="bibr" target="#b23">[24]</ref> proposed a bilinear structure to compute the pairwise feature interactions by two independent CNNs. And it turns out that higherorder features interaction can make the features highly discriminative <ref type="bibr" target="#b7">[8]</ref>. To model the subtle differences between two fine-grained sub-categories, attention mechanism <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13]</ref> and metric learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b6">7]</ref> are often used. Besides, Zhang et al. <ref type="bibr" target="#b42">[43]</ref> proposed to unify CNN with spatially weighted representation by Fisher Vectors, which achieves high performances on CUB-200-2011. Although promising performance has been achieved, these methods are all at the expense of higher computational cost, and are prohibitive to deploy on low-end devices.</p><p>Few works rely on external data to help facilitate recognition. Cui et al. <ref type="bibr" target="#b5">[6]</ref> proposed to use Earth Mover's Distance to estimate the domain similarity, and transfer the knowledge from the source domain which is similar to the target domain. In <ref type="bibr" target="#b19">[20]</ref>, Krause et al. collected millions of images with tags from Web and utilized the Web data by transfer learning. However, both methods make use of a large amount of class-specific labels for transfer learning. In this paper, we demonstrate that for fine-grained recognition, transferring attribute features is a powerful proxy to improve the recognition accuracy at low cost, without knowing the class-specific labels of the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>Data augmentation can greatly alleviate overfitting in training deep networks. Simple transformations such as horizontal flipping, color space augmentations, and random cropping are widely used in recognition tasks to improve generalization. Recently, automatic data augmentation techniques, e.g., AutoAugment <ref type="bibr" target="#b4">[5]</ref>, are proposed to search for a better augmentation strategy among a large pool of candidates. Differently, Mixup <ref type="bibr" target="#b37">[38]</ref> combined two samples linearly in pixel level, where the target of the synthetic image was the linear combination of one-hot label. Though not meaningful for human perception, Mixup has been demonstrated surprisingly effective for the recognition task. Following Mixup, there are a few variants <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15]</ref> as well as a recent effort named Cutmix <ref type="bibr" target="#b36">[37]</ref>, which combined Mixup and Cutout <ref type="bibr" target="#b8">[9]</ref> by cutting and pasting patches. However, all these methods would inevitably introduce unreasonable noise due to augmentation operations on random patches of an image, without considering their semantic meanings. Our method is similar to these methods in mixing two samples for data augmentation. Differently, the mixing operation is only performed around those semantic meaningful regions, which enables the model for more stable training and is beneficial for generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we describe our proposed attribute-based data augmentation strategy in detail. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the core ingredients of the proposed method consist of three modules: 1) Automatic attribute mining, which aims at discovering attributes with only image level labels. 2) Attribute Mix data augmentation, which mixes attribute features from any two images for new image generation. 3) Attribute Mix+, which enriches training samples via mining images from the same generic domain at attribute level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attribute Mining</head><p>The attribute level features are the core of the following data augmentation operation. We first elaborate how to obtain attribute level features with only image level labels. Denote {x, y}, y ∈ {0, 1} C as a training image and its corresponding one-hot label with y c = 1, where C is the number of fine-grained sub-categories. Without loss of generality, assuming that all fine-grained sub-categories share k attributes, we simply convert the C class level labels to more detailed, k × C attribute level labels for attribute discovery, as shown in <ref type="figure">Fig. 3</ref>. Specifically, the one-hot label y of image x is extended to multi-hot label y A , while C i=1 y i = 1 and kC i=1 y A i = k, with each none zero hot regarded as one attribute corresponding to a specific sub-category. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>  <ref type="figure">Fig. 3</ref>. All fine-grained sub-categories from the same generic domain often share the same parts. The one-hot label is extended to multi-hot label from C dimension to k * C dimension, where each sub-category is endowed with k attributes. For simplicity, we order the attributes belonging to the same sub-category at adjacent locations.</p><p>simply remove all the fully connected layers, and add a 1 × 1 convolutional layer to produce feature maps f ∈ R h×w×kC with kC channels, here every adjacent k channels correspond to k attributes for a certain sub-category. These feature maps are fed into a GAP (Global Average Pooling) layer to aggregate attribute level features for classification. The multiple attribute mining is proceeded as follows:</p><p>1 Following the above procedures, we are able to train an attribute level classification network automatically and obtain a series of masks M c , which correspond to different attributes of an object. <ref type="figure">Fig. 4</ref> shows an example of what the Attribute Mining procedure learns. It can be shown that these attributes coarsely correspond to different parts of an object, e.g., the birds' heads, wings, and tails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attribute Mix</head><p>After obtaining the attribute regions, we introduce a simple data augmentation strategy on attribute level to facilitate fine-grained training. The proposed </p><formula xml:id="formula_0">x = (1 − M b,k ) x a + λ M b,k x a + (1 − λ) M b,k x b y = λy A a + (1 − λ)y A b ,<label>(1)</label></formula><p>where M b,k is the transformed binary mask from x b to x a , with the mask center-aligned with M a,k , and denotes the region that needs to be mixed. 1 is a binary mask filled with ones, and is the element-wise multiplication operation. Like Mixup <ref type="bibr" target="#b37">[38]</ref>, the combination ratio λ between two regions is sampled from the beta distribution Beta(α, α). In all our experiments, we set α = 1, meaning that λ is sampled from the uniform distribution (0, 1). An illustration of the Attribute Mix operation is shown in <ref type="figure" target="#fig_0">Fig.1</ref> , as well as some comparison results that generated by Mixup and CutMix operations. Compared with Mixup and CutMix which inevitably introduce some meaningless samples (e.g. random patches, background noise), Attribute Mix only focuses on the foreground attribute regions and is more suitable for fine-grained categorization.</p><p>Though more semantically meaningful our Attribute Mix operation is, the generated virtual samples still suffer large domain gap with the original, natural images. As a result, memorizing these samples would deteriorates the model generalization. To address this issue, we introduce a time decay learning strategy to limit the probability of applying Attribute Mix operation. This is achieved by controlling the mixing ratio λ in Eq. (1), i.e., we introduce a variable γ(t), which increases from 0 to 1 as the training proceeds, and limit γ(t) ≤ λ ≤ 1. In this way, λ is sampled from Beta(α, α) distribution, and only when λ is larger than γ(t), the generated samples are used for training. As the training process goes on, the mixed operations between two images decay, and finally degenerate to using the original images. In the experimental section, we will validate its effectiveness in improving model generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attribute Mix+</head><p>In principle, the attribute features are shared among images from the same super-category, regardless of their specific sub-category labels. This conveniently enables us to expand the training samples at attribute level to images from the generic domain. In this section, we enrich the training samples in another view, i.e., transfer attributes from images with only generic labels. This is achieved by generating soft, attribute level labels via a standard semi-supervised learning strategy over a large amount of images with low cost, general labels. Since attributes are shared among all the sub-categories, it is not necessary for images from the generic domain that belong to the same sub-categories with the target domain.</p><p>Using the model trained with Attribute Mix strategy, we conduct inference over the images from the generic domain, and produce attribute level probabilities by using a softmax layer. Specifically, denoting the model output as z ∈ R kC , we reshape the output to z ∈ R k×C , where each row corresponds to the same attribute among different sub-categories. The softmax operation is conducted over the row dimension to obtain the probability p ∈ R k×C :</p><formula xml:id="formula_1">p i,j = exp( z i,j /T )) C c=1 exp( z i,c /T )) ,<label>(2)</label></formula><p>where T is a temperature parameter that controls the smooth degree of the probability.</p><p>Entropy ranking. In many semi-supervised learning methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>, it is a common underlying assumption that classifiers' decision boundary should not pass through high-density regions of the marginal data distribution. Similarly, in our experiment, the collected data with generic labels inevitably contain noise that would probably hurt the performance, and some of them are with too smooth soft-labels that can not provide any valuable attribute information. To address this issue, we propose an entropy ranking strategy to select images adaptively. Specifically, given an image x g from the generic domain with soft attribute label p, its entropy is calculated as follows:</p><formula xml:id="formula_2">H(x g ) = − k i=1 C c=1 p i,c log 2 p i,c ,<label>(3)</label></formula><p>where p i,c denotes the probability that image x g contains attribute i of finegrained sub-category c. H(x g ) is large if the attribute distribution is smooth, and reaches its maximum when all attributes obtain the same probability, which we think carry no valuable information for fine-grained training. Based on this property, we set the criteria that samples with entropy H(x g ) that higher than threshold τ to be filtered out. In the ablation study, we will validate its effectiveness for achieving stable results, especially when noisy images exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation Details</head><p>Datasets. The empirical evaluation is performed on three widely used finegrained benchmarks: Caltech-USCD Birds-200-2011 <ref type="bibr" target="#b33">[34]</ref>, Standford Cars <ref type="bibr" target="#b20">[21]</ref>, and FGVC-Aircraft <ref type="bibr" target="#b24">[25]</ref>, which belong to three generic domains: birds, cars and aircraft, respectively. Each dataset is endowed with specific statistic properties, which are crucial for recognition performance. CUB-200-2011 is the most widely used fine-grained dataset, which contains 11,788 images spanning 200 sub-species. Standford Cars consists of 16,185 images with 196 classes, which are produced by different manufacturers, while FGVC-Aircraft consists of 10,000 images with 100 species. In current view, these are all small scale datasets. We Implementation details. In our implementation, all experiments are based on ResNet-50 <ref type="bibr" target="#b15">[16]</ref> backbone. We first construct our baseline model over three datasets for following comparisons. During network training, the input images are randomly cropped (scale s ∼ U (0.15, 1)) to 448 × 448 pixels after being resized to 512 × 512 pixels and randomly flipped. For Standford Cars, color jittering is used for extra data augmentation. We train the models for 80 epochs, using Stochastic Gradient Descent (SGD) with the momentum of 0.9, weight decay of 0.0001. The learning rate is set to 0.001, which decays by a factor of 10 every 30 epochs. During inference, the original image is center cropped and resized to 448 × 448. Benefiting from the powerful features, our baseline models have achieved considerable pleasing performance over these datasets. In the following, we will validate the effectiveness of the proposed method, even over such high baselines.</p><p>For multi-hot attribute level classification, a standard binary cross-entropy loss is used. We increase the training epochs to 300 after introducing Attribute Mixed samples, since it needs more rounds to converge for these augmented data. When mining attributes from the generic domain, we use the model pretrained with Attribute Mix to inference over data from the generic domains to produce soft attribute level labels. During inference, the multi-hot attribute level classification network outputs the predicted scores of k * C attributes, and we simply combine the predicted scores for k attributes of each sub-category. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In this section, we investigate some parameters which are important for recognition performance. Unless otherwise specified, all experiments are conducted on CUB-200-2011. Effects of number of attributes k. Here we inspect the recognition performance w.r.t. the number of attributes k during Attribute Mixing. The performances for different choices of k are shown in <ref type="table">Table 1</ref>. If we set k too small, the model cannot mine adequate attributes of an object and the improvement is marginal. Specifically, k = 1 denotes the baseline without multiple attribute mixing. While for larger k, the model is at the risk of including background clutters, and making the optimization difficult.</p><p>Impact of hyperparameter α. The hyperparameter α in Eq. (1) plays an important role during mixing, which controls the strength of interpolation between attributes from two training samples. Here we try different choices with α ∈ {0.25, 0.5, 1, 2, 4}. The performances of different α are shown in the <ref type="table">Table 2</ref>, and the best performance can be achieved when α is set to 1.</p><p>Effects of adaptive Attribute Mix. We introduce the time decay strategy to alleviate the overfitting over the augmented samples. The probability of applying Attribute Mix decays from 1 to 0 as the cosine curve. The comparison results on CUB-200-2011 are shown in <ref type="table">Table 3</ref>. It shows that Attribute Mix with the time decay learning strategy achieves higher accuracy of 88.4%, which surpasses Attribute Mix without that strategy by 0.6 points.</p><p>Comparison with image level image mining. In Section 3.3, the images from the generic domain are used to mine attribute level features. In order to validate the advantages of attribute level features, we compare with the traditional, semi-supervised learning using only image level labels. Specifically, the image level pseudo labels directly leverage the information from unlabeled data Effects of τ for entropy ranking. When leverage the data from generic domain, we introduce the entropy ranking to select samples that share attributes contributing most for the fine-grained recognition. The entropy ranking mechanism investigates the correlations between the mined images from generic domain and fine-grained domain, and is robust to noisy images that probably do not belong to the same generic domain. In order to inspect the effectiveness of entropy ranking at length, we intentionally introduce some extra images with labels different from the generic labels, and test the performance of our method under such a situation. Specifically, we choose external dataset PASCAL VOC 2007 <ref type="bibr" target="#b11">[12]</ref> and 2012 <ref type="bibr" target="#b10">[11]</ref> as noisy images, which both contain 20 object classes. This is a dataset with multi-label images, and the number of samples that include birds is 1,377, only a small ratio (around 6.4%) over the whole dataset. Overall, this dataset can be treated as adding noisy images (around 16.3%) to the generic domain dataset. We evaluate our method using these noisy data, and the results with different thresholds τ are shown in the T able 5. When the threshold τ is set to 1.5, entropy ranking mechanism can filter out most of the noisy samples in VOC 07 + 12, only 1, 833 samples reserved. We claim that the advantages of ranking mechanism are obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-arts</head><p>We now move on to compare our proposed method with state-of-the-art works on above mentioned fine-grained datasets. In <ref type="table">Table 6</ref>, we show the comparison results on CUB-200-2011, Standford Cars and FGVC-Aircraft. For fair comparison, we choose recent works which use similar backbone with us. MAMC <ref type="bibr" target="#b31">[32]</ref>  introduces complex attention modules to model the subtle visual differences. In <ref type="bibr" target="#b44">[45]</ref>, And bilinear interaction with high computational complexity is used in <ref type="bibr" target="#b44">[45]</ref> to learn fine-grained image representations. S3N <ref type="bibr" target="#b9">[10]</ref> and MGE-CNN <ref type="bibr" target="#b38">[39]</ref> use multiple ResNet-50 as multiple branches, which greatly increases the complexity. As for our method, our proposed Attribute Mix achieves a superior accuracy of 88.4%, 94.9% and 92.0% on CUB-200-2011, Standford Cars and FGVC-Aircraft without any complicated modules.</p><p>Compared with the other data-mixing augmentation methods, Attribute Mix outperforms the Mixup and CutMix at least 2.2 points, which further demonstrates that Attribute Mix is more suitable for fine-grained categorization. In order to validate the generality of our method, we also incorporate Attribute Mix into the advanced methods S3N and MGE-CNN, following all the parameters setting in <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b38">[39]</ref>, and promote the accuracy to 89.2%, 95.2% and 93.4% with S3N, 89.3%, 95.0% and 92.9% with MGE-CNN. It can be seen that Attribute Mix achieves the state-of-the-art performances on all these fine-grained datasets without any complex changes on the baseline model, and it can be also easily combined with the other SOTA methods to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presented a general data augmentation framework for fine-grained recognition. Our proposed method, named Attribute Mix, conducts data augmentation via mixing two images at attribute level, and can greatly improve the performance without increasing the inference budgets. Furthermore, based on the principle that the attribute level features can be seamlessly transferred from the generic domain to the fine-grained domain regardless of their specific labels, we enrich the training samples with attribute level labels using images from the generic domain with low labelling cost, and further boost the performance. Our proposed method is a general framework for data augmentation at low cost, and can be combined with most state-of-the-art fine-grained recognition methods to further improve the performance. Experiments conducted on several widely used fine-grained benchmarks have demonstrated the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:2004.02684v2 [cs.CV] 9 Jul 2020 (a) Image xa (b) Image x b (c) Ma,1 xa (d) M b,1 x b Data augmentation via Mixup, CutMix and Attribute Mix. (a)(b) are samples xa and x b drawn at random from CUB-200-2011. (c)(d) are two mined discriminative attribute regions from xa and x b , respectively. (e)(f) are virtual samples generated by Mixup and CutMix. (g)(h) are two virtual samples generated by Attribute Mix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The framework of the proposed Attribute Mix data augmentation. First, an automatic attribute learning approach is proposed to mine attribute features from a small scale fine-grained samples, implemented by training a multi-hot attribute level deep classification network through iteratively masking out the most discriminative parts. These attributes information can be used to generate new samples by Attribute Mix and shared between images from the same generic domain at attribute level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. Training multi-hot attribute level classification network with original images and attribute level multi-hot labels y A ck:(c+1)k−1 = 1. 2. For an image with label y A c , picking out the corresponding feature map at the kcth channel f (:, :, kc), generating the attention map according to the activations and upsampling to the original image size, thus we obtain the most discriminative region mask M c,1 of an image. 3. M c,1 is used to erase the original image x to get erased image x Mc,1 , and the corresponding multi-hot label, y A ck changes from 1 to 0. 4. Using x Mc,1 as a new training sample, and do the above three steps to obtain masks M c,i , i=2,...,k for all remained attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Fig. 4 .</head><label>34</label><figDesc>Illustration of multiple attribute mining. Example images show the mined attributes when k = 3. These attributes approximately correspond to birds' heads, wings and tails, respectively. Attribute Mix operation constructs synthetic examples by intentionally mixing the corresponding attribute regions from any two images. Specifically, given two samples (x a , y A a ) and (x b , y A b ) with x a , x b ∈ R W ×H×3 , and random picked attribute masks M a,k , M b,k ∈ {0, 1} W ×H , the generated training sample ( x, y) is obtained by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>use the default training/test split for experiments, which gives us around 30 training examples per class for birds, 40 training examples per category for cars, and approximately 66 training examples per category for aircraft.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, for a typical CNN, we</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>inde = 4</cell></row><row><cell cols="2">Fine-grained domain</cell><cell>Attributes</cell><cell>inde = 14</cell></row><row><cell></cell><cell></cell><cell>heads</cell></row><row><cell></cell><cell></cell><cell></cell><cell>American</cell></row><row><cell></cell><cell></cell><cell>legs</cell><cell>Goldenfitch</cell></row><row><cell>Painted</cell><cell>American</cell><cell></cell></row><row><cell>Bunting</cell><cell>Goldfinch</cell><cell>wings</cell></row><row><cell>Lazuli</cell><cell></cell><cell></cell></row><row><cell>Bunting</cell><cell></cell><cell></cell><cell>× 1</cell><cell>(  *  ) × 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Lazuli bunting</cell><cell>One-hot</cell><cell>Multi-hot</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Performances comparison for different α on Attributes Mix Performances comparison for different k on Attributes Mix</figDesc><table><row><cell>Dataset</cell><cell>Hyperparameter α</cell><cell>Acc.</cell></row><row><cell></cell><cell>0.25</cell><cell>87.9</cell></row><row><cell>CUB-200-2011</cell><cell>0.5 1</cell><cell>88.2 88.4</cell></row><row><cell></cell><cell>2</cell><cell>87.8</cell></row><row><cell>Dataset</cell><cell>Number of</cell><cell>Acc.</cell></row><row><cell></cell><cell>attributes k</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>85.3</cell></row><row><cell>CUB-200-2011</cell><cell>2 3</cell><cell>87.9 88.4</cell></row><row><cell></cell><cell>4</cell><cell>88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Impact of adaptive Attribute Mix on CUB-200-2011 top-1 Acc. Comparison between attribute level and image-level sample mining. We use the model's prediction on attribute level and image level separately to generate the soft-labels over those images from the generic domain. As shown inTable 4, our proposed attributes level features achieve much higher accuracy of 89.6%, which surpasses the image level result 88.3% by 1.3 points. It is not a small gain considering the high baseline we used and the difficulty of CUB-200-2011 dataset.</figDesc><table><row><cell>Dataset</cell><cell>Adaptive</cell><cell>Acc.</cell></row><row><cell cols="2">CUB-200-2011</cell><cell>88.4 87.8</cell></row><row><cell>Dataset</cell><cell>Methods</cell><cell>Acc.</cell></row><row><cell>CUB-200-2011</cell><cell>image-level attribute level</cell><cell>88.0 89.2</cell></row><row><cell cols="3">D U L without considering the attribute information. For fair comparison, both</cell></row><row><cell cols="3">methods use ResNet-101 model and the Attribute Mix is not applied during</cell></row><row><cell>training.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Impact of threshold in entropy ranking. * denotes that using ResNet50 as backbone without any changes.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>τ</cell><cell>Acc.</cell></row><row><cell></cell><cell></cell><cell>1.0</cell><cell>89.0</cell></row><row><cell></cell><cell></cell><cell>1.5</cell><cell>89.4</cell></row><row><cell cols="2">CUB-200-2011</cell><cell>2.0</cell><cell>89.3</cell></row><row><cell></cell><cell></cell><cell>2.5</cell><cell>89.1</cell></row><row><cell></cell><cell></cell><cell>3.0</cell><cell>88.8</cell></row><row><cell cols="5">Table 6. Comparisions with state-of-the-art methods on CUB-200-2011, Standford</cell></row><row><cell>Cars and FGVC-Aircrafts</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Backbone CUB Cars Aircrafts</cell></row><row><cell>Baseline</cell><cell>R-50  *</cell><cell></cell><cell>85.3 91.7</cell><cell>88.5</cell></row><row><cell>MAMC [32]</cell><cell>R-50</cell><cell></cell><cell>86.2 92.8</cell><cell>-</cell></row><row><cell>DBTNet [45]</cell><cell>R-50</cell><cell></cell><cell>87.5 94.1</cell><cell>91.2</cell></row><row><cell>S3N [10]</cell><cell>R-50</cell><cell></cell><cell>88.5 94.7</cell><cell>92.8</cell></row><row><cell>MGE-CNN [39]</cell><cell>R-50</cell><cell></cell><cell>88.5 93.9</cell><cell>-</cell></row><row><cell>Mixup [38]</cell><cell>R-50  *</cell><cell></cell><cell>85.9 92.3</cell><cell>89.2</cell></row><row><cell>CutMix [37]</cell><cell>R-50  *</cell><cell></cell><cell>86.2 92.3</cell><cell>88.7</cell></row><row><cell>Attribute Mix</cell><cell>R-50  *</cell><cell></cell><cell>88.4 94.9</cell><cell>92.0</cell></row><row><cell cols="2">Attribute Mix (MGE-CNN) R-50</cell><cell></cell><cell>89.3 95.0</cell><cell>92.9</cell></row><row><cell>Attribute Mix (S3N)</cell><cell>R-50</cell><cell></cell><cell>89.2 95.2</cell><cell>93.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="714" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<title level="m">Bird species categorization using pose normalized deep convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1153" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>voc2007) results</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3714" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two-phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3534" to="3543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<title level="m">Fine-grained visual classification of aircraft</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weaklysupervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05236</idno>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8331" to="8340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fine-grained pose prediction, normalization, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07063</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4279" to="4288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
