<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose estimator and tracker using temporal flow maps for limbs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihye</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Transdisciplinary Studies</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieun</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Ajou University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
							<email>sungheonpark@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Transdisciplinary Studies</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
							<email>nojunk@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Transdisciplinary Studies</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pose estimator and tracker using temporal flow maps for limbs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For human pose estimation in videos, it is significant how to use temporal information between frames. In this paper, we propose temporal flow maps for limbs (TML) and a multi-stride method to estimate and track human poses. The proposed temporal flow maps are unit vectors describing the limbs' movements. We constructed a network to learn both spatial information and temporal information end-to-end. Spatial information such as joint heatmaps and part affinity fields is regressed in the spatial network part, and the TML is regressed in the temporal network part. We also propose a data augmentation method to learn various types of TML better. The proposed multi-stride method expands the data by randomly selecting two frames within a defined range. We demonstrate that the proposed method efficiently estimates and tracks human poses on the PoseTrack 2017 and 2018 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human pose estimation (HPE) is one of the most significant tasks in computer vision. Over the past few years, static imagebased pose estimation for either a single person or multiple people has achieved high accuracy using convolutional neural networks (CNNs). Deeply-structured networks as well as iterative networks have been proposed for this task to take advantage of their large receptive fields and rich representation power.</p><p>In case of multiple people pose estimation, there are two major approaches: top-down and bottom-up approaches. The bottom-up approach <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b9">[10]</ref> detects the body joints of all people at once and then estimates human poses individually. On the other hand, the top-down approach <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref> consists of a human detector that detects human bounding boxes and a single person pose estimator that locates and groups body joints in each bounding box.</p><p>Recently, HPE in videos has grabbed attentions as an extension of HPE in a single image. For HPE in videos, human pose tracking should be performed as well as the pose estimation. Many researches have exploited temporal information in various ways for tracking. Such methods as a bounding box tracking algorithm, optical flow, similarity of estimated shape, temporal flow fields (TFF) and so on have been applied for this task <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Among them, the work of Xiao et al. <ref type="bibr" target="#b14">[15]</ref> is a representative top-down approach. They detected the pose based on the extracted bounding boxes and proposed a box tracking method for pose tracking, which is a combination of a box propagation using optical flow and a flow-based pose similarity. Likewise, Ibrahim et al. <ref type="bibr" target="#b7">[8]</ref> used the similarity of the poses to track the pose while it is a bottom-up approach. In case of <ref type="bibr" target="#b15">[16]</ref>, they proposed an online pose tracking algorithm called pose flow, which is an association of the same person in different frames. They created the optimized pose flow using several scores such as mean score of all keypoints. In summary, these studies proposed tracking methods based on the similarity of estimated poses.</p><p>On the other hand, Andreas et al. <ref type="bibr" target="#b1">[2]</ref> represented an association of poses as temporal vector maps called temporal flow fields (TFF). TFF indicates the flow of a joint between two frames. They estimated poses through the heatmaps and part affinity fields <ref type="bibr" target="#b0">[1]</ref> and used a similarity measure in a bipartite graph matching to track the poses. However, when estimating TFF, using only joint location may not be enough to track the poses. Tracking only a single joint may lead to a lack of representation power or may be vulnerable to occlusion of joints. Therefore, if a limb that connects two joints is tracked, it is expected to provide richer representation for the tracker and to enhance robustness to occlusion. In addition, considering frames of multiple strides rather than only two consecutive frames can further improve the robustness and performance of the network.</p><p>To this end, in this paper, we propose a pose estimator and tracker based on TML which is designed to represent a temporal movement of a person by estimating the direction of limbs' movement. More specifically, we subdivide each limb into several sections equally in each frame. Then, 2D unit vectors that represent the direction of corresponding limb sections between two frames are calculated, which are used to build each limb's temporal maps. A huge amount of data is needed to train the TML because the maps have to learn extensive information. Thus, we develop a multi-stride method as a data augmentation method to learn various types of TML. In other words, we randomly take the two frames within a given time range. <ref type="figure">Figure 1</ref> shows the overall flow of inference in the proposed method. During inference, we process three frames as a frame set at a time. First, we extract poses on each frame in the form of joint heatmaps and part affinity fields at the spatial part. The extracted poses are tracked between the first and <ref type="figure">Fig. 1</ref>. An inference flow of proposed method. A set of frames (F t−1 , Ft, F t+1 ) is defined for temporal inference. Two frames that are (F t−1 , Ft) or (Ft, F t+1 ) are input into the network as a pair. First, poses are estimated using the spatial part on the each frame and TML are extracted by the temporal part at the same time. To track poses, we calculate the association score of each person using the TML and the joint distance score. The optimal connection is found by using a bipartite method. In order to refine the middle of frame Ft, we need to get the associated information between (F t−1 , F t+1 ) through the TML and the joint distance. If the pose is connected between F t−1 and F t+1 , we added the average pose of between (F t−1 , F t+1 ). Note that, the time interval of TML is 1 at inference stage but it can be a greater number at training stage which will be described in the multi-stride method.</p><p>the second frame of the three frames by associated scores obtained from a TML score and a joint distance. The second frame lies in the middle of the consecutive three frames. After the same procedure is applied to the second frame and the third frame, the second frame is refined by analyzing the association scores of the three frames. The frame set is selected at one frame interval. This makes the model stable since the information from the frames back and forth adjust the result of the intermediate frame.</p><p>Thus, the contributions of our work are as follows: 1) We propose the TML to represent directions of limbs' movement.</p><p>2) A multi-stride method is proposed to train various TML as a data augmentation method.</p><p>3) The current poses are refined by associated score of the previous frame and the next frame.</p><p>We evaluated the proposed method on the PoseTrack 2017 and 2018 datasets <ref type="bibr" target="#b16">[17]</ref>. To prove the effectiveness of the proposed method, we made a comparison of our work with state-of-the-art algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single person pose estimation</head><p>Over the past few years, many CNN based methods in single person pose estimation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b25">[26]</ref> used very deep networks. Also, a recursive methodology has been adopted in many competitive methods.</p><p>Newell et al. <ref type="bibr" target="#b20">[21]</ref> proposed a model with multiple hourglass modules that repeats bottom-up and top-down processing and Wei et al. <ref type="bibr" target="#b24">[25]</ref> proposed a convolution version of the pose machine <ref type="bibr" target="#b21">[22]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi person pose estimation</head><p>Multi-person pose estimation <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref> methods can be categorized as top-down and bottom-up approaches. The top-down approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref> firstly detect a person's bounding box and estimate single pose on the extracted bounding box. On the other hand, the bottom-up approaches <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b9">[10]</ref> firstly detect parts of people and then determine poses in the input image by connecting the candidate parts.</p><p>Cao et al. <ref type="bibr" target="#b0">[1]</ref> proposed part affinity fields (PAFs) to associate body parts and determined the pose using the PAFs. Doering et al. <ref type="bibr" target="#b1">[2]</ref> and Zhu et al. <ref type="bibr" target="#b9">[10]</ref> suggested modified version of methods based on this model. DeeperCut <ref type="bibr" target="#b6">[7]</ref> is a graph decomposition method to re-define a variable number of consistent body part configurations. The performances of stateof-the-art multi-person pose estimation methods are pretty good for a single frame. However, to apply the methods on real applications, we need to combine them with tracking algorithms for video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Human pose estimation with tracking</head><p>Several methods have been proposed to estimate and track human poses on videos <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref>. These methods can be divided into two groups depending on whether the learned temporal information is used or not. For the methods that do not use the learned temporal information, they track the pose by applying optical flow, box tracking algorithm, and so on. Xiu et al. <ref type="bibr" target="#b15">[16]</ref> proposed a pose tracker based on a pose flow that is a flow structure indicating the same person in different frames by pose distance. Xiao et al. <ref type="bibr" target="#b14">[15]</ref> tracked the pose to use a flow-based pose tracking algorithm based on box propagation using optical flow and a flow-based pose similarity. Instead of naively connecting the relationships between detected poses, several papers trained sequential information. Radwan et al. <ref type="bibr" target="#b7">[8]</ref> used a bi-directional long-short term memory (LSTM) framework to learn the consistencies of the human body shapes. Doering et al. <ref type="bibr" target="#b1">[2]</ref> proposed temporal flow fields that are vector fields to indicate the direction of joints.</p><p>However, tracking only a single joint may not contain enough temporal information due to lack of representation power and also it may be vulnerable to occlusion of joints. Therefore, in this paper, instead of a single joint point, a limb connecting two adjacent joints is tracked, which is expected to resolve the above mentioned problems. Also, during both training and testing, we consider a pair of frames with more than one time interval rather than only using two consecutive frames for the robustness of the proposed architecture. <ref type="figure">Fig. 2</ref>. The structure of the proposed network. The spatial and the temporal parts are combined together in a single network. On the spatial part, joint heatmaps (H circle) and part affinity fields (A circle) are regressed. Outputs from the spatial part and features from the final layers of the VGG parts are fed into the temporal part. The temporal part regresses the TML (L circle). Pixel-wise L2 losses are used to optimize all the outputs. V t−1 and Vt mean the extracted features from VGG parts at t − 1 and t frame respectively. Sp stage6 is the concatenated spatial features of t − 1 frame and t frame at the stage6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In order to estimate and track human poses using a single network, we constructed a network consisting of two sub-parts (a spatial part and a temporal part) as shown in <ref type="figure">Figure 2</ref>. We used the network presented in <ref type="bibr" target="#b0">[1]</ref> for the spatial part, which has iterative stages. The stage consists of two branches, one for joint heatmaps and the other for part affinity fields. In the proposed network, we take two frames as inputs. Each frame passes through the VGG network to extract features <ref type="bibr" target="#b27">[28]</ref>. The features of each frame are fed into each spatial part of the network in parallel. The features of two input frames and the output of the spatial part's last stage are concatenated and fed into the temporal part. The temporal part has a single branch to train the TML. Same as the spatial part, we apply the iterative stages. Since the last stage outputs of the spatial part are fed into each stage of the temporal part, the spatial and the temporal information affect each other through the end-to-end learning. We calculate the pixel-wise L2 loss as a loss function for each map at all stages.</p><p>Below is a more detailed description on each part.</p><p>• Spatial parts: Spatial parts are made up of six stages to learn the joint heatmaps and the part affinity fields. VGG features of two frames are fed into each spatial part. The losses of the joint heatmaps (H circle in <ref type="figure">Figure 2</ref>) and the part affinity fields (A circle in <ref type="figure">Figure 2</ref>) are calculated at each stage as in <ref type="bibr" target="#b0">[1]</ref>. • Temporal parts: The temporal parts resemble the single branch of the spatial part and are made up of three stages to learn the TML. Each stage of the temporal parts has three 3 × 3 convolutions and two 1 × 1 convolutions. The first stage takes the concatenated features as inputs: the VGG features of the two input frames, and the joint heatmaps and the part affinity fields from the last stage in the spatial parts. The second and the third stages additionally use the TML of the previous stage as an input. Iterative stages gradually improve the accuracy of the TML The loss of the TML (L circle in <ref type="figure">Figure  2</ref>) is calculated at each stage using a pixel-wise L2 loss function. The number of stages, three, was found experimentally.</p><p>As the spatial parts are the same network presented in <ref type="bibr" target="#b0">[1]</ref>, we will focus on the temporal part in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal flow Maps for Limb movement (TML)</head><p>The TML is a set of vector fields representing flows of person's limbs. In this paper, a limb denotes a part linking two joints such as an upper arm and calf. An example of the vector fields of a single limb is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b). To obtain these new type of maps, first we divide each limb at regular intervals to multiple parts. <ref type="figure" target="#fig_1">Figure 3</ref>(a) shows an example of a divided limb at frame F t1 and F t2 . In the figure, the red line means a limb and each yellow dot line shows the same parts on the same limb between frames. A separated part (S t,p,l,n ), which means an n-th separated part on l-th limb on the p-th person at the frame t, is used to calculate the movement direction between two frames. Based on the pair (S t1,p,l,n , S t2,p,l,n ), we calculate a unit vector v as follows: v = (S t1,p,l,n − S t2,p,l,n ) S t1,p,l,n − S t2,p,l,n 2 .</p><p>Here, n, l and p represent the index of a separated part, a limb and a person respectively, and t 1 and t 2 are the frame indices. The part S is represented by a two dimensional vector corresponding to the position of the part and thus v is also a two-dimensional vector. Then, the L for the l-th limb is encoded through the unit vector v for each pixel s = (x, y) which is the limb passes through at the time interval t 1 and t 2 . To draw the TML, we applied the similar process of part affinity field in <ref type="bibr" target="#b0">[1]</ref>.</p><formula xml:id="formula_1">L l,p (s) = v if s ⊆ C 0 otherwise.<label>(2)</label></formula><p>According to the condition (C), each pixel is determined to whether it is on the path of limb movement at the time interval t 1 and t 2 . More concretely, in our case, the pixels belonging to the line segment (S t1,p,l,n , S t2,p,l,n ) with a constant width is filled with the value of v and the other pixels remain as zero.</p><p>When the TML of multi person are overlapped at the same position, it is averaged to preserve the scales of the output. Thus, the final TML for the l-th joint averages the TML of the joints of all people appeared in the image as follows:</p><formula xml:id="formula_2">L l (s) = 1 P (s) P (s) p=1 L l,p (s), if P (s) ≥ 1 0 if P (s) = 0,<label>(3)</label></formula><p>where P (s) means the number of non-zero vectors at pixel s. All of n divided parts follow the above process to make the TML. <ref type="figure" target="#fig_1">Figure 3(b)</ref> shows a visualization of the TML of a single limb that is a part of left lower arm in x and y directions. The closer the value to −1(+1), the brighter (darker) it becomes. In <ref type="figure" target="#fig_1">Figure 3</ref>(a), we can see that the left hand of the person moves to the left-down side. Then, the direction of x channel is − while the direction of y channel becomes + as shown in <ref type="figure" target="#fig_1">Figure 3(b)</ref>. Thus, it is confirmed that the direction is different in each pixel of the TML. Unlike optical flow <ref type="bibr" target="#b28">[29]</ref> representing directions and magnitudes at each location, the TML only represents the directions using unit vectors. Because the TML does not contain magnitude information, it is more prone to change of time interval between frames. The multi-stride method for data augmentation, which will be describe in the next subsection, helps to alleviate this issue and successfully trains the network using video frames with different sampling rates.</p><p>Furthermore, the TML channel can be set as an individual channel for each limb <ref type="figure" target="#fig_1">(Figure 3(b)</ref>) or as an accumulated channel which accumulates the TML of all limbs <ref type="figure" target="#fig_1">(Figure 3(c)</ref>). The number of individual channels becomes the number of limbs × 2 (x, y coordinate channel) while the accumulated channel has only 2 channels (x, y coordinate channel). We will show the efficiency of different types of channel in the evaluation section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-stride method</head><p>The TML has temporal information on the joint location. We need a huge dataset containing various situations and poses to learn the maps. If one trains the TML by using only a fixed time interval in videos sequentially, only limited types of maps can be obtained, which usually contains very small movements. Thus, we propose a multi-stride method which uses a pair of frames with various time interval as a data augmentation method. To generate the various time interval, we randomly select two frames within a given time range. <ref type="figure" target="#fig_2">Figure 4</ref> shows the examples of the TML on a small-motion video. If we only use the time interval of one shown in <ref type="figure" target="#fig_2">Figure  4</ref>(a), we cannot get a direction of large motions in this video. On the other hands, if we use various time intervals, additional maps can be obtained as shown in <ref type="figure" target="#fig_2">Figure 4(b)</ref> and (c), making it possible to express a case where the motion is varied even in a small movement.</p><p>Furthermore, our multi-stride method can be used to refine poses at inference time. The proposed refining method can be useful when a frame misses a person but the preceding and the next frames successfully target the person. In this case, because our multi-stride method randomly selected two frames at the training time and the network learned this situation, we can extract the TML and track the pose between frame F t and F t+2 . More details can be found in the Section III-C. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inference</head><p>We define a set of frames consisting of three frames (F t−1 , F t , F t+1 ) as shown in <ref type="figure">Figure 1</ref> for temporal inference which associates joint candidates in different frames. First, on each frame, we estimate the joint candidates using the joint heatmaps and spatially connect the candidates using part affinity fields as in <ref type="bibr" target="#b0">[1]</ref>. The heatmaps and part affinity fields are created by the spatial part as shown in <ref type="figure">Figure 2</ref>.</p><p>Based on the connected joint candidates denoted as I, we track the poses. We calculate the associated score of each person in different frames. The associated score is calculated by a linear combination of a score of the TML (S T ) and a score of joint distance (S d ):</p><formula xml:id="formula_3">S = αS T + (1 − α)S d ,<label>(4)</label></formula><p>where α is a hyper-parameter which is set to 0.5 in our experiments. We measure the score of a candidate movement on each TML by calculating the line integral. More specifically, we extract two joint candidates I t1 j and I t2 j in different frames at time t 1 and t 2 corresponding to the joint j and make a normalized directional vector between the two joint candidates. Then the value of the TML corresponding to the line segment (I t1 j , I t2 j ) is obtained to take inner product with the directional vector. This is done for all the points in the line segment and integrated as follows:</p><formula xml:id="formula_4">S T = 1 n J n J j=1 u=1 u=0 L l (K(u)) · I t1 j − I t2 j I t1 j − I t2 j 2 du. (5)</formula><p>Here, I is a joint candidate and n J is the number of joints for a person which is determined in the spatial part. K(u) indicates interpolated points in the line segment (I t1 j , I t2 j ) where u ∈ {0, 1}, i.e., K(u) = (1 − u) · I t1 j + u · I t2 j . This score measures the plausibility of joint association between frames using the TML.</p><p>We measured the joint distance (S d ) between the frames using the Euclidean distance.</p><formula xml:id="formula_5">S d = 1 n J n J j=1 I t1 j − I t2 j<label>(6)</label></formula><p>Both scores are given a different weight by using the variable α which is determined through experiments. Finally, we find the optimal connection by applying a bipartite graph <ref type="bibr" target="#b1">[2]</ref>. After this, we refine the poses that have disappeared at the intermediate frame and come out again as shown in the second and the third images in <ref type="figure">Figure 1</ref>. More specifically, those situation means that the pose are not extracted on the frame F t but extracted and tracked between the frames (F t−1 , F t+1 ). To improve the situation, after pairs of frames with the time interval of one (F t−1 , F t ), (F t , F t+1 ) are processed, then to recover the missing person or joint in the frame F t , a pair of frames with the time interval of two (F t−1 , F t+1 ) is inputted to the proposed network followed by the above tracking method. After that, the poses or the joints missed in the middle of the frame F t are filled with average locations of those in F t−1 and F t+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>In order to prove the efficiency of the proposed method, experiments on the PoseTrack 2017 and 2018 datasets <ref type="bibr" target="#b16">[17]</ref> have been performed. PoseTrack datasets are large-scale benchmarks for human pose estimation and tracking. The PoseTrack datasets have various videos of human activities including fishing, running, tennis and so on. The datasets include a wide range of pose variations from a monotonous pose to a complex pose. PoseTrack datasets have the videos more than 500 sequences that are expected to be more than 20K frames. It is composed of 250 videos for training, 50 videos for validation and 214 videos for testing. PoseTrack 2018 dataset annotated more data than 2017.</p><p>The annotation types of PoseTrack 2017 and 2018 are different. The joints of PoseTrack 2018 added more parts such as ears and shoulder on top of the joints of PoseTrack 2017 and a new order of joints has been set. However, at the test time, mean average precision (mAP), multiple object tracker accuracy (MOTA) and multiple object tracking precision (MOTP) are evaluated in the annotation order of PoseTrack 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>We used the open-source library Caffe <ref type="bibr" target="#b29">[30]</ref> to implement our model. Our model was trained with a weight decay of 0.0005, a momentum of 0.9 and a learning rate of 0.00005. We used the pre-trained model of <ref type="bibr" target="#b0">[1]</ref> trained on COCO keypoints dataset <ref type="bibr" target="#b30">[31]</ref> as a base network. At the training time, we needed to change the joint order and to add some parts such as ears to middle of head from Postrack to COCO to use the pretrained model parameter, because COCO data and PoseTrack data have different order of joints.</p><p>At the training time, we applied data augmentation methods such as random crop, random rotate and so on. We set the scaling and rotation parameters based on the first frame among the two images. After a scaling and a rotation, we randomly select a person and crop a region such that the center of the selected person is located at the center of the region. The scaling, the rotation and selected person information of the first frame are applied equally in the second frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation</head><p>MOTA, MOTP and mAP are used to evaluate the performance <ref type="bibr" target="#b31">[32]</ref>. <ref type="table" target="#tab_0">Table I</ref> shows the results of the proposed methods by different settings -using different numbers (1 or 3) of iterative stages in the temporal part (#stage), using channel accumulation of TML instead of using individual channels for each joint ( * ), and a tracking method only using distance score by setting α in (4) as 0 (distance). Through the experiment, we empirically decide the number of subdivide each limb to 20 pieces to make the TML.</p><p>To make the temporal network part having as few parameters as possible while maintaining high performance, we experimented with different number of repetition stages, 1, 3 and 6. The spatial part used a fixed six stages. <ref type="table" target="#tab_0">Table I</ref> only compares the performances with one and three iterative stages in the temporal part, because the experimental result of the iterative 6 stages is lower than that of 3 stages and has a huge number of parameters.</p><p>Similar to optical flow <ref type="bibr" target="#b28">[29]</ref>, we accumulate all limb movements in one map called accumulated channel map as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c). On the Table I, ( * ) means that the network used the accumulated TML. Basically, we use a map with a channel for each limb called individual channel map. The number of channel on individual channel map is (the number of (x, y) channels = 2)×(the number of limbs), but the accumulated channel map has only two (x, y) channels. In all the tested networks, the accumulated channel map obtained lower accuracy than the individual channel map. Huge amount of the directional information of each limb is lost in the accumulated map, because the map includes some problems, e.g., different limbs overlap in the same location and have an averaging effect on that point. We implemented and compared the performance of the Joint-Flow map to show that the map created using limbs is more efficient than the map created using joints. The Joint-Flow map is constructed as a direction in which the joint moves between two frames. The Joint-Flow map follows the equation of (2) but uses the joint location instead of separated part s. The mAPs of Joint-Flow are higher than the TML , but MOTAs are lower. This results shows the difficulty of tracking using the Joint-Flow, because the Joint-Flow map has less information than the TML. Moreover, we compared with JointFlow [2] that proposed a temporal map about joint movement as shown in <ref type="table" target="#tab_0">Table II</ref>. On the PoseTrack 2017 test set, our results are better than those of the JointFlow <ref type="bibr" target="#b1">[2]</ref>.</p><p>Because the proposed method is the bottom-up approach, it is possible to detect many joint candidates on the same part. Thus, a non-maximum suppression (NMS) is applied for joints to reduce confusion after estimating joint location. (+) in <ref type="table" target="#tab_0">Table  I</ref> means that first we detect joints using the joint heatmaps and refine the joint using NMS. Reducing the confusing candidates increases tracking performance by around 0.4% in mAP and 0.6% in MOTA.</p><p>The sum of the TML score and the joint distance score is used for the association score to track poses. We experimented to see how the joint distance affects to association score. On <ref type="table" target="#tab_0">Table I</ref>, (Distance) means that only joint distances of a person <ref type="figure">Fig. 5</ref>. An example of pose refinement using multi-stride inputs during the inference. The person at the right side of input images (red line) is tracked from F t−1 to F t+1 , but the pose of the person is not detected at Ft. By associating the poses at F t−1 and F t+1 , we can retrieve the missed pose at Ft. On the other hand, we cannot refine the person on the left side (pink line), because it is only estimated at the F t−1 . is used in the calculation of association score. To enable this, at inference time, we use the same structure as TML+ and set α to 0. Only using the distance score incurs more confusion with nearby people and the resultant MOTA is by far lower than others on average. However, we need to use the distance score to handle the case of no motion. Thus, we apply α to 0.5 in all the other cases.</p><p>One of our contributions is the refining method for the middle frame pose. We refine the pose on the middle of frame by analyzing between three frames. (++) in <ref type="table" target="#tab_0">Table I, Table II,   and Table III</ref> means that the refining method is applied. <ref type="figure">Figure  5</ref> shows an example result of the refined pose. The pose on F t is refined through the association between frames F t−1 and F t+1 . In case of the person on the right side (red line), the person is tracked at the F t−1 and the F t+1 , but not tracked at the F t . Through the refining method, an average pose between F t−1 and F t+1 is added on the frame F t . Unfortunately, the person on the left side (pink line) can not be tracked through the refining method, because the pose is not estimated at the F t+1 . <ref type="figure" target="#fig_3">Figure 6</ref> shows qualitative results of pose estimation and tracking. Poses are estimated and tracked well in a variety of environments even when several people move close together or quickly. Because our association score considers the distance score, poses that have a little movement can also be tracked as shown in the fourth row on <ref type="figure" target="#fig_3">Figure 6</ref>. Unfortunately, if the poses nearly occludes each other as in the last row of <ref type="figure" target="#fig_3">Figure  6</ref>, the pose is likely to be missed. For future work, we may propagate the pose through the TML and refine the estimated pose by comparing it with the propagated pose to address this.</p><p>We compare our method with the state-of-the-art methods on the PoseTrack 2017 and 2018 test datasets as shown in <ref type="table" target="#tab_0">Table II</ref>. Though the proposed method shows a lower performance than the highest record <ref type="bibr" target="#b14">[15]</ref>, the result of the proposed network is the best among the bottom-up approaches.</p><p>Because the PoseTrack challenge was held on the September 2018, papers using the PoseTrack 2018 data have not been published yet. We could not compare the proposed method with other methods on the PoseTrack 2018 validate data. However, we can compare results of state-of-the-art on the PoseTrack 2018 test data through the results on the PoseTrack leader-board site as shown in the <ref type="table" target="#tab_0">Table III</ref>. We cannot compare the structures of the networks, but ours shows the best performance among the ones trained only using COCO data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We propose a multi-stride pose estimator and tracker. It tracks the joints based on the TML which is a unit vector map representing the human flow. The multi-stride method has been used to train various temporal flow maps. Our method utilizes both the spatial and temporal information. Spatial information such as joint heatmaps and part affinity fields is regressed by the spatial part and TML is regressed by the temporal part. The combined network can be trained in an end-to-end manner influencing each other. We demonstrate the efficiency of the proposed method on the PoseTrack 2017 and 2018 datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>which has been proposed by Ramakrishna et al. Features in these networks possess a large receptive field which extracts an efficient representation of human context. Advances in the single person pose estimation have made it possible to proceed research on multi-person pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>An example of TML (a) Illustration explaining how to obtain the TML using the frames Ft 1 and Ft 2 . We subdivide each limb into several parts and calculate the unit vector of each pair (connected by the yellow lines, S t 1 ,p,l,n and S t 2 ,p,l,n ).(b) Visualization of the left arm TML on x(top) and y(bottom) coordinates. (c) Accumulated TML for all limbs on x(top) and y(bottom) coordinates. The values of TML are between the range of -1 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of the TML of x coordinate with various time intervals. Consecutive image sequences are shown from left to right. (a), (b) and (c) are the right arm TML of the left person with the different time intervals, 1, 2 and 4 respectively. Using various strides, it is possible to get the TML of both small and large movements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The qualitative results of the proposed multi-stride pose estimator and tracker. The images are in chronological order from left to right. Tracked poses are displayed in the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>ESTIMATION AND TRACKING RESULTS OF THE PROPOSED METHODS ON THE POSETRACK2017 AND 2018 VALIDATION DATA. #STAGE MEANS THE NUMBER OF STACKED STAGES IN THE TEMPORAL PART. JOINT-FLOW HAS A DIFFERENT TYPE OF TEMPORAL MAP THAT IS CREATED BY JOINT MOVEMENT. BASICALLY, THE PROPOSED TML HAS TWO CHANNELS (x AND y) FOR EACH LIMB. ( * ) MEANS THE METHOD IN WHICH THE TML OF ALL LIMBS ARE ACCUMULATED IN A SINGLE MAP FOR X AND Y DIRECTIONS. + ADOPTED THE NON-MAXIMUM SUPPRESSION (NMS) FOR JOINTS. ++INDICATES THAT THE PROPOSED REFINING METHOD FOR THE MIDDLE FRAME POSE IS APPLIED. DISTANCE MEANS THAT THE TML IS NOT USED IN THE CALCULATION OF THE ASSOCIATION SCORE IN (4) BY SETTING α TO 0, WHICH MEANS THAT IT ONLY USES THE TORSO DISTANCE OF A PERSON FOR THE ASSOCIATED SCORE.</figDesc><table><row><cell>data</cell><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MOTA</cell><cell></cell><cell></cell><cell></cell><cell>mAP</cell></row><row><cell></cell><cell></cell><cell cols="3">#stage Head Shou</cell><cell>Elb</cell><cell>Wri</cell><cell>Hip</cell><cell cols="3">Knee Ankl Total</cell><cell></cell></row><row><cell></cell><cell>Joint-Flow( * )</cell><cell>1</cell><cell>70.6</cell><cell>70.1</cell><cell>50.6</cell><cell>37.5</cell><cell>53.9</cell><cell>41.8</cell><cell>30.3</cell><cell>52</cell><cell>73.1</cell></row><row><cell></cell><cell>Joint-Flow</cell><cell>1</cell><cell>48.5</cell><cell>48.3</cell><cell>30.3</cell><cell>19.3</cell><cell>33.9</cell><cell>23</cell><cell>13.5</cell><cell>32.1</cell><cell>73.2</cell></row><row><cell></cell><cell>TML( * )</cell><cell>1</cell><cell>72</cell><cell>70.6</cell><cell>52.1</cell><cell>37.7</cell><cell>53.8</cell><cell>41.3</cell><cell>30.9</cell><cell>52.6</cell><cell>71.3</cell></row><row><cell>2017</cell><cell>TML</cell><cell>1</cell><cell>70.1</cell><cell>69.5</cell><cell>51.9</cell><cell>40.5</cell><cell>53.8</cell><cell>43.5</cell><cell>32.7</cell><cell>52.9</cell><cell>72.9</cell></row><row><cell></cell><cell>TML</cell><cell>3</cell><cell>74.7</cell><cell>74.1</cell><cell>61.7</cell><cell>49.4</cell><cell>59</cell><cell>52.6</cell><cell>43.7</cell><cell>60.3</cell><cell>70.9</cell></row><row><cell></cell><cell>TML+</cell><cell>3</cell><cell>75.1</cell><cell>74.6</cell><cell>62.5</cell><cell>50.1</cell><cell>59.5</cell><cell>53</cell><cell>44.2</cell><cell>60.9</cell><cell>71.3</cell></row><row><cell></cell><cell>Distance+</cell><cell>3</cell><cell>49.9</cell><cell>50.1</cell><cell>40.5</cell><cell>31.5</cell><cell>37.7</cell><cell>32.4</cell><cell>26.7</cell><cell>39.2</cell><cell>71.3</cell></row><row><cell></cell><cell>TML++</cell><cell>3</cell><cell>75.5</cell><cell>75.1</cell><cell>62.9</cell><cell>50.7</cell><cell>60</cell><cell>53.4</cell><cell>44.5</cell><cell>61.3</cell><cell>71.5</cell></row><row><cell>2018</cell><cell>TML++</cell><cell>3</cell><cell>76</cell><cell>76.9</cell><cell>66.1</cell><cell>56.4</cell><cell>65.1</cell><cell>61.6</cell><cell>52.4</cell><cell>65.7</cell><cell>74.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II POSE</head><label>II</label><figDesc>ESTIMATION AND TRACKING PERFORMANCE ON POSETRACK 2017</figDesc><table><row><cell></cell><cell cols="3">TEST DATASET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Method</cell><cell>mAP</cell><cell cols="4">MOTA MOTP Prec.</cell><cell>Rec</cell></row><row><cell></cell><cell>Poseflow [16]</cell><cell>63</cell><cell cols="2">51</cell><cell>16.9</cell><cell>71.2</cell><cell>78.9</cell></row><row><cell>Top-down</cell><cell>MVIG</cell><cell>63.2</cell><cell cols="2">50.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Xiao et al. [15]</cell><cell>74.6</cell><cell cols="2">57.8</cell><cell>62.6</cell><cell>79.4</cell><cell>80.3</cell></row><row><cell></cell><cell>JointFlow [2]</cell><cell>63.6</cell><cell cols="2">53</cell><cell>23.2</cell><cell>82.1</cell><cell>70.6</cell></row><row><cell>Bottom-up</cell><cell>Jin et al. [5]</cell><cell>59.16</cell><cell cols="2">50.59</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TML++</cell><cell>68.78</cell><cell cols="2">54.46</cell><cell>85.2</cell><cell>80</cell><cell>76.1</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">POSE ESTIMATION AND TRACKING PERFORMANCE ON POSETRACK 2018</cell></row><row><cell></cell><cell cols="3">TEST DATASET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Additional training data MOTA</cell><cell>mAP</cell><cell cols="3">Wrists AP Ankles AP</cell></row><row><cell>Xiao et al. [15]</cell><cell>+COCO+Other</cell><cell cols="2">61.37</cell><cell>74.03</cell><cell>73</cell><cell></cell><cell>69.05</cell></row><row><cell>ALG</cell><cell>+COCO+Other</cell><cell cols="2">60.79</cell><cell>74.85</cell><cell>72.62</cell><cell></cell><cell>71.11</cell></row><row><cell>Miracle</cell><cell>+COCO+Other</cell><cell cols="2">57.36</cell><cell>70.9</cell><cell>68.19</cell><cell></cell><cell>66.06</cell></row><row><cell>CMP</cell><cell>+COCO</cell><cell cols="2">54.47</cell><cell>64.67</cell><cell>61.78</cell><cell></cell><cell>60.86</cell></row><row><cell>PR</cell><cell>+COCO</cell><cell cols="2">44.54</cell><cell>59.05</cell><cell>50.16</cell><cell></cell><cell>49.4</cell></row><row><cell>TML++</cell><cell>+COCO</cell><cell cols="2">54.86</cell><cell>67.81</cell><cell>60.2</cell><cell></cell><cell>56.85</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint flow: Temporal flow fields for multi person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">BMVC</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards multi-person pose tracking: Bottom-up and topdown methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual path networks for multi-person human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Global pose refinement using bidirectional long-short term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geocke</surname></persName>
		</author>
		<ptr target="https://posetrack.net/workshops/iccv2017/pdfs/MPR.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint multi-person pose estimation and semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6080" to="6089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation for posetrack with enhanced part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detectand-Track: Efficient Pose Estimation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2274" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3711" to="3719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pose flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">BMVC</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ensafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial PoseNet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5669" to="5678" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1290" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ArtTrack: Articulated Multi-person Tracking in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mot16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
