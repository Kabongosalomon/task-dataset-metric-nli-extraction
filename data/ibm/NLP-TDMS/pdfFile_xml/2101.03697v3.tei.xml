<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RepVGG: Making VGG-style ConvNets Great Again</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<postCode>SY23 3FL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cnsunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RepVGG: Making VGG-style ConvNets Great Again</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple but powerful architecture of convolutional neural network, which has a VGG-like inferencetime body composed of nothing but a stack of 3 × 3 convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the trainingtime and inference-time architecture is realized by a structural re-parameterization technique so that the model is named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy, which is the first time for a plain model, to the best of our knowledge. On NVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster than ResNet-101 with higher accuracy and show favorable accuracy-speed trade-off compared to the stateof-the-art models like EfficientNet and RegNet. The code and trained models are available at https://github. com/megvii-model/RepVGG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A classic Convolutional Neural Network (ConvNet), VGG <ref type="bibr" target="#b29">[31]</ref>, achieved huge success in image recognition with a simple architecture composed of a stack of conv, ReLU, and pooling. With Inception <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b17">19]</ref>, ResNet <ref type="bibr" target="#b11">[12]</ref> and DenseNet <ref type="bibr" target="#b16">[17]</ref>, a lot of research interests were shifted to well-designed architectures, making the models more and more complicated. for EfficientNet-B3 <ref type="bibr" target="#b33">[35]</ref> and 224 for the others.</p><p>automatic <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b21">23]</ref> or manual <ref type="bibr" target="#b26">[28]</ref> architecture search, or a searched compound scaling strategy <ref type="bibr" target="#b33">[35]</ref>. Though many complicated ConvNets deliver higher accuracy than the simple ones, the drawbacks are significant. 1) The complicated multi-branch designs (e.g., residualaddition in ResNet and branch-concatenation in Inception) make the model difficult to implement and customize, slow down the inference and reduce the memory utilization. 2) Some components (e.g., depthwise conv in Xception <ref type="bibr" target="#b2">[3]</ref> and MobileNets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">30]</ref> and channel shuffle in ShuffleNets <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b39">41]</ref>) increase the memory access cost and lack supports of various devices. With so many factors affecting the inference speed, the amount of floating-point operations (FLOPs) does not precisely reflect the actual speed. Though some novel models have lower FLOPs than the oldfashioned ones like VGG and ResNet-18/34/50 <ref type="bibr" target="#b11">[12]</ref>  <ref type="figure" target="#fig_2">Figure 2</ref>: Sketch of RepVGG architecture. RepVGG has 5 stages and conducts down-sampling via stride-2 convolution at the beginning of a stage. Here we only show the first 4 layers of a specific stage. As inspired by ResNet <ref type="bibr" target="#b11">[12]</ref>, we also use identity and 1 × 1 branches, but only for training.</p><p>may not run faster <ref type="table" target="#tab_4">(Table. 4</ref>). Consequently, VGG and the original versions of ResNets are still heavily used for realworld applications in both academia and industry.</p><p>In this paper, we propose RepVGG, a VGG-style architecture which outperforms many complicated models ( <ref type="figure" target="#fig_0">Fig.  1</ref>). RepVGG has the following advantages.</p><p>• The model has a VGG-like plain (a.k.a. feed-forward) topology 1 without any branches, which means every layer takes the output of its only preceding layer as input and feeds the output into its only following layer. • The model's body uses only 3 × 3 conv and ReLU. • The concrete architecture (including the specific depth and layer widths) is instantiated with no automatic search <ref type="bibr" target="#b42">[44]</ref>, manual refinement <ref type="bibr" target="#b26">[28]</ref>, compound scaling <ref type="bibr" target="#b33">[35]</ref>, nor other heavy designs. It is challenging for a plain model to reach a comparable level of performance as the multi-branch architectures. An explanation is that a multi-branch topology, e.g., ResNet, makes the model an implicit ensemble of numerous shallower models <ref type="bibr" target="#b34">[36]</ref>, so that training a multi-branch model avoids the gradient vanishing problem.</p><p>Since the benefits of multi-branch architecture are all for training and the drawbacks are undesired for inference, we propose to decouple the training-time multibranch and inference-time plain architecture via structural re-parameterization, which means converting the architecture from one to another via transforming its parameters. To be specific, a network structure is coupled with a set of parameters, e.g., a conv layer is represented by a 4th-order kernel tensor. If the parameters of a certain structure can be converted into another set of parameters coupled by another structure, we can equivalently replace the former with the latter, so that the overall network architecture is changed.</p><p>Specifically, we construct the training-time RepVGG using identity and 1×1 branches, which is inspired by ResNet but in a different way that the branches can be removed by structural re-parameterization ( <ref type="figure" target="#fig_2">Fig. 2,4</ref>). After training, we perform the transformation with simple algebra, as an identity branch can be regarded as a degraded 1×1 conv, and the latter can be further regarded as a degraded 3 × 3 conv, so that we can construct a single 3 × 3 kernel with the trained parameters of the original 3 × 3 kernel, identity and 1 × 1 branches and batch normalization (BN) <ref type="bibr" target="#b17">[19]</ref> layers. Consequently, the transformed model has a stack of 3 × 3 conv layers, which is saved for test and deployment.</p><p>Notably, the body of an inference-time RepVGG only has one single type of operator: 3 × 3 conv followed by ReLU, which makes RepVGG fast on generic computing devices like GPUs. Even better, RepVGG allows for specialized hardware to achieve even higher speed because given the chip size and power consumption, the fewer types of operators we require, the more computing units we can integrate onto the chip. Consequently, an inference chip specialized for RepVGG can have an enormous number of 3×3-ReLU units and fewer memory units (because the plain topology is memory-economical, as shown in <ref type="figure">Fig. 3</ref>). Our contributions are summarized as follows.</p><p>• We propose RepVGG, a simple architecture with favorable speed-accuracy trade-off compared to the state-of-the-arts. • We propose to use structural re-parameterization to decouple a training-time multi-branch topology with an inference-time plain architecture. • We show the effectiveness of RepVGG in image classification and semantic segmentation, and the efficiency and ease of implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">From Single-path to Multi-branch</head><p>After VGG <ref type="bibr" target="#b29">[31]</ref> raised the top-1 accuracy of ImageNet classification to above 70%, there have been many innovations in making ConvNets complicated for high performance, e.g., the contemporary GoogLeNet <ref type="bibr" target="#b31">[33]</ref> and later Inception models <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b17">19]</ref> adopted elaborately designed multi-branch architectures, ResNet <ref type="bibr" target="#b11">[12]</ref> proposed a simplified two-branch architecture, and DenseNet <ref type="bibr" target="#b16">[17]</ref> made the topology more complicated by connecting lower-level layers with numerous higher-level ones. Neural architecture search (NAS) <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b33">35]</ref> and manual designing space design <ref type="bibr" target="#b26">[28]</ref> can generate ConvNets with higher performance but at the costs of vast computing resources or manpower. Some large versions of NAS-generated models are even not trainable on ordinary GPUs, hence limiting the applications. Except for the inconvenience of implementation, the complicated models may reduce the degree of parallelism <ref type="bibr" target="#b22">[24]</ref> hence slow down the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Effective Training of Single-path Models</head><p>There have been some attempts to train ConvNets without branches. However, the prior works mainly sought to make the very deep models converge with reasonable accuracy, but not achieve better performance than the complicated models. Consequently, the methods and resultant models were neither simple nor practical. An initialization method <ref type="bibr" target="#b35">[37]</ref> was proposed to train extremely deep plain ConvNets. With a mean-field-theory-based scheme, 10,000-layer networks were trained over 99% accuracy on MNIST and 82% on CIFAR-10. Though the models were not practical (even LeNet-5 <ref type="bibr" target="#b19">[21]</ref> can reach 99.3% accuracy on MNIST and VGG-16 can reach above 93% on CIFAR-10), the theoretical contributions were insightful. A recent work <ref type="bibr" target="#b23">[25]</ref> combined several techniques including Leaky ReLU, max-norm and careful initialization. On ImageNet, it showed that a plain ConvNet with 147M parameters could reach 74.6% top-1 accuracy, which was 2% lower than its reported baseline (ResNet-101, 76.6%, 45M parameters).</p><p>Notably, this paper is not merely a demonstration that plain models can converge reasonably well, and does not intend to train extremely deep ConvNets like ResNets. Rather, we aim to build a simple model with reasonable depth and favorable accuracy-speed trade-off, which can be simply implemented with the most common components (e.g., regular conv and BN) and simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Re-parameterization</head><p>DiracNet <ref type="bibr" target="#b37">[39]</ref> is a re-parameterization method related to ours. It builds deep plain models by encoding the kernel of a conv layer asŴ = diag(a)I + diag(b)W norm , whereŴ is the eventual weight used for convolution (a 4thorder tensor viewed as a matrix), a and b are learned vectors, and W norm is the normalized learnable kernel. Compared to ResNets with comparable amount of parameters, the top-1 accuracy of DiracNet is 2.29% lower on CIFAR-100 (78.46% vs. 80.75%) and 0.62% lower on ImageNet (72.21% of DiracNet-34 vs. 72.83% of ResNet-34). Dirac-Net differs from our method in two aspects. 1) The trainingtime behavior of RepVGG is implemented by the actual dataflow through a concrete structure which can be later converted into another, while DiracNet merely uses another mathematical expression of conv kernels for easier optimization. In other words, a training-time RepVGG is a real multi-branch model, but a DiracNet is not.</p><p>2) The performance of a DiracNet is higher than a normally parameterized plain model but lower than a comparable ResNet, while RepVGG models outperform ResNets by a large margin. Asym Conv Block (ACB) <ref type="bibr" target="#b9">[10]</ref>, DO-Conv <ref type="bibr" target="#b0">[1]</ref> and ExpandNet <ref type="bibr" target="#b10">[11]</ref> can also be viewed as structural reparameterization in the sense that they convert a block into a conv. Compared to our method, the difference is that they are designed for component-level improvements and used as a drop-in replacement for conv layers in any architecture, while our structural re-parameterization is critical for training plain ConvNets, as shown in Sect. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Winograd Convolution</head><p>RepVGG uses only 3 × 3 conv because it is highly optimized by some modern computing libraries like NVIDIA cuDNN <ref type="bibr" target="#b1">[2]</ref> and Intel MKL [18] on GPU and CPU. <ref type="table" target="#tab_0">Table. 1</ref> shows the theoretical FLOPs, actual running time and computational density (measured in Tera FLoating-point Operations Per Second, TFLOPS) 2 tested with cuDNN 7.5.0 on a 1080Ti GPU. The theoretical computational density of 3×3 conv is around 4× as the others, suggesting the total theoretical FLOPs is not a comparable proxy for the actual speed among different architectures. Winograd <ref type="bibr" target="#b18">[20]</ref> is a classic algorithm for accelerating 3 × 3 conv (only if the stride is 1), which has been well supported (and enabled by default) by libraries like cuDNN and MKL. For example, with the standard F (2 × 2, 3 × 3) Winograd, the amount of multiplications (MULs) of a 3 × 3 conv is reduced to <ref type="bibr">4 9</ref> of the original. Since the multiplications are much more time-consuming than additions, we count the MULs to measure the computational costs with Winograd support (denoted by Wino MULs in <ref type="table">Table.</ref>  <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5)</ref>. Note that the specific computing library and hardware determine whether to use Winograd for each operator because small-scale convolutions may not be accelerated due to the memory overhead. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Building RepVGG via Structural Re-param</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Simple is Fast, Memory-economical, Flexible</head><p>There are at least three reasons for using simple Con-vNets: they are fast, memory-economical and Flexible.</p><p>Fast Many recent multi-branch architectures have lower theoretical FLOPs than VGG but may not run faster. For example, VGG-16 has 8.4× FLOPs as EfficientNet-B3 <ref type="bibr" target="#b33">[35]</ref> but runs 1.8× faster on 1080Ti <ref type="table" target="#tab_4">(Table. 4</ref>), which means the computational density of the former is 15× as the latter. Except for the acceleration brought by Winograd conv, the discrepancy between FLOPs and speed can be attributed to two important factors that have considerable affection on speed but are not taken into account by FLOPs: the memory access cost (MAC) and degree of parallelism <ref type="bibr" target="#b22">[24]</ref>. For example, though the required computations of branch addition or concatenation are negligible, the MAC is significant. Moreover, MAC constitutes a large portion of time usage in groupwise convolution. On the other hand, a model with high degree of parallelism could be much faster than another one with low degree of parallelism, under the same FLOPs. As multi-branch topology is widely adopted in Inception and auto-generated architectures, multiple small operators are used instead of a few large ones. A prior work <ref type="bibr" target="#b22">[24]</ref> reported that the number of fragmented operators (i.e. the number of individual conv or pooling operations in one building block) in NASNET-A <ref type="bibr" target="#b41">[43]</ref> is 13, which is unfriendly to devices with strong parallel computing powers like GPU and introduces extra overheads such as kernel launching and synchronization. In contrast, this number is 2 or 3 in ResNets, and we make it 1: a single conv.</p><p>Memory-economical The multi-branch topology is memory-inefficient because the results of every branch need to be kept until the addition or concatenation, significantly raising the peak value of memory occupation. <ref type="figure">Fig. 3</ref> shows that the input to a residual block need to be kept until the addition. Assuming the block maintains the feature map size, the peak value of extra memory occupation is 2× as the input. In contrast, a plain topology allows the memory occupied by the inputs to a specific layer to be immediately released when the operation is finished. When designing specialized hardware, a plain ConvNet allows deep memory optimizations and reduces the costs of memory units so that we can integrate more computing units onto the chip.</p><p>Flexible</p><p>The multi-branch topology imposes constraints on the architectural specification. For example, ResNet requires the conv layers to be organized as residual blocks, which limits the flexibility because the last conv layers of every residual block have to produce tensors of the same shape, or the shortcut addition will not make sense. Even worse, multi-branch topology limits the application of channel pruning <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b13">14]</ref>, which is a practical technique to remove some unimportant channels, and some methods can optimize the model structure by automatically discovering  <ref type="figure">Figure 3</ref>: Peak memory occupation in residual and plain model. If the residual block maintains the size of feature map, the peak value of extra memory occupied by feature maps will be 2× as the input. The memory occupied by the parameters is small compared to the features hence ignored.</p><p>the appropriate width of each layer <ref type="bibr" target="#b7">[8]</ref>. However, multibranch models make pruning tricky and result in significant performance degradation or low acceleration ratio <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b8">9]</ref>. In contrast, a plain architecture allows us to freely configure every conv layer according to our requirements and prune to obtain a better performance-efficiency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training-time Multi-branch Architecture</head><p>Plain ConvNets have many strengths but one fatal weakness: the poor performance. For example, with modern components like BN <ref type="bibr" target="#b17">[19]</ref>, a VGG-16 can reach over 72% top-1 accuracy on ImageNet, which seems outdated. Our structural re-parameterization method is inspired by ResNet, which explicitly constructs a shortcut branch to model the information flow as y = x + f (x) and uses a residual block to learn f . When the dimensions of x and f (x) do not match, it becomes y = g(x)+f (x), where g(x) is a convolutional shortcut implemented by a 1×1 conv. An explanation for the success of ResNets is that such a multibranch architecture makes the model an implicit ensemble of numerous shallower models <ref type="bibr" target="#b34">[36]</ref>. Specifically, with n blocks, the model can be interpreted as an ensemble of 2 n models, since every block branches the flow into two paths.</p><p>Since the multi-branch topology has drawbacks for inference but the branches seem beneficial to training <ref type="bibr" target="#b34">[36]</ref>, we use multiple branches to make an only-training-time ensemble of numerous models. To make most of the members shallower or simpler, we use ResNet-like identity (only if the dimensions match) and 1 × 1 branches so that the training-time information flow of a building block is y = x + g(x) + f (x). We simply stack several such blocks to construct the training-time model. From the same perspective as <ref type="bibr" target="#b34">[36]</ref>, the model becomes an ensemble of 3 n members with n such blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Re-param for Plain Inference-time Model</head><p>In this subsection, we describe how to convert a trained block into a single 3 × 3 conv layer for inference. Note that we use BN in each branch before the addition <ref type="figure" target="#fig_4">(Fig. 4)</ref>. Formally, we use W (3) ∈ R C2×C1×3×3 to denote the kernel of a 3 × 3 conv layer with C 1 input channels and C 2 out-put channels, and W (1) ∈ R C2×C1 for the kernel of 1 × 1 branch. We use µ <ref type="bibr" target="#b2">(3)</ref> , σ <ref type="bibr" target="#b2">(3)</ref> , γ <ref type="bibr" target="#b2">(3)</ref> , β <ref type="bibr" target="#b2">(3)</ref> as the accumulated mean, standard deviation and learned scaling factor and bias of the BN layer following 3 × 3 conv, µ <ref type="bibr" target="#b0">(1)</ref> , σ <ref type="bibr" target="#b0">(1)</ref> , γ <ref type="bibr" target="#b0">(1)</ref> , β <ref type="bibr" target="#b0">(1)</ref> for the BN following 1 × 1 conv, and µ (0) , σ (0) , γ (0) , β (0) for the identity branch. Let M (1) ∈ R N ×C1×H1×W1 , M <ref type="bibr" target="#b1">(2)</ref> ∈ R N ×C2×H2×W2 be the input and output, respectively, and * be the convolution operator.</p><formula xml:id="formula_0">If C 1 = C 2 , H 1 = H 2 , W 1 = W 2 , we have M (2) = bn(M (1) * W (3) , µ (3) , σ (3) , γ (3) , β (3) ) + bn(M (1) * W (1) , µ (1) , σ (1) , γ (1) , β (1) ) + bn(M (1) , µ (0) , σ (0) , γ (0) , β (0) ) .<label>(1)</label></formula><p>Otherwise, we simply use no identity branch, hence the above equation only has the first two terms. Here bn is the inference-time BN function, formally,</p><formula xml:id="formula_1">∀1 ≤ i ≤ C 2 , bn(M, µ, σ, γ, β) :,i,:,: = (M :,i,:,: − µ i ) γ i σ i + β i . (2)</formula><p>We first convert every BN and its preceding conv layer into a conv with a bias vector. Let {W , b } be the kernel and bias converted from {W, µ, σ, γ, β}, we have</p><formula xml:id="formula_2">W i,:,:,: = γ i σ i W i,:,:,: , b i = − µ i γ i σ i + β i .<label>(3)</label></formula><p>Then it is easy to verify that ∀1 ≤ i ≤ This transformation also applies to the identity branch because an identity can be viewed as a 1 × 1 conv with an identity matrix as the kernel. After such transformations, we will have one 3 × 3 kernel, two 1 × 1 kernels, and three bias vectors. Then we obtain the final bias by adding up the three bias vectors, and the final 3 × 3 kernel by adding the 1×1 kernels onto the central point of 3×3 kernel, which can be easily implemented by first zero-padding the two 1 × 1 kernels to 3 × 3 and adding the three kernels up, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. Note that the equivalence of such transformations requires the 3 × 3 and 1 × 1 layer to have the same stride, and the padding configuration of the latter shall be one pixel less than the former. For example, for a 3×3 layer that pads the input by one pixel, which is the most common case, the 1 × 1 layer should have padding = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Architectural Specification</head><p>Table. 2 shows the specification of RepVGG including the depth and width. RepVGG is VGG-style in the sense that it adopts a plain topology and heavily uses 3 × 3 conv, but it does not use max pooling like VGG because we desire the body to have only one type of operator. We arrange the 3×3 layers into 5 stages, and the first layer of a stage downsamples with the stride = 2. For image classification, we use  For the ease of visualization, we assume C 2 = C 1 = 2, thus the 3 × 3 layer has four 3 × 3 matrices and the kernel of 1 × 1 layer is a 2 × 2 matrix. global average pooling followed by a fully-connected layer as the head. For other tasks, the task-specific heads can be used on the features produced by any layer. We decide the numbers of layers of each stage following three simple guidelines. 1) The first stage operates with large resolution, which is time-consuming, so we use only one layer for lower latency.</p><p>2) The last stage shall have more channels, so we use only one layer to save the parameters. 3) We put the most layers into the second last stage (with 14 × 14 output resolution on ImageNet), following ResNet and its recent variants <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b36">38]</ref> (e.g., ResNet-101 uses 69 layers in its 14 × 14-resolution stage). We let the five stages have 1, 2, 4, 14, 1 layers respectively to construct an instance named RepVGG-A. We also build a deeper RepVGG-B, which has 2 more layers in stage2, 3 and 4. We use RepVGG-A to compete against other lightweight and middleweight models including ResNet-18/34/50, and RepVGG-B against the high-performance ones.</p><p>We determine the layer width by uniformly scaling the classic width setting of [64, 128, 256, 512] (e.g., VGG and ResNets). We use multiplier a to scale the first four stages and b for the last stage, and usually set b &gt; a because we desire the last layer to have richer features for the classification or other down-stream tasks. Since RepVGG has only one layer in the last stage, a larger b does not significantly increase the latency nor the amount of parameters. Specifically, the width of stage2, 3, 4, 5 is [64a, 128a, 256a, 512b], respectively. To avoid large-scale conv on high-resolution feature maps, we scale down stage1 if a &lt; 1 but do not scale it up, so that the width of stage1 is min <ref type="figure" target="#fig_4">(64, 64a)</ref>.</p><p>To further reduce the parameters and computations, we may optionally interleave groupwise 3 × 3 conv layers with dense ones to trade accuracy for efficiency. Specifically, we set the number of groups g for the 3rd, 5th, 7th, ..., 21st layer of RepVGG-A and the additional 23rd, 25th and 27th layers of RepVGG-B. For the simplicity, we set g as 1, 2, or 4 globally for such layers without layer-wise tuning. We do not use adjacent groupwise conv layers because that would disable the inter-channel information exchange and bring a side effect <ref type="bibr" target="#b39">[41]</ref>: outputs from a certain channel would be derived from only a small fraction of input channels. Note that the 1 × 1 branch shall have the same g as the 3 × 3 conv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compare RepVGG with the baselines on ImageNet, justify the significance of structural re-parameterization by a series of ablation studies and comparisons, and verify the generalization performance on semantic segmentation <ref type="bibr" target="#b40">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">RepVGG for ImageNet Classification</head><p>We compare RepVGG with the classic and state-of-theart models including VGG-16 <ref type="bibr" target="#b29">[31]</ref>, ResNet <ref type="bibr" target="#b11">[12]</ref>, ResNeXt <ref type="bibr" target="#b36">[38]</ref>, EfficientNet <ref type="bibr" target="#b33">[35]</ref>, and RegNet <ref type="bibr" target="#b26">[28]</ref> on ImageNet-1K <ref type="bibr" target="#b5">[6]</ref>, which comprises 1.28M images for training and 50K for validation. We use EfficientNet-B0/B3 and RegNet-3.2GF/12GF as the representatives for middleweight and heavyweight state-of-the-art models, respectively. We vary the multipliers a and b to generate a series of RepVGG models to compare against the baselines <ref type="table">(Table.</ref> 3).</p><p>We first compare RepVGG against ResNets <ref type="bibr" target="#b11">[12]</ref>, which are the most common benchmarks. We use RepVGG-A0/A1/A2 for the comparisons with ResNet-18/34/50, respectively. To compare against the larger models, we construct the deeper RepVGG-B0/B1/B2/B3 with increasing width. For those RepVGG models with interleaved groupwise layers, we postfix g2/g4 to the model name.</p><p>For training the lightweight and middleweight models, we only use the simple data augmentation pipeline including random cropping and left-right flipping, following the official PyTorch example <ref type="bibr" target="#b25">[27]</ref>. We use a global batch size of 256 on 8 GPUs, a learning rate initialized as 0.1 and cosine annealing for 120 epochs, standard SGD with momentum coefficient of 0.9 and weight decay of 10 −4 on the kernels of conv and fully-connected layers. For the heavyweight models including RegNetX-12GF, EfficientNet-B3 and RepVGG-B3, we use 5-epoch warmup, cosine learning rate annealing for 200 epochs, label smoothing <ref type="bibr" target="#b32">[34]</ref> and mixup <ref type="bibr" target="#b38">[40]</ref> (following <ref type="bibr" target="#b12">[13]</ref>), and a data augmentation pipeline of Autoaugment <ref type="bibr" target="#b4">[5]</ref>, random cropping and flipping. RepVGG-B2 and its g2/g4 variants are trained in both settings. We test the speed of every model with a batch size of 128 on a 1080Ti GPU 4 by first feeding 50 batches to warm the hardware up, then 50 batches with time usage recorded. For the fair comparison, we test all the models on the same GPU, and all the conv-BN sequences of the baselines are also converted into a conv with bias (Eq. 3). <ref type="table">Table.</ref> 4 shows the favorable accuracy-speed tradeoff of RepVGG: RepVGG-A0 is 1.25% and 33% better than ResNet-18 in terms of accuracy and speed, RepVGG-A1 is 0.29%/64% better than ResNet-34, RepVGG-A2 is 0.17%/83% better than ResNet-50. With interleaved groupwise layers (g2/g4), the RepVGG models are further accelerated with reasonable accuracy decrease: RepVGG-B1g4 is 0.37%/101% better than ResNet-101, and RepVGG-B1g2 is impressively 2.66× as fast as ResNet-152 with the same accuracy. Though the number of parameters is not our primary concern, all the above RepVGG models are more parameter-efficient than ResNets. Compared to the classic VGG-16, RepVGG-B2 has only 58% parameters, runs 10% faster and shows 6.57% higher accuracy. Compared to the highest-accuracy (74.5%) VGG to the best of our knowledge trained with RePr <ref type="bibr" target="#b24">[26]</ref> (a pruning-based training method), RepVGG-B2 outperforms by 4.28% in accuracy.</p><p>Compared with the state-of-the-art baselines, RepVGG also shows favorable performance, considering its simplicity: RepVGG-A2 is 1.37%/59% better than EfficientNet-B0, RepVGG-B1 performs 0.39% better than RegNetX-3.2GF and runs slightly faster. Notably, RepVGG models reach above 80% accuracy with 200 epochs <ref type="table" target="#tab_5">(Table. 5</ref>), which is the first time for plain models to catch up with the state-of-the-arts, to the best of our knowledge. Compared to RegNetX-12GF, RepVGG-B3 runs 31% faster, which is impressive considering that RepVGG does not require a lot  of manpower to refine the design space like RegNet <ref type="bibr" target="#b26">[28]</ref>, and the architectural hyper-parameters are set casually. As two proxies of computational complexity, we count the theoretical FLOPs and Wino MULs as described in Sect. 2.4. For example, we found out that none of the conv in EfficientNet-B0/B3 is accelerated by Winograd algorithm. <ref type="table" target="#tab_4">Table. 4</ref> shows Wino MULs is a better proxy on GPU, e.g., ResNet-152 runs slower than VGG-16 with lower theoretical FLOPs but higher Wino MULs. Of course, the actual speed should always be the golden standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Structural Re-parameterization is the Key</head><p>In this subsection, we verify the significance of our structural re-parameterization technique <ref type="table" target="#tab_6">(Table. 6</ref>). All the models are trained from scratch for 120 epochs with the same simple training settings described above. First, we conduct ablation studies by removing the identity and/or 1 × 1 branch from every block of RepVGG-B0. With both branches removed, the training-time model degrades into an ordinary plain model and only achieves 72.39% accuracy. The accuracy is lifted to 73.15% with 1 × 1 or 74.79% with identity. The accuracy of the full featured RepVGG-B0 is 75.14%, which is 2.75% higher than the ordinary plain model. Seen from the inference speed of the training-time (i.e., not yet converted) models, removing the identity and 1 × 1 branches via structural re-parameterization brings significant speedup.</p><p>Then we construct a series of variants and baselines for comparison on RepVGG-B0 <ref type="table" target="#tab_7">(Table. 7</ref>). Again, all the models are trained from scratch in 120 epochs.</p><p>• Identity w/o BN removes the BN in identity branch.</p><p>• Post-addition BN removes the BN layers in the three branches and appends a BN layer after the addition. In other words, the position of BN is changed from preaddition to post-addition. • +ReLU in branches inserts ReLU into each branch (after BN and before addition). Since such a block cannot be converted into a single conv layer, it is of no practical use, and we merely desire to see whether more nonlinearity will bring higher performance. • DiracNet <ref type="bibr" target="#b37">[39]</ref> adopts a well-designed reparameterization of conv kernels, as introduced in Sect. 2.2. We use its official PyTorch code to build the layers to replace the original 3 × 3 conv. • Trivial Re-param is a simpler re-parameterization of conv kernels by directly adding an identity kernel to the 3 × 3 kernel, which can be viewed a degraded version of DiracNet (Ŵ = I + W <ref type="bibr" target="#b37">[39]</ref>). • Asymmetric Conv Block (ACB) <ref type="bibr" target="#b9">[10]</ref> can be viewed as another form of structural re-parameterization. We compare with ACB to see whether the improvement of our structural re-parameterization is due to the component-level over-parameterization (i.e., the extra parameters making every 3 × 3 conv stronger). • Residual Reorg builds each stage by re-organizing it in a ResNet-like manner (2 layers per block). Specifically, the resultant model has one 3×3 layer in the first and last stages and 2, 3, 8 residual blocks in stage2, 3, 4, and uses shortcuts just like ResNet-18/34. We reckon the superiority of structural re-param over DiractNet and Trivial Re-param lies in the fact that the former relies on the actual dataflow through a concrete structure with nonlinear behavior (BN), while the latter merely uses another mathematical expression of conv kernels. The former "re-param" means "using the params of a structure to parameterize another structure", but the latter means "computing the params first with another set of params, then using them for other computations". With nonlinear components like a training-time BN, the former cannot be approximated by the latter. As evidences, the accuracy is decreased  The comparison with ACB suggests the success of RepVGG should not be simply attributed to the effect of over-parameterization of every component, since ACB uses more parameters but yields inferior performance. As a double check, we replace every 3 × 3 conv of ResNet-50 with a RepVGG block and train from scratch for 120 epochs. The accuracy is 76.34%, which is merely 0.03% higher than the ResNet-50 baseline, suggesting that RepVGGstyle structural re-parameterization is not a generic overparameterization technique, but a methodology critical for training powerful plain ConvNets. Compared to Residual Reorg, a real residual network with the same number of 3 × 3 conv and additional shortcuts for both training and inference, RepVGG outperforms by 0.58%, which is not surprising since RepVGG has far more branches. For example, the branches make stage4 of RepVGG an ensemble of 2 × 3 15 = 2.8 × 10 7 models <ref type="bibr" target="#b34">[36]</ref>, while the number for Residual Reorg is 2 8 = 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic Segmentation</head><p>We verify the generalization performance of ImageNetpretrained RepVGG for semantic segmentation on Cityscapes <ref type="bibr" target="#b3">[4]</ref>  <ref type="table" target="#tab_8">(Table. 8</ref>). We use the PSPNet <ref type="bibr" target="#b40">[42]</ref> framework, a poly learning rate policy with base of 0.01 and power of 0.9, weight decay of 10 −4 and a global Following the official PSPNet-50/101 <ref type="bibr" target="#b40">[42]</ref> which uses dilated conv in the last two stages of ResNet-50/101, we also make all the 3 × 3 conv layers in the last two stages of RepVGG-B1g2/B2 dilated. However, the current inefficient implementation of 3 × 3 dilated conv (though the FLOPs is the same as 3 × 3 regular conv) slows down the inference. For the ease of comparison, we build another two PSPNets (denoted by fast) with dilation only in the last 5 layers (i.e., the last 4 layers of stage4 and the only layer of stage5), so that the PSPNets run slightly faster than the ResNet-50/101-backbone counterparts. RepVGG backbones outperform ResNet-50 and ResNet-101 by 1.71% and 1.01% respectively in mean IoU with higher speed, and RepVGG-B1g2-fast outperforms the ResNet-101 backbone by 0.37 in mIoU and runs 62% faster. Interestingly, dilation seems more effective for larger models, as using more dilated conv layers does not improve the performance compared to RepVGG-B1g2-fast, but raises the mIoU of RepVGG-B2 by 1.05% with reasonable slowdown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Limitations</head><p>RepVGG models are fast, simple and practical ConvNets designed for the maximum speed on GPU and specialized hardware, less concerning the number of parameters. They are more parameter-efficient than ResNets but may be less favored than the mobile-regime models like MobileNets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b14">15]</ref> and ShuffleNets <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b22">24]</ref> for low-power devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed RepVGG, a simple architecture with a stack of 3 × 3 conv and ReLU, which is especially suitable for GPU and specialized inference chips. With our structural re-parameterization method, it reaches over 80% top-1 accuracy on ImageNet and shows favorable speed-accuracy trade-off compared to the state-of-the-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Some recent architectures are based on * This work is supported by The National Key Research and Development Program of China (No. 2017YFA0700800), the National Natural Science Foundation of China (No.61925107, No.U1936202) and Beijing Academy of Artificial Intelligence (BAAI). Xiaohan Ding is funded by the Baidu Scholarship Program 2019. This work is done during Xiaohan Ding and Ningning Ma's internship at MEGVII Technology. † Corresponding author. Top-1 accuracy on ImageNet vs. actual speed. Left: lightweight and middleweight RepVGG and baselines trained in 120 epochs. Right: heavyweight models trained in 200 epochs. The speed is tested on the same 1080Ti with a batch size of 128, full precision (fp32), single crop, and measured in examples/second. The input resolution is 300</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, they arXiv:2101.03697v3 [cs.CV] 29 Mar 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>C 2 ,</head><label>2</label><figDesc>bn(M * W, µ, σ, γ, β) :,i,:,: = (M * W ) :,i,:,: + b i . (4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Structural re-parameterization of a RepVGG block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Speed test with varying kernel size and batch size = 32, input channels = output channels = 2048, resolution = 56×56, stride = 1 on NVIDIA 1080Ti. The results of time usage are average of 10 runs after warming up the hardware.</figDesc><table><row><cell>Kernel</cell><cell>Theoretical</cell><cell>Time</cell><cell>Theoretical</cell></row><row><cell>size</cell><cell>FLOPs (B)</cell><cell>usage (ms)</cell><cell>TFLOPS</cell></row><row><cell>1 × 1</cell><cell>420.9</cell><cell>84.5</cell><cell>9.96</cell></row><row><cell>3 × 3</cell><cell>3788.1</cell><cell>198.8</cell><cell>38.10</cell></row><row><cell>5 × 5</cell><cell>10522.6</cell><cell>2092.5</cell><cell>10.57</cell></row><row><cell>7 × 7</cell><cell>20624.4</cell><cell>4394.3</cell><cell>9.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Architectural specification of RepVGG. Here 2 × 64a means stage2 has 2 layers each with 64a channels.</figDesc><table><row><cell cols="3">Stage Output size RepVGG-A</cell><cell>RepVGG-B</cell></row><row><cell>1</cell><cell cols="3">112 × 112 1 × min(64, 64a) 1 × min(64, 64a)</cell></row><row><cell>2</cell><cell>56 × 56</cell><cell>2 × 64a</cell><cell>4 × 64a</cell></row><row><cell>3</cell><cell>28 × 28</cell><cell>4 × 128a</cell><cell>6 × 128a</cell></row><row><cell>4</cell><cell>14 × 14</cell><cell>14 × 256a</cell><cell>16 × 256a</cell></row><row><cell>5</cell><cell>7 × 7</cell><cell>1 × 512b</cell><cell>1 × 512b</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>RepVGG models defined by multipliers a and b.</figDesc><table><row><cell>Name</cell><cell>Layers of each stage</cell><cell>a</cell><cell>b</cell></row><row><cell>RepVGG-A0</cell><cell>1, 2, 4, 14, 1</cell><cell cols="2">0.75 2.5</cell></row><row><cell>RepVGG-A1</cell><cell>1, 2, 4, 14, 1</cell><cell>1</cell><cell>2.5</cell></row><row><cell>RepVGG-A2</cell><cell>1, 2, 4, 14, 1</cell><cell cols="2">1.5 2.75</cell></row><row><cell>RepVGG-B0</cell><cell>1, 4, 6, 16, 1</cell><cell>1</cell><cell>2.5</cell></row><row><cell>RepVGG-B1</cell><cell>1, 4, 6, 16, 1</cell><cell>2</cell><cell>4</cell></row><row><cell>RepVGG-B2</cell><cell>1, 4, 6, 16, 1</cell><cell>2.5</cell><cell>5</cell></row><row><cell>RepVGG-B3</cell><cell>1, 4, 6, 16, 1</cell><cell>3</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results trained on ImageNet with simple data augmentation in 120 epochs. The speed is tested on 1080Ti with a batch size of 128, full precision (fp32), and measured in examples/second. We count the theoretical FLOPs and Wino MULs as described in Sect. 2.4. The baselines are our implementations with the same training settings.</figDesc><table><row><cell>Model</cell><cell>Top-1 acc</cell><cell>Speed</cell><cell>Params (M)</cell><cell>Theo FLOPs (B)</cell><cell>Wino MULs (B)</cell></row><row><cell>RepVGG-A0</cell><cell cols="2">72.41 3256</cell><cell>8.30</cell><cell>1.4</cell><cell>0.7</cell></row><row><cell>ResNet-18</cell><cell cols="3">71.16 2442 11.68</cell><cell>1.8</cell><cell>1.0</cell></row><row><cell>RepVGG-A1</cell><cell cols="3">74.46 2339 12.78</cell><cell>2.4</cell><cell>1.3</cell></row><row><cell>RepVGG-B0</cell><cell cols="3">75.14 1817 14.33</cell><cell>3.1</cell><cell>1.6</cell></row><row><cell>ResNet-34</cell><cell cols="3">74.17 1419 21.78</cell><cell>3.7</cell><cell>1.8</cell></row><row><cell>RepVGG-A2</cell><cell cols="3">76.48 1322 25.49</cell><cell>5.1</cell><cell>2.7</cell></row><row><cell cols="3">RepVGG-B1g4 77.58 868</cell><cell>36.12</cell><cell>7.3</cell><cell>3.9</cell></row><row><cell cols="3">EfficientNet-B0 75.11 829</cell><cell>5.26</cell><cell>0.4</cell><cell>-</cell></row><row><cell cols="3">RepVGG-B1g2 77.78 792</cell><cell>41.36</cell><cell>8.8</cell><cell>4.6</cell></row><row><cell>ResNet-50</cell><cell cols="2">76.31 719</cell><cell>25.53</cell><cell>3.9</cell><cell>2.8</cell></row><row><cell>RepVGG-B1</cell><cell cols="2">78.37 685</cell><cell>51.82</cell><cell>11.8</cell><cell>5.9</cell></row><row><cell cols="3">RegNetX-3.2GF 77.98 671</cell><cell>15.26</cell><cell>3.2</cell><cell>2.9</cell></row><row><cell cols="3">RepVGG-B2g4 78.50 581</cell><cell>55.77</cell><cell>11.3</cell><cell>6.0</cell></row><row><cell>ResNeXt-50</cell><cell cols="2">77.46 484</cell><cell>24.99</cell><cell>4.2</cell><cell>4.1</cell></row><row><cell>RepVGG-B2</cell><cell cols="2">78.78 460</cell><cell>80.31</cell><cell>18.4</cell><cell>9.1</cell></row><row><cell>ResNet-101</cell><cell cols="2">77.21 430</cell><cell>44.49</cell><cell>7.6</cell><cell>5.5</cell></row><row><cell>VGG-16</cell><cell cols="3">72.21 415 138.35</cell><cell>15.5</cell><cell>6.9</cell></row><row><cell>ResNet-152</cell><cell cols="2">77.78 297</cell><cell>60.11</cell><cell>11.3</cell><cell>8.1</cell></row><row><cell>ResNeXt-101</cell><cell cols="2">78.42 295</cell><cell>44.10</cell><cell>8.0</cell><cell>7.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results on ImageNet trained in 200 epochs with Autoaugment [5], label smoothing and mixup.</figDesc><table><row><cell>Model</cell><cell cols="4">Acc Speed Params FLOPs MULs</cell></row><row><cell cols="2">RepVGG-B2g4 79.38 581</cell><cell>55.77</cell><cell>11.3</cell><cell>6.0</cell></row><row><cell cols="2">RepVGG-B3g4 80.21 464</cell><cell>75.62</cell><cell>16.1</cell><cell>8.4</cell></row><row><cell>RepVGG-B3</cell><cell cols="2">80.52 363 110.96</cell><cell>26.2</cell><cell>12.9</cell></row><row><cell cols="2">RegNetX-12GF 80.55 277</cell><cell>46.05</cell><cell>12.1</cell><cell>10.9</cell></row><row><cell cols="2">EfficientNet-B3 79.31 224</cell><cell>12.19</cell><cell>1.8</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies with 120 epochs on RepVGG-B0. The inference speed w/o re-param (examples/s) is tested with the models before conversion (batch size=128). Note again that all the models have the same final structure.</figDesc><table><row><cell>Identity branch</cell><cell>1 × 1 branch</cell><cell>Accuracy</cell><cell>Inference speed w/o re-param</cell></row><row><cell></cell><cell></cell><cell>72.39</cell><cell>1810</cell></row><row><cell></cell><cell></cell><cell>74.79</cell><cell>1569</cell></row><row><cell></cell><cell></cell><cell>73.15</cell><cell>1230</cell></row><row><cell></cell><cell></cell><cell>75.14</cell><cell>1061</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison with variants and baselines on RepVGG-B0 trained in 120 epochs. RepVGG block can be equivalently converted into a single conv for inference, the inferencetime equivalence does not imply the training-time equivalence, as we cannot construct a conv layer to have the same training-time behavior as a RepVGG block.</figDesc><table><row><cell>Variant and baseline</cell><cell>Accuracy</cell></row><row><cell>Identity w/o BN</cell><cell>74.18</cell></row><row><cell>Post-addition BN</cell><cell>73.52</cell></row><row><cell>Full-featured reparam</cell><cell>75.14</cell></row><row><cell>+ReLU in branch</cell><cell>75.69</cell></row><row><cell>DiracNet [39]</cell><cell>73.97</cell></row><row><cell>Trivial Re-param</cell><cell>73.51</cell></row><row><cell>ACB [10]</cell><cell>73.58</cell></row><row><cell>Residual Reorg</cell><cell>74.56</cell></row><row><cell cols="2">by removing the BN and improved by adding ReLU. In</cell></row><row><cell>other words, though a</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Semantic segmentation on Cityscapes<ref type="bibr" target="#b3">[4]</ref> tested on the validation subset. The speed (examples/second) is tested with a batch size of 16, full precision (fp32), and input resolution of 713×713 on the same 1080Ti GPU. GPUs for 40 epochs. For the fair comparison, we only change the ResNet-50/101 backbone to RepVGG-B1g2/B2 and keep other settings identical.</figDesc><table><row><cell>Backbone</cell><cell cols="3">Mean IoU Mean pixel acc Speed</cell></row><row><cell>RepVGG-B1g2-fast</cell><cell>78.88</cell><cell>96.19</cell><cell>10.9</cell></row><row><cell>ResNet-50</cell><cell>77.17</cell><cell>95.99</cell><cell>10.4</cell></row><row><cell>RepVGG-B1g2</cell><cell>78.70</cell><cell>96.27</cell><cell>8.0</cell></row><row><cell>RepVGG-B2-fast</cell><cell>79.52</cell><cell>96.36</cell><cell>6.9</cell></row><row><cell>ResNet-101</cell><cell>78.51</cell><cell>96.30</cell><cell>6.7</cell></row><row><cell>RepVGG-B2</cell><cell>80.57</cell><cell>96.50</cell><cell>4.5</cell></row><row><cell>batch size of 16 on 8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, a network topology only focuses on how the components connect to others, an architecture refers to the topology together with the specification of components like depth and width, and a structure may refer to any component or part of the architecture.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As a common practice, we count a multiply-add as a single operation when counting the theoretical FLOPs, but hardware vendors like NVIDIA usually count it as two operations when reporting the TFLOPS.<ref type="bibr" target="#b2">3</ref> Our results are manually tested operator-by-operator with cuDNN 7.5.0, 1080Ti. For each stride-1 3 × 3 conv, we test its time usage along with a stride-2 counterpart of the same FLOPs. We assume the former uses F (2 × 2, 3 × 3) Winograd if the latter runs significantly slower. Such a testing method is approximate hence the results are for reference only.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use such a batch size because it is large enough to realize 100% GPU utilization of every tested model to simulate the actual application scenario pursuing the maximum QPS (Queries Per Second), and our GPU memory is insufficient for EfficientNet-B3 with a batch size of 256.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do-conv: Depthwise over-parameterized convolutional layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12030</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Centripetal sgd for pruning very deep convolutional networks with complicated structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4943" to="4953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximated oracle filter pruning for destructive cnn width optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auto-balanced filter pruning for efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Expandnets: Linear over-parameterization to train compact convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4013" to="4021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Pruning filters for efficient convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with neural networks without skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamila</forename><surname>Oyebade K Oyedotun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ottersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1756" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Repr: Improved training of convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Dinei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Florêncio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10666" to="10675" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/examples/blob/master/imagenet/main.py" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5393" to="5402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00388</idno>
		<title level="m">Diracnets: Training very deep neural networks without skip-connections</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
