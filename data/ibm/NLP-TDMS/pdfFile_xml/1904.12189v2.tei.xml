<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning metrics for persistence-based summaries and applications for graph classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Learning metrics for persistence-based summaries and applications for graph classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently a new feature representation framework based on a topological tool called persistent homology (and its persistence diagram summary) has gained much momentum. A series of methods have been developed to map a persistence diagram to a vector representation so as to facilitate the downstream use of machine learning tools. In these approaches, the importance (weight) of different persistence features are usually pre-set. However often in practice, the choice of the weight-function should depend on the nature of the specific data at hand. It is thus highly desirable to learn a best weight-function (and thus metric for persistence diagrams) from labelled data. We study this problem and develop a new weighted kernel, called WKPI, for persistence summaries, as well as an optimization framework to learn the weight (and thus kernel). We apply the learned kernel to the challenging task of graph classification, and show that our WKPI-based classification framework obtains similar or (sometimes significantly) better results than the best results from a range of previous graph classification frameworks on benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years a new data analysis methodology based on a topological tool called persistent homology has started to attract momentum. The persistent homology is one of the most important developments in the field of topological data analysis, and there have been fundamental developments both on the theoretical front (e.g, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref>), and on algorithms / implementations (e.g, <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>). On the high level, given a domain X with a function f : X → R on it, the persistent homology summarizes "features" of X across multiple scales simultaneously in a single summary called the persistence diagram (see the second picture in <ref type="figure" target="#fig_0">Figure 1</ref>). A persistence diagram consists of a multiset of points in the plane, where each point p = (b, d) intuitively corresponds to the birth-time (b) and death-time (d) of some (topological) features of X w.r.t. f . Hence it provides a concise representation of X, capturing multi-scale features of it simultaneously. Furthermore, the persistent homology framework can be applied to complex data (e.g, 3D shapes, or graphs), and different summaries could be constructed by putting different descriptor functions on input data.</p><p>Due to these reasons, a new persistence-based feature vectorization and data analysis framework ( <ref type="figure" target="#fig_0">Figure  1</ref>) has become popular. Specifically, given a collection of objects, say a set of graphs modeling chemical compounds, one can first convert each shape to a persistence-based representation. The input data can now be viewed as a set of points in a persistence-based feature space. Equipping this space with appropriate distance or kernel, one can then perform downstream data analysis tasks (e.g, clustering).</p><p>The original distances for persistence diagram summaries unfortunately do not lend themselves easily to machine learning tasks. Hence in the last few years, starting from the persistence landscape <ref type="bibr" target="#b7">[8]</ref>, there have been a series of methods developed to map a persistence diagram to a vector representation to facilitate machine learning tools <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref>. Recent ones include Persistence Scale-Space kernel <ref type="bibr" target="#b43">[44]</ref>, Persistence Images <ref type="bibr" target="#b0">[1]</ref>, Persistence Weighted Gaussian kernel (PWGK) <ref type="bibr" target="#b33">[34]</ref>, Sliced Wasserstein kernel <ref type="bibr" target="#b12">[13]</ref>, and Persistence Fisher kernel <ref type="bibr" target="#b34">[35]</ref>.</p><p>In these approaches, when computing the distance or kernel between persistence summaries, the importance (weight) of different persistence features are often pre-determined. In persistence images <ref type="bibr" target="#b0">[1]</ref> and PWGK <ref type="bibr" target="#b33">[34]</ref>, the importance of having a weight-function for the birth-death plane (containing the persistence points) has been emphasized and explicitly included in the formulation of their kernels. However, before using these kernels, the weight-function needs to be pre-set.</p><p>On the other hand, as recognized by <ref type="bibr" target="#b26">[27]</ref>, the choice of the weight-function should depend on the nature of the specific type of data at hand. For example, for the persistence diagrams computed from atomic configurations of molecules, features with small persistence could capture the local packing patterns which are of utmost importance and thus should be given a larger weight; while in many other scenarios, small persistence leads to noise with low importance. However, in general researchers performing data analysis tasks may not have such prior insights on input data. Thus it is natural and highly desirable to learn a best weight-function from labelled data.</p><p>Our work. We study the problem of learning an appropriate metric (kernel) for persistence summaries from labelled data, and apply the learnt kernel to the challenging graph classification task.</p><p>(1) Metric learning for persistence summaries: We propose a new weighted-kernel (called WKPI), for persistence summaries based on persistence images representations. Our WKPI kernel is positive semi-definite and its induced distance is stable. A weight-function used in this kernel directly encodes the importance of different locations in the persistence diagram. We next model the metric learning problem for persistence summaries as the problem of learning (the parameters of) this weight-function from a certain function class. In particular, the metric-learning is formulated as an optimization problem over a specific cost function we propose. This cost function has a simple matrix view which helps both conceptually clarify its meaning and simplify the implementation of its optimization.</p><p>(2) Graph classification application: Given a set of objects with class labels, we first learn a best WKPI-kernel as described above, and then use the learned WKPI to further classify objects. We implemented this WKPI-classification framework, and apply it to a range of graph data sets. Graph classification is an important problem, and there has been a large literature on developing effective graph representations (e.g, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b40">41]</ref>, including the very recent persistent-homology enhanced WL-kernel <ref type="bibr" target="#b44">[45]</ref>), and graph neural networks (e.g, graph neural networks <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32]</ref>) to classify graphs. The problem is challenging as graph data are less structured. We perform our WKPI-classification framework on various benchmark graph data sets as well as new neuron-cell data sets. Our learnt WKPI performs consistently better than other persistence-based kernels. Most importantly, when compared with existing state-of-the-art graph classification frameworks, our framework shows similar or (sometimes significantly) better performance in almost all cases than the best results by existing approaches. We note that <ref type="bibr" target="#b26">[27]</ref> is the first to recognize the importance of using labelled data to learn a task-optimal representation of topological signatures. They developed an end-to-end deep neural network for this purpose, using a novel and elegant design of the input layer to implicitly learn a task-specific representation. Very recently, in a parallel and independent development of our work, Carrière et al. <ref type="bibr" target="#b11">[12]</ref> built an interesting new neural network based on the DeepSet architecture <ref type="bibr" target="#b51">[52]</ref>, which can achieve an end-to-end learning for multiple persistence representations in a unified manner. Compared to these developments, we instead explicitly formulate the metric-learning problem for persistence-summaries, and decouple the metric-learning (which can also be viewed as representation-learning) component from the downstream data analysis tasks. Also as shown in Section 4, our WKPI-classification framework (using SVM) achieves better results on graph classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Persistence-based framework</head><p>We first give an informal description of persistent homology below. See <ref type="bibr" target="#b22">[23]</ref> for more detailed exposition on the subject. Suppose we are given a shape X (in our later graph classification application, X is a graph). Imagine we inspect X through a filtration of X, which is a sequence of growing subsets of X: X 1 ⊆ X 2 ⊆ · · · ⊆ X n = X. As we scan X, sometimes a new feature appears in X i , and sometimes an existing feature disappears upon entering X j . Using the topological object called homology classes to describe these features (intuitively components, independent loops, voids, and their high dimensional counter-parts), the birth and death of topological features can be captured by the persistent homology, in the form of a persistence diagram DgX. Specifically, for each dimension k, Dg k X consists of a multi-set of points in the plane (which we call the birth-death plane R 2 ): each point (b, d) in it, called a persistence-point, indicates that a certain k-dimensional homological feature is created upon entering X b and destroyed upon entering X d . In the remainder of the paper, we often omit the dimension k for simplicity: when multiple dimensions are used for persistence features, we will apply our construction to each dimension and concatenate the resulting vector representations.</p><p>A common way to obtain a meaningful filtration of X is via the sublevel-set filtration induced by a descriptor function f on X. More specifically, given a function f : X → R, let X ≤a := {x ∈ X | f (x) ≤ a} be its sublevel-set at a. Let a 1 &lt; a 2 &lt; · · · &lt; a n be n real values. The sublevel-set filtration w.r.t. f is: X ≤a 1 ⊆ X ≤a 2 ⊆ · · · ⊆ X ≤an ; and its persistence diagram is denoted by Dgf . Each persistence-point p = (a i , a j ) ∈ Dgf indicates the function values when some topological features are created (when entering X ≤a i ) and destroyed (in X ≤a j ), and the persistence of this feature is its life-time pers(p) = |a j − a i |. See <ref type="figure" target="#fig_1">Figure 2</ref> (a) for a simple example where X = R. If one sweeps X top-down in decreasing function values, one gets the persistence diagram induced by the super-levelset filtration of X w.r.t. f in an analogous way. Finally, if one tracks the change of topological features in the levelset f −1 (a), one obtains the so-called levelset zigzag persistence <ref type="bibr" target="#b9">[10]</ref> (which contains the information captured by the extended persistence <ref type="bibr" target="#b17">[18]</ref>).</p><p>Graph Setting. Given a graph G = (V, E), a descriptor function f defined on V or E will induce a filtration and its persistence diagrams. Suppose f : V → R is defined on the node set of G (e.g, the node degree). Then we can extend f to E by setting f (u, v) = max{f (u), f (v)}, and the sublevel-set at a is defined as</p><formula xml:id="formula_0">G ≤a := {σ ∈ V ∪ E | f (σ) ≤ a}.</formula><p>Similarly, if we are given f : E → R, then we can extend f to V by setting f (u) = min u∈e,e∈E f (e). When scanning G via the sublevel-set filtration of f , connected components in the swept subgraphs will be created and merged, and new cycles will be created. The formal events are encoded in the 0-dimensional persistence diagram. The the 1-dimensional features (cycles), however, we note that cycles created will never be killed, as they are present in the total space X = G. To this end, we use the so-called extended persistence introduced in <ref type="bibr" target="#b17">[18]</ref> which can record information of cycles. Now given a collection of shapes Ξ, we can compute a persistence diagram DgX for each X ∈ Ξ, which maps the set Ξ to a set of points in the space of persistence diagrams. There are natural distances defined for persistence diagrams, including the bottleneck distance and the Wasserstein distance, both of which have been well studied (e.g, stability under them <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref>) with efficient implementations available <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. However, to facilitate downstream machine learning tasks, it is desirable to further map the persistence diagrams to another "vector" representation. Below we introduce one such representation, called the persistence images <ref type="bibr" target="#b0">[1]</ref>, as our new kernel is based on it.</p><p>Let A be a persistence diagram (containing a multiset of persistence-points). Following <ref type="bibr" target="#b0">[1]</ref>, set T : R 2 → R 2 to be the linear transformation <ref type="bibr" target="#b0">1</ref>  ).</p><formula xml:id="formula_1">where for each (x, y) ∈ R 2 , T (x, y) = (x, y − x). Let T (A) be the transformed diagram of A. Let φ u : R 2 → R be a differentiable</formula><p>Definition 2.1 ([1]) Let α : R 2 → R be a non-negative weight-function for the persistence plane R 2 . Given a persistence diagram A, its persistence surface ρ A :</p><formula xml:id="formula_2">R 2 → R (w.r.t. α) is defined as: for any z ∈ R 2 , ρ A (z) = u∈T (A) α(u)φ u (z).</formula><p>The persistence image is a discretization of the persistence surface. Specifically, fix a grid on a rectangular region in the plane with a collection P of N rectangles (pixels). The persistence image for a diagram A is PI A = { PI[p] } p∈P consists of N numbers (i.e, a vector in R N ), one for each pixel p in the grid P with PI[p] := p ρ A dydx.</p><p>The persistence image can be viewed as a vector in R N . One can then compute distance between two persistence diagrams A 1 and A 2 by the L 2 -distance PI 1 − PI 2 2 between their persistence images (vectors) PI 1 and PI 2 . The persistence images have several nice properties, including stability guarantees; see <ref type="bibr" target="#b0">[1]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Metric learning frameworks</head><p>Suppose we are given a set of n objects Ξ (sampled from a hidden data space S), classified into k classes. We want to use these labelled data to learn a good distance for (persistence image representations of) objects from Ξ which hopefully is more appropriate at classifying objects in the data space S. To do so, below we propose a new persistence-based kernel for persistence images, and then formulate an optimization problem to learn the best weight-function so as to obtain a good distance metric for Ξ (and data space S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Weighted persistence image kernel (WKPI)</head><p>From now on, we fix the grid P (of size N ) to generate persistence images (so a persistence image is a vector in R N ). Let p s be the center of the s-th pixel p s in P, for s ∈ {1, 2, · · · , N }. We now propose a new kernel for persistence images. A weight-function refers to a non-negative real-valued function on R 2 . Definition 3.1 Let ω : R 2 → R be a weight-function. Given two persistence images PI and PI , the</p><formula xml:id="formula_3">(ω-)weighted persistence image kernel (WKPI) is defined as: k w (PI, PI ) := N s=1 ω(p s )e − (PI(s)−PI (s)) 2 2σ 2 .</formula><p>Remark 0: We could use the persistence surfaces (instead of persistence images) to define the kernel (with the summation replaced by an integral). Since for computational purpose, one still needs to approximate the integral in the kernel via some discretization, we choose to present our work using persistence images directly. Our Lemma 3.2 and Theorem 3.4 still hold (with slightly different stability bound) if we use the kernel defined for persistence surfaces. Remark 1: One can choose the weight-function from different function classes. Two popular choices are: mixture of m 2D Gaussians; and degree-d polynomials on two variables. Remark 2: There are other natural choices for defining a weighted kernel for persistence images. For example,</p><formula xml:id="formula_4">we could use k(PI, PI ) = N s=1 e − ω(ps)(PI(s)−PI (s)) 2 2σ 2</formula><p>, which we refer this as altWKPI. Alternatively, one could use the weight function used in PWGK kernel <ref type="bibr" target="#b33">[34]</ref> directly. Indeed, we have implemented all these choices, and our experiments show that our WKPI kernel leads to better results than these choices for almost all datasets (see Appendix Section 2). In addition, note that PWGK kernel <ref type="bibr" target="#b33">[34]</ref> contains cross terms ω(x) · ω(y) in its formulation, meaning that there are quadratic number of terms (w.r.t the number of persistence points) to calculate the kernel, making it more expensive to compute and learn for complex objects (e.g, for the neuron data set, a single neuron tree could produce a persistence diagrams with hundreds of persistence points). The rather simple proof of the above lemma is in Appendix Section 1.1. By Lemma 3.2, the WKPI kernel gives rise to a Hilbert space. We can now introduce the WKPI-distance, which is the pseudo-metric induced by the inner product on this Hilbert space. Definition 3.3 Given two persistence diagrams A and B, let PI A and PI B be their corresponding persistence images. Given a weight-function ω :</p><formula xml:id="formula_5">R 2 → R, the (ω-weighted) WKPI-distance is: D ω (A, B) := k w (PI A , PI A ) + k w (PI B , PI B ) − 2k w (PI A , PI B ).</formula><p>Stability of WKPI-distance. Given two persistence diagrams A and B, two traditional distances between them are the bottleneck distance d B (A, B) and the p-th Wasserstein distance d W,p (A, B). Stability of these two distances w.r.t. changes of input objects or functions defined on them have been studied <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref>. Similar to the stability study on persistence images, below we prove WKPI-distance is stable w.r.t. small perturbation in persistence diagrams as measured by d W,1 . (Very informally, view two persistence diagrams A and B as two (appropriate) measures (with special care taken to the diagonals), and d W,1 (A, B) is roughly the "earth-mover" distance between them to convert the measure corresponding to A to that for B.)</p><p>To simplify the presentation of Theorem 3.4, we use unweighted persistence images w.r.t. Gaussian, meaning in Definition 2.1, (1) the weight function α is the constant function α = 1; and (2) the distribution</p><formula xml:id="formula_6">φ u is the Gaussian φ u (z) = 1 2πτ 2 e − z−u 2 2τ 2</formula><p>. (Our result below can be extended to the case where φ u is not Gaussian.) The proof of the theorem below follows from results of <ref type="bibr" target="#b0">[1]</ref> and can be found in Appendix Section 1.2.</p><formula xml:id="formula_7">Theorem 3.4 Given a weight-function ω : R 2 → R, set c w = ω ∞ = sup z∈R 2 ω(z).</formula><p>Given two persistence diagrams A and B, with corresponding persistence images PI A and PI B , we have that:</p><formula xml:id="formula_8">D ω (A, B) ≤ 20cw π · 1 σ·τ · d W,1 (A, B),</formula><p>where σ is the width of the Gaussian used to define our WKPI kernel (Def. 3.1), and τ is that for the Gaussian φ u to define persistence images (Def. 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remarks:</head><p>We can obtain a more general bound for the case where the distribution φ u is not Gaussian. Furthermore, we can obtain a similar bound when our WKPI-kernel and its induced WKPI-distance is defined using persistence surfaces instead of persistence images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization problem for metric-learning</head><p>Suppose we are given a collection of objects Ξ = {X 1 , . . . , X n } (sampled from some hidden data space S), already classified (labeled) to k classes C 1 , . . . , C k . In what follows, we say that i ∈ C j if X i has class-label j. We first compute the persistence diagram A i for each object X i ∈ Ξ. (The precise filtration we use to do so will depend on the specific type of objects. Later in Section 4, we will describe filtrations used for graph data). Let {A 1 , . . . , A n } be the resulting set of persistence diagrams. Given a weight-function ω, its induced WKPI-distance between A i and A j can also be thought of as a distance for the original objects X i and X j ; that is, we can set D ω (X i , X j ) := D ω (A i , A j ). Our goal is to learn a good distance metric for the data space S (where Ξ are sampled from) from the labels. We will formulate this as learning a best weight-function ω * so that its induced WKPI-distance fits the class-labels of X i 's best. Specifically, for any t ∈ {1, 2, · · · , k}, set:</p><formula xml:id="formula_9">cost ω (t, t) = i,j∈Ct D ω 2 (A i , A j ); and cost ω (t, ·) = i∈Ct,j∈{1,2,··· ,n} D ω 2 (A i , A j ).</formula><p>Intuitively, cost ω (t, t) is the total in-class (square) distances for C t ; while cost ω (t, ·) is the total distance from objects in class C t to all objects in Ξ. A good metric should lead to relatively smaller distance between objects from the same class, but larger distance between objects from different classes. We thus propose the following optimization problem, which is related to k-way spectral clustering where the distance for an edge cost(t,·) . The optimal distance problem aims to find the best weight-function ω * from a certain function class F so that the total-cost is minimized; that is: T C * = min ω∈F T C(ω); and ω * = argmin ω∈F T C(ω).</p><formula xml:id="formula_10">(A i , A j ) is D 2 ω (A i , A j ):</formula><p>Matrix view of optimization problem. We observe that our cost function can be re-formulated into a matrix form. This provides us with a perspective from the Laplacian matrix of certain graphs to understand the cost function, and helps to simplify the implementation of our optimization problem, as several programming languages popular in machine learning (e.g Python and Matlab) handle matrix operations more efficiently (than using loops). More precisely, recall our input is a set Ξ of n objects with labels from k classes. We set up the following matrices:</p><formula xml:id="formula_11">L = G − Λ; Λ = Λ ij n×n , where Λ ij = D ω 2 (A i , A j ) for i, j ∈ {1, 2, · · · , n}; G = g ij n×n , where g ij = n =1 Λ i if i = j 0 if i = j H = h ti k×n where h ti =    1 √ costω(t,·) i ∈ C t 0 otherwise</formula><p>Viewing Λ as distance matrix of objects {X 1 , . . . , X n }, L is then its Laplacian matrix. We have the following main theorem, which essentially is similar to the trace-minimization view of k-way spectral clustering (see e.g, Section 6.5 of <ref type="bibr" target="#b30">[31]</ref>). The proof for our specific setting is in Appendix 1.3. Note that all matrices, L, G, Λ, and H, are dependent on the (parameters of) weight-function ω, and in the following corollary of Theorem 3.6, we use the subscript of ω to emphasize this dependence.</p><formula xml:id="formula_12">Corollary 3.7 The Optimal distance problem is equivalent to min ω k − Tr(H ω L ω H T ω ) , subject to H ω G ω H T ω = I.</formula><p>Solving the optimization problem. In our implementation, we use (stochastic) gradient descent to find a (locally) optimal weight-function ω * for the minization problem. Specifically, given a collection of objects Ξ with labels from k classes, we first compute their persistence diagrams via appropriate filtrations, and obtain a resulting set of persistence diagrams {A 1 , . . . , A n }. We then aim to find the best parameters for the weight-function ω * to minimize T r(HLH T ) = k t=1 h t Lh T t subject to HGH T = I (via Corollary 3.7). For example, assume that the weight-function ω is from the class F of mixture of m number of 2D non-negatively weighted (spherical) Gaussians. Each weight-function ω :</p><formula xml:id="formula_13">R 2 → R ∈ F is thus determined by 4m parameters {x r , y r , σ r , w r | r ∈ {1, 2, · · · , m}} with ω(z) = w r e − (zx−xr ) 2 +(zy −yr ) 2 σ 2 r</formula><p>. We then use (stochastic) gradient decent to find the best parameters to minimize T r(HLH T ) subject to HGH T = I. Note that the set of persistence diagrams / images will be fixed through the optimization process.</p><p>From the proof of Theorem 3.6 (in Appendix 1.3), it turns out that condition HGH T = I is satisfied as long as the multiplicative weight w r of each Gaussian in the mixture is non-negative. Hence during the gradient descent, we only need to make sure that this holds 2 . It is easy to write out the gradient of T C(ω) w.r.t. each parameter {x r , y r , σ r , w r | r ∈ {1, 2, · · · , m}} in matrix form. For example,</p><formula xml:id="formula_14">∂T C(ω) ∂xr = −( k t=1 ∂ht ∂xr Lh T t + h t ∂L ∂xr h T t + h t L ∂h T t ∂xr ); where h t = h t1 , h t2 , .</formula><p>.., h tn is the t-th row vector of H. While this does not improve the asymptotic complexity of computing the gradient (compared to using the formulation of cost function in Definition 3.5), these matrix operations can be implemented much more efficiently than using loops in languages such as Python and Matlab. For large data sets, we use stochastic gradient decent, by sampling a subset of s &lt;&lt; n number of input persistence images, and compute the matrices H, D, L, G as well as the cost using the subsampled data points. The time complexity of one iteration in updating parameters is O(s 2 N ), where N is the size of a persistence image (recall, each <ref type="bibr" target="#b1">2</ref> In our implementation, we add a penalty term m r=1 c exp(wr ) to total-cost k − T r(HLH T ), to achieve this in a "soft" manner. persistence image is a vector in R N ). In our implementation, we use Armijo-Goldstein line search scheme to update the parameters in each (stochastic) gradient decent step. The optimization procedure terminates when the cost function converges or the number of iterations exceeds a threshold. Overall, the time complexity of our optimization procedure is O <ref type="figure" target="#fig_1">(Rs 2 N )</ref> where R is the number of iterations, s is the minibatch size, and N is the size (# pixels) of a single persistence image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We show the effectiveness of our metric-learning framework and the use of the learned metric via graph classification applications. In particular, given a set of graphs Ξ = {G 1 , . . . , G n } coming from k classes, we first compute the unweighted persistence images A i for each graph G i , and apply the framework from Section 3.1 to learn the "best" weight-function ω * : R 2 → R on the birth-death plane R 2 using these persistence images {A 1 , . . . , A n } and their labels. We then perform graph classification using kernel-SVM with the learned ω * -WKPI kernel. We refer to this framework as WKPI-classification framework. We show two sets of experiments. Section 4.1 shows that our learned WKPI kernel significantly outperforms existing persistence-based representations. In Section 4.2, we compare the performance of WKPI-classification framework with various state-of-the-art methods for the graph classification task over a range of data sets. More details / results can be found in Appendix Section 2.</p><p>Setup for our WKPI-based framework. In all our experiments, we assume that the weight-function comes from the class F of mixture of m 2D non-negatively weighted Gaussians as described in the end of Section 3.2. We take m and the width σ in our WKPI kernel as hyperparameters: Specifically, we search among m ∈ {3, 4, 5, 6, 7, 8} and σ ∈ {0.001, 0.01, 0.1, 1, 10, 100}. The 10 * 10-fold nested cross validation are applied to evaluate our algorithm: There are 10 folds in outer loop for evaluation of the model with selected hyperparameters and 10 folds in inner loop for hyperparameter tuning. We then repeat this process 10 times (although the results are extremely close whether repeating 10 times or not). Our optimization procedure terminates when the change of the cost function remains ≤ 10 −4 or the iteration number exceeds 2000. One important question is to initialize the centers of the Gaussians in our mixture. There are three strategies that we consider. (1) We simply sample m centers in the domain of persistence images randomly.</p><p>(2) We collect all points in the persistence diagrams {A 1 , . . . , A n } derived from the training data Ξ, and perform a k-means algorithm to identify m means. (3) We perform a k-center algorithm to those points to identify m centers. Strategies (2) and (3) usually outperform strategy <ref type="bibr" target="#b0">(1)</ref>. Thus in what follows we only report results from using k-means and k-centers as initialization, referred to as WKPI-kM and WKPI-kC, respectively. Our code is published in https://github.com/topology474/WKPI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with other persistence-based methods</head><p>We compare our methods with state-of-the-art persistence-based representations, including the Persistence Weighted Gaussian Kernel (PWGK) <ref type="bibr" target="#b33">[34]</ref>, original Persistence Image (PI) <ref type="bibr" target="#b0">[1]</ref>, and Sliced Wasserstein (SW) Kernel <ref type="bibr" target="#b12">[13]</ref>. Furthermore, as mentioned in Remark 2 after Definition 3.1, we can learn weight functions in PWGK by the optimizing the same cost function (via replacing our WKPI-distance with the one computed from PWGK kernel); and we refer to this as trainPWGK. We can also use an alternative kernel for persistence images as described in Remark 2, and then optimize the same cost function using distance computed from this kernel; we refer to this as altWKPI. We will compare our methods both with existing approaches, as well as with these two alternative metric-learning approaches (trainPWGK and altWKPI).</p><p>Generation of persistence diagrams. Neuron cells have natural tree morphology, rooted at the cell body (soma), with dendrite and axon branching out, and are commonly modeled as geometric trees. See <ref type="figure" target="#fig_0">Figure  1</ref> in the Appendix for an example. Given a neuron tree T , following <ref type="bibr" target="#b36">[37]</ref>, we use the descriptor function f : T → R where f (x) is the geodesic distance from x to the root of T along the tree. To differentiate the dendrite and axon part of a neuron cell, we further negate the function value if a point x is in the dendrite. We then use the union of persistence diagrams A T induced by both the sublevel-set and superlevel-set filtrations w.r.t. f . Under these filtrations, intuitively, each point (b, d) in the birth-death plane R 2 corresponds to the creation and death of certain branch feature for the input neuron tree. The set of persistence diagrams obtained this way (one for each neuron tree) is the input to our WKPI-classification framework.</p><p>Results on neuron datasets. Neuron-Binary dataset consists of 1126 neuron trees from two classes; while Neuron-Multi contains 459 neurons from four classes. As the number of trees is not large, we use all training data to compute the gradients in the optimization process instead of mini-batch sampling. Persistence images are both needed for the methodology of <ref type="bibr" target="#b0">[1]</ref> and as input for our WKPI-distance, and its resolution is fixed at roughly 40 × 40 (see Appendix 2.2 for details). For persistence image (PI) approach of <ref type="bibr" target="#b0">[1]</ref>, we experimented both with the unweighted persistence images (PI-CONST), and one, denoted by (PI-PL), where the weight function α : R 2 → R is a simple piecewise-linear (PL) function adapted from what's proposed in <ref type="bibr" target="#b0">[1]</ref>; see Appendix 2.2 for details. Since PI-PL performs better than PI-CONST on both datasets, <ref type="table" target="#tab_0">Table 1</ref> only shows the results of PI-PL. The classification accuracy of various methods is given in <ref type="table" target="#tab_0">Table 1</ref>. Our results are consistently better than other topology-based approaches, as well as alternative metric-learning approaches; not only for the neuron datasets as in <ref type="table" target="#tab_0">Table 1</ref>, but also for graph benchmark datasets shown in <ref type="table" target="#tab_2">Table 3</ref> of Appendix Section 2.2, and often by a large margin. In Appendix Section 2.1, we also show the heatmaps indicating the learned weight function ω : R 2 → R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph classification task</head><p>We use a range of benchmark datasets: (1) several datasets on graphs derived from small chemical compounds or protein molecules: NCI1 and NCI109 <ref type="bibr" target="#b46">[47]</ref>, PTC <ref type="bibr" target="#b24">[25]</ref>, PROTEIN <ref type="bibr" target="#b6">[7]</ref>, DD <ref type="bibr" target="#b21">[22]</ref> and MUTAG <ref type="bibr" target="#b19">[20]</ref>;</p><p>(2) two datasets on graphs representing the response relations between users in Reddit: REDDIT-5K (5 classes) and REDDIT-12K (11 classes) <ref type="bibr" target="#b50">[51]</ref>; and (3) two datasets on IMDB networks of actors/actresses: IMDB-BINARY (2 classes), and IMDB-MULTI (3 classes). See Appendix Section 2.2 for descriptions of these datasets, and their statistics (sizes of graphs etc).</p><p>Many graph classification methods have been proposed in the literature, with different methods performing better on different datasets. Thus we include multiple approaches to compare with, to include state-of-theart results on different datasets: six graph-kernel based approaches: RetGK <ref type="bibr" target="#b52">[53]</ref>, FGSD <ref type="bibr" target="#b47">[48]</ref>, Weisfeiler-Lehman kernel (WL) <ref type="bibr" target="#b46">[47]</ref>, Weisfeiler-Lehman optimal assignment kernel (WL-OA) <ref type="bibr" target="#b32">[33]</ref>, Deep Graphlet kernel (DGK) <ref type="bibr" target="#b50">[51]</ref>, and the very recent persistent Weisfeiler-Lehman kernel (P-WL-UC) <ref type="bibr" target="#b44">[45]</ref>, Persistence Fisher kernel (PF) <ref type="bibr" target="#b34">[35]</ref>; two graph neural networks: PATCHYSAN (PSCN) <ref type="bibr" target="#b41">[42]</ref>, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b48">[49]</ref>; and the topology-signature-based neural networks <ref type="bibr" target="#b26">[27]</ref>.</p><p>Classification results. To generate persistence summaries, we want to put a meaningful descriptor function on input graphs. We consider two choices: (a) the Ricci-curvature function f c : G → R, where f c (x) is the discrete Ricci curvature for graphs as introduced in <ref type="bibr" target="#b37">[38]</ref>; and (b) Jaccard-index function f J : G → R which measures edge similarities in a graph. See Appendix 2.2 for details. Graph classification results are in <ref type="table" target="#tab_1">Table  2</ref>: here Ricci curvature function is used for the small chemical compounds datasets (NCI1, NCI9, PTC and MUTAG), while Jaccard function is used for proteins datasets (PROTEIN and DD) and the social/IMDB networks (IMDB's and REDDIT's). Results of previous methods are taken from their respective papers -only a subset of the aforementioned previous methods are included in <ref type="table" target="#tab_1">Table 2</ref> due to space limitation. Comparisons with more methods are in Appendix Section 2.2. We rerun the two best performing approahes GIN and RetGK using the exactly same nested cross validation setup as ours. The results are also in Appendix Section 2.2, which is similar to those in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Except for MUTAG and IMDB-MULTI, the performances of our WKPI-framework are similar or better than the best of other methods. Our WKPI-framework performs well on both chemical graphs and social graphs, while some of the earlier work tend to work well on one type of the graphs. Furthermore, note that the chemical / molecular graphs usually have attributes associated with them. Some existing methods use these attributes in their classification <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53]</ref>. Our results however are obtained purely based on graph structure without using any attributes. In terms of variance, the standard deviations of our methods tend to be on-par with graph kernel based previous approaches; and are usually much better (smaller) than the GNN based approaches (i.e, PSCN and GIN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding remarks</head><p>This paper introduces a new weighted-kernel for persistence images (WKPI), together with a metric-learning framework to learn the best weight-function for WKPI-kernel from labelled data. Very importantly, we apply the learned WKPI-kernel to the task of graph classification, and show that our new framework achieves similar or better results than the best results among a range of previous graph classification approaches.</p><p>In our current framework, only a single descriptor function of each input object (e.g, a graph) is used to derive a persistence-based representation. It will be interesting to extend our framework to leverage multiple descriptor functions (so as to capture different types of information) simultaneously and effectively. Recent work on multidimensional persistence would be useful in this effort. Another interesting question is to study how to incorporate categorical attributes associated to graph nodes (or points in input objects) effectively. Indeed, real-valued attributed can potentially be used as a descriptor function to generate persistence-based summaries. But the handling of categorical attributes via topological summarization is much more challenging, especially when there is no (prior-known) correlation between these attributes (e.g, the attribute is simply a number from {1, 2, · · · , s}, coming from s categories. The indices of these categories may carry no meaning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Missing details from Section 3 A.1 Proof of Lemma 3.2</head><p>Consider an arbitrary collection of n persistence images {PI 1 , . . . , PI n } (i.e, a collection of n vectors in R N ). Set K = [k ij ] n×n to be the n × n kernel matrix where k ij = k w (PI i , PI j ). Now given any vector v = (v 1 , v 2 , ..., v n ) T , we have that:</p><formula xml:id="formula_15">v T Kv = n i,j=1 v i v j k ij = n i,j=1 v i v j m s=1 ω(p s )e − (PI i (s)−PI j (s)) 2 2σ 2 = m s=1 ω(p s ) n i,j=1 v i v j e − (PI i (s)−PI j (s)) 2 2σ 2 .</formula><p>Because Gaussian kernel is positive semi-definite and the weight-function ω is non-negative, v T Kv ≥ 0 for any v ∈ R N . Hence the WKPI kernel is positive semi-definite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 3.4</head><p>By Definitions 3.1 and 3.3, combined with the fact that 1 − e −x ≤ x for any x ∈ R, we have that:</p><formula xml:id="formula_16">D ω 2 (A, B) = k w (PI A , PI A ) + k w (PI B , PI B ) − 2k w (PI A , PI B ) = 2 N s=1 ω(p s ) − 2 n s=1 ω(p s )e − (PI A (s)−PI B (s)) 2 σ 2 = 2 N s=1 ω(p s )(1 − e − (PI A (s)−PI B (s)) 2 σ 2 ) ≤ 2c w N s=1 (1 − e − (PI A (s)−PI B (s)) 2 σ 2 ) ≤ 2 c w σ 2 n s=1 (PI A (s) − PI B (s)) 2 ≤ 2 c w σ 2 ||PI A − PI B || 2 2</formula><p>Furthermore, by Theorem 10 of <ref type="bibr" target="#b0">[1]</ref>, when the distribution φ u to in Definition 2.1 is the normalized</p><formula xml:id="formula_17">Gaussian φ u (z) = 1 2πτ 2 e − z−u 2 2τ 2</formula><p>, and the weight function α = 1, we have that PI A − PI B 2 ≤ 10 π · 1 τ · d W,1 <ref type="figure">(A, B)</ref>. (Intuitively, view two persistence diagrams A and B as two (appropriate) measures, and d W,1 <ref type="figure">(A, B)</ref> is then the "earth-mover" distance between them so as to convert the measure corresponding to A to that for B, where the cost is measured by the total L 1 -distance that all mass have to travel.) Combining this with the inequalities for D ω 2 <ref type="figure">(A, B)</ref> above, the theorem then follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Theorem 3.6</head><p>We first show the following properties of matrix L which will be useful for the proof later.</p><p>Lemma A.1 The matrix L is symmetric and positive semi-definite. Furthermore, for every vector f ∈ R n , we have</p><formula xml:id="formula_18">f T Lf = 1 2 n i,j=1 Λ ij (f i − f j ) 2 (1)</formula><p>Proof: By construction, it is easy to see that L is symmetric as matrices Λ and G are. The positive semi-definiteness follows from Eqn (1) which we prove now.</p><formula xml:id="formula_19">f T Lf = f T Gf − f T Λf = n i=1 f 2 i g ii − n i,j=1 f i f j Λ ij = 1 2 n i=1 f 2 i g ii + n j=1 f 2 j g jj − n i,j=1 2f i f j Λ ij = 1 2 n i=1 f 2 i n j=1 Λ ij + n j=1 f 2 j n i=1 Λ ji − n i,j=1 2f i f j Λ ij = 1 2 n i,j=1 Λ ij · (f 2 i + f 2 j − 2f i f j ) = 1 2 n i,j=1 Λ ij (f i − f j ) 2</formula><p>The lemma then follows. We now prove the statement in Theorem 3.6. Recall that the definition of various matrices, and that h t 's are the row vectors of matrix H. For simplicity, in the derivations below, we use D(i, j) to denote the ω-induced WKPI-distance D ω (A i , A j ) between persistence diagrams A i and A j . Applying Lemma A.1, we have:</p><formula xml:id="formula_20">Tr(HLH T ) = k t=1 (HLH T ) tt = k t=1 h t Lh T t = k t=1 1 2 · n j 1 ,j 2 =1 D 2 (j 1 , j 2 )(h t,j 1 − h t,j 2 ) 2 = k t=1 1 2 · n j 1 ,j 2 =1 D 2 (j 1 , j 2 )(h 2 t,j 1 + h 2 t,j 2 − 2h t,j 1 h t,j 2 ).<label>(2)</label></formula><p>Now by definition of h ti , it is non-zero only when i ∈ C t . Combined with Eqn (2), it then follows that:</p><formula xml:id="formula_21">Tr(HLH T ) = k t=1 1 2 · j 1 ∈Ct,j 2 ∈[1,n] D 2 (j 1 , j 2 ) cost ω (t, ·) + j 1 ∈[1,n],j 2 ∈Ct D 2 (j 1 , j 2 ) cost ω (t, ·) − 2 j 1 ,j 2 ∈Ct D 2 (j 1 , j 2 ) cost ω (t, ·) = k t=1 1 2 j 1 ∈Ct,j 2 / ∈Ct D 2 (j 1 , j 2 ) cost ω (t, ·) + j 1 / ∈Ct,j 2 ∈Ct D 2 (j 1 , j 2 ) cost ω (t, ·) = k t=1 j 1 ∈At,j 2 / ∈At D 2 (j 1 , j 2 ) cost ω (t, ·) = k t=1 cost ω (t, ·) − cost ω (t, t) cost ω (t, ·) = k − T C(ω)</formula><p>This proves the first statement in Theorem 3.6. We now show that the matrix HGH T is the k × k identity matrix I. Specifically, first consider s = t ∈ [1, k]; we claim:</p><formula xml:id="formula_22">(HGH T ) st = h s Gh T t = n j 1 ,j 2 =1 h sj 1 G j 1 j 2 h tj 2 = 0.</formula><p>It equals to 0 because h sj 1 is non-zero only for j 1 ∈ C s , while h tj 2 is non-zero only for j 2 ∈ C t . However, for such a pair of j 1 and j 2 , obviously j 1 = j 2 , which means that G j 1 j 2 = 0. Hence the sum is 0 for all possible j 1 and j 2 's. Now for the diagonal entries of the matrix HGH T , we have that for any t ∈ <ref type="bibr">[1, k]</ref>:</p><formula xml:id="formula_23">(HGH T ) tt = h t Gh T t = n j 1 ,j 2 =1 h tj 1 G j 1 ,j 2 h tj 2 = j 1 ,j 2 ∈Ct G j 1 j 2 cost ω (t, ·) = j 1 ∈Ct G j 1 j 1 cost ω (t, ·) = j 1 ∈Ct n =1 D 2 (j 1 , ) cost ω (t, ·) = j 1 ∈Ct, ∈[1,n] D 2 (j 1 , ) cost ω (t, ·) = cost ω (t, ·) cost ω (t, ·) = 1.</formula><p>This finishes the proof that HGH T = I, and completes the proof of Theorem 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More details for Experiments B.1 More on neuron experiments</head><p>Description of neuron datasets. Neuron cells have natural tree morphology (see <ref type="figure" target="#fig_6">Figure 3</ref> (a) for an example), rooted at the cell body (soma), with dentrite and axon branching out. Furthermore, this tree morphology is important in understanding neurons. Hence it is common in the field of neuronscience to model a neuron as a (geometric) tree (see <ref type="figure" target="#fig_6">Figure 3</ref> (b) for an example downloaded from NeuroMorpho.Org <ref type="bibr" target="#b1">[2]</ref>).</p><p>Our NEURON-BINARY dataset consists of 1126 neuron trees classified into two (primary) classes: interneuron and principal neurons (data partly from the Blue Brain Project <ref type="bibr" target="#b39">[40]</ref> and downloaded from http://neuromorpho.org/). The second NEURON-MULTI dataset is a refinement of the 459 interneuron class into four (secondary) classes: basket-large, basket-nest, neuglia and martino. Setup for persistence images. Persistence-images are both needed for the methodology of <ref type="bibr" target="#b0">[1]</ref> and as input for our WKPI-distance. For each dataset, the persistence image for each object inside is computed within the rectangular bounding box of the points from all persistence diagrams of input trees. The y-direction is then discretized to 40 uniform intervals, while the x-direction is discretized accordingly so that each pixel is a square. For persistence image (PI) approach of <ref type="bibr" target="#b0">[1]</ref>, we show results both for the unweighted persistence images (PI-CONST), and one, denoted by PI-PL, where the weight function α : </p><p>Weight function learnt. In <ref type="figure" target="#fig_8">Figure 4</ref> we show the heatmaps of the learned weight-function ω * for both datasets. Interestingly, we note that the important branching features (points in the birth-death plane with high ω * values) separating the two primary classes (i.e, for Neuron-Binary dataset) is different from those important for classifying neurons from one of the two primary classes (the interneuron class) into the four secondary classes (i.e, the Neuron-Multi dataset). Also high importance (weight) points may not have high persistence. In the future, it would be interesting to investigate whether the important branch features are also biochemically important. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 More on graph classification experiments</head><p>Benchmark datasets for graph classification. Below we first give a brief description of the benchmark datasets we used in our experiments. These are collected from the literature. NCI1 and NCI109 <ref type="bibr" target="#b46">[47]</ref> consist of two balanced subsets of datasets of chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines, respectively. PTC <ref type="bibr" target="#b24">[25]</ref> is a dataset of graph structures of chemical molecules from rats and mice which is designed for the predictive toxicology challenge 2000-2001.</p><p>DD <ref type="bibr" target="#b21">[22]</ref> is a data set of 1178 protein structures. Each protein is represented by a graph, in which the nodes are amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart. They are classified according to whether they are enzymes or not. PROTEINS <ref type="bibr" target="#b6">[7]</ref> contains graphs of protein. In each graph, a node represents a secondary structure element (SSE) within protein structure, i.e. helices, sheets and turns. Edges connect nodes if they are neighbours along amino acid sequence or neighbours in protein structure space. Every node is connected to its three nearest spatial neighbours. MUTAG <ref type="bibr" target="#b19">[20]</ref> is a dataset collecting 188 mutagenic aromatic and heteroaromatic nitro compounds labelled according to whether they have a mutagenic effect on the Gramnegtive bacterium Salmonella typhimurium. REDDIT-5K and REDDIT-12K <ref type="bibr" target="#b50">[51]</ref> consist of graph representing the discussions on the online forum Reddit. In these datasets, nodes represent users and edges between two nodes represent whether one of these two users leave comments to the other or not. In REDDIT-5K, graphs are collected from 5 sub-forums, and they are labelled by to which sub-forums they belong. In REDDIT-12K, there are 11 sub-forums involved, and the labels are similar to those in REDDIT-5K. IMDB-BINARY and IMDB-MULTI <ref type="bibr" target="#b50">[51]</ref> are dataset consists of networks of 1000 actors or actresses who played roles in movies in IMDB. In each graph, a node represents an actor or actress, and an edge connects two nodes when they appear in the same movie. In IMDB-BINARY, graphs are classified into Action and Romance genres. In IMDB-MULTI, they are collected from three different genres: Comedy, Romance and Sci-Fi.</p><p>The statistics of these datasets are provided in <ref type="table" target="#tab_2">Table 3</ref>. In our experiments, for REDDIT-12K dataset, due to the larger size of the dataset (with about 13K graphs), we deploy the EigenPro method ( <ref type="bibr" target="#b38">[39]</ref>, code available at https://github.com/EigenPro/EigenPro-matlab), which is a preconditioned (stochastic) gradient descent iteration) to significantly improve the efficiency of kernel-SVM.</p><p>Persistence generation. To generate persistence diagram summaries, we want to put a meaningful descriptor function on input graphs. We consider two choices in our experiments: (a) the Ricci-curvature function f c : G → R, where f c (x) is a discrete Ricci curvature for graphs as introduced in <ref type="bibr" target="#b37">[38]</ref>; and (b) Jaccard-index </p><formula xml:id="formula_25">= 1 − W (m α u , m α v )/d(u, v) where W (·, ·)</formula><p>is Wasserstein distance between two measures and d(u, v) is the distance between two nodes, and probability measure m α u around node u is defined as</p><formula xml:id="formula_26">m α x (x) =      α x = u (1 − α)/n u x ∈ N (u) 0 otherwise<label>(4)</label></formula><p>n u = |N (u)| and α is a parameter within [0, 1]. In this paper, we set α = 0.5.</p><p>In particular, the Jaccard-index of an edge (u, v) ∈ G in the graph is defined as</p><formula xml:id="formula_27">ρ(u, v) = |N N (u)∩N N (v)| |N N (u)∪N N (v)| , where N N (x)</formula><p>refers to the set of neighbors of node x in G. The Jaccard index has been commonly used as a way to measure edge-similarity <ref type="bibr" target="#b2">3</ref> . As in the case for neuron data sets, we take the union of the 0-th persistence diagrams induced by both the sublevel-set and the superlevel-set filtrations of the descriptor function f , and convert it to a persistence image as input to our WKPI-classification framework <ref type="bibr" target="#b3">4</ref> .</p><p>In all results reported in main text and in <ref type="table" target="#tab_3">Table 4</ref>, Ricci curvature function is used for the small chemical compounds data sets (NCI1, NCI9, PTC and MUTAG), while Jaccard function is used for the two proteins datasets (PROTEIN and DD) as well as the social/IMDB networks (IMDB's and REDDIT's). Both 0-dim and 1-dim extented persistence diagrams are employed. In general, we observe that Ricci curvature is more sensitive to accurate graph local structure, while Jaccard function is better for noisy graphs (with noisy edge). In <ref type="figure" target="#fig_9">Figure 5</ref>, we show the heatmaps of the weight function before and after our metric learning for NCI1 and REDDIT-5K datasets. In particular, the left column shows the heatmaps of the initialized weight function, while the right column shows the heatmaps of the optimal weight function as learned by our algorithm.</p><p>Additional results. Many graph classification methods have been proposed in the literature. We compare our results with a range of existing approaches, which includes state-of-the-art results on different datasets: six graph-kernel based approaches: RetGK <ref type="bibr" target="#b52">[53]</ref>, FGSD <ref type="bibr" target="#b47">[48]</ref>, Weisfeiler-Lehman kernel (WL) <ref type="bibr" target="#b46">[47]</ref>, Weisfeiler-Lehman optimal assignment kernel (WL-OA) <ref type="bibr" target="#b32">[33]</ref>, Deep Graphlet kernel (DGK) <ref type="bibr" target="#b50">[51]</ref>, and the very recent persistent Weisfeiler-Lehman kernel (P-WL-UC) <ref type="bibr" target="#b4">5</ref>  <ref type="bibr" target="#b44">[45]</ref>; two graph neural networks: PATCHYSAN (PSCN) Initial weight function Learnt weight function [42], Graph Isomorphism Network (GIN) <ref type="bibr" target="#b48">[49]</ref>; as well as the topology-signature-based neural networks <ref type="bibr" target="#b26">[27]</ref>. Additional results of comparing our results with more existing methods are given in <ref type="table" target="#tab_3">Table 4</ref>. The results of DL-TDA (topological signature based deep learning framework) <ref type="bibr" target="#b26">[27]</ref> are not listed in <ref type="table" target="#tab_3">Table 4</ref>, as only the classification accuracy for REDDIT-5K (accuracy 54.5%) and REDDIT-12K (44.5%) are given in their paper (although their paper contains many more results on other objects, such as images). While also not listed in this table, we note that our results also outperform the newly independently proposed general neural network architecture for persistence representations reported in the very recent preprint <ref type="bibr" target="#b11">[12]</ref>. Comparison with other topological-based non-neural network approaches are given below.</p><p>Topological-based methods on graph data. Here we compare our WKPI-framework with the performance of several state-of-the-art persistence-based classification frameworks, including: PWGK <ref type="bibr" target="#b33">[34]</ref>, SW <ref type="bibr" target="#b12">[13]</ref>, PI <ref type="bibr" target="#b0">[1]</ref> and PF <ref type="bibr" target="#b34">[35]</ref>. We also compare it with two alternative ways to learn the metric for persistence-based representations: trainPWGK is the version of PWGK <ref type="bibr" target="#b33">[34]</ref> where we learn the weight function in its formulation, using the same cost-function as what we propose in this paper for our WKPI kernel functions. altWKPI is the alternative formulation of a kernel for persistence images where we set the kernel to be k(PI, PI ) = N s=1 e − ω(ps)(PI(s)−PI (s)) 2 2σ 2</p><p>, instead of our WKPI-kernel as defined in Definition 3.1.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A persistence-based data analysis framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a): As we sweep the curve bottom-up in increasing f -values, at certain critical moments new 0-th homological features (connected components) are created, or destroyed (i.e, components merge). For example, a component is created when passing x 4 and killed when passing x 6 , giving rise to the persistence-point (f 4 , f 6 ) in the persistence diagram (f i := f (x i )). (b) shows the graph of a persistence surface (where z-axis is the function ρ A ), and (c) is its corresponding persistence image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>probability distribution with mean u ∈ R 2 (e.g, the normalized Gaussian where for any z ∈ R 2 , φ u (z) = 1 2πτ 2 e − z−u 2 2τ 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 3 . 2</head><label>32</label><figDesc>The WKPI kernel is positive semi-definite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 3 . 5 (</head><label>35</label><figDesc>Optimization problem) Given a weight-function ω : R 2 → R, the total-cost of its induced WKPI-distance over Ξ is defined as: T C(ω) := k t=1 cost(t,t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 3 . 6</head><label>36</label><figDesc>The total-cost can also be represented by T C(ω) = k − Tr(HLH T ), where Tr(·) is the trace of a matrix. Furthermore, HGH T = I, where I is the k × k identity matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>(a) An neuron cell (downloaded from Wikipedia)and (b) an example of a neuron tree (downloaded from NeuroMorpho.Org).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>R 2 →</head><label>2</label><figDesc>R (for Definition 2.1) is the following piecewise-linear function (modified from one proposed by Adams et al. [1]) where b the largest persistence for any persistent-point among all persistence diagrams. b |y − x| &lt; b and y &gt; 0 |−y−x| b | − y − x| &lt; b and y &lt; 0 1 otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Heatmaps of the learned weight-function ω * for Neuron-Binary (left) and Neuron-Multi (right) datasets. Each point in this plane indicates the birth-death of some branching feature. Warmer color (e.g, red) indicates higher ω * value. xand y-axies are birth / death time measured by the descriptor function f (modified geodesic function, where for points in dendrites they are negation of the distance to root).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Heatmap of initialized weight function (left column) and that of the learnt weight-function ω * (right column). Top row shows results for NCI1 data set; while bottom row contains those for REDDIT-5K data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy on neuron dataset. Our results are WKPI-km and WKPI-kc. BINARY 80.5±0.4 85.3±0.7 83.7±0.3 82.1±2.1 84.6±2.4 89.6 ±2.2 86.4±2.4 NEURON-MULTI 45.1±0.3 57.6±0.6 44.2±0.3 54.3±2.3 49.7±2.4 56.6±2.7 59.3±2.3</figDesc><table><row><cell>Datasets</cell><cell cols="2">Existing approaches</cell><cell></cell><cell cols="2">Alternative metric learning</cell><cell cols="2">Our WKPI framework</cell></row><row><cell></cell><cell>PWGK</cell><cell>SW</cell><cell>PI-PL</cell><cell>altWKPI</cell><cell>trainPWGK</cell><cell cols="2">WKPI-km WKPI-kc</cell></row><row><cell>NEURON-Average</cell><cell>62.80</cell><cell>71.45</cell><cell>63.95</cell><cell>68.20</cell><cell>67.15</cell><cell>73.10</cell><cell>72.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Graph classification accuracy + standard deviation. Our results are last two columns.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell cols="3">Previous approaches</cell><cell></cell><cell></cell><cell cols="2">Our approaches</cell></row><row><cell></cell><cell>RetGK</cell><cell>WL</cell><cell>DGK</cell><cell>P-WL-UC</cell><cell>PF</cell><cell>PSCN</cell><cell>GIN</cell><cell>WKPI-kM</cell><cell>WKPI-kC</cell></row><row><cell>NCI1</cell><cell>84.5±0.2</cell><cell cols="2">85.4±0.3 80.3±0.5</cell><cell>85.6±0.3</cell><cell cols="2">81.7±0.8 76.3±1.7</cell><cell>82.7±1.6</cell><cell>87.5±0.5</cell><cell>84.5±0.5</cell></row><row><cell>NCI109</cell><cell>-</cell><cell cols="2">84.5±0.2 80.3±0.3</cell><cell>85.1±0.3</cell><cell>78.5±0.5</cell><cell>-</cell><cell>-</cell><cell>85.9±0.4</cell><cell>87.4±0.3</cell></row><row><cell>PTC</cell><cell>62.5±1.6</cell><cell cols="2">55.4±1.5 60.1±2.5</cell><cell>63.5±1.6</cell><cell cols="2">62.4±1.8 62.3±5.7</cell><cell>66.6±6.9</cell><cell>62.7±2.7</cell><cell>68.1±2.4</cell></row><row><cell>PROTEIN</cell><cell>75.8±0.6</cell><cell cols="2">71.2±0.8 75.7±0.5</cell><cell>75.9±0.8</cell><cell cols="2">75.2±2.1 75.0±2.5</cell><cell>76.2±2.6</cell><cell>78.5±0.4</cell><cell>75.2±0.4</cell></row><row><cell>DD</cell><cell>81.6±0.3</cell><cell>78.6±0.4</cell><cell>-</cell><cell>78.5±0.4</cell><cell cols="2">79.4±0.8 76.2±2.6</cell><cell>-</cell><cell>82.0±0.5</cell><cell>80.3±0.4</cell></row><row><cell>MUTAG</cell><cell>90.3±1.1</cell><cell cols="2">84.4±1.5 87.4±2.7</cell><cell>85.2±0.3</cell><cell cols="2">85.6±1.7 89.0±4.4</cell><cell>90.0±8.8</cell><cell>85.8±2.5</cell><cell>88.3±2.6</cell></row><row><cell>IMDB-BINARY</cell><cell>71.9±1.0</cell><cell cols="2">70.8±0.5 67.0±0.6</cell><cell>73.0±1.0</cell><cell cols="2">71.2±1.0 71.0±2.3</cell><cell>75.1±5.1</cell><cell>70.7±1.1</cell><cell>75.1±1.1</cell></row><row><cell>IMDB-MULTI</cell><cell>47.7±0.3</cell><cell cols="2">49.8±0.5 44.6±0.4</cell><cell>-</cell><cell cols="3">48.6±0.7 45.2±2.8 52.3 ±2.8</cell><cell>46.4±0.5</cell><cell>49.5±0.4</cell></row><row><cell>REDDIT-5K</cell><cell>56.1±0.5</cell><cell cols="2">51.2±0.3 41.3±0.2</cell><cell>-</cell><cell cols="2">56.2±1.1 49.1±0.7</cell><cell>57.5±1.5</cell><cell>59.1±0.5</cell><cell>59.5±0.6</cell></row><row><cell>REDDIT-12K</cell><cell>48.7±0.2</cell><cell cols="2">32.6±0.3 32.2±0.1</cell><cell>-</cell><cell cols="2">47.6±0.5 41.3±0.4</cell><cell>-</cell><cell>47.4±0.6</cell><cell>48.4±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the benchmark graph datasets</figDesc><table><row><cell>Dataset</cell><cell cols="4">#classes #graphs average #nodes average #edges</cell></row><row><cell>NCI1</cell><cell>2</cell><cell>4110</cell><cell>29.87</cell><cell>32.30</cell></row><row><cell>NCI109</cell><cell>2</cell><cell>4127</cell><cell>29.68</cell><cell>31.96</cell></row><row><cell>PTC</cell><cell>2</cell><cell>344</cell><cell>14.29</cell><cell>14.69</cell></row><row><cell>PROTEIN</cell><cell>2</cell><cell>1113</cell><cell>39.06</cell><cell>72.82</cell></row><row><cell>DD</cell><cell>2</cell><cell>1178</cell><cell>284.32</cell><cell>715.66</cell></row><row><cell>IMDB-BINARY</cell><cell>2</cell><cell>1000</cell><cell>19.77</cell><cell>96.53</cell></row><row><cell>IMDB-MULTI</cell><cell>3</cell><cell>1500</cell><cell>13.00</cell><cell>65.94</cell></row><row><cell>REDDIT-5K</cell><cell>5</cell><cell>4999</cell><cell>508.82</cell><cell>594.87</cell></row><row><cell>REDDIT-12K</cell><cell>11</cell><cell>12929</cell><cell>391.41</cell><cell>456.89</cell></row><row><cell>function f</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>J : G → R. Then Ollivier's Ricci curvature between two nodes u and v is κ α uv</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy on graphs. Our results are in columns WKPI-kM and WKPI-kC.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Previous approaches</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Our appraches</cell></row><row><cell></cell><cell>RetGK</cell><cell>WL</cell><cell cols="6">WL-OA DGK FGSD PSCN GIN P-WL-UC</cell><cell cols="2">WKPI-kM WKPI-kC</cell></row><row><cell>NCI1</cell><cell>84.5</cell><cell>85.4</cell><cell>86.1</cell><cell>80.3</cell><cell>79.8</cell><cell>76.3</cell><cell>82.7</cell><cell>85.6</cell><cell>87.5</cell><cell>84.5</cell></row><row><cell>NCI109</cell><cell>-</cell><cell>84.5</cell><cell>86.3</cell><cell>80.3</cell><cell>78.8</cell><cell>-</cell><cell>-</cell><cell>85.1</cell><cell>85.9</cell><cell>87.4</cell></row><row><cell>PTC</cell><cell>62.5</cell><cell>55.4</cell><cell>63.6</cell><cell>60.1</cell><cell>62.8</cell><cell>62.3</cell><cell>66.6</cell><cell>63.5</cell><cell>62.7</cell><cell>68.1</cell></row><row><cell>PROTEIN</cell><cell>75.8</cell><cell>71.2</cell><cell>76.4</cell><cell>75.7</cell><cell>72.4</cell><cell>75.0</cell><cell>76.2</cell><cell>75.9</cell><cell>78.5</cell><cell>75.2</cell></row><row><cell>DD</cell><cell>81.6</cell><cell>78.6</cell><cell>79.2</cell><cell>-</cell><cell>77.1</cell><cell>76.2</cell><cell>-</cell><cell>78.5</cell><cell>82.0</cell><cell>80.3</cell></row><row><cell>MUTAG</cell><cell>90.3</cell><cell>84.4</cell><cell>84.5</cell><cell>87.4</cell><cell>92.1</cell><cell>89</cell><cell>90</cell><cell>85.2</cell><cell>85.8</cell><cell>88.3</cell></row><row><cell>IMDB-BINARY</cell><cell>71.9</cell><cell>70.8</cell><cell>-</cell><cell>67.0</cell><cell>71.0</cell><cell>71.0</cell><cell>75.1</cell><cell>73.0</cell><cell>70.7</cell><cell>75.4</cell></row><row><cell>IMDB-MULTI</cell><cell>47.7</cell><cell>49.8</cell><cell>-</cell><cell>44.6</cell><cell>45.2</cell><cell>45.2</cell><cell>52.3</cell><cell>-</cell><cell>46.4</cell><cell>49.5</cell></row><row><cell>REDDIT-5K</cell><cell>56.1</cell><cell>51.2</cell><cell>-</cell><cell>41.3</cell><cell>47.8</cell><cell>49.1</cell><cell>57.5</cell><cell>-</cell><cell>59.1</cell><cell>59.5</cell></row><row><cell>REDDIT-12K</cell><cell>48.7</cell><cell>32.6</cell><cell>-</cell><cell>32.2</cell><cell>-</cell><cell>41.3</cell><cell>-</cell><cell>-</cell><cell>47.4</cell><cell>48.4</cell></row><row><cell>Average</cell><cell>-</cell><cell>66.39</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.99</cell><cell>71.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracy on graphs for topology-based methods.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell cols="3">Existing TDA approaches</cell><cell></cell><cell cols="2">Alternative metric learning</cell><cell cols="2">Our WKPI framework</cell></row><row><cell></cell><cell cols="3">PWGK PI-CONST PI-PL</cell><cell>SW</cell><cell>PF</cell><cell>trainPWGK</cell><cell>altWKPI</cell><cell cols="2">WKPI-kM WKPI-kC</cell></row><row><cell>NCI1</cell><cell>73.3</cell><cell>72.5</cell><cell>72.1</cell><cell>80.1</cell><cell>81.7</cell><cell>76.5</cell><cell>77.4</cell><cell>87.2</cell><cell>84.7</cell></row><row><cell>NCI109</cell><cell>71.5</cell><cell>74.3</cell><cell>73.1</cell><cell>75.5</cell><cell>78.5</cell><cell>77.2</cell><cell>81.2</cell><cell>85.5</cell><cell>86.9</cell></row><row><cell>PTC</cell><cell>62.2</cell><cell>61.3</cell><cell>64.2</cell><cell>64.5</cell><cell>62.4</cell><cell>62.5</cell><cell>64.2</cell><cell>61.1</cell><cell>64.3</cell></row><row><cell>PROTEIN</cell><cell>73.6</cell><cell>72.2</cell><cell>69.1</cell><cell>76.4</cell><cell>75.2</cell><cell>74.8</cell><cell>75.1</cell><cell>77.4</cell><cell>75.6</cell></row><row><cell>DD</cell><cell>75.2</cell><cell>74.2</cell><cell>76.8</cell><cell>78.9</cell><cell>79.4</cell><cell>76.4</cell><cell>72.5</cell><cell>79.8</cell><cell>79.1</cell></row><row><cell>MUTAG</cell><cell>82.0</cell><cell>85.2</cell><cell>83.5</cell><cell>87.1</cell><cell>85.6</cell><cell>86.4</cell><cell>88.5</cell><cell>85.5</cell><cell>88.0</cell></row><row><cell>IMDB-BINARY</cell><cell>66.8</cell><cell>65.5</cell><cell>69.7</cell><cell>69.6</cell><cell>71.2</cell><cell>71.8</cell><cell>67.3</cell><cell>70.6</cell><cell>75.4</cell></row><row><cell>IMDB-MULTI</cell><cell>43.4</cell><cell>42.5</cell><cell>46.4</cell><cell>48.7</cell><cell>48.6</cell><cell>45.8</cell><cell>45.3</cell><cell>47.1</cell><cell>48.8</cell></row><row><cell>REDDIT-5K</cell><cell>47.6</cell><cell>52.2</cell><cell>51.7</cell><cell>53.8</cell><cell>56.2</cell><cell>53.5</cell><cell>54.7</cell><cell>58.7</cell><cell>59.3</cell></row><row><cell>REDDIT-12K</cell><cell>38.5</cell><cell>43.3</cell><cell>45.7</cell><cell>48.3</cell><cell>47.6</cell><cell>43.7</cell><cell>42.1</cell><cell>45.2</cell><cell>44.5</cell></row><row><cell>Average</cell><cell>63.41</cell><cell>64.3</cell><cell>65.23</cell><cell cols="2">68.29 68.64</cell><cell>66.86</cell><cell>66.83</cell><cell>69.81</cell><cell>70.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Graph classification accuracy of GIN and RetGK on graph benchmarks with the same nested cross validation setup</figDesc><table><row><cell></cell><cell>NCI1</cell><cell>NCI109</cell><cell>PTC</cell><cell>PROTEIN</cell><cell>DD</cell><cell>MUTAG</cell><cell cols="2">IMDB-BIN IMDB-MULTI</cell><cell>Reddit5K</cell><cell>Reddit12K</cell></row><row><cell cols="4">RetGK 84.5±0.2 84.8±0.2 62.9±1.6</cell><cell>75.4±0.6</cell><cell cols="2">81.6±0.4 90.0±1.1</cell><cell>72.3±1.0</cell><cell>47.7±0.4</cell><cell>55.8 ±0.5</cell><cell>48.5± 0.2</cell></row><row><cell>GIN</cell><cell cols="3">82.4±1.6 86.5±1.5 67.8±6.5</cell><cell>76.7±2.6</cell><cell cols="2">81.1±2.5 89.0±7.5</cell><cell>75.6±5.3</cell><cell>52.4±3.1</cell><cell>57.2±1.5</cell><cell>47.9± 2.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In fact, we can define our kernel without transforming the persistence diagram. We use the transformation simply to follow the same convention as persistence images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We modify our persistence algorithm slightly to handle the edge-valued Jaccard index function<ref type="bibr" target="#b3">4</ref> We expect that using the 0-th zigzag persistence diagrams will provide better results. However, we choose to use only 0-th standard persistence as it can be easily implemented to run in O(n log n) time using a simple union-find data structure.<ref type="bibr" target="#b4">5</ref> Note that results for three version of persistent WL kernels are reported in their paper. We take the one (P-WL-UC, with uniform node labels) that performs the best from theirTable 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Chao Chen and Justin Eldridge for useful discussions related to this project. We would also like to thank Giorgio Ascoli for helping provide the neuron dataset. This work is partially supported by National Science Foundation via grants CCF-1740761, CCF-1733798, and RI-1815697, as well as by National Health Institute under grant R01EB022899.</p><p>We use the same setup as our WKPI-framework to train these two metrics, and use their resulting kernels for SVM to classify the benchmark graph datasets. WKPI-framework outperforms the existing approaches and alternative metric learning methods on all datasets except MUTAG. WKPI-kM (i.e, WKPI-kmeans) and KWPI-kC (i.e, WKPI-kcenter) improve the accuracy by 3.9% − 11.9% and 5.4% − 13.5%, respectively. Besides, we show results by another experimental setup. In 10-fold cross validation, choose m and σ leading to the smallest cost function value, then evaluate the classifier on the test set. Repeat this process 10 times. That is, m and σ are not the hyperparameters of the SVM classifiers, but are determined by the metrics learning. We refer to these two approaches as WKPI-kM and WKPI-kC in accordance with the initialization methods. The classification accuracy of all these methods are reported in <ref type="table">Table 5</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Persistence images: a stable vector representation of persistent homology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Emerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chepushtanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Motta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ziegelmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="218" to="252" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">org: a central resource for neuronal morphologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Donohue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neuromorpho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="9247" to="9251" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A quantum jensen-shannon graph kernel for unattributed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="344" to="355" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ripser</surname></persName>
		</author>
		<ptr target="https://github.com/Ripser/ripser" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Phat -persistent homology algorithms toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reininghaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Software -ICMS 2014</title>
		<editor>H. Hong and C. Yap</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="137" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding and predicting links in graphs: A persistent homology perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04049</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical topological data analysis using persistence landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bubenik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="102" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zigzag persistence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="405" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zigzag persistent homology and real-valued functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morozov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Annu. ACM Sympos</title>
		<meeting>25th Annu. ACM Sympos</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The theory of multidimensional persistence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomorodian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="93" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A general neural network architecture for persistence diagrams and graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carriere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chazal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lacombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Umeda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09378</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sliced Wasserstein kernel for persistence diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carrière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oudot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="664" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proximity of persistence modules and their diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chazal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oudot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th ACM Sympos</title>
		<meeting>25th ACM Sympos</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The structure and stability of persistence modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chazal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oudot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SpringerBriefs in Mathematics</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The gudhi library: simplicial complexes and persistent homology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Boissonnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yvinec</surname></persName>
		</author>
		<ptr target="http://gudhi.gforge.inria.fr/python/latest/index.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stability of persistence diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extending persistence using Poincaré and Lefschetz duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="103" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lipschitz functions have Lp-stable persistence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mileyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of computational mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="139" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simba: An efficient tool for approximating Rips-filtration persistence via simplicial batch-collapse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Leibniz International Proceedings in Informatics (LIPIcs)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>24th Annual European Symposium on Algorithms (ESA 2016</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computational Topology : an Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topological persistence and simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Letscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomorodian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Comput. Geom</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="511" to="533" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Helma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="108" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A linear-time graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2009. ICDM&apos;09. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning with topological signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1634" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometry helps to compare persistence diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nigmetov</surname></persName>
		</author>
		<idno>22:1.4:1-1.4:20</idno>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Algorithmics</title>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">HERA: software to compute distances for persistence diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nigmetov</surname></persName>
		</author>
		<ptr target="https://bitbucket.org/grey$_$narn/hera" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Barcodes of towers and a streaming algorithm for persistent homology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Symposium on Computational Geometry</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
	<note>Schloss Dagstuhl-Leibniz-Zentrum für Informatik GmbH</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Trace optimization and eigenproblems in dimension reduction methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kokiopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical Linear Algebra with Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="565" to="602" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kernel method for persistence diagrams via kernel embedding and weight factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kusano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hiraoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">189</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Persistence Fisher kernel: A Riemannian manifold kernel for persistence diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10028" to="10039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Metrics for comparing neuronal tree shapes based on persistent homology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">182184</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ricci curvature of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Yau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tohoku Mathematical Journal, Second Series</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="605" to="627" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Diving into the shallows: a computational perspective on large-scale shallow learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3778" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reconstruction and simulation of neocortical microcircuitry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Reimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdellah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alonso-Nanclares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Antille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arsever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="456" to="492" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient graph kernels by randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="378" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Svn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="488" to="495" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A stable multi-scale kernel for topological machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reininghaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4741" to="4748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A persistent weisfeiler-lehman procedure for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Linear-size approximations to the Vietoris-Rips filtration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheehy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th. Annu. Sympos</title>
		<meeting>28th. Annu. Sympos</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Proceeding Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">How powerful are graph neural networks? International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A mixed Weisfeiler-Lehman graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-based Representations in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Retgk: Graph kernels based on return probabilities of random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3968" to="3978" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
