<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggí</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, <ref type="bibr" target="#b19">Ramachandran et al. (2019)</ref> showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent advances in Natural Language Processing (NLP) are largely attributed to the rise of the transformer <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref>. Pre-trained to solve an unsupervised task on large corpora of text, transformer-based architectures, such as GPT-2 <ref type="bibr" target="#b18">(Radford et al., 2018)</ref>, BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> and Transformer-XL , seem to possess the capacity to learn the underlying structure of text and, as a consequence, to learn representations that generalize across tasks. The key difference between transformers and previous methods, such as recurrent neural networks <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref> and convolutional neural networks (CNN), is that the former can simultaneously attend to every word of their input sequence. This is made possible thanks to the attention mechanism-originally introduced in Neural Machine Translation to better handle long-range dependencies <ref type="bibr" target="#b2">(Bahdanau et al., 2015)</ref>. With self-attention in particular, the similarity of two words in a sequence is captured by an attention score measuring the distance of their representations. The representation of each word is then updated based on those words whose attention score is highest.</p><p>Inspired by its capacity to learn meaningful inter-dependencies between words, researchers have recently considered utilizing self-attention in vision tasks. Self-attention was first added to CNN by either using channel-based attention <ref type="bibr" target="#b12">(Hu et al., 2018)</ref> or non-local relationships across the image <ref type="bibr" target="#b22">(Wang et al., 2018)</ref>. More recently,  augmented CNNs by replacing some convolutional layers with self-attention layers, leading to improvements on image classification and object detection tasks. Interestingly, <ref type="bibr" target="#b19">Ramachandran et al. (2019)</ref> noticed that, even though state-of-the art results are reached when attention and convolutional features are combined, under same computation and model size constraints, self-attention-only architectures also reach competitive image classification accuracy.</p><p>These findings raise the question, do self-attention layers process images in a similar manner to convolutional layers? From a theoretical perspective, one could argue that transfomers have the capacity to simulate any function-including a CNN. Indeed, <ref type="bibr" target="#b17">Pérez et al. (2019)</ref> showed that a multilayer attention-based architecture with additive positional encodings is Turing complete under some strong theoretical assumptions, such as unbounded precision arithmetic. Unfortunately, universality results do not reveal how a machine solves a task, only that it has the capacity to do so. Thus, the question of how self-attention layers actually process images remains open.</p><p>Contributions. In this work, we put forth theoretical and empirical evidence that self-attention layers can (and do) learn to behave similar to convolutional layers: I. From a theoretical perspective, we provide a constructive proof showing that self-attention layers can express any convolutional layers. Specifically, we show that a single multi-head self-attention layer using relative positional encoding can be re-parametrized to express any convolutional layer.</p><p>II. Our experiments show that the first few layers of attention-only architectures <ref type="bibr" target="#b19">(Ramachandran et al., 2019)</ref> do learn to attend on grid-like pattern around each query pixel, similar to our theoretical construction.</p><p>Strikingly, this behavior is confirmed both for our quadratic encoding, but also for relative encoding that is learned. Our results seem to suggest that localized convolution is the right inductive bias for the first few layers of an image classifying network. We provide an interactive website 2 to explore how self-attention exploits localized position-based attention in lower layers and contentbased attention in deeper layers. For reproducibility purposes, our code is publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND ON ATTENTION MECHANISMS FOR VISION</head><p>We here recall the mathematical formulation of self-attention layers and emphasize the role of positional encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THE MULTI-HEAD SELF-ATTENTION LAYER</head><p>Let X ∈ R T ×Din be an input matrix consisting of T tokens in of D in dimensions each. While in NLP each token corresponds to a word in a sentence, the same formalism can be applied to any sequence of T discrete objects, e.g. pixels. A self-attention layer maps any query token t ∈ [T ] from D in to D out dimensions as follows:</p><p>Self-Attention(X) t,: := softmax (A t,: ) XW val ,</p><p>where we refer to the elements of the T × T matrix</p><formula xml:id="formula_1">A := XW qry W key X<label>(2)</label></formula><p>as attention scores and the softmax output 3 as attention probabilities. The layer is parametrized by a query matrix W qry ∈ R Din×D k , a key matrix W key ∈ R Din×D k and a value matrix W val ∈ R Din×Dout .For simplicity, we exclude any residual connections, batch normalization and constant factors.</p><p>A key property of the self-attention model described above is that it is equivariant to reordering, that is, it gives the same output independently of how the T input tokens are shuffled. This is problematic for cases we expect the order of things to matter. To alleviate the limitation, a positional encoding is learned for each token in the sequence (or pixel in an image), and added to the representation of the token itself before applying self-attention</p><formula xml:id="formula_2">A := (X + P )W qry W key (X + P ) ,<label>(3)</label></formula><p>where P ∈ R T ×Din contains the embedding vectors for each position. More generally, P may be substituted by any function that returns a vector representation of the position.</p><p>It has been found beneficial in practice to replicate this self-attention mechanism into multiple heads, each being able to focus on different parts of the input by using different query, key and value matrices. In multi-head self-attention, the output of the N h heads of output dimension D h are concatenated and projected to dimension D out as follows:</p><formula xml:id="formula_3">MHSA(X) := concat h∈[N h ] Self-Attention h (X) W out + b out<label>(4)</label></formula><p>and two new parameters are introduced: the projection matrix W out ∈ R N h D h ×Dout and a bias term b out ∈ R Dout .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ATTENTION FOR IMAGES</head><p>Convolutional layers are the de facto choice for building neural networks that operate on images. We recall that, given an image tensor X ∈ R W ×H×Din of width W , height H and D in channels, the output of a convolutional layer for pixel (i, j) is given by</p><formula xml:id="formula_4">Conv(X) i,j,: := (δ1,δ2)∈∆ ∆ K X i+δ1,j+δ2,: W δ1,δ2,:,: + b,<label>(5)</label></formula><p>where W is the K × K × D in × D out weight tensor 4 , b ∈ R Dout is the bias vector and the set</p><formula xml:id="formula_5">∆ ∆ K := − K 2 , · · · , K 2 × − K 2 , · · · , K 2</formula><p>contains all possible shifts appearing when convolving the image with a K × K kernel.</p><p>In the following, we review how self-attention can be adapted from 1D sequences to images.</p><p>With images, rather than tokens, we have query and key pixels q, k ∈ [W ] × [H]. Accordingly, the input is a tensor X of dimension W × H × D in and each attention score associates a query and a key pixel.</p><p>To keep the formulas consistent with the 1D case, we abuse notation and slice tensors by using a 2D index vector: if p = (i, j), we write X p,: and A p,: to mean X i,j,: and A i,j,:,: , respectively. With this notation in place, the multi-head self attention layer output at pixel q can be expressed as follows:</p><formula xml:id="formula_6">Self-Attention(X) q,: = k softmax (A q,: ) k X k,: W val<label>(6)</label></formula><p>and accordingly for the multi-head case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">POSITIONAL ENCODING FOR IMAGES</head><p>There are two types of positional encoding that has been used in transformer-based architectures: the absolute and relative encoding (see also <ref type="table">Table 3</ref> in the Appendix).</p><p>With absolute encodings, a (fixed or learned) vector P p,: is assigned to each pixel p. The computation of the attention scores we saw in eq. (2) can then be decomposed as follows:</p><p>A abs q,k = (X q,: + P q,: )W qry W key (X k,: + P k,: ) = X q,: W qry W key X k,: + X q,: W qry W key P k,: + P q,: W qry W key X k,: + P q,: W qry W key P k,:</p><p>where q and k correspond to the query and key pixels, respectively.</p><p>The relative positional encoding was introduced by . The main idea is to only consider the position difference between the query pixel (pixel we compute the representation of) and the key pixel (pixel we attend) instead of the absolute position of the key pixel:</p><p>A rel q,k := X q,: W qry W key X k,: + X q,: W qry W key r δ + u W key X k,:</p><formula xml:id="formula_8">+ v W key r δ<label>(8)</label></formula><p>In this manner, the attention scores only depend on the shift δ := k − q. Above, the learnable vectors u and v are unique for each head, whereas for every shift δ the relative positional encoding r δ ∈ R Dp is shared by all layers and heads. Moreover, now the key weights are split into two types:</p><p>W key pertain to the input and W key to the relative position of pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SELF-ATTENTION AS A CONVOLUTIONAL LAYER</head><p>This section derives sufficient conditions such that a multi-head self-attention layer can simulate a convolutional layer. Our main result is the following: Theorem 1. A multi-head self-attention layer with N h heads of dimension D h , output dimension D out and a relative positional encoding of dimension D p ≥ 3 can express any convolutional layer of kernel size √ N h × √ N h and min(D h , D out ) output channels.</p><p>The theorem is proven constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer. In the proposed construction, the attention scores of each self-attention head should attend to a different relative shift within the set ∆ ∆ K = {− K/2 , . . . , K/2 } 2 of all pixel shifts in a K × K kernel. The exact condition can be found in the statement of Lemma 1.</p><p>Then, Lemma 2 shows that the aforementioned condition is satisfied for the relative positional encoding that we refer to as the quadratic encoding:</p><formula xml:id="formula_9">v (h) := −α (h) (1, −2∆ (h) 1 , −2∆ (h) 2 ) r δ := ( δ 2 , δ 1 , δ 2 ) W qry = W key := 0 W key := I (9)</formula><p>The learned parameters</p><formula xml:id="formula_10">∆ (h) = (∆ (h) 1 , ∆<label>(h)</label></formula><p>2 ) and α (h) determine the center and width of attention of each head, respectively. On the other hand, δ = (δ 1 , δ 2 ) is fixed and expresses the relative shift between query and key pixels.</p><p>It is important to stress that the above encoding is not the only one for which the conditions of Lemma 1 are satisfied. In fact, in our experiments, the relative encoding learned by the neural network also matched the conditions of the lemma (despite being different from the quadratic encoding). Nevertheless, the encoding defined above is very efficient in terms of size, as only D p = 3 dimensions suffice to encode the relative position of pixels, while also reaching similar or better empirical performance (than the learned one).</p><p>The theorem covers the general convolution operator as defined in eq. (17). However, machine learning practitioners using differential programming frameworks <ref type="bibr" target="#b16">(Paszke et al., 2017;</ref><ref type="bibr" target="#b0">Abadi et al., 2015)</ref> might question if the theorem holds for all hyper-parameters of 2D convolutional layers:</p><p>• Padding: a multi-head self-attention layer uses by default the "SAME" padding while a convolutional layer would decrease the image size by K − 1 pixels. The correct way to alleviate these boundary effects is to pad the input image with K/2 zeros on each side. In this case, the cropped output of a MHSA and a convolutional layer are the same.</p><p>• Stride: a strided convolution can be seen as a convolution followed by a fixed pooling operation-with computational optimizations. Theorem 1 is defined for stride 1, but a fixed pooling layer could be appended to the Self-Attention layer to simulate any stride.</p><p>• Dilation: a multi-head self-attention layer can express any dilated convolution as each head can attend a value at any pixel shift and form a (dilated) grid pattern.</p><p>Remark for the 1D case. Convolutional layers acting on sequences are commonly used in the literature for text <ref type="bibr" target="#b14">(Kim, 2014)</ref>, as well as audio (van den Oord et al., 2016) and time series <ref type="bibr" target="#b8">(Franceschi et al., 2019)</ref>. Theorem 1 can be straightforwardly extended to show that multi-head self-attention with N h heads can also simulate a 1D convolutional layer with a kernel of size K = N h with min(D h , D out ) output channels using a positional encoding of dimension D p ≥ 2. Since we have not tested empirically if the preceding construction matches the behavior of 1D self-attention in practice, we cannot claim that it actually learns to convolve an input sequence-only that it has the capacity to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF OF MAIN THEOREM</head><p>The proof follows directly from Lemmas 1 and 2 stated below:</p><p>Lemma 1. Consider a multi-head self-attention layer consisting of N h = K 2 heads, D h ≥ D out and let f : [N h ] → ∆ ∆ K be a bijective mapping of heads onto shifts. Further, suppose that for every head the following holds:</p><formula xml:id="formula_11">softmax(A (h) q,: ) k = 1 if f (h) = q − k 0 otherwise.<label>(10)</label></formula><p>Then, for any convolutional layer with a K × K kernel and D out output channels, there exists {W</p><formula xml:id="formula_12">(h) val } h∈[N h ] such that MHSA(X) = Conv(X) for every X ∈ R W ×H×Din .</formula><p>Attention maps for pixel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter matrices</head><p>Multi-Head Self-Attention Layer val . We show attention maps computed for a query pixel at position q.</p><p>Proof. Our first step will be to rework the expression of the Multi-Head Self-Attention operator from equation <ref type="formula" target="#formula_0">(1)</ref> and equation <ref type="formula" target="#formula_3">(4)</ref> such that the effect of the multiple heads becomes more transparent:</p><formula xml:id="formula_13">MHSA(X) = b out + h∈[N h ] softmax(A (h) )X W (h) val W out [(h − 1)D h + 1 : hD h + 1] W (h) (11) Note that each head's value matrix W (h) val ∈ R Din×D h and each block of the projection matrix W out of dimension D h × D out are learned.</formula><p>Assuming that D h ≥ D out , we can replace each pair of matrices by a learned matrix W (h) for each head. We consider one output pixel of the multi-head self-attention:</p><formula xml:id="formula_14">MHSA(X) q,: = h∈[N h ] k softmax(A (h) q,: ) k X k,: W (h) + b out<label>(12)</label></formula><p>Due to the conditions of the Lemma, for the h-th attention head the attention probability is one when k = q − f (h) and zero otherwise. The layer's output at pixel q is thus equal to</p><formula xml:id="formula_15">MHSA(X) q = h∈[N h ] X q−f (h),: W (h) + b out<label>(13)</label></formula><p>For K = √ N h , the above can be seen to be equivalent to a convolutional layer expressed in eq. 17: there is a one to one mapping (implied by map f ) between the matrices W (h) for h = [N h ] and the matrices W k1,k2,:,:</p><formula xml:id="formula_16">for all (k 1 , k 2 ) ∈ [K] 2 .</formula><p>Remark about D h and D out . It is frequent in transformer-based architectures to set</p><formula xml:id="formula_17">D h = D out /N h , hence D h &lt; D out .</formula><p>In that case, W (h) can be seen to be of rank D out − D h , which does not suffice to express every convolutional layer with D out channels. Nevertheless, it can be seen that any D h out of D out outputs of MHSA(X) can express the output of any convolutional layer with D h output channels. To cover both cases, in the statement of the main theorem we assert that the output channels of the convolutional layer should be min(D h , D out ). In practice, we advise to concatenate heads of dimension D h = D out instead of splitting the D out dimensions among heads to have exact re-parametrization and no "unused" channels. Lemma 2. There exists a relative encoding scheme {r δ ∈ R Dp } δ∈Z 2 with D p ≥ 3 and parameters W qry , W key , W key , u with D p ≤ D k such that, for every ∆ ∈ ∆ ∆ K there exists some vector v (conditioned on ∆) yielding softmax(A q,: ) k = 1 if k − q = ∆ and zero, otherwise.</p><p>Proof. We show by construction the existence of a D p = 3 dimensional relative encoding scheme yielding the required attention probabilities.</p><p>As the attention probabilities are independent of the input tensor X, we set W key = W qry = 0 which leaves only the last term of eq. (8). Setting W key ∈ R D k ×Dp to the identity matrix (with appropriate row padding), yields A q,k = v r δ where δ := k − q. Above, we have assumed that D p ≤ D k such that no information from r δ is lost. Now, suppose that we could write:</p><formula xml:id="formula_18">A q,k = −α( δ − ∆ 2 + c)<label>(14)</label></formula><p>for some constant c. In the above expression, the maximum attention score over A q,: is −αc and it is reached for A q,k with δ = ∆. On the other hand, the α coefficient can be used to scale arbitrarily the difference between A q,∆ and the other attention scores.</p><p>In this way, for δ = ∆, we have</p><formula xml:id="formula_19">lim α→∞ softmax(A q,: ) k = lim α→∞ e −α( δ−∆ 2 +c) k e −α( (k−q )−∆ 2 +c) = lim α→∞ e −α δ−∆ 2 k e −α (k−q )−∆ 2 = 1 1 + lim α→∞ k =k e −α (k−q )−∆ 2 = 1</formula><p>and for δ = ∆, the equation becomes lim α→∞ softmax(A q,: ) k = 0, exactly as needed to satisfy the lemma statement.</p><p>What remains is to prove that there exist v and {r δ } δ∈Z 2 for which eq. (14) holds. Expanding the RHS of the equation</p><formula xml:id="formula_20">, we have −α( δ − ∆ 2 + c) = −α( δ 2 + ∆ 2 − 2 δ, ∆ + c) . Now if we set v = −α (1, −2∆ 1 , −2∆ 2 ) and r δ = ( δ 2 , δ 1 , δ 2 ), then A q,k = v r δ = −α( δ 2 − 2∆ 1 δ 1 − 2∆ 2 δ 2 ) = −α( δ 2 − 2 δ, ∆ ) = −α( δ − ∆ 2 − ∆ 2 ),</formula><p>which matches eq. (14) with c = − ∆ 2 and the proof is concluded.</p><p>Remark on the magnitude of α. The exact representation of one pixel requires α (or the matrices W qry and W key ) to be arbitrary large, despite the fact that the attention probabilities of all other pixels converge exponentially to 0 as α grows. Nevertheless, practical implementations always rely on finite precision arithmetic for which a constant α suffices to satisfy our construction. For instance, since the smallest positive float32 scalar is approximately 10 −45 , setting α = 46 would suffice to obtain hard attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>The aim of this section is to validate the applicability of our theoretical results-which state that self-attention can perform convolution-and to examine whether self-attention layers in practice do actually learn to operate like convolutional layers when trained on standard image classification tasks. In particular, we study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. We find that, for both cases, the attention probabilities learned tend to respect the conditions of Lemma 1, supporting our hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IMPLEMENTATION DETAILS</head><p>We study a fully attentional model consisting of six multi-head self-attention layers. As it has already been shown by  that combining attention features with convolutional features improves performance on Cifar-100 and ImageNet, we do not focus on attaining state-of-the-art performance. Nevertheless, to validate that our model learns a meaningful classifier, we compare it to the standard ResNet18 <ref type="bibr" target="#b10">(He et al., 2015)</ref> on the CIFAR-10 dataset <ref type="bibr">(Krizhevsky et al.)</ref>. In all experiments, we use a 2 × 2 invertible down-sampling <ref type="bibr" target="#b13">(Jacobsen et al., 2018)</ref> on the input to reduce the size of the image. As the size of the attention coefficient tensors (stored during forward) scales quadratically with the size of the input image, full attention cannot be applied to bigger images. The fixed size representation of the input image is computed as the average pooling of the last layer representations and given to a linear classifier.    We used the PyTorch library <ref type="bibr" target="#b16">(Paszke et al., 2017)</ref> and based our implementation on PyTorch Transformers 5 . We release our code on Github 6 and hyper-parameters are listed in <ref type="table" target="#tab_2">Table 2</ref> (Appendix).</p><p>Remark on accuracy. To verify that our self-attention models perform reasonably well, we display in <ref type="figure">Figure 6</ref> the evolution of the test accuracy on CIFAR-10 over the 300 epochs of training for our self-attention models against a small ResNet <ref type="table">(Table 1)</ref>. The ResNet is faster to converge, but we cannot ascertain whether this corresponds to an inherent property of the architecture or an artifact of the adopted optimization procedures. Our implementation could be optimized to exploit the locality of Gaussian attention probabilities and reduce significantly the number of FLOPS. We observed that learned embeddings with content-based attention were harder to train probably due to their increased number of parameters. We believe that the performance gap can be bridged to match the ResNet performance, but this is not the focus of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">QUADRATIC ENCODING</head><p>As a first step, we aim to verify that, with the relative position encoding introduced in equation <ref type="formula">(9)</ref>, attention layers learn to behave like convolutional layers. We train nine attention heads at each layer to be on par with the 3 × 3 kernels used predominantly by the ResNet architecture. The center of attention of each head h is initialized to ∆ (h) ∼ N (0, 2I 2 ). <ref type="figure" target="#fig_3">Figure 3</ref> shows how the initial positions of the heads (different colors) at layer 4 changed during training. We can see that after optimization, the heads attend on specific pixel of the image forming a grid around the query pixel. Our intuition that Self-Attention applied to images learns convolutional filters around the queried pixel is confirmed. <ref type="figure" target="#fig_4">Figure 4</ref> displays all attention head at each layer of the model at the end of the training. It can be seen that in the first few layers the heads tend to focus on local patterns (layers 1 and 2), while deeper layers (layers 3-6) also attend to larger patterns by positioning the center of attention further from the queried pixel position. We also include in the Appendix a plot of the attention positions for a higher number of heads (N h = 16). <ref type="figure" target="#fig_0">Figure 14</ref> displays both local patterns similar to CNN and long range dependencies. Interestingly, attention heads do not overlap and seem to take an arrangement maximizing the coverage of the input space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LEARNED RELATIVE POSITIONAL ENCODING</head><p>We move on to study the positional encoding used in practice by fully-attentional models on images.</p><p>We implemented the 2D relative positional encoding scheme used by <ref type="bibr" target="#b19">(Ramachandran et al., 2019;</ref>: we learn a D p /2 position encoding vector for each row and each column pixel shift. Hence, the relative positional encoding of a key pixel at position k with a query pixel at position q is the concatenation of the row shift embedding δ 1 and the column shift embedding δ 2 (where δ = k − q). We chose D p = D out = 400 in the experiment. We differ from their (unpublished) implementation in the following points: (i) we do not use convolution stem and ResNet bottlenecks for downsampling, but only a 2 × 2 invertible downsampling layer <ref type="bibr" target="#b13">(Jacobsen et al., 2018)</ref> at input, (ii) we use D h = D out instead of D h = D out /N h backed by our theory that the effective number of learned filters is min(D h , D out ).</p><p>At first, we discard the input data and compute the attention scores solely as the last term of eq. (8).</p><p>The attention probabilities of each head at each layer are displayed on <ref type="figure">Figure 5</ref>. The figure confirms our hypothesis for the first two layers and partially for the third: even when left to learn the positional encoding scheme from randomly initialized vectors, certain self-attention heads (depicted on the left) learn to attend to individual pixels, closely matching the condition of Lemma 1 and thus Theorem 1. At the same time, other heads pay attention to horizontally-symmetric but non-localized patterns, as well as to long-range pixel inter-dependencies.</p><p>We move on to a more realistic setting where the attention scores are computed using both positional and content-based attention (i.e., q k + q r in <ref type="bibr" target="#b19">(Ramachandran et al., 2019)</ref>) which corresponds to a full-blown standalone self-attention model.</p><p>The attention probabilities of each head at each layer are displayed in <ref type="figure">Figure 6</ref>. We average the attention probabilities over a batch of 100 test images to outline the focus of each head and remove the dependency on the input image. Our hypothesis is confirmed for some heads of layer 2 and 3: even when left to learn the encoding from the data, certain self-attention heads only exploit positionbased attention to attend to distinct pixels at a fixed shift from the query pixel reproducing the receptive field of a convolutional kernel. Other heads use more content-based attention (see Figures 8 to 10 in Appendix for non-averaged probabilities) leveraging the advantage of Self-Attention over CNN which does not contradict our theory. In practice, it was shown by  that combining CNN and self-attention features outperforms each taken separately. Our experiments shows that such combination is learned when optimizing an unconstrained fully-attentional model.</p><p>The similarity between convolution and multi-head self-attention is striking when the query pixel is slid over the image: the localized attention patterns visible in <ref type="figure">Figure 6</ref> follow the query pixel. This characteristic behavior materializes when comparing <ref type="figure">Figure 6</ref> with the attention probabilities at a different query pixel (see <ref type="figure">Figure 7</ref> in Appendix). Attention patterns in layers 2 and 3 are not only localized but stand at a constant shift from the query pixel, similarly to convolving the receptive field of a convolutional kernel over an image. This phenomenon is made evident on our interactive website 7 . This tool is designed to explore different components of attention for diverse images with or without content-based attention. We believe that it is a useful instrument to further understand how MHSA learns to process images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>In this section, we review the known differences and similarities between CNNs and transformers.</p><p>The use of CNN networks for text-at word level <ref type="bibr" target="#b9">(Gehring et al., 2017)</ref> or character level <ref type="bibr" target="#b14">(Kim, 2014)</ref>-is more seldom than transformers (or RNN). Transformers and convolutional models have been extensively compared empirically on tasks of Natural Language Processing and Neural Machine Translation. It was observed that transformers have a competitive advantage over convolutional model applied to text <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref>. It is only recently that ; <ref type="bibr" target="#b19">Ramachandran et al. (2019)</ref> used transformers on images and showed that they achieve similar accuracy as ResNets. However, their comparison only covers performance and number of parameters and FLOPS but not expressive power.</p><p>Beyond performance and computational-cost comparisons of transformers and CNN, the study of expressiveness of these architectures has focused on their ability to capture long-term dependencies . Another interesting line of research has demonstrated that transformers are Turingcomplete <ref type="bibr" target="#b6">(Dehghani et al., 2018;</ref><ref type="bibr" target="#b17">Pérez et al., 2019)</ref>, which is an important theoretical result but is not informative for practitioners. To the best of our knowledge, we are the first to show that the class of functions expressed by a layer of self-attention encloses all convolutional filters.</p><p>The closest work in bridging the gap between attention and convolution is due to <ref type="bibr" target="#b1">Andreoli (2019)</ref>. They cast attention and convolution into a unified framework leveraging tensor outerproduct. In this framework, the receptive field of a convolution is represented by a "basis" tensor A ∈ R K×K×H×W ×H×W . For instance, the receptive field of a classical K × K convolutional kernel would be encoded by A ∆,q,k = 1{k − q = ∆} for ∆ ∈ ∆ ∆ K . The author distinguishes this index-based convolution with content-based convolution where A is computed from the value of the input, e.g., using a key/query dot-product attention. Our work moves further and presents sufficient conditions for relative positional encoding injected into the input content (as done in practice) to allow content-based convolution to express any index-based convolution. We further show experimentally that such behavior is learned in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We showed that self-attention layers applied to images can express any convolutional layer (given sufficiently many heads) and that fully-attentional models learn to combine local behavior (similar to convolution) and global attention based on input content. More generally, fully-attentional models seem to learn a generalization of CNNs where the kernel pattern is learned at the same time as the filters-similar to deformable convolutions <ref type="bibr" target="#b4">(Dai et al., 2017;</ref><ref type="bibr" target="#b24">Zampieri, 2019)</ref>. Interesting directions for future work include translating existing insights from the rich CNNs literature back to transformers on various data modalities, including images, text and time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Jean-Baptiste Cordonnier is thankful to the Swiss Data Science Center (SDSC) for funding this work. Andreas Loukas was supported by the Swiss National Science Foundation (project Deep Learning for Graph Structured Data, grant number PZ00P2 179981).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A MORE EXAMPLES WITH CONTENT-BASED ATTENTION</head><p>We present more examples of attention probabilities computed by self-attention model. <ref type="figure">Figure 7</ref> shows average attention at a different query pixel than <ref type="figure" target="#fig_0">Figure 6. Figures 8 to 10</ref> display attention for single images.      <ref type="table">Table 3</ref>: Types of positional encoding used by transformers models applied to text (top) and images (bottom). When multiple encoding types have been tried, we report the one advised by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D GENERALIZED LEMMA 1</head><p>We present a generalization of Lemma 1 that replaces the necessity of hard attention (to single pixels) by a milder assumption: the attention probabilities should span the grid receptive field.  Proof. Our first step will be to rework the expression of the Multi-Head Self-Attention operator from equation <ref type="formula" target="#formula_0">(1)</ref> and equation <ref type="formula" target="#formula_3">(4)</ref> such that the effect of the multiple heads becomes more transparent:</p><formula xml:id="formula_21">MHSA(X) = b out + h∈[N h ] softmax(A (h) )X W (h) val W out [(h − 1)D h + 1 : hD h + 1] W (h)<label>(15)</label></formula><p>Note that each head's value matrix W (h) val ∈ R Din×D h and each block of the projection matrix W out of dimension D h × D out are learned. Assuming that D h ≥ D out , we can replace each pair of matrices by a learned matrix W (h) for each head. We consider one output pixel of the multi-head self-attention and drop the bias term for simplicity:</p><formula xml:id="formula_22">MHSA(X) q,: = h∈[N h ] k a (h) q,k X k,: W (h) = k X k,: h∈[N h ] a (h) q,k W (h) W SA q,k ∈R D in ×D out ,<label>(16)</label></formula><p>with a </p><formula xml:id="formula_23">W conv q,k ∈R D in ×D out .<label>(17)</label></formula><p>Equality between equations <ref type="formula" target="#formula_0">(16)</ref> and <ref type="formula" target="#formula_0">(17)</ref>  Sufficient. Given that row(E q ) ⊆ row(A q ), there exists Φ ∈ R K 2 ×N h such that E q = ΦA q and a valid decomposition is</p><formula xml:id="formula_24">W SA = W conv Φ which gives W SA A q = V conv q .</formula><p>Necessary. Assume there exists x ∈ R HW such that x ∈ row(E q ) and x ∈ row(A q ) and set x to be a row of V conv q . Then, W SA A q = V conv q for any W SA and there is no possible decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E GENERALIZED QUADRATIC POSITIONAL ENCODING</head><p>We noticed the similarity of the attention probabilities in the quadratic positional encoding (Section 3) to isotropic bivariate Gaussian distributions with bounded support:</p><formula xml:id="formula_25">softmax(A q,: ) k = e −α (k−q)−∆ 2 k ∈[W ]×[H] e −α (k −q)−∆ 2 .<label>(18)</label></formula><p>Building on this observation, we further extended our attention mechanism to non-isotropic Gaussian distribution over pixel positions. Each head is parametrized by a center of attention ∆ and a covariance matrix Σ to obtain the following attention scores,</p><formula xml:id="formula_26">A q,k = − 1 2 (δ − ∆) Σ −1 (δ − ∆) = − 1 2 δ Σ −1 δ + δ Σ −1 ∆ − 1 2 ∆ Σ −1 ∆ ,<label>(19)</label></formula><p>where, once more, δ = k − q. The last term can be discarded because the softmax is shift invariant and we rewrite the attention coefficient as a dot product between the head target vector v and the relative position encoding r δ (consisting of the first and second order combinations of the shift in pixels δ):</p><formula xml:id="formula_27">v = 1 2 (2(Σ −1 ∆) 1 , 2(Σ −1 ∆) 2 , −Σ −1 1,1 , −Σ −1 2,2 , −2 · Σ −1 1,2 ) and r δ = (δ 1 , δ 2 , δ 2 1 , δ 2 2 , δ 1 δ 2 ) .</formula><p>Evaluation. We trained our model using this generalized quadratic relative position encoding. We were curious to see if, using the above encoding the self-attention model would learn to attend to non-isotropic groups of pixels-thus forming unseen patterns in CNNs. Each head was parametrized by ∆ ∈ R 2 and Σ −1/2 ∈ R 2×2 to ensure that the covariance matrix remained positive semi-definite. We initialized the center of attention to ∆ (h) ∼ N (0, 2I 2 ) and Σ −1/2 = I 2 + N (0, 0.01I 2 ) so that initial attention probabilities were close to an isotropic Gaussian. <ref type="figure" target="#fig_0">Figure 12</ref> shows that the network did learn non-isotropic attention probability patterns, especially in high layers. Nevertheless, the fact that we do not obtain any performance improvement seems to suggest that attention non-isotropy is not particularly helpful in practice-the quadratic positional encoding suffices. <ref type="figure" target="#fig_0">Figure 12</ref>: Centers of attention of each attention head (different colors) for the 6 self-attention layers using non-isotropic Gaussian parametrization. The central black square is the query pixel, whereas solid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.</p><p>Pruning degenerated heads. Some non-isotropic attention heads attend on "non-intuitive" patches of pixels: either attending a very thin stripe of pixels, when Σ −1 was almost singular, or attending all pixels uniformly, when Σ −1 was close to 0 (i.e. constant attention scores). We asked ourselves, are such attention patterns indeed useful for the model or are these heads degenerated and unused? To find out, we pruned all heads having largest eigen-values smaller than 10 −5 or condition number (ratio of the biggest and smallest eigen-values) greater than 10 5 . Specifically in our model with 6-layer and 9-heads each, we pruned [2, 4, 1, 2, 6, 0] heads from the first to the last layer. This means that these layers cannot express a 3 × 3 kernel anymore. As shown in yellow on <ref type="figure" target="#fig_2">fig. 2</ref>, this ablation initially hurts a bit the performance, probably due to off biases, but after a few epochs of continued training with a smaller learning rate (divided by 10) the accuracy recovers its unpruned value. Hence, without sacrificing performance, we reduce the size of the parameters and the number of FLOPS by a fourth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F INCREASING THE NUMBER OF HEADS</head><p>For completeness, we also tested increasing the number of heads of our architecture from 9 to 16.  <ref type="table">Table 4</ref>: Number of parameters and accuracy on CIFAR-10 per model. SA stands for Self-Attention. <ref type="figure" target="#fig_0">Figure 14</ref>: Centers of attention for 16 attention heads (different colors) for the 6 self-attention layers using quadratic positional encoding. The central black square is the query pixel, whereas solid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.</p><p>Similar to <ref type="figure" target="#fig_4">Figure 4</ref>, we see that the network distinguishes two main types of attention patterns. Localized heads (i.e., those that attend to nearly individual pixels) appear more frequently in the first few layers. The self-attention layer uses these heads to act in a manner similar to how convolutional layers do. Heads with less-localized attention become more common at higher layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of a Multi-Head Self-Attention layer applied to a tensor image X. Each head h attends pixel values around shift ∆ (h) and learn a filter matrix W (h)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. SA learned emb. + content-based att.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Test accuracy on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Centers of attention of each attention head (different colors) at layer 4 during the training with quadratic relative positional encoding. The central black square is the query pixel, whereas solid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Centers of attention of each attention head (different colors) for the 6 self-attention layers using quadratic positional encoding. The central black square is the query pixel, whereas solid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Attention probabilities of each head (column) at each layer (row) using learned relative positional encoding without content-based attention. The central black square is the query pixel. We reordered the heads for visualization and zoomed on the 7x7 pixels around the query pixel. Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using learned relative positional encoding and content-content based attention. Attention maps are averaged over 100 test images to display head behavior and remove the dependence on the input content. The black square is the query pixel. More examples are presented in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using learned relative positional encoding and content-content attention. We present the average of 100 test images. The black square is the query pixel. Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using learned relative positional encoding and content-content based attention. The query pixel (black square) is on the frog head. Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using learned relative positional encoding and content-content based attention. The query pixel (black square) is on the horse head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using learned relative positional encoding and content-content based attention. The query pixel (black square) is on the building in the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Factorization of the vectorized weight matrices V conv q and V SA q used to compute the output at position q for an input image of dimension H × W . On the left: a convolution of kernel 2 × 2, on the right: a self-attention with N h = 5 heads. D in = 2, D out = 3 in both cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>= softmax(A (h) q,: ) k . We rewrite the output of a convolution at pixel q in the same manner: Conv(X) q,: = ∆∈∆ ∆ K X q+∆,: W ∆,:,: = k∈[H]×[W ] X k,: 1 {k−q∈∆ ∆ K } W k−q,:,:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>: Self-attention network parameters</cell><cell></cell></row><row><cell cols="2">C POSITIONAL ENCODING REFERENCES</cell><cell></cell></row><row><cell>Model</cell><cell>type of positional encoding</cell><cell>relative</cell></row><row><cell></cell><cell>sinusoids learned quadratic</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The conditions of this Lemma are still satisfied by Lemma 2, hence Theorem 1 follows. Lemma 3. Consider a multi-head self-attention layer consisting of N h ≥ K 2 heads, D h ≥ D out and let ω : [H]×[W ] → [HW ] be a pixel indexing. Then, for any convolutional layer with a K × K kernel and D out output channels, there exists {W (h) val } h∈[N h ] and W out such that MHSA(X) = Conv(X) for every X ∈ R W ×H×Din if and only if, for all q ∈ [H] × [W ], 8 span({e ω(q+∆) ∈ R HW : ∆ ∈ ∆ ∆ K }) ⊆ span({vect(softmax(A (h)</figDesc><table /><note>q,: )) : h ∈ [N h ]}) .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>holds for any input X if and only if the linear transformations for each pair of key/query pixels are equal, i.e. W conv q,k = W SA q,k ∀q, k. We vectorize the weight matrices into matrices of dimension D in D out × HW as V conv The matrix V conv q has a restricted support: only the columns associated with a pixel shift ∆ ∈ ∆ ∆ K in the receptive field of pixel q can be non-zero. This leads to the factorization V conv q = W conv E q displayed inFigure 11where W conv ∈ R DinDout×K 2 and E q ∈ R K 2 ×HW . Given an ordering of the shifts ∆ ∈ ∆ ∆ K indexed by j, set (W conv ) :,j = vect(W ∆,:,: ) and (E q ) j,: = e ω(q+∆) . On the other hand, we decompose V SA q = W SA A q with (W SA ) :,h = vect(W (h) ) and (A q ) h,i = a The proof is concluded by showing that row(E q ) ⊆ row(A q ) is a necessary and sufficient condition for the existence of a W SA such that any V conv q = W conv E q can be written as W SA A q .</figDesc><table><row><cell></cell><cell></cell><cell>q</cell><cell>:= [vect(W conv q,k )] k∈[H]×[W ]</cell></row><row><cell>and V SA q</cell><cell cols="2">:= [vect(W SA q,k )] k∈[H]×[W ] . Hence, to show that Conv(X) = MHSA(X) for all X, we</cell></row><row><cell cols="2">must show that V conv q</cell><cell>= V SA q for all q.</cell></row><row><cell></cell><cell></cell><cell>(h) q,ω(i) .</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">epfml.github.io/attention-cnn 3 softmax (At,:) k = exp(A t,k )/ p exp(At,p)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To simplify notation, we index the first two dimensions of the tensor from − K/2 to K/2 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">github.com/huggingface/pytorch-transformers 6 github.com/epfml/attention-cnn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">epfml.github.io/attention-cnn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">the vectorization operator vect(·) flattens a matrix into a vector</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg ; Martin Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas. Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolution, attention and structure embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Andreoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2019 workshop on Graph Representation Learning</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09925</idno>
		<title level="m">Attention Augmented Convolutional Networks</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/1703.06211</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Universal transformers. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno>abs/1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised scalable representation learning for multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Dieuleveut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno>abs/1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">i-revnet: Deep invertible networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On the turing completeness of modern neural network architectures. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marinkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barceló</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. CoRR, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Geometric deep learning for volumetric computational fluid dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Zampieri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
