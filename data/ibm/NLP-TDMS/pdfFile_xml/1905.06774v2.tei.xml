<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RICHLY ACTIVATED GRAPH CONVOLUTIONAL NETWORK FOR ACTION RECOGNITION WITH INCOMPLETE SKELETONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RICHLY ACTIVATED GRAPH CONVOLUTIONAL NETWORK FOR ACTION RECOGNITION WITH INCOMPLETE SKELETONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action Recognition</term>
					<term>Skeleton Data</term>
					<term>Graph Convolutional Network</term>
					<term>Activation Maps</term>
					<term>Occlusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current methods for skeleton-based human action recognition usually work with completely observed skeletons. However, in real scenarios, it is prone to capture incomplete and noisy skeletons, which will deteriorate the performance of traditional models. To enhance the robustness of action recognition models to incomplete skeletons, we propose a multi-stream graph convolutional network (GCN) for exploring sufficient discriminative features distributed over all skeleton joints. Here, each stream of the network is only responsible for learning features from currently unactivated joints, which are distinguished by the class activation maps (CAM) obtained by preceding streams, so that the activated joints of the proposed method are obviously more than traditional methods. Thus, the proposed method is termed richly activated GCN (RA-GCN), where the richly discovered features will improve the robustness of the model. Compared to the state-of-the-art methods, the RA-GCN achieves comparable performance on the NTU RGB+D dataset. Moreover, on a synthetic occlusion dataset, the performance deterioration can be alleviated by the RA-GCN significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Skeleton-based human action recognition methods become increasingly important in many applications and achieve great progress, due to the superiority in background adaptability, robustness to light intensity and less computational cost. Skeleton data is composed of 3D coordinates of multiple spatial and temporal skeleton joints, which can be either collected by multimodal sensors such as Kinect or directly estimated from 2D images by pose estimation methods. Traditional methods usually deal with skeleton data in two ways. One way is to connect these joints into a whole vector, then model temporal information using RNN-based methods Spatial occlusion sample <ref type="figure">Fig. 1</ref>. The demonstration of the occlusion dataset based on the NTU RGB+D dataset. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. The other way is to treat or expand temporal sequences of joints into images, then utilize CNN-based methods to recognize actions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. However, the spatial structure information among skeleton joints is hard to be ultilized effectively by both the RNN-based and CNNbased methods, though researchers propose some additional constraints or dedicated network structures to strenuously encode the spatial structure of skeleton joints. Recently, Yan et al. <ref type="bibr" target="#b10">[11]</ref> firstly apply graph-based methods to skeleton-based action recognition, and propose the spatial temporal graph convolutional networks (ST-GCN) to extract features embedded in the spatial configuration and the temporal dynamics. Additionally, there are also some action recognition methods using graphic techniques, such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. But these are mainly based on RGB videos, instead of skeleton data.</p><p>All these above methods assume that the complete skeleton joints can be well captured, while the incomplete case is not considered. However, it is often difficult to obtain a complete skeleton sequence in real scenarios. For example, pedestrians may be occluded by parked vehicles or other contextual objects observed. Meanwhile, when facing to incomplete skeletons, traditional methods will have varying degrees of performance deterioration. Therefore, how to recognize  The pipeline of RA-GCN with three baseline models. Each baseline model is an ST-GCN network with ten layers. The two numbers under the ST-GCN layers are input channels and output channels, respectively. Other layers contain the same input and output channels. Each layer with different input and output channels uses a temporal stride 2 to reduce the sequence length. GAP is global average pooling operation, and ⊗ and ⊕ denote element-wise multiplication and concatenation, respectively. actions with incomplete skeletons is a challenging problem.</p><p>Many researchers are exploring to extract non-local features from all positions in the input feature maps, such as <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>. Inspired by this, we propose a multi-stream graph convolutional network (GCN) to explore sufficient discriminative features for robust action recognitions. Here, a subtle technique, class activation maps (CAM), is utilized to distinguish the discriminative skeleton joints activated by each stream. The activation maps obtained by preceding streams are accumulated as a mask matrix to inform the new stream about which joints have been already activated. Then, the new stream will be forced to explore discriminative features from unactivated joints. Therefore, the proposed method is called richly activated GCN (RA-GCN), where the richly discovered discriminative features will improve the robustness of the model to incomplete skeletons. The experimental results on the NTU RGB+D dataset <ref type="bibr" target="#b16">[17]</ref> show that the RA-GCN achieves comparable performance to the state-of-the-art methods. Furthermore, for the case of incomplete skeletons, we construct a synthetic occlusion dataset, where the joints in the NTU dataset are partially occluded over both spatial and temporal dimensions. Some examples on the two types of occlusions are shown in <ref type="figure">Fig.1</ref>. On the new dataset, the RA-GCN significantly alleviates the performance deterioration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RICHLY ACTIVATED GCN</head><p>In order to enhance the robustness of action recognition models, we propose the RA-GCN to explore sufficient discrim-inative features from the training skeleton sequences. The overview of RA-GCN is presented in <ref type="figure" target="#fig_1">Fig.2</ref>. Suppose that V is the number of joints in one skeleton, M is the number of skeletons in one frame and T is the number of frames in one sequence. Then, the size of input data x is C × T × V × M , where C = 3 denotes the 3D coordinates of each joint.</p><p>The proposed network consists of three main steps. Firstly, in the preprocessing module, for extracting more informative features, the input data x is transformed into x , whose size is 3C × T × V × M . The preprocessing module is composed of two parts. The first part extracts motion features by computing temporal difference</p><formula xml:id="formula_0">x t = x[t + 1] − x[t], where x[t]</formula><p>means the input data of the t-th frame. The second part calculates the relative coordinates x r between all joints and the center joint (center trunk) in each frame. Then, x will be obtained by concatenating x, x t and x r . Secondly, for each stream, the skeleton joints in x will be filtered by the element-wise product with a mask matrix, which records the currently unactivated joints. These joints are distinguished by accumulating the activated maps obtained by the activation modules of preceding streams. Here, the mask matrix is initialized to all-one matrix with the same size as x . After the masking operation, the input data of each stream only contains unactivated joints, and subsequently passes through an ST-GCN network <ref type="bibr" target="#b10">[11]</ref> to obtain a feature representation based on partial skeleton joints. Finally, the features of all streams are concatenated in the output module, and a softmax layer is used to obtain the final class of input x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline Model</head><p>The baseline model is the ST-GCN <ref type="bibr" target="#b10">[11]</ref>, which is composed of several spatial convolutional blocks and temporal convolutional blocks. Concretely, the spatial graph convolutional block can be implemented by the following formulation:</p><formula xml:id="formula_1">f out = Dmax d=0 W d f in (Λ − 1 2 d A d Λ − 1 2 d ⊗ M d ),<label>(1)</label></formula><p>where D max is the predefined maximum distance, f in and f out are the input and output feature maps respectively, A d denotes the adjacency matrix for graphic distance d, Λ ii d = k A ik d + α is the normalized diagonal matrix, A ik d denotes the element of the i-th row and k-th column of A d and α is set to a small value, e.g. 10 −4 , to avoid the empty rows in Λ d . For each adjacency matrix, we accompany it with a learnable matrix M d , which expresses the importance of each edge.</p><p>After the spatial graph convolutional block, a 1 × L Conv layer is used to extract temporal information of the feature map f out , where L is the temporal window size. Both spatial and temporal convolutional blocks are followed with a Batch-Norm layer and a ReLU layer, and the total ST-GCN layer contains a residual connection. Besides, an adaptive dropout layer is added between the spatial and temporal convolutional blocks to avoid overfitting. More details of the ST-GCN will be found in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Activation Module</head><p>The activation module in the RA-GCN is constructed to distinguish the activated joints of each stream, then guide the learning process of the new stream by accumulating the activated maps of preceding streams. This procedure can be implemented mainly by the CAM technique <ref type="bibr" target="#b17">[18]</ref>. The original CAM technique is to localize class-specific image regions in a single forward-pass, and M c is defined as the activation map for class c, where each spatial point is</p><formula xml:id="formula_2">M c (x, y) = k w c k f k (x, y).<label>(2)</label></formula><p>In this formulation, f k (x, y) is the feature map before the global average pooling operation, and w c k is the weight of the k-th channel for class c. In this paper, we replace the coordinates (x, y) in an image with the frame number t and the joint number i in a skeleton sequence, by which we are able to locate the activated joints. These joints can also be regarded as the attention joints of the corresponding streams. Here, the class c is selected as the true class. Then, the mask matrix of stream s is calculated as follows.</p><formula xml:id="formula_3">mask s = ( s−1 i=1 mask i ) ⊗ (1 − Sof tmax(M s−1 c )),<label>(3)</label></formula><p>where denotes the element-wise product of all mask matrices before the s-th stream. Specially, the mask matrix of the baseline t1 t2 t3 t4 t5</p><p>RA-GCN 2 streams RA-GCN 3 streams <ref type="figure">Fig. 3</ref>. An example of activated joints of all streams for the baseline model, the RA-GCN whth 2 streams and 3 streams. The red points denote the activated joints, while the blue points denote the unactivated joints. Best viewed in color.</p><p>first stream is an all-one matrix. Finally, the input of stream s will be obtained by</p><formula xml:id="formula_4">x s = x ⊗ mask s ,<label>(4)</label></formula><p>where x is the skeleton data after preprocessing. Eq. 4 illustrates that the input of stream s only consists of the joints which are not activated by previous streams. Thus the RA-GCN will explore discriminative features from all joints sufficiently. <ref type="figure">Fig.3</ref> shows an example of the activated joints of all streams for the baseline model, the RA-GCN whth 2 streams and 3 streams, from which we can observe that the RA-GCN discovers significantly more activated joints than the baseline model. The code of RA-GCN is availabel on https://github.com/yfsong0709/RA-GCNv1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and Implemental Details</head><p>In this section, we evaluate the performance of the RA-GCN on a large-scale dataset NTU RGB+D <ref type="bibr" target="#b16">[17]</ref>, which is the currently largest indoor action recognition dataset. This dataset contains 56880 video samples collected by Microsoft Kinect v2, and consists of 60 action classes performed by 40 subjects. The maximum frame number T is set to 300 for simplicity. The authors of this dataset recommend two benchmarks: (1) cross-subject (CS) contains 40320 and 16560 samples for training and evaluation, which splits 40 subjects into training and evaluation groups; (2) cross-view (CV) contains 37920 and 18960 samples, which uses camera 2 and 3 for training and camera 1 for evaluation.</p><p>Before training RA-GCN, we need to pre-train an ST-GCN network with transformed skeleton data to get the baseline model, and the training setting is the same as <ref type="bibr" target="#b10">[11]</ref>. Then, initialize all streams of RA-GCN with this baseline model. year CS CV H-BRNN <ref type="bibr" target="#b18">[19]</ref> 2015 59.1 64.0 PA-LSTM <ref type="bibr" target="#b16">[17]</ref> 2016 62.9 70.3 VA-LSTM <ref type="bibr" target="#b2">[3]</ref> 2017 79.4 87.6 ST-GCN (baseline) <ref type="bibr" target="#b10">[11]</ref> 2018 81.5</p><p>88.3 HCN <ref type="bibr" target="#b8">[9]</ref> 2018 86.5 91.1 SR-TSL <ref type="bibr" target="#b4">[5]</ref> 2018 84.8 92.4 PB-GCN <ref type="bibr" target="#b19">[20]</ref> 2018 87.5 93.2 *2s RA-GCN -85.8 93.0 *3s RA-GCN -85.9 93.5 *: 2s denotes two streams and 3s denotes three streams Finally, finetune the RA-GCN model. All of our experiments are running on two TITAN X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental results on complete skeletons</head><p>We compare the performance of RA-GCN against previous state-of-the-art methods on the NTU RGB+D dataset. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our method achieves better performance than other models on the CV benchmark, though it is only 1.6% less than PB-GCN <ref type="bibr" target="#b19">[20]</ref> on the CS benchmark. Compared to the baseline, our method outperforms by 4.4% and 5.2%, respectively. The RA-GCN only achieves comparable performance to the state-of-the-art method, because the RA-GCN aims to discover more discriminative joints, while the most actions can be recognized by only a few main joints. However, when these main joints are occluded, the performance of traditional methods will deteriorate significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation studies</head><p>In Section 2.1, we introduce two hyperparameters for the baseline model, D max for maximum distance and L for temporal window size. These two hyperparameters have a great impact on our model. We test six groups of parameters and the experimental results are given in <ref type="table" target="#tab_2">Table 2</ref>. It is observed that our model achieves the best accuracy when D max = 2 and L = 5 on the CS benchmark. As to the CV benchmark, D max and L are optimally set to 3 and 9, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Experimental results on incomplete skeletons</head><p>To validate the robustness of our method to incomplete skeletons, we construct a synthetic occlusion dataset based on the NTU-RGB+D dataset, where some joints are selected to be occluded (set to 0) over both spatial and temporal dimensions. For spatial occlusion, we train the testing models with complete skeletons, then evaluate them with skeletons without part 1, 2, 3, 4, 5, which denote left arm, right arm, two hands, two legs and trunk, respectively. For temporal occlusion, we randomly occlude a block of frames in first 100 frames, because the lengths of many sequences are less than 100. Some examples of the occlusion dataset are demonstrated in <ref type="figure">Fig.1</ref>. On the synthetic occlusion dataset, we test the baseline model <ref type="bibr" target="#b10">[11]</ref>, SR-TSL <ref type="bibr" target="#b4">[5]</ref>, RA-GCN with 2 streams and 3 streams. The experimental results are displayed in <ref type="table" target="#tab_3">Table 3</ref>, from which it is revealed that the 3s RA-GCN greatly outperforms the other models in most occlusion experiments except occluding part 2. Especially, in temporal occlusion experiments, 3s RA-GCN achieves an increasing superiority to other models. Thus the performance deterioration can be alleviated by the proposed methods. We also find that when some important joints, such as right arms, are occluded, some action categories, e.g. handshaking, cannot be inferred by other joints. The proposed method will fail in such case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we have proposed a novel model named RA-GCN, which achieves much better than the baseline model and improves the robustness of the model. With extensive experiments on the NTU RGB+D dataset, we verify the effectiveness of our model in occlusion scenarios.</p><p>In the future, we will add the attention module into our model, in order to make each stream focus more on certain discriminative joints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work is sponsored by National Key R&amp;D Program of China (2016YFB1001002), National Natural Science Foundation of China (61525306, 61633021, 61721004, 61420106015) and CAS-AIR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The pipeline of RA-GCN with three baseline models. Each baseline model is an ST-GCN network with ten layers. The two numbers under the ST-GCN layers are input channels and output channels, respectively. Other layers contain the same input and output channels. Each layer with different input and output channels uses a temporal stride 2 to reduce the sequence length. GAP is global average pooling operation, and ⊗ and ⊕ denote element-wise multiplication and concatenation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1905.06774v2 [cs.CV] 17 May 2019</figDesc><table><row><cell cols="3">Data Preprocessing</cell><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell>X'</cell><cell>BatchNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GAP</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>9,64</cell><cell>64,128 Activation</cell><cell cols="2">128,256</cell><cell cols="2">feature weight</cell><cell>256</cell><cell>60</cell></row><row><cell></cell><cell>Temporal</cell><cell></cell><cell cols="2">mask2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>difference</cell><cell></cell><cell>X'</cell><cell>BatchNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GAP</cell><cell></cell></row><row><cell>X</cell><cell>Relative coordinate</cell><cell>X'</cell><cell></cell><cell>9,64</cell><cell cols="2">64,128 Activation</cell><cell>128,256</cell><cell></cell><cell>feature weight</cell><cell>256</cell><cell>60</cell><cell>Softmax</cell><cell>class</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">mask3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>X'</cell><cell>BatchNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GAP</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>9,64</cell><cell>64,128 Activation</cell><cell></cell><cell cols="2">128,256</cell><cell>feature weight</cell><cell>256</cell><cell>60</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Multi-stream Models</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of methods on NTU RGB+D (%)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison</figDesc><table><row><cell cols="3">of different maximum distances and</cell></row><row><cell cols="2">temporal window sizes on NTU RGB+D (%)</cell><cell></cell></row><row><cell></cell><cell>CS</cell><cell>CV</cell></row><row><cell>D max = 1, L = 5</cell><cell>85.2</cell><cell>90.5</cell></row><row><cell>D max = 2, L = 5</cell><cell>85.9</cell><cell>91.6</cell></row><row><cell>D max = 3, L = 5</cell><cell>85.8</cell><cell>92.2</cell></row><row><cell>D max = 1, L = 9</cell><cell>85.2</cell><cell>91.7</cell></row><row><cell>D max = 2, L = 9</cell><cell>85.4</cell><cell>92.7</cell></row><row><cell>D max = 3, L = 9</cell><cell>85.0</cell><cell>93.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Occlusion experiments on the CS benchmark (%) ] 80.7 71.4 60.5 62.6 77.4 50.2 SR-TSL [5] 84.8 70.6 54.3 48.6 74.3 56.2 2s RA-GCN 85.8 72.8 58.3 73.2 80.3 70.6 3s RA-GCN 85.9 73.4 60.4 73.5 81.1 70.6</figDesc><table><row><cell>spatial</cell><cell></cell><cell></cell><cell cols="2">occluded part</cell><cell></cell><cell></cell></row><row><cell>occlusion</cell><cell>none</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>baseline [11*difference</cell><cell>5.2</cell><cell>2.0</cell><cell cols="3">-0.1 10.9 3.7</cell><cell>20.4</cell></row><row><cell>temporal</cell><cell></cell><cell cols="4">occluded frame number</cell><cell></cell></row><row><cell>occlusion</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell cols="7">baseline [11] 80.7 69.3 57.0 44.5 34.5 24.0</cell></row><row><cell>SR-TSL [5]</cell><cell cols="6">84.8 70.9 62.6 48.8 41.3 28.8</cell></row><row><cell cols="7">2s RA-GCN 85.8 82.0 74.7 64.9 52.5 38.6</cell></row><row><cell cols="7">3s RA-GCN 85.9 81.9 75.0 66.3 54.4 40.6</cell></row><row><cell>*difference</cell><cell>5.2</cell><cell cols="5">12.6 18.0 21.8 19.9 16.6</cell></row><row><cell cols="7">*: the difference between 3s RA-GCN and baseline model</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatiotemporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On geometric features for skeleton-based action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WACV. IEEE</publisher>
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2136" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="103" to="118" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME Workshops. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Investigation of different skeleton features for cnn-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME Workshops. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="786" to="792" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5323" to="5332" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Directed acyclic graph kernels for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bags-of-daglets for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1550" to="1554" />
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear cross-view sample enrichment for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7794" to="7803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="369" to="378" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
