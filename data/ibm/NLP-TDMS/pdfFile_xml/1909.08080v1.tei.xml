<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Video Super-Resolution through Recurrent Latent Space Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Fuoli</surname></persName>
							<email>dario.fuoli@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
							<email>shuhang.gu@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
							<email>radu.timofte@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Video Super-Resolution through Recurrent Latent Space Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the recent trend for ultra high definition displays, the demand for high quality and efficient video superresolution (VSR) has become more important than ever. Previous methods adopt complex motion compensation strategies to exploit temporal information when estimating the missing high frequency details. However, as the motion estimation problem is a highly challenging problem, inaccurate motion compensation may affect the performance of VSR algorithms. Furthermore, the complex motion compensation module may also introduce a heavy computational burden, which limits the application of these methods in real systems. In this paper, we propose an efficient recurrent latent space propagation (RLSP) algorithm for fast VSR. RLSP introduces high-dimensional latent states to propagate temporal information between frames in an implicit manner. Our experimental results show that RLSP is a highly efficient and effective method to deal with the VSR problem. We outperform current state-of-the-art method <ref type="bibr" target="#b14">[15]</ref> with over 70× speed-up. arXiv:1909.08080v1 [eess.IV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Super-resolution aims to obtain high-resolution (HR) images from its low-resolution (LR) observations. It provides a practical solution to enhance existing images as well as alleviating the pressure of data transportation. One category of methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11]</ref> takes a single LR image as input. The single image super-resolution (SISR) problem has been intensively studied for many years and is still an active topic in the area of computer vision. Another category of approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36]</ref>, i.e. video superresolution (VSR), takes LR video as input. In contrast to SISR methods, which can only rely on natural image priors for estimation of high resolution details, VSR exploits temporal information for improved recovery of image details.</p><p>A key issue to the success of VSR algorithms, is how to take full advantage from temporal information <ref type="bibr" target="#b27">[28]</ref>. In the early years, different methods have been suggested to model the subpixel-level motion between LR observations,   <ref type="bibr" target="#b30">[31]</ref> and DUF <ref type="bibr" target="#b14">[15]</ref>.</p><p>including the Bilateral prior <ref type="bibr" target="#b7">[8]</ref> and Bayesian estimation model <ref type="bibr" target="#b23">[24]</ref> have been adopted to solve the VSR problem. In recent years, the success of deep learning in other vision tasks inspired the research to apply convolutional neural networks (CNN) also to VSR. Following a similar strategy, already adopted in conventional VSR algorithms, most of existing deep learning based VSR methods divide the task into two sub-problems: motion estimation and the following compensation procedure. In the last several years, a large number of elaborately designed models have been proposed to capture the subpixel motion between input LR frames. However, as subpixel-level alignment of images is a highly challenging problem, these types of approaches may generate blurred estimations, when the motion compensation module fails to generate accurate motion estimation. Furthermore, the complex motion estimation and compensation modules are often computationally expensive, which makes these methods unable to handle HR video in real time.</p><p>To address the accuracy issue, Jo et al. <ref type="bibr" target="#b14">[15]</ref> proposed dynamic upsampling filters (DUF) to perform VSR without explicit motion compensation. In their solution, motion information is implicitly captured with dynamic upsampling filters and the HR frame is directly constructed by local filtering of the center input frame. Such an implicit formulation avoids conducting motion compensation in the image space and helps DUF to obtain state-of-the-art VSR results. However, as DUF needs to estimate dynamic filters in each location, the algorithm suffers from heavy computation as well as putting a burden on memory for processing large size images.</p><p>In this paper, we propose a recurrent latent space propagation (RLSP) method for efficient VSR. RLSP follows a similar strategy as FRVSR <ref type="bibr" target="#b30">[31]</ref>, which utilizes a recurrent architecture to avoid processing LR input frames multiple times. In contrast to FRVSR, which adopts explicit motion estimation and warping operations to exploit temporal information, RLSP introduces high dimensional latent states to propagate temporal information in an implicit way.</p><p>In <ref type="figure" target="#fig_1">Fig. 1</ref>, we present the trade-off between runtime and accuracy (average PSNR) for state-of-the-art VSR approaches on the Vid4 dataset <ref type="bibr" target="#b23">[24]</ref>. The proposed RLSP approach achieves a better balance between speed and performance than the competing methods. RLSP achieves about 10× and 70× speed-up over the methods FRVSR and DUF, respectively, while maintaining similar accuracy. Furthermore, despite its efficiency, by utilizing more filters in our model, RLSP can also be pushed to pursue state-of-the-art VSR accuracy. Our model RLSP 7-256 achieves the highest PSNR on the Vid4 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single Image Super-Resolution With the rise of deep learning, especially convolutional neural networks (CNN) <ref type="bibr" target="#b18">[19]</ref>, learning based super-resolution models have shown to be superior in terms of accuracy compared to classical interpolation methods, such as bilinear and bicubic interpolation and similar approaches. One of the earliest methods to apply convolution for superresolution is SRCNN, proposed by <ref type="bibr" target="#b5">[6]</ref>. SRCNN uses a shallow network of only 3 convolutional layers. VDSR <ref type="bibr" target="#b16">[17]</ref> shows substantial improvements by using a much deeper network of 20 layers combined with residual learning. In order to get visually more pleasing images, photorealistic and natural looking, the accuracy to the ground truth is traded off by method such as SRGAN <ref type="bibr" target="#b20">[21]</ref>, EnhanceNet <ref type="bibr" target="#b29">[30]</ref>, and <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26]</ref> that introduce alternative loss functions <ref type="bibr" target="#b9">[10]</ref> to super-resolution. An overview of recent methods in the field of SISR is provided by <ref type="bibr" target="#b33">[34]</ref>. Video Super-Resolution Super-resolution can be generalized from images to videos. Videos additionally provide temporal information among frames, which can be exploited to improve interpolation quality. Non-deep learning video super-resolution problems are often solved by formulating demanding optimization problems, leading to slow evaluation times <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Many deep learning based VSR methods are composed of multiple independent processing pipelines, motivated by prior knowledge and inspired by traditional computer vision tools. To leverage temporal information, a natural extension to SISR is combining multiple low-resolution frames to produce a single high-resolution estimate <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref>. Kappeler et al. <ref type="bibr" target="#b15">[16]</ref> combine several adjacent frames. Noncenter frames are motion compensated by calculating optical flow and warping towards the center frame. All frames are then concatenated and followed by 3 convolution layers. Tao et al. <ref type="bibr" target="#b32">[33]</ref> produce a single high-resolution frame y t from up to 7 low-resolution input frames x t−3:t+3 . First, motion is estimated in low resolution and a preliminary high-resolution frame is computed through a subpixel motion compensation layer. The final output is computed by applying an encoder-decoder style network with an intermediate convolutional LSTM <ref type="bibr" target="#b11">[12]</ref> layer. Liu et al. <ref type="bibr" target="#b24">[25]</ref> calculate multiple high-resolution estimates in parallel branches, each processing an increasing number of low-resolution frames. Additionally, a temporal modulation branch computes weights according to which the respective high-resolution estimates are aggregated, forming the final high-resolution output. Caballero et al. <ref type="bibr" target="#b2">[3]</ref> extract motion flow maps between adjacent frames and center frame x t . The frames are warped according to the flow maps towards frame x t . These frames are then processed with a spatiotemporal network, by either direct concatenation and convolution, gradually applying several convolution and concatenation steps or applying 3D convolutions <ref type="bibr" target="#b34">[35]</ref>. Jo et al. <ref type="bibr" target="#b14">[15]</ref> propose DUF, a network without explicit motion estimation. Dynamic upsampling filters and residuals are calculated from a batch of adjacent input frames. The center frame is filtered and added with the residuals to get the final output.</p><p>A more powerful approach to process sequential data like video, is to use recurrent connections between time steps. Methods using a fixed number of input frames are inherently limited by the information content in those frames. Recurrent models however, are able to leverage information from a potentially unlimited number of frames. Sajjadi et al. <ref type="bibr" target="#b30">[31]</ref> use an optical flow network, followed by a superresolution network. Optical flow is calculated between x t−1 and x t to warp the previous output y t−1 towards t. The final output y t is calculated from the warped previous output and the current low-resolution input frame x t . The two networks are trained jointly. Huang et al. <ref type="bibr" target="#b12">[13]</ref> propose a bidirectional recurrent network using 2D and 3D convolutions with recurrent connections between time steps. A forward pass and a backward pass are combined to produce the final output frames. Because of its nature, this method can not be applied online.  RLSP does not rely on a dedicated motion compensation module and instead introduces a recurrent hidden state, to efficiently leverage temporal information implicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Video super-resolution (VSR) maps a LR video x to a HR video y by a given scaling factor r. At time t, a single frame y t ∈ R rH×rW ×C represents the reconstructed HR frame of x t ∈ R H×W ×C , where H and W are spatial dimensions and C is the number of color channels. In contrast to SISR, the temporal dimension provides additional information, which can be leveraged when generating a single frame y t . As a natural choice for sequential data, we therefore propose a recurrent neural network (RNN). The model is fully defined by its cell, illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>As done in many non-recurrent methods, we feed several adjacent LR frames x t−1:t+1 to produce y t . These frames are concatenated along the channel axis, together with the recurrent inputs h t−1 and y t−1 . To concatenate and align the previous HR output y t−1 with the LR tensors, y t−1 is shuffled down by the scaling factor r, see Sec. 3.1. The combined input is then fed to n convolution layers with ReLU activation function. In the last stage, the hidden state h t for the next iteration and the HR output's residuals in LR space are produced. The residuals are added with the nearest neighbor interpolated frame x * t represented in LR space and shuffled up by scaling factor r, to finally generate the output y t (see Sec. 3.2). All input frames are in RGB color space, while the output represents the brightness channel Y of YCbCr color space. Chroma channels are upscaled separately with bicubic interpolation. All our models are trained with a scaling factor of r = 4.</p><p>The model is a recurrent, fully convolutional network. It is therefore not limited to a fixed input size and can accommodate video data of any dimensions. With the exception of higher level contextual information (along the spatial and temporal axes), super-resolution is a highly locality based interpolation problem. Thus, a convolutional neural network is a sensible choice. Since the receptive field grows with the number of convolution layers, the network is still able to detect complex structure across an extended neighborhood. In order to optimize information flow, much care is taken to keep local alignment throughout the network and is achieved by using operations which keep local integrity, also see Sec. 3.1 and Sec. 3.2. We aim for efficiency and therefore use a fixed number of n = 7 layers, the filter's spatial dimensions are set to 3 × 3 for all models. The number of filters f is adapted per model.</p><p>In the following sections, the model's core elements are discussed in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Shuffling</head><p>To realize the mapping from LR to HR, the spatial dimensions need to be transformed at some point in the model. The most relevant processing is kept in LR and spatial expansion is executed at the last stage of the processing chain. Because the output is fed back, an inverse transformation from HR to LR is applied. Shuffling performs these transformations by reducing the channel dimension Z of a tensor t with factor r 2 and extending both spatial dimensions H and W with factor r and vice versa for the inverse transformation. Since shuffling is a bijective transformation these operations are reversible:</p><formula xml:id="formula_0">t LR ∈ R H×W ×Z ×r − − → t HR ∈ R rH×rW ×Z/r 2 (1) t HR ∈ R H×W ×Z /r − → t LR ∈ R H/r×W/r×r 2 Z (2)</formula><p>To get a single channel HR output image with upscaling factor r = 4, the LR tensor's channel dimension needs to be Z = 16. Therefore, the last layer has 16 filters. A very  </p><formula xml:id="formula_1">t LR ∈ R 2×2×4 ×2 − − → t HR ∈ R 4×4×1 .</formula><p>important characteristic of this transformation is that it retains local integrity. All pixels along the channel dimension in LR are rearranged in their corresponding local HR interpolation area. This enables a smooth localized information flow from LR input to HR output. The shuffling operation has been adopted in previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref> to change the spatial resolution of image/feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual Learning</head><p>Reducing the sample rate inherently leads to loss of frequency components above the Nyquist frequency. Due to the lower Nyquist frequency in LR space, the main information loss occurs in the spatial high-frequency components. Low-frequency components below the Nyquist frequency can be fully retained when downsampling. This a priori knowledge is used by introducing a residual connection directly from the LR input frame x t to the output frame y t . First, x t is converted from RGB color space to the brightness channel Y (from YCbCr color space) and replicated 16 times to match the residual's dimension. This procedure is effectively nearest-neighbor interpolation. No information is altered during this process, which means the network in parallel does not need to allocate complexity to learn this transformation, as it would be the case with other methods, e.g. bicubic interpolation. In contrast to FRVSR, which does not adopt this strategy, all complexity can be used to reconstruct only the missing high-frequency components. All LR inputs and nearest-neighbor interpolated HR output (represented in LR space) are properly aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feedback</head><p>Feeding back the output naturally helps to improve continuity between frames and reduces flickering, which can occur in models with limited temporal connectivity. Because of high correlation between adjacent frames, having a reference of the previous output y t−1 also supplies additional, already processed HR information when producing the HR estimate y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hidden State</head><p>In order to propagate complex, abstract information across time, a hidden state h t is added to the processing chain. The hidden state is realized by carrying forward the feature maps from the previous iteration and feeding them back to the input through concatenation. This keeps the network structure in line with the prior on locality. Since the whole network is fully convolutional, the hidden state's spatial dimensions are dynamically adjusted according to the input size of x. The hidden state can be seen as a set of vectors in a locality based latent space R f , characterized by vectors v ∈ R f with entries along the channel axis, see <ref type="figure" target="#fig_5">Fig. 4</ref>. Since every instance in a feature map is calculated by the same convolution kernel, each feature map represents a dimension in the latent space R f . In contrast to using just feedback as done in <ref type="bibr" target="#b30">[31]</ref>, which is bound to pass HR information from y t−1 only, the hidden state is not limited in the type of information that can be propagated. It is theoretically possible to propagate past information across the whole time axis. Because at time instance t, the next frame x t+1 is fed to the input already, the hidden state also allows every frame x t to be processed twice before estimating y t . This essentially increases the receptive field and the number of processing layers for a single frame, even though both processing steps share their weights. Because of recurrence, these two steps can be efficiently distributed over two time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss</head><p>The loss function is defined as the pixel-wise meansquared-error (MSE) between all k pixels in the ground truth frames y * and the network's output y:</p><formula xml:id="formula_2">L = 1 k y * − y 2 2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We adhere to the experimental setup from <ref type="bibr" target="#b30">[31]</ref> and use the same dataset. Originally, it contained 40 high resolution videos (720p, 1080p, 4k), downloaded from vimeo.com, but 3 videos were not online anymore, so we train on the available 37 videos instead. Following the same procedure as in <ref type="bibr" target="#b30">[31]</ref> we produce 40,000 random crops of size 20 × 256 × 256 × 3, which serve as HR ground truth sequences y * ∈ R T ×H×W ×C . To get the corresponding LR sequences x, Gaussian blur with σ = 1.5 is applied and every 4-th pixel in both spatial dimensions is sampled as done in both methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b14">15]</ref> that we use for comparison. To monitor training progress and generalization, we downloaded 10 additional high resolution videos from youtube.com and generated validation sequences with the same procedure as used for generating the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>To train our RNN, the cell is unrolled along the time axis to accommodate for a training clip length of 10 frames. For training, we randomly sample 12 consecutive frames from the training clips, which contain 20 frames. The 2 additional frames are used to feed x t−1 at the beginning and x t+1 at the end. The weights are initialized with Xavier initialization <ref type="bibr" target="#b8">[9]</ref> and the network is trained with batches of size 4 with Adam optimizer <ref type="bibr" target="#b17">[18]</ref>. Because of the network's recurrent structure, the hidden state h t−1 , and the previous estimate y t−1 need to be initialized. Both tensors are initialized with zeros. In our experimental setup, our fastest model has 7 layers with 48 filters per layer, denoted as RLSP 7-48, while our most accurate model is implemented with 7 layers and 256 filters (RLSP 7-256). The networks are trained with decreasing learning rate, starting at 10 −4 . For RLSP 7-48 the learning rate is divided by 10 after 2M and 3M iterations. For all other models, the learning rate is divided by 10 after 2M and 4M steps. The models are selected according to the lowest moving average on the validation loss at convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>We investigate our models in terms of runtime, accuracy, temporal consistency, information flow and provide images for qualitative comparison. We compare our models with state-of-the-art video super-resolution methods DUF <ref type="bibr" target="#b14">[15]</ref> and FRVSR <ref type="bibr" target="#b30">[31]</ref> on Vid4 <ref type="bibr" target="#b23">[24]</ref> benchmark. To the best of our knowledge DUF achieves the highest accuracy while FRVSR is very efficient with the best tradeoff between accuracy and runtime to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation</head><p>We compare different configurations found in other VSR methods by applying them to our architecture and assess the impact of each part in terms of accuracy and runtimes. A SISR implementation of our network serves as a baseline. For that matter, all recurrent connections are removed and only the current frame x t is fed to produce y t . As an extension to SISR, a batch of 3 consecutive frames x t−1:t+1 is fed to the network. This approach is further expanded by introducing a recurrent feedback connection y t−1 . Finally, our proposed locality based hidden state is added. All configurations are trained on the same data with the settings, described in Sec Adding adjacent frames already improves PSNR substantially compared to the SISR implementation by 1.3dB. Feeding back the previous output improves PSNR by 0.23dB. Our proposed locality based hidden state further improves the PSNR score substantially by 0.55dB. The experiments show, that adjacent frames and our locality based hidden state, have the strongest impact on the video performance. As expected, the runtime increases for more complex configurations. The models without recurrence need 12ms to produce a single Full HD frame. Adding feedback (y t−1 ) and the hidden state (h t−1 ) to the network increases runtime by 2ms and another 5ms, respectively. The complete configuration RLSP 7-64 achieves a PSNR value comparable to recent state-of-the-art by gaining 1.98dB compared to the SISR implementation. RLSP 7-64 can generate 50fps Full HD in real time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Temporal Consistency</head><p>An important aspect in VSR is temporal consistency between consecutive frames. Methods with limited temporal connectivity often expose flickering and other artefacts along the time domain. This property can not be analyzed by per frame PSNR values. Therefore, we provide temporal profiles to visually assess the temporal continuity of our RLSP method compared to others. For that matter, a single pixel line (red line in <ref type="figure" target="#fig_7">Fig. 5</ref>) is recorded along the whole sequence and stacked vertically. Temporal profiles of high quality videos with smooth temporal transitions expose sharp detailed images. DUF-52 is the most limited method in terms of exploiting temporal information, as it only uses adjacent frames without any recurrent connections. FRVSR 10-128 uses feedback to leverage temporal information. Our method additionally propagates a latent state, which increases temporal connectivity even further. These properties are also reflected in the temporal profiles in <ref type="figure" target="#fig_7">Fig. 5</ref>. The vertical stripes in DUF-52's profile are blurred out, which indicates discontinuities between consecutive frames. FRVSR 10-128 shows increased performance, while our method RLSP 7-128 exhibits the sharpest stripes among all methods. Our method can therefore produce better temporal consistency overall.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Information Flow over Time</head><p>Unlike many existing methods, our model is not limited by the number of LR input frames to extract information over time. To investigate the range of information flow, the same model (RLSP 7-128) is evaluated on a Full HD sequence from the validation set, but initialized at different instances in time. The first run is started 100 frames ahead of the second run. Therefore, the first run has already accumulated information over 100 frames, when the second run is initialized at frame 0. The respective PSNR values per frame and the difference between the two runs are plotted for 200 frames in <ref type="figure" target="#fig_8">Fig. 6</ref>. The experiment shows that the model can propagate information over almost 175 frames until the two runs finally converge. Accumulated information from the first 100 frames can be leveraged to get up to 0.2dB higher PSNR values over a long period of 150 frames. Interestingly, the two runs collapse at the beginning, but quickly separate again, which leads to the conclusion, that the model saves information, based on considering a large horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Initialization</head><p>Due to the recurrent nature, our method is dependent on previously processed information. Because the available information content is at its lowest at the beginning, it takes a couple of frames to gather temporal information. This phenomenon can be observed in <ref type="figure" target="#fig_9">Fig. 7</ref>, where the first 6 frames of sequence city are shown. We compare our method RLSP 7-128 with DUF-52, which suffers from initialization until frame 2 as well. The fine structure of the building in the first frame can not be fully reconstructed, but as more information is processed, the quality increases quickly until the correct structure is revealed after frame 4. This behaviour is also represented in the PSNR values in <ref type="figure" target="#fig_10">Fig. 8</ref>. Our models start with lower PSNR at the beginning, but quickly catch up with DUF to then surpass it. Unfortunately, the first 2 and the last two frames of FRVSR 10-128 are not provided by the authors, probably, because these frames are not considered for PSNR evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Accuracy and Runtimes</head><p>To compare performance of the proposed network with other methods we calculate the average PSNR over all se-  quences from Vid4 and measure runtimes to produce a single Full HD (1920x1080) frame. The ideal method attains fast runtimes and high PSNR. Video PSNR is calculated from the MSE over the total number of pixels in a single video sequence. The final reported value is the average of each sequence's PSNR value. Sometimes, methods use encoder-decoder parts in their architectures and therefore need to restrict input sizes. It is therefore common to crop the spatial dimensions on test sets to accommodate for this drawback. Our method can process any input dimension and cropping would not be required. However, to objectively compare our method to others, we follow the evaluation strategy described in <ref type="bibr" target="#b14">[15]</ref> and directly include the reported values from that paper. The values for FRVSR 10-128 <ref type="bibr" target="#b30">[31]</ref> are recalculated in the same way from the provided output images. Because the outputs for model FRVSR 3-64 are not provided, we simply add the same difference in PSNR on top of the reported value in the paper, which was gained, when calculating the new PSNR value for FRVSR 10-128. Our runtimes are measured on a NVIDIA TITAN Xp with our unoptimized code. DUF and FRVSR runtimes are taken from the respective papers. The results are listed in Tab. 2 and displayed in <ref type="figure" target="#fig_1">Fig. 1</ref>. RLSP 7-64 shows comparable PSNR to FRVSR 10-128 and DUF-16, but is 10× and 20× faster, respectively. RLSP 7-128 achieves a good trade-off between accuracy and speed. It reaches state-of-the-art accuracy, while reducing runtimes by two orders of magnitude compared to DUF-52. Our direct implementation without any optimization can produce 25fps of Full HD video in real-time. Because the focus in this work is on performance, we keep the layer count constant at 7 and instead vary the number of filters. Due to the high parallelizability, this allows to increase complexity without putting too much burden on computation time. By further increasing the number of filters, RLSP 7-256 achieves the highest reported PSNR to date on Vid4, improving 0.65dB over FRVSR 10-128 and 0.21dB over DUF 52 while still being more than 2× faster than FRVSR and 30× faster than DUF, respectively.</p><p>To investigate evolution of accuracy across time, average PSNR per frame over all sequences in vid4 is computed and plotted in <ref type="figure" target="#fig_10">Fig. 8</ref>. All methods have to deal with incomplete initialization. Since our RLSP approach profits greatly from past information, PSNR values are lower at the beginning compared to DUF-52. However, RLSP 7-128 improves quickly and is able to surpass all other methods from frame 6 until the end. We also provide images for visual comparison in <ref type="figure">Fig. 9</ref> for each sequence in Vid4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced RLSP, a new end-to-end trainable recurrent video super-resolution architecture with locality based latent space propagation, without relying on a dedicated motion estimation module. Due to the ability of effectively leveraging temporal information over long periods of time, RLSP reduces runtimes drastically, while still maintaining state-of-the-art accuracy. Because our network is shallow and wide, a large amount of computation can be run in parallel, which is also responsible for its efficiency and could enable even faster runtimes for dedicated hardware implementations. Even though the network structure is designed to be highly efficient, we could show, that it is still possible to improve accuracy by increasing complexity, e.g. adding more filters. Our RLSP achieves the best accuracy on Vid4 benchmark while being more than 70× faster than DUF, the former state-of-the-art. Accuracy could also be further improved by investigating different configurations of kernel sizes, alternative convolution types or numbers of layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Quantitative comparison of PSNR values on Vid4 and computation times to produce a single Full HD (1920x1080) frame with other state-of-the-art methods FRVSR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>RLSP Architecture. The recurrent cell is shown at time t, ⊗ denotes concatenation along the channel dimension, ⊕ denotes element-wise addition. Information is propagated over time through hidden state h and feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Shuffling:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Locality based hidden state. Entries along the channel dimension represent vectors v ∈ R f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. 4.2. The experiments are conducted with n = 7 layers and f = 64 filters. The PSNR values on Vid4 and Full HD runtimes are shown in Tab. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Temporal profiles for calendar. The profiles are produced from the red line, shown on the left</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Information flow over time for model RLSP 7-128 on a validation video. Top: Absolute PSNR values, Bottom: Difference in PSNR per frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Initialization artefacts for DUF-52 and RLSP 7-128. The first 6 frames of city are shown. DUF-52 is fully initialized at frame 2, when all input frames xt−2:t+2 are available. RLSP 7-128 reconstructs the full structure starting from frame 4, while DUF-52 still exhibits artefacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Average PSNR values over all 4 videos in Vid4 for the top models of FRVSR<ref type="bibr" target="#b30">[31]</ref>, DUF<ref type="bibr" target="#b14">[15]</ref> and our models RLSP 7-64 and RLSP 7-128. The full average is calculated for the first 34 frames, constrained by the shortest sequence city. The first and last two frames of FRVSR 10-128 are not provided by the authors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results for different network configurations on Vid4.</figDesc><table><row><cell>Inputs</cell><cell cols="2">PSNR [dB] Runtime [ms]</cell></row><row><cell>x t</cell><cell>24.91</cell><cell>12</cell></row><row><cell>x t−1:t+1</cell><cell>26.20</cell><cell>12</cell></row><row><cell>x t−1:t+1 + y t−1</cell><cell>26.43</cell><cell>14</cell></row><row><cell>x t−1:t+1 + y t−1 + h t−1</cell><cell>26.89</cell><cell>19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Visual comparison on Vid4. From top to bottom: calendar, foliage, city, walk. Comparison of PSNR values on Vid4 and runtimes between methods FRVSR [31], DUF [15] and ours. Bicubic interpolation is included as a baseline. All runtimes are computed on Full HD.</figDesc><table><row><cell>GT</cell><cell>Bicubic</cell><cell>FRVSR 10-128 [31]</cell><cell>DUF-52 [15]</cell><cell>RLSP 7-64</cell><cell>RLSP 7-128</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum a posteriori video super-resolution using a new multichannel image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Belekos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1451" to="1464" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new adaptive video super-resolution algorithm with improved robustness to innovations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Borsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C M</forename><surname>Bermudez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="673" to="686" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pixel recursive super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse representation-based multiple frame video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="765" to="781" />
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uhd video super-resolution using low-rank and sparse decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Erfanian</forename><surname>Ebadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Guerra</forename><surname>Ones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and Robust Multiframe Super Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
			<date type="published" when="2004-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatio-temporal transformer network for video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="106" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012-01" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video superresolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="De" to=" cember" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Lopez</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">End-to-End Learning of Video Super-Resolution with Motion Compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Super-resolution image reconstruction: a technical overview. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="21" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Photorealistic Video Super Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pérez-Pellitero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frame-Recurrent Video Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image superresolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-memory convolutional neural network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2530" to="2544" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
