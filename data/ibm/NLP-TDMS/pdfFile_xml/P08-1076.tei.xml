<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Sequential Labeling and Segmentation using Giga-word Scale Unlabeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-06">June 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Communication Science Laboratories</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>Ohio</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
							<email>isozaki@cslab.kecl.ntt.co.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">NTT Corp</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Sequential Labeling and Segmentation using Giga-word Scale Unlabeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-06">June 2008</date>
						</imprint>
					</monogr>
					<note>Proceedings of ACL-08: HLT, pages 665-673,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL&apos;00 and &apos;03 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today, we can easily find a large amount of unlabeled data for many supervised learning applications in Natural Language Processing (NLP). Therefore, to improve performance, the development of an effective framework for semi-supervised learning (SSL) that uses both labeled and unlabeled data is attractive for both the machine learning and NLP communities. We expect that such SSL will replace most supervised learning in real world applications.</p><p>In this paper, we focus on traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER). These are also typical supervised learning applications in NLP, and are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. <ref type="bibr" target="#b0">Ando and Zhang (2005)</ref> reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL'00 shared task data <ref type="bibr" target="#b15">(Tjong Kim Sang and Buchholz, 2000</ref>) and NER with the CoNLL'03 shared task data <ref type="bibr" target="#b16">(Tjong Kim Sang and Meulder, 2003)</ref>.</p><p>One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data.</p><p>To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data. We first propose a scalable model for SSL. Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data <ref type="bibr">(Mar- cus et al., 1994)</ref> for POS tagging, CoNLL'00 shared task data for syntactic chunking, and CoNLL'03 shared task data for NER. We used up to 1G-words (one billion tokens) of unlabeled data to explore the performance improvement with respect to the unlabeled data size. In addition, we investigate the performance improvement for 'unseen data' from the viewpoint of unlabeled data coverage. Finally, we compare our results with those provided by the best current systems.</p><p>The contributions of this paper are threefold. First, we present a simple, scalable, but powerful task-independent model for semi-supervised sequential labeling and segmentation. Second, we report the best current results for the widely used test collections described above. Third, we confirm that the use of more unlabeled data in SSL can really lead to further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conditional Model for SSL</head><p>We design our model for SSL as a natural semisupervised extension of conventional supervised conditional random fields (CRFs) ( <ref type="bibr" target="#b7">Lafferty et al., 2001</ref>). As our approach for incorporating unlabeled data, we basically follow the idea proposed in ( <ref type="bibr" target="#b14">Suzuki et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conventional Supervised CRFs</head><p>Let x ∈ X and y ∈ Y be an input and output, where X and Y represent the set of possible inputs and outputs, respectively. C stands for the set of cliques in an undirected graphical model G(x, y), which indicates the interdependency of a given x and y. y c denotes the output from the corresponding clique c. Each clique c ∈ C has a potential function Ψ c . Then, the CRFs define the conditional probability p(y|x) as a product of Ψ c s. In addition, let f = (f 1 , . . ., f I ) be a feature vector, and λ = (λ 1 , . . ., λ I ) be a parameter vector, whose lengths are I. p(y|x; λ) on a CRF is defined as follows:</p><formula xml:id="formula_0">p(y|x; λ) = 1 Z(x) c Ψ c (y c , x; λ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">Z(x) = y∈Y c∈C Ψ c (y c , x; λ)</formula><p>is the partition function. We generally assume that the potential function is a non-negative real value function. Therefore, the exponentiated weighted sum over the features of a clique is widely used, so that,</p><formula xml:id="formula_2">Ψ c (y c , x; λ)=exp(λ · f c (y c , x)) where f c (y c , x)</formula><p>is a feature vector obtained from the corresponding clique c in G(x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-supervised Extension for CRFs</head><p>Suppose we have J kinds of probability models (PMs). The j-th joint PM is represented by p j (x j , y; θ j ) where θ j is a model parameter. x j = T j (x) is simply an input x transformed by a predefined function T j . We assume x j has the same graph structure as x. This means p j (x j , y) can be factorized by the cliques c in G(x, y). That is, p j (x j , y; θ j )= c p j (x jc , y c ; θ j ). Thus, we can incorporate generative models such as Bayesian networks including (1D and 2D) hidden Markov models (HMMs) as these joint PMs. Actually, there is a difference in that generative models are directed graphical models while our conditional PM is an undirected. However, this difference causes no violations when we construct our approach.</p><p>Let us introduce λ =(λ 1 , . . ., λ I , λ I+1 , . . ., λ I+J ), and h = (f 1 , . . ., f I , log p 1 , . . ., log p J ), which is the concatenation of feature vector f and the loglikelihood of J-joint PMs. Then, we can define a new potential function by embedding the joint PMs;</p><formula xml:id="formula_3">Ψ c (y c , x; λ , Θ) = exp(λ · f c (y c , x)) · j p j (x jc , y c ; θ j ) λ I+j = exp(λ · h c (y c , x)).</formula><p>where Θ = {θ j } J j=1 , and h c (y c , x) is h obtained from the corresponding clique c in G(x, y). Since each p j (x jc , y c ) has range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, which is nonnegative, Ψ c can also be used as a potential function. Thus, the conditional model for our SSL can be written as:</p><formula xml:id="formula_4">P (y|x; λ , Θ) = 1 Z (x) c Ψ c (y c , x; λ , Θ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">Z (x) = y∈Y c∈C Ψ c (y c , x; λ , Θ).</formula><p>Hereafter in this paper, we refer to this conditional model as a 'Joint probability model Embedding style SemiSupervised Conditional Model', or JESS-CM for short.</p><p>Given labeled data, D l ={(x n , y n )} N n=1 , the MAP estimation of λ under a fixed Θ can be written as:</p><formula xml:id="formula_6">L 1 (λ |Θ) = n log P (y n |x n ; λ , Θ) + log p(λ ),</formula><p>where p(λ ) is a prior probability distribution of λ . Clearly, JESS-CM shown in Equation 2 has exactly the same form as Equation 1. With a fixed Θ, the log-likelihood, log p j , can be seen simply as the feature functions of JESS-CM as with f i . Therefore, embedded joint PMs do not violate the global convergence conditions. As a result, as with supervised CRFs, it is guaranteed that λ has a value that achieves the global maximum of L 1 (λ |Θ). Moreover, we can obtain the same form of gradient as that of supervised CRFs ( <ref type="bibr" target="#b11">Sha and Pereira, 2003)</ref>, that is,</p><formula xml:id="formula_7">L 1 (λ |Θ) = E ˜ P (Y,X ;λ ,Θ) h(Y, X ) − n E P (Y|x n ;λ ,Θ) h(Y, x n ) + log p(λ ).</formula><p>Thus, we can easily optimize L 1 by using the forward-backward algorithm since this paper solely focuses on a sequence model and a gradient-based optimization algorithm in the same manner as those used in supervised CRF parameter estimation. We cannot naturally incorporate unlabeled data into standard discriminative learning methods since the correct outputs y for unlabeled data are unknown. On the other hand with a generative approach, a well-known way to achieve this incorporation is to use maximum marginal likelihood (MML) parameter estimation, i.e., ( <ref type="bibr" target="#b10">Nigam et al., 2000)</ref>.</p><formula xml:id="formula_8">Given unlabeled data D u = {x m } M m=1</formula><p>, MML estimation in our setting maximizes the marginal distribution of a joint PM over a missing (hidden) variable y, namely, it maximizes m log y∈Y p(x m , y; θ). Following this idea, there have been introduced a parameter estimation approach for non-generative approaches that can effectively incorporate unlabeled data ( <ref type="bibr" target="#b14">Suzuki et al., 2007)</ref>. Here, we refer to it as 'Maximum Discriminant Functions sum' (MDF) parameter estimation. MDF estimation substitutes p(x, y) with discriminant functions g(x, y). Therefore, to estimate the parameter Θ of JESS-CM by using MDF estimation, the following objective function is maximized with a fixed λ :</p><formula xml:id="formula_9">L 2 (Θ|λ ) = m log y∈Y g(x m , y; λ , Θ) + log p(Θ),</formula><p>where p(Θ) is a prior probability distribution of Θ. Since the normalization factor does not affect the determination of y, the discriminant function of JESS-CM shown in Equation 2 is defined as g(x, y; λ , Θ) = c∈C Ψ c (y c , x; λ , Θ). With a fixed λ , the local maximum of L 2 (Θ|λ ) around the initialized value of Θ can be estimated by an iterative computation such as the EM algorithm <ref type="bibr">(Demp- ster et al., 1977</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scalability: Efficient Training Algorithm</head><p>A parameter estimation algorithm of λ and Θ can be obtained by maximizing the objective functions L 1 (λ |Θ) and L 2 (Θ|λ ) iteratively and alternately. <ref type="figure" target="#fig_0">Figure 1</ref> summarizes an algorithm for estimating λ and Θ for JESS-CM.</p><p>This paper considers a situation where there are many more unlabeled data M than labeled data N , that is, N &lt;&lt; M . This means that the calculation cost for unlabeled data is dominant. Thus, in order to make the overall parameter estimation procedure</p><formula xml:id="formula_10">Input: training data D = {D l , Du} where labeled data D l = {(x n , y n )} N n=1 , and unlabeled data Du = {x m } M m=1</formula><p>Initialize:</p><formula xml:id="formula_11">Θ (0) ← uniform distribution, t ← 0 do 1. t ← t + 1 2. (Re)estimate λ : maximize L 1 (λ |Θ) with fixed Θ ← Θ (t−1) using D l . 3. Estimate Θ (t) : (Initial values = Θ (t−1) )</formula><p>update one step toward maximizing L 2 (Θ|λ ) with fixed λ using Du.</p><formula xml:id="formula_12">do until |Θ (t) −Θ (t−1) | |Θ (t−1) | &lt; .</formula><p>Reestimate λ : perform the same procedure as 1. scalable for handling large scale unlabeled data, we only perform one step of MDF estimation for each t as explained on 3. in <ref type="figure" target="#fig_0">Figure 1</ref>. In addition, the calculation cost for estimating parameters of embedded joint PMs (HMMs) is independent of the number of HMMs, J, that we used ( <ref type="bibr" target="#b14">Suzuki et al., 2007)</ref>. As a result, the cost for calculating the JESS-CM parameters, λ and Θ, is essentially the same as executing T iterations of the MML estimation for a single HMM using the EM algorithm plus T + 1 time optimizations of the MAP estimation for a conventional supervised CRF if it converged when t = T . In addition, our parameter estimation algorithm can be easily performed in parallel computation.</p><formula xml:id="formula_13">Output: a JESS-CM, P (y|x, λ , Θ (t) ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Comparison with Hybrid Model</head><p>SSL based on a hybrid generative/discriminative approach proposed in ( <ref type="bibr" target="#b14">Suzuki et al., 2007)</ref> has been defined as a log-linear model that discriminatively combines several discriminative models, p D i , and generative models, p G j , such that:</p><formula xml:id="formula_14">R(y|x; Λ, Θ, Γ) = i p D i (y|x; λ i ) γi j p G j (x j , y; θ j ) γj y i p D i (y|x; λ i ) γi j p G j (x j , y; θ j ) γj ,</formula><p>where</p><formula xml:id="formula_15">Λ={λ i } I i=1</formula><p>, and</p><formula xml:id="formula_16">Γ={{γ i } I i=1 , {γ j } I+J j=I+1</formula><p>}. With the hybrid model, if we use the same labeled training data to estimate both Λ and Γ, γ j s will become negligible (zero or nearly zero) since p D i is already fitted to the labeled training data while p G j are trained by using unlabeled data. As a solution, a given amount of labeled training data is divided into two distinct sets, i.e., 4/5 for estimating Λ, and the remaining 1/5 for estimating Γ ( <ref type="bibr" target="#b14">Suzuki et al., 2007)</ref>. Moreover, it is necessary to split features into several sets, and then train several corresponding discriminative models separately and preliminarily. In contrast, JESS-CM is free from this kind of additional process, and the entire parameter estimation procedure can be performed in a single pass. Surprisingly, although JESS-CM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure, JESS-CM provides F -scores of 94.45 and 88.03 for CoNLL'00 and '03 data, respectively, which are 0.15 and 0.83 points higher than those reported in ( <ref type="bibr" target="#b14">Suzuki et al., 2007)</ref> for the same configurations. This performance improvement is basically derived from the full benefit of using labeled training data for estimating the parameter of the conditional model while the combination weights, Γ, of the hybrid model are estimated solely by using 1/5 of the labeled training data. These facts indicate that JESS-CM has several advantageous characteristics compared with the hybrid model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In our experiments, we report POS tagging, syntactic chunking and NER performance incorporating up to 1G-words of unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Set</head><p>To compare the performance with that of previous studies, we selected widely used test collections. For our POS tagging experiments, we used the Wall Street Journal in PTB III ( <ref type="bibr" target="#b9">Marcus et al., 1994)</ref> with the same data split as used in <ref type="bibr" target="#b12">(Shen et al., 2007</ref>). For our syntactic chunking and NER experiments, we used exactly the same training, development and test data as those provided for the shared tasks of CoNLL'00 <ref type="bibr" target="#b15">(Tjong Kim Sang and Buchholz, 2000</ref>) and CoNLL'03 <ref type="bibr">(Tjong Kim Sang and Meul- der, 2003</ref>), respectively. The training, development and test data are detailed in <ref type="table" target="#tab_1">Table 1</ref> 1 .</p><p>The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07). As regards the TIP- <ref type="bibr">1</ref> The second-order encoding used in our NER experiments is the same as that described in <ref type="bibr" target="#b11">(Sha and Pereira, 2003</ref>) except removing IOB-tag of previous position label.    STER corpus, we extracted all the Wall Street Journal articles published between 1990 and 1992. With the English Gigaword corpus, we extracted articles from five news sources published between 1994 and 1996. The unlabeled data used in this paper is detailed in <ref type="table" target="#tab_2">Table 2</ref>. Note that the total size of the unlabeled data reaches 1G-words (one billion tokens).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Design of JESS-CM</head><p>We used the same graph structure as the linear chain CRF for JESS-CM. As regards the design of the feature functions f i , <ref type="table" target="#tab_4">Table 3</ref> shows the feature templates used in our experiments. In the    resources such as gazetteers for NER, we used none. All our features can be automatically extracted from the given training data.</p><note type="other">(a) POS tagging:(total 47 templates) [</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ys], [y s−1:s ], {[ys, pf-N s ], [ys, sf-Ns]} 9 N =1 , {[ys, wdu], [ys, wtp u ], [y s−1:s , wtp u ]} s+2 u=s−2 , {[ys, wd u−1:u ], [ys, wtp u−1:u ], [y s−1:s , wtp u−1:u ]}</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design of Joint PMs (HMMs)</head><p>We used first order HMMs for embedded joint PMs since we assume that they have the same graph structure as JESS-CM as described in Section 2.2. To reduce the required human effort, we simply used the feature templates shown in <ref type="table" target="#tab_4">Table 3</ref> to generate the features of the HMMs. With our design, one feature template corresponded to one HMM. This design preserves the feature whereby each HMM emits a single symbol from a single state (or transition). We can easily ignore overlapping features that appear in a single HMM. As a result, 47, 39 and 79 distinct HMMs are embedded in the potential functions of JESS-CM for POS tagging, chunking and NER experiments, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Tunable Parameters</head><p>In our experiments, we selected Gaussian and Dirichlet priors as the prior distributions in L 1 and L 2 , respectively. This means that JESS-CM has two tunable parameters, σ 2 and η, in the Gaussian and Dirichlet priors, respectively. The values of these tunable parameters are chosen by employing a binary line search. We used the value for the best performance with the development set 2 . However, it may be computationally unrealistic to retrain the entire procedure several times using 1G-words of unlabeled data. Therefore, these tunable parameter values are selected using a relatively small amount of unlabeled data (17M-words), and we used the selected values in all our experiments. The left graph in <ref type="figure" target="#fig_3">Figure 2</ref> shows typical η behavior. The left end is equivalent to optimizing L 2 without a prior, and the right end is almost equivalent to considering p j (x j , y) for all j to be a uniform distribution. This is why it appears to be bounded by the performance obtained from supervised CRF. We omitted the influence of σ 2 because of space constraints, but its behavior is nearly the same as that of supervised CRF.</p><p>Unfortunately, L 2 (Θ|λ ) may have two or more local maxima. Our parameter estimation procedure does not guarantee to provide either the global optimum or a convergence solution in Θ and λ space. An example of non-convergence is the oscillation of the estimated Θ. That is, Θ traverses two or more local maxima. Therefore, we examined its convergence property experimentally. The right graph in <ref type="figure" target="#fig_3">Figure 2</ref> shows a typical convergence property. Fortunately, in all our experiments, JESS-CM converged in a small number of iterations. No oscillation is observed here. <ref type="table" target="#tab_6">Table 4</ref> shows the performance of JESS-CM using 1G-words of unlabeled data and the performance gain compared with supervised CRF, which is trained under the same conditions as JESS-CM except that joint PMs are not incorporated. We emphasize that our model achieved these large improvements solely using unlabeled data as additional resources, without introducing a sophisticated model, deep feature engineering, handling external hand-     <ref type="figure">Figure 3</ref>: Performance changes with respect to unlabeled data size in JESS-CM crafted resources, or task dependent human knowledge (except for the feature design). Our method can greatly reduce the human effort needed to obtain a high performance tagger or chunker. <ref type="figure">Figure 3</ref> shows the learning curves of JESS-CM with respect to the size of the unlabeled data, where the x-axis is on the logarithmic scale of the unlabeled data size (Mega-word). The scale at the top of the graph shows the ratio of the unlabeled data size to the labeled data size. We observe that a small amount of unlabeled data hardly improved the performance since the supervised CRF results are competitive. It seems that we require at least dozens of times more unlabeled data than labeled training data to provide a significant performance improvement. The most important and interesting behavior is that the performance improvements against the unlabeled data size are almost linear on a logarithmic scale within the size of the unlabeled data used in our experiments. Moreover, there is a possibility that the performance is still unsaturated at the 1G-word unlabeled data point. This suggests that increasing the unlabeled data in JESS-CM may further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Impact of Unlabeled Data Size</head><p>Suppose J=1, the discriminant function of JESS-CM is g(x, y) = A(x, y)p 1 (x 1 , y; θ 1 ) λ I+1 where A(x, y) = exp(λ · c f c <ref type="figure">(y c , x)</ref>). Note that both A(x, y) and λ I+j are given and fixed during the MDF estimation of joint PM parameters Θ. Therefore, the MDF estimation in JESS-CM can be regarded as a variant of the MML estimation (see Section 2.2), namely, it is MML estimation with a bias, A(x, y), and smooth factors, λ I+j . MML estimation can be seen as modeling p(x) since it is equivalent to maximizing m log p(x m ) with marginalized hidden variables y, where y∈Y p(x, y) = p(x). Generally, more data will lead to a more accurate model of p(x). With our method, as with modeling p(x) in MML estimation, more unlabeled data is preferable since it may provide more accurate modeling. This also means that it provides better 'clusters' over the output space since Y is used as hidden states in HMMs. These are intuitive explanations as to why more unlabeled data in JESS-CM produces better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expected Performance for Unseen Data</head><p>We try to investigate the impact of unlabeled data on the performance of unseen data. We divide the test set (or the development set) into two disjoint sets: L.app and L.neg app. L.app is a set of sentences constructed by words that all appeared in the Labeled training data. L.¬app is a set of sentences that have at least one word that does not appear in the Labeled training data. <ref type="table" target="#tab_8">Table 5</ref> shows the performance with these two sets obtained from both supervised CRF and JESS-CM with 1G-word unlabeled data. As the supervised CRF results, the performance of the L.¬app sets is consistently much lower than that of the cor-</p><formula xml:id="formula_17">(a) POS tagging (b) Chunking (c) NER eval. data development test test development test L.¬app L.app L.¬app L.app L.¬app L.app L.¬app L.app L.¬app L.</formula><p>app rates of sentences (46.1%) (53.9%) (40.4%) (59.6%) (70.7%) <ref type="bibr">(</ref>   responding L.app sets. Moreover, we can observe that the ratios of L.¬app are not so small; nearly half (46.1% and 40.4%) in the PTB III data, and more than half (70.7%, 54.3% and 64.3%) in CoNLL'00 and '03 data, respectively. This indicates that words not appearing in the labeled training data are really harmful for supervised learning. Although the performance with L.¬app sets is still poorer than with L.app sets, the JESS-CM results indicate that the introduction of unlabeled data effectively improves the performance of L.¬app sets, even more than that of L.app sets. These improvements are essentially very important; when a tagger and chunker are actually used, input data can be obtained from anywhere and this may mostly include words that do not appear in the given labeled training data since the labeled training data is limited and difficult to increase. This means that the improved performance of L.¬app can link directly to actual use. <ref type="table" target="#tab_8">Table 5</ref> also shows the ratios of sentences that are constructed from words that all appeared in the 1G-word Unlabeled data used in our experiments (U.app) in the L.¬app and L.app. This indicates that most of the words in the development or test sets are covered by the 1G-word unlabeled data. This may be the main reason for JESS-CM providing large performance gains for both the overall and L.¬app set performance of all three tasks.   <ref type="bibr" target="#b0">and Zhang, 2005)</ref> 94.39 15M-word unlabeled data ( <ref type="bibr" target="#b14">Suzuki et al., 2007)</ref> 94.36 17M-word unlabeled data ( <ref type="bibr" target="#b18">Zhang et al., 2002)</ref> 94.17 full parser output ( <ref type="bibr" target="#b6">Kudo and Matsumoto, 2001)</ref>    <ref type="bibr">30-31 Aug. 1996 and</ref><ref type="bibr">6-7 Dec. 1996</ref> Reuters news articles, respectively. We find that temporal proximity leads to better performance. This aspect can also be explained as U.app. Basically, the U.app increase leads to improved performance.</p><p>The evidence provided by the above experiments implies that increasing the coverage of unlabeled data offers the strong possibility of increasing the expected performance of unseen data. Thus, it strongly encourages us to use an SSL approach that includes JESS-CM to construct a general tagger and chunker for actual use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison with Previous Top Systems and Related Work</head><p>In POS tagging, the previous best performance was reported by <ref type="bibr" target="#b12">(Shen et al., 2007)</ref> as summarized in <ref type="table" target="#tab_11">Table 7</ref>. Their method uses a novel sophisticated model that learns both decoding order and labeling, while our model uses a standard first order Markov model. Despite using such a simple model, our method can provide a better result with the help of unlabeled data. system dev. test additional resources JESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data 93.66 89.36 37M-word unlabeled data ( <ref type="bibr" target="#b0">Ando and Zhang, 2005</ref>) 93.15 89.31 27M-word unlabeled data ( <ref type="bibr" target="#b4">Florian et al., 2003)</ref> 93.87 88.76 own large gazetteers, 2M-word labeled data ( <ref type="bibr" target="#b14">Suzuki et al., 2007)</ref> N/A 88.41 27M-word unlabeled data [sup. <ref type="bibr">CRF (baseline)]</ref> 91.74 86.35 - <ref type="table">Table 9</ref>: NER results of the previous top systems for CoNLL'03 shared task data evaluated by F β=1 score</p><p>As shown in <ref type="table" target="#tab_13">Tables 8 and 9</ref>, the previous best performance for syntactic chunking and NER was reported by <ref type="bibr" target="#b0">(Ando and Zhang, 2005)</ref>, and is referred to as 'ASO-semi'. ASO-semi also incorporates unlabeled data solely as additional information in the same way as JESS-CM. ASO-semi uses unlabeled data for constructing auxiliary problems that are expected to capture a good feature representation of the target problem. As regards syntactic chunking, JESS-CM significantly outperformed ASO-semi for the same 15M-word unlabeled data size obtained from the Wall Street Journal in 1991 as described in <ref type="bibr" target="#b0">(Ando and Zhang, 2005</ref>). Unfortunately with NER, JESS-CM is slightly inferior to ASO-semi for the same 27M-word unlabeled data size extracted from the Reuters corpus. In fact, JESS-CM using 37M-words of unlabeled data provided a comparable result. We observed that ASOsemi prefers 'nugget extraction' tasks to 'field segmentation' tasks ( <ref type="bibr" target="#b5">Grenager et al., 2005</ref>). We cannot provide details here owing to the space limitation. Intuitively, their word prediction auxiliary problems can capture only a limited number of characteristic behaviors because the auxiliary problems are constructed by a limited number of 'binary' classifiers. Moreover, we should remember that ASOsemi used the human knowledge that 'named entities mostly consist of nouns or adjectives' during the auxiliary problem construction in their NER experiments. In contrast, our results require no such additional knowledge or limitation. In addition, the design and training of auxiliary problems as well as calculating SVD are too costly when the size of the unlabeled data increases. These facts imply that our SSL framework is rather appropriate for handling large scale unlabeled data.</p><p>On the other hand, ASO-semi and JESS-CM have an important common feature. That is, both methods discriminatively combine models trained by using unlabeled data in order to create informative feature representation for discriminative learning. Unlike self/co-training approaches <ref type="bibr" target="#b2">(Blum and Mitchell, 1998)</ref>, which use estimated labels as 'correct labels', this approach automatically judges the reliability of additional features obtained from unlabeled data in terms of discriminative training. <ref type="bibr" target="#b1">Ando and Zhang (2007)</ref> have also pointed out that this methodology seems to be one key to achieving higher performance in NLP applications.</p><p>There is an approach that combines individually and independently trained joint PMs into a discriminative model ( <ref type="bibr" target="#b8">Li and McCallum, 2005</ref>). There is an essential difference between this method and JESS-CM. We categorize their approach as an 'indirect approach' since the outputs of the target task, y, are not considered during the unlabeled data incorporation. Note that ASO-semi is also an 'indirect approach'. On the other hand, our approach is a 'direct approach' because the distribution of y obtained from JESS-CM is used as 'seeds' of hidden states during MDF estimation for join PM parameters (see Section 4.1). In addition, MDF estimation over unlabeled data can effectively incorporate the 'labeled' training data information via a 'bias' since λ included in A(x, y) is estimated from labeled training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a simple yet powerful semi-supervised conditional model, which we call JESS-CM. It is applicable to large amounts of unlabeled data, for example, at the giga-word level. Experimental results obtained by using JESS-CM incorporating 1G-words of unlabeled data have provided the current best performance as regards POS tagging, syntactic chunking, and NER for widely used large test collections such as PTB III, CoNLL'00 and '03 shared task data, respectively. We also provided evidence that the use of more unlabeled data in SSL can lead to further improvements. Moreover, our experimental analysis revealed that it may also induce an improvement in the expected performance for unseen data in terms of the unlabeled data coverage. Our results may encourage the adoption of the SSL method for many other real world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Parameter estimation algorithm for JESS-CM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s+2 u=s−1 (b) Syntactic chunking: (total 39 templates) [ys], [y s−1:s ], {[ys, wdu], [ys, pos u ], [ys, wdu, pos u ], [y s−1:s , wdu], [y s−1:s , pos u ]} s+2 u=s−2 , {[ys, wd u−1:u ], [ys, pos u−1:u ], {[y s−1:s , pos u−1:u ]} s+2 u=s−1 , (c) NER: (total 79 templates) [ys], [y s−1:s ], {[ys, wdu], [ys, lwdu], [ys, pos u ], [ys, wtp u ], [y s−1:s , lwdu], [y s−1:s , pos u ], [y s−1:s , wtp u ]} s+2 u=s−2 , {[ys, lwd u−1:u ], [ys, pos u−1:u ], [ys, wtp u−1:u ], [y s−1:s , pos u−1:u ], [y s−1:s , wtp u−1:u ]} s+2 u=s−1 , [ys, pos s−1:s:s+1 ], [ys, wtp s−1:s:s+1 ], [y s−1:s , pos s−1:s:s+1 ], [y s−1:s , wtp s−1:s:s+1 ], [ys, wd4ls], [ys, wd4rs], {[ys, pf-N s ], [ys, sf-Ns], [y s−1:s , pf-N s ], [y s−1:s , sf-Ns]} 4 N =1 wd: word, pos: part-of-speech lwd : lowercase of word, wtp: 'word type', wd4{l,r}: words within the left or right 4 tokens {pf,sf}-N: N character prefix or suffix of word</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Typical behavior of tunable parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Details of training, development, and test data 
(labeled data set) used in our experiments 

data 
abbr. (time period) 
# of sent. 
# of words 
Tipster 
wsj 04/90-03/92 
1,624,744 
36,725,301 
Reuters 
reu 09/96-08/97* 13,747,227 215,510,564 
Corpus 
*(excluding 06-07/12/96) 
English 
afp 05/94-12/96 
5,510,730 135,041,450 
Gigaword apw 11/94-12/96 
7,207,790 154,024,679 
ltw 04/94-12/96 
3,094,290 
72,928,537 
nyt 07/94-12/96 15,977,991 357,952,297 
xin 01/95-12/96 
1,740,832 
40,078,312 
total 
all 
48,903,604 1,012,261,140 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Unlabeled data used in our experiments</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>table , s indi- cates a focused token position. X s−1:s represents the bi-gram of feature X obtained from s − 1 and s po- sitions. {X u } B u=A indicates that u ranges from A to B. For example, {X u } s+2 u=s−2 is equal to</head><label>,</label><figDesc></figDesc><table>five feature 
templates, {X s−2 , X s−1 , X s , X s+1 , X s+2 }. 'word 
type' or wtp represents features of a word such as 
capitalization, the existence of digits, and punctua-
tion as shown in (Sutton et al., 2006) without regular 
expressions. Although it is common to use external </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Feature templates used in our experiments 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙 񮽙 񮽙</head><label>3</label><figDesc></figDesc><table>(a) Influence of η 
(b) Changes in performance 
in Dirichlet prior 
and convergence property 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Results for POS tagging (PTB III data), syntactic chunking (CoNLL'00 data), and NER (CoNLL'03 data) incorporated with 1G-words of unlabeled data, and the performance gain from supervised CRF 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙</head><label>4</label><figDesc></figDesc><table>񮽙 

񮽙񮽙񮽙 񮽙 

񮽙񮽙 񮽙 񮽙 

񮽙񮽙 񮽙 񮽙 

񮽙 񮽙񮽙 񮽙 

񮽙 񮽙 񮽙 񮽙 

񮽙 񮽙񮽙 񮽙 

񮽙 񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 

񮽙񮽙񮽙񮽙 
񮽙 񮽙񮽙 񮽙 

񮽙񮽙 񮽙񮽙񮽙 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with L.¬app and L.app sets obtained from both supervised CRF and JESS-CM with 1G-word 
unlabeled data evaluated by the entire sentence accuracies, and the ratio of U.app. 

unlab. data 
dev (Aug. 30-31) test (Dec. 06-07) 
(period) 
#sent. #wds F β=1 
U.app F β=1 
U.app 
reu(Sep.) 1.0M 17M 93.50 
82.0% 88.27 
69.7% 
reu(Oct.) 1.3M 20M 93.04 
71.0% 88.82 
72.0% 
reu(Nov.) 1.2M 18M 92.94 
68.7% 89.08 
74.3% 
reu(Dec.)* 
9M 15M 92.91 
67.0% 89.29 
84.4% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Influence of U.app in NER experiments: *(ex-
cluding Dec. 06-07) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 shows</head><label>6</label><figDesc></figDesc><table>the relation between JESS-CM per-
formance and U.app in the NER experiments. The 
development data and test data were obtained from 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>POS tagging results of the previous top systems 
for PTB III data evaluated by label accuracy 

system 
test additional resources 
JESS-CM (CRF/HMM) 
95.15 1G-word unlabeled data 
94.67 15M-word unlabeled data 
(Ando </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 8 : Syntactic chunking results of the previous top systems for CoNLL'00 shared task data (F β=1 score)</head><label>8</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> Since CoNLL&apos;00 shared task data has no development set, we divided the labeled training data into two distinct sets, 4/5 for training and the remainder for the development set, and determined the tunable parameters in preliminary experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A High-Performance Semi-Supervised Learning Method for Text Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-2005</title>
		<meeting>of ACL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-view Feature Generation Model for Semi-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML-2007</title>
		<meeting>of ICML-2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining Labeled and Unlabeled Data with Co-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Learning Theory 11</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Named Entity Recognition through Classifier Combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2003</title>
		<meeting>of CoNLL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Field Segmentation Models for Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-2005</title>
		<meeting>of ACL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="371" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chunking with Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML-2001</title>
		<meeting>of ICML-2001</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-Supervised Sequence Modeling with Syntactic Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI-2005</title>
		<meeting>of AAAI-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="813" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text Classification from Labeled and Unlabeled Documents using EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shallow Parsing with Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT/NAACL-2003</title>
		<meeting>of HLT/NAACL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided Learning for Bidirectional Sequence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-2007</title>
		<meeting>of ACL-2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="760" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing Weight Undertraining in Structured Discriminative Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sindelar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HTL-NAACL 2006</title>
		<meeting>of HTL-NAACL 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="89" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemiSupervised Structured Output Learning Based on a Hybrid Generative and Discriminative Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2000 Shared Task: Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2000 and LLL-2000</title>
		<meeting>of CoNLL-2000 and LLL-2000</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2003</title>
		<meeting>of CoNLL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature-rich Part-ofspeech Tagging with a Cyclic Dependency Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL-2003</title>
		<meeting>of HLT-NAACL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Text Chunking based on a Generalization of Winnow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="637" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
