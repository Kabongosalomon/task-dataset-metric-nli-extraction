<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A-CNN: Annularly Convolutional Neural Networks on Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Komarichev</surname></persName>
							<email>artem.komarichev@wayne.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichun</forename><surname>Zhong</surname></persName>
							<email>zichunzhong@wayne.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Hua</surname></persName>
							<email>jinghua@wayne.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A-CNN: Annularly Convolutional Neural Networks on Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Analyzing the geometric and semantic properties of 3D point clouds through the deep networks is still challenging due to the irregularity and sparsity of samplings of their geometric structures. This paper presents a new method to define and compute convolution directly on 3D point clouds by the proposed annular convolution. This new convolution operator can better capture the local neighborhood geometry of each point by specifying the (regular and dilated) ring-shaped structures and directions in the computation. It can adapt to the geometric variability and scalability at the signal processing level. We apply it to the developed hierarchical neural networks for object classification, part segmentation, and semantic segmentation in large-scale scenes. The extensive experiments and comparisons demonstrate that our approach outperforms the state-of-the-art methods on a variety of standard benchmark datasets (e.g., ModelNet10, ModelNet40, ShapeNetpart, S3DIS, and ScanNet).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, the ability to understand and analyze 3D data is becoming increasingly important in computer vision and computer graphics communities. During the past few years, the researchers have applied deep learning methods to analyze 3D objects inspired by the successes of these techniques in 2D images and 1D texts. Traditional lowlevel handcrafted shape descriptors suffer from not being able to learn the discriminative and sufficient features from 3D shapes <ref type="bibr" target="#b0">[1]</ref>. Recently, deep learning techniques have been applied to extract hierarchical and effective information from 3D shape features captured by low-level descriptors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6]</ref>. 3D deep learning methods are widely used in shape classification, segmentation, and recognition, etc. But all these methods are still constrained by the representation power of the shape descriptors.</p><p>One of the main challenges to directly apply deep learning methods to 3D data is that 3D objects can be represented in different formats, i.e., regular / structured representation (e.g., multi-view images and volumes), and irregular / unstructured representation (e.g., point clouds and meshes). There are extensive approaches based on regular / structured representation, such as multi-view convolutional neural networks (CNNs) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10]</ref> and 3D volumetric / grid CNN methods and its variants <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9]</ref>. These methods can be conveniently developed and implemented in 3D data structure, but they easily suffer from the heavy computation and large memory expense. So it is better to define the deep learning computations based on 3D shapes directly, i.e., irregular / unstructured representation, such as point cloud based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42]</ref>. However, defining the convolution on the irregular / unstructured representation of 3D objects is not an easy task. Very few methods on point clouds have defined an effective and efficient convolution on each point. Meanwhile, several approaches have been proposed to develop convolutional networks on 2D manifolds <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>. Their representations (e.g., 3D surface meshes) have point positions as well as connectivities, which makes it relatively easier to define the convolution operator on them.</p><p>In this work, we present a new method to define and compute convolutions directly on 3D point clouds effectively and efficiently by the proposed annular convolutions. This new convolution operator can better capture local neighborhood geometry of each point by specifying the (regular and dilated) ring-shaped structures and directions in the computation. It can adapt to the geometric variability and scalability at the signal processing level. Then, we apply it along with the developed hierarchical neural networks to object classification, part segmentation, and semantic segmentation in large-scale scene as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The key contributions of our work are as follows:</p><p>• We propose a new approach to define convolutions on point cloud. The proposed annular convolutions can define arbitrary kernel sizes on each local ring-shaped region, and help to capture better geometric representations of 3D shapes;</p><p>• We propose a new multi-level hierarchical method based on dilated rings, which leads to better capturing and abstracting shape geometric details. The new dilated strategy on point clouds benefits our proposed closed-loop convolutions and poolings;</p><p>• Our proposed network models present new state-ofthe-art performance on object classification, part segmentation, and semantic segmentation of large-scale scenes using a variety of standard benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Due to the scope of our work, we focus only on recently related deep learning methods, which are proposed on different 3D shape representations.</p><p>Volumetric Methods. One traditional way to analyze a 3D shape is to convert it into the regular volumetric occupancy grid and then apply 3D CNNs <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27]</ref>. The major limitation of these approaches is that 3D convolutions are more expensive in computations than 2D cases. In order to make the computation affordable, the volume grid size is usually in a low resolution. However, lower resolution means loosing some shape geometric information, especially in analyzing large-scale 3D shapes / scenes. To overcome these problems, octree-based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> have been proposed to allow applying 3D CNNs on higher / adaptive resolution grids. PointGrid <ref type="bibr" target="#b15">[16]</ref> is a 3D CNN that incorporates a constant number of points within each grid cell and allows it to learn better local geometric details. Similarly, Hua et al. <ref type="bibr" target="#b8">[9]</ref> presented a 3D convolution operator based on a uniform grid kernel for semantic segmentation and object recognition on point clouds.</p><p>Point Cloud based Methods. PointNet <ref type="bibr" target="#b25">[26]</ref> is the first attempt of applying deep learning directly on point clouds. PointNet model is invariant to the order of points, but it considers each point independently without including local region information. PointNet++ <ref type="bibr" target="#b27">[28]</ref> is a hierarchical extension of PointNet model and learns local structures of point clouds at different scales. But <ref type="bibr" target="#b27">[28]</ref> still considers every point in its local region independently. In our work, we address the aforementioned issues by defining the convolution operator that learns the relationship between neighbor-ing points in a local region, which helps to better capture the local geometric properties of the 3D object.</p><p>Klokov et al. <ref type="bibr" target="#b12">[13]</ref> proposed a new deep learning architecture called Kd-networks, which uses kd-tree structure to construct a computational graph on point clouds. KC-Net <ref type="bibr" target="#b31">[32]</ref> improves PointNet model by considering the local neighborhood information. It defines a set of learnable point-set kernels for local neighboring points and presents a pooling method based on a nearest-neighbor graph. PCNN <ref type="bibr" target="#b2">[3]</ref> is another method to apply convolutional neural networks to point clouds by defining extension and restriction operators, and mapping point cloud functions to volumetric functions. SO-Net <ref type="bibr" target="#b16">[17]</ref> is a permutation invariant network that utilizes spatial distribution of point clouds by building a self-organizing map. There are also some spectral convolution methods on point clouds, such as SyncSpecCNN <ref type="bibr" target="#b43">[44]</ref> and spectral graph convolution <ref type="bibr" target="#b35">[36]</ref>. Point2Sequence <ref type="bibr" target="#b18">[19]</ref> learns the correlation of different areas in a local region by using attention mechanism, but it does not propose a convolution on point clouds. PointCNN <ref type="bibr" target="#b17">[18]</ref> is a different method that proposes to transform neighboring points to the canonical order and then apply convolution.</p><p>Recently, there are several approaches proposed to process and analyze large-scale point clouds from indoor and outdoor environments. Engelmann et al. <ref type="bibr" target="#b7">[8]</ref> extended Point-Net model to exploit the large-scale spatial context. Ye et al. <ref type="bibr" target="#b41">[42]</ref> proposed a pointwise pyramid pooling to aggregate features at local neighborhoods as well as two-directional hierarchical recurrent neural networks (RNNs) to learn spatial contexts. However, these methods do not define convolutions on large-scale point clouds to learn geometric features in the local neighborhoods. TangentConv <ref type="bibr" target="#b34">[35]</ref> is another method that defines the convolution on point clouds by projecting the neighboring points on tangent planes and applying 2D convolutions on them. The orientation of the tangent image is estimated according to the local point / shape curvature, but as we know the curvature computation on the local region of the point clouds is not stable and not robust (see the discussion in Sec. 3.4), which makes it orientationdependent. Instead, our method proposes an annular convolution, which is invariant to the orientations of local patches. Also, ours does not require additional input features while theirs needs such features (e.g., depth, height, etc.).</p><p>Mesh based Methods. Besides point cloud based methods, several approaches have been proposed to develop convolutional networks on 3D meshes for shape analysis. Geodesic CNN <ref type="bibr" target="#b21">[22]</ref> is an extension of the Euclidean CNNs to non-Euclidean domains and is based on a local geodesic system of polar coordinates to extract local patches. Anisotropic CNN <ref type="bibr" target="#b3">[4]</ref> is another generalization of Euclidean CNNs to non-Euclidean domains, where classical convolutions are replaced by projections over a set of oriented anisotropic diffusion kernels. Mixture Model Net-works (MoNet) <ref type="bibr" target="#b23">[24]</ref> generalizes deep learning methods to non-Euclidean domains (graphs and manifolds) by combining previous methods, e.g., classical Euclidean CNN, Geodesic CNN, and Anisotropic CNN. MoNet proposes a new type of kernel in parametric construction. Directionally Convolutional Networks (DCN) <ref type="bibr" target="#b40">[41]</ref> applies convolution operation on the triangular mesh of 3D shapes to address part segmentation problem by combining local and global features. Lastly, Surface Networks <ref type="bibr" target="#b13">[14]</ref> propose upgrades to Graph Neural Networks to leverage extrinsic differential geometry properties of 3D surfaces for increasing their modeling power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work, we propose a new end-to-end framework named as annularly convolutional neural networks (A-CNN) that leverages the neighborhood information to better capture local geometric features of 3D point clouds. In this section, we introduce main technique components of the A-CNN model on point clouds that include: regular and dilated rings, constraint-based k-nearest neighbors (k-NN) search, ordering neighbors, annular convolution, and pooling on rings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Regular and Dilated Rings on Point Clouds</head><p>To extract local spatial context of the 3D shape, Point-Net++ <ref type="bibr" target="#b27">[28]</ref> proposes multi-scale architecture. The major limitation of this approach is that multiple scaled regions may have overlaps (i.e., same neighboring points could be duplicately included in different scaled regions), which reduces the performance of the computational architecture. Overlapped points at different scales lead to redundant information at the local region, which limits a network to learn more discriminative features.</p><p>In order to address the above issue, our proposed framework is aimed to leverage a neighborhood at different scales more wisely. We propose two ring-based schemes, i.e., regular rings and dilated rings. Comparing to multi-scale strategy, the ring-based structure does not have overlaps (no duplicated neighboring points) at the query point's neighborhood, so that each ring contains its own unique points, as illustrated in Sec. A of Supplementary Material.</p><p>The difference between regular rings and dilated rings is that dilated rings have empty space between rings. The idea of proposed dilated rings is inspired by dilated convolutions on image processing <ref type="bibr" target="#b44">[45]</ref>, which benefits from aggregating multi-scale contextual information. Although each ring may define the same number of computation / operation parameters (e.g., number of neighboring points), the coverage area of each ring is different (i.e., dilated rings will have larger coverage than the regular rings) as depicted in <ref type="figure">Fig. 7</ref>. Regular rings can be considered as dilated rings with the dilation factor equal to 0. <ref type="figure">Figure 2</ref>: The comparison of the regular and dilated ring-shaped structures (such as with two rings). We can see that comparing two sectors (e.g., black solid points) in the regular and dilated rings, the dilated rings cover larger space by using the same number of neighbors as in regular rings. Moreover, each ring contains unique neighboring points comparing to the other ring.</p><p>The proposed regular rings and dilated rings will contribute to neighboring point search, convolution, and pooling in the follow-up processes. First, for k-NN algorithm, we constrain search areas in the local ring-shaped neighborhood to ensure no overlap. Second, the convolutions defined on rings cover larger areas with the same kernel sizes without increasing the number of convolution parameters. Third, the regular / dilated ring architectures will help to aggregate more discriminative features after applying maxpooling at each ring of the local region. We will discuss them in more detail in the following subsections.</p><p>To justify the aforementioned statements, we will compare multi-scale approach with our proposed multi-ring scheme on object classification task in the ablation study (Sec. 5.4). The results show that ring-based structure captures better local geometric features than previous multiscale method, since it achieves higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Constraint-based K-NN Search</head><p>In the original PointNet++ model, the ball query algorithm returns the first K neighbors found inside a search ball specified by a radius R and query point q i , so that it cannot guarantee that the closest points will always be found. However, our proposed k-NN search algorithm guarantees returning closest points inside the searching area by using the Euclidean metric. Each ring is defined by two parameters: the inner radius R inner and the outer radius R outer (in <ref type="figure">Fig. 7)</ref>; therefore, the constraint-based k-NN search ensures that the closest and unique points will be found in each ring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ordering Neighbors</head><p>In order to learn relationships between neighboring points in a local regions, we need first to order points in a clockwise / counterclockwise manner and then apply annular convolutions. Our proposed ordering operator consists of two main steps: projection and ordering. The importance of the projection before ordering is that the dot product has its restriction in ordering points. By projecting points on a tangent plane at a query point q i , we effectively order neighbors in clockwise / counterclockwise direction by taking use of cross product and dot product together. The detailed explanations of normal estimation, orthogonal projection, and ordering are given in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Normal Estimation on Point Clouds</head><p>Normal is an important geometric property of a 3D shape. We use it as a tool for projecting and ordering neighboring points at a local domain. The simplest normal estimation method approximates the normal n i at the given point q i by calculating the normal of the local tangent plane T i at that point, which becomes a least-square plane fitting estimation problem <ref type="bibr" target="#b29">[30]</ref>. To calculate normal n i , one needs to compute eigenvalues and eigenvectors of the covariance matrix C as:</p><formula xml:id="formula_0">C = 1 K K j=1 (x j − q i ) · (x j − q i ) T , C · v γ = λ γ · v γ , γ ∈ {0, 1, 2},<label>(1)</label></formula><p>where K is the number of neighboring points x j s around query point q i (e.g., K = 10 in our experiments), λ γ and v γ are the γth eigenvalue and eigenvector of the covariance matrix C, respectively. The covariance matrix C is symmetric and positive semi-definite. The eigenvectors v γ form an orthogonal frame, in respect to the local tangent plane T i . The eigenvector v 0 that corresponds to the smallest eigenvalue λ 0 is the estimated normal n i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Orthogonal Projection</head><p>After extracting neighbors x j , j ∈ {1, ..., K} for a query point q i , we calculate projections p j s of these points on a tangent plane T i described by a unit normal n i (estimated in Sec. 3.3.1) as: <ref type="figure">Fig. 3</ref> (a) illustrates the orthogonal projection of neighboring points on a ring.</p><formula xml:id="formula_1">p j = x j − ((x j − q i ) · n i ) · n i , j ∈ {1, ..., K}. (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Counterclockwise Ordering</head><p>Firstly, we use the geometric definition of the dot product to compute the angle between two vectors c (i.e., starts from the query point q i and connects with a randomly starting point, such as p 1 ) and p j − q i (i.e., starts from the query point q i and connects with other neighboring points p j ):</p><formula xml:id="formula_2">cos(θ pj ) = c · (p j − q i ) ||c||||p j − q i || .<label>(3)</label></formula><p>We know that cos(θ pj ) lies in [−1, 1], which corresponds to angles between [0 • , 180 • ]. In order to sort the neighboring points around the query point between   <ref type="figure">Figure 3</ref>: The illustration of the proposed annular convolution on a ring. (a) Projection: qi is a query point. After applying the constraint-based k-NN search, neighboring points X = {xj|j = 1, ..., K} are extracted on a ring. Given the normal ni at query point qi, we project the searched points on the tangent plane Ti. (b) Counterclockwise Ordering: After projection, we randomly pick a starting point as our reference direction c and order points in counterclockwise. It is worth mentioning that we order original points [x1, x2, ..., xj, ..., xK ] based on their projections. (c) Annular Convolution: Depending on the kernel size, we copy several original points from the beginning position and concatenate them to the end of the ordered points. Finally, we apply annular convolution with the given kernel.</p><p>[0 • , 360 • ), we must to decide which semicircle the considered point p j belongs to as follows:</p><formula xml:id="formula_3">sign pj = (c × (p j − q i )) · n i ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">sign pj ≥ 0 is θ pj ∈ [0 • , 180 • ], and sign pj &lt; 0 is θ pj ∈ (180 • , 360 • ).</formula><p>Then, we can recompute the cosine value of the angle as:</p><formula xml:id="formula_5">∠ pj = −cos(θ pj ) − 2 sign pj &lt; 0 cos(θ pj ) sign pj ≥ 0.<label>(5)</label></formula><p>Now the values of the angles lie in (−3, 1], which maps angles between [0 • , 360 • ). Finally, we sort neighboring points x j by descending the value of ∠ pj to obtain the counterclockwise order. <ref type="figure">Fig. 3</ref> (b) illustrates the process of ordering in a local neighborhood. The neighboring points can be ordered in the clockwise manner, if we sort neighboring points x j by ascending the value of ∠ pj .</p><p>Our experiments show in Sec. 5.4 that ordering points in the local regions is an important step in our framework and our model achieves better classification accuracy with ordered points than without ordering them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Annular Convolution on Rings</head><p>Through the previous computation, we have the ordered neighbors represented as an array [x 1 , x 2 , ..., x K ]. In or-der to develop the annular convolution, we need to loop the array of neighbors with respect to the size of the kernel (e.g., 1 × 3, 1 × 5, ...) on each ring. For example, if the convolutional kernel size is 1 × 3, we need to take the first two neighbors and concatenate them with the ending elements in the original array to construct a new circular array</p><formula xml:id="formula_6">[x 1 , x 2 , ..., x K , x 1 , x 2 ]</formula><p>. Then, we can perform the standard convolutions on this array as shown in <ref type="figure">Fig. 3 (c)</ref>.</p><p>There are some nice properties of the proposed annular convolutions as follows: <ref type="formula" target="#formula_0">(1)</ref> The annular convolution is invariant to the orientation of the local patch. That is because the neighbors are organized and ordered in a closed loop in each ring by concatenating the beginning with the end of the neighboring points' sequence. Therefore, we can order neighbors based on any random starting position, which does not negatively affect the convolution results. Compared with some previous convolutions defined on 3D shapes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35]</ref>, they all need to compute the real principal curvature direction as the reference direction to define the local patch operator, which is not robust and cumbersome. In particular, 3D shapes have large areas of flat and spherical regions, where the curvature directions are arbitrary. (2) As we know, in reality, the normal direction flipping issues are widely existing in point clouds, especially the large-scale scene datasets. Under the annular convolution strategy, no matter the neighboring points are ordered in clockwise or counterclockwise manner, the results are the same. (3) Another advantage of annular convolution is that we can define an arbitrary kernel size, instead of just 1 × 1 kernels <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>. Therefore, the annular convolution can provide the ability to learn the relationship between ordered points inside each ring as shown in <ref type="figure">Fig. 3 (c)</ref>.</p><p>Annular convolutions can be applied on both regular and dilated rings. By applying annular convolutions with the same kernel size on different rings, we can cover and convolve larger areas by using the dilated structure, which helps us to learn larger spatial contextual information in the local regions. The importance of annular convolutions is shown in the ablation study in Sec. 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Pooling on Rings</head><p>After applying a set of annular convolutions sequentially, the resulting convolved features encode information about its closest neighbors in each ring as well as spatial remoteness from a query point. Then we aggregate the convolved features across all neighbors on each ring separately. We apply the max-pooling strategy in our framework. Our proposed ring-based scheme allows us to aggregate more discriminative features. The extracted max-pooled features contain the encoded information about neighbors and the relationship between them in the local region, unlike the pooling scheme in PointNet++ <ref type="bibr" target="#b27">[28]</ref>, where each neighbor is considered independently from its neighbors. In our pooling process, the non-overlapped regions (rings) will aggregate different types of features in each ring, which can uniquely describe each local region (ring) around the query point. The multi-scale approach in PointNet++ does not guarantee this and might aggregate the same features at different scales, which is redundant information for a network. The (regular and dilated) ring-based scheme helps to avoid extracting duplicate information but rather promotes extracting multi-level information from different regions (rings). This provides a network with more diverse features to learn from. After aggregating features at different rings, we concatenate and feed them to another abstract layer to further learn hierarchical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A-CNN Architecture</head><p>Our proposed A-CNN model follows a design where the hierarchical structure is composed of a set of abstract layers. Each abstract layer consists of several operations performed sequentially and produces a subset of input points with newly learned features. Firstly, we subsample points by using Farthest Point Sampling (FPS) algorithm <ref type="bibr" target="#b22">[23]</ref> to extract centroids randomly distributed on the surface of each object. Secondly, our constraint-based k-NN extracts neighbors of a centroid for each local region (i.e., regular / dilated rings) and then we order neighbors in a counterclockwise manner using projection. Finally, we apply sequentially a set of annular convolutions on the ordered points and maxpool features across neighbors to produce new feature vectors, which uniquely describe each local region.</p><p>Given the point clouds of 3D shapes, our proposed endto-end network is able to classify and segment the objects. In the following, we discuss the classification and segmentation network architectures on 3D point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification Network</head><p>The classification network is illustrated at the top of <ref type="figure" target="#fig_3">Fig. 4</ref>. It consists of two major parts: encoder and classification. The encoder extracts features from each ring independently inside every layer and concatenates them at the end to process further to extract high-level features. The proposed architecture includes both regular rings and dilated rings. We end up using two rings per layer, because it gives us pretty good experimental results as shown in the Sec. 5. It can be easily extended to more than two rings per layer, if necessary.</p><p>We use regular rings in the first layer and dilated rings in the second layer in the encoder. Annular convolutions with the kernel sizes 1 × 3 and stride 1 are applied in the first two layers, followed by a batch normalization <ref type="bibr" target="#b11">[12]</ref> (BN) and a rectified linear unit <ref type="bibr" target="#b24">[25]</ref> (ReLU). Different rings of the same query point are processed in parallel. Then, the aggregated features from each ring concatenate together to propagate to the next layer. The last layer in the encoder performs convolutions with kernel sizes 1 × 1 followed by BN and ReLU layers, where only spatial positions of the sampled points are the numbers of points as input, after the first and second layer, respectively. K and K are the unordered and ordered points inside the local rings, respectively. c is the number of classification classes. m is the number of segmentation classes. "FPS" stands for Farthest Point Sampling algorithm. "mlp" stands for multi-layer perceptron. conv1×3(F1, F2, ..., Fn) stands for annular convolutions with the kernel size 1 × 3 applied sequentially with corresponding feature map sizes Fi, i ∈ 1, ..., n.</p><p>are considered. After that aggregated high-level features are fed to the set of fully-connected layers with integrated dropout <ref type="bibr" target="#b32">[33]</ref> and ReLU layers to calculate probability of each class. The output size of the classification network is equal to the number of classes in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Segmentation Network</head><p>The segmentation network shares encoder part with the classification network as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. In order to predict the segmentation label per point, we need to upsample the sampled points in the encoder back to the original point cloud size. As pointed out by <ref type="bibr" target="#b45">[46]</ref>, the consecutive feature propagation proposed by <ref type="bibr" target="#b27">[28]</ref> is not the most efficient approach. Inspired from <ref type="bibr" target="#b45">[46]</ref>, we propagate features from different levels from the encoder directly to the original point cloud size, and concatenate them by allowing the network to learn the most important features from different levels as well as to learn the relationship between them.</p><p>The output of each level has different sizes due to the hierarchical feature extractions, so we have to restore hierarchical features from each level back to the original point size by using an interpolation method <ref type="bibr" target="#b27">[28]</ref>. The interpolation method is based on the inverse squared Euclidean distance weighted average of the three nearest neighbors as:</p><formula xml:id="formula_7">f (l+1) (x) = 3 j=1 f (l) (x j ) w j (x) 3 j=1 w j (x) ,<label>(6)</label></formula><p>where w j (x) = 1 d(x,xj ) 2 is an inverse squared Euclidean distance weight. Then, we concatenate upsampled features from different levels and pass them through 1 × 1 convolution to reduce feature space and learn the relationship between features from different levels. Finally, the segmentation class distribution for each point is calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our A-CNN model on various tasks such as point cloud classification, part segmentation, and largescale scene segmentation. In the following subsections, we demonstrate more details on each task. It is noted that for the comparison experiments, best results in the tables are shown in bold font.</p><p>All models in this paper are trained on a single NVIDIA Titan Xp GPU with 12 GB GDDR5X. The training time of our model is faster than that of PointNet++ model. More details about the network configurations, training settings and timings in our experiments can be found in Sec. B and Tab. 5 of Supplementary Material. The source code of the framework will be made available later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Point Cloud Classification</head><p>We evaluate our classification model on two datasets: ModelNet10 and ModelNet40 <ref type="bibr" target="#b39">[40]</ref>. ModelNet is a largescale 3D CAD model dataset. ModelNet10 is a subset of ModelNet dataset that consists of 10 different classes with 3991 training and 908 testing objects. ModelNet40 includes 40 different classes with 9843 objects for training and 2468 objects for testing. Point clouds with 10,000 points and normals are sampled from meshes, normalized into a unit sphere, and provided by <ref type="bibr" target="#b27">[28]</ref>.</p><p>For experiments on ModelNet10 and ModelNet40, we sample 1024 points with normals, where normals are only used to order points in the local region. For data augmentation, we randomly scale object sizes, shift object positions, and perturb point locations. For better generalization, we apply point shuffling in order to generate different centroids for the same object at different epochs.</p><p>In Tab. 1, we compare our method with several state-ofthe-art methods in the shape classification results on both ModelNet10 and ModelNet40 datasets. Our model achieves better accuracy among the point cloud based methods (with 1024 points), such as PointNet <ref type="bibr" target="#b25">[26]</ref>, PointNet++ <ref type="bibr" target="#b27">[28]</ref> (5K points + normals), Kd-Net (depth 15) <ref type="bibr" target="#b12">[13]</ref>, Pointwise CNN <ref type="bibr" target="#b8">[9]</ref>, KCNet <ref type="bibr" target="#b31">[32]</ref>, PointGrid <ref type="bibr" target="#b15">[16]</ref>, PCNN <ref type="bibr" target="#b2">[3]</ref>, and PointCNN <ref type="bibr" target="#b17">[18]</ref>. Our model is slightly better than Point2Sequence <ref type="bibr" target="#b18">[19]</ref> on ModelNet10 and shows comparable performance on ModelNet40.</p><p>Meanwhile, our model performs better than other volumetric approaches, such as O-CNN <ref type="bibr" target="#b36">[37]</ref> and AO-CNN <ref type="bibr" target="#b37">[38]</ref>; while we are a little worse than SO-Net <ref type="bibr" target="#b16">[17]</ref>, which uses denser input points, i.e., 5000 points with normals as the input (1024 points in our A-CNN); MVCNN-MultiRes <ref type="bibr" target="#b26">[27]</ref>, which uses multi-view 3D volumes to represent an object (i.e., 20 views of 30 × 30 × 30 volume); the VRN Ensemble <ref type="bibr" target="#b4">[5]</ref>, which involves an ensemble of six models.</p><p>We also provide some feature visualization results in Sec. C of Supplementary Material, including global feature (e.g., t-SNE clustering) visualization and local feature (e.g., the magnitude of the gradient per point) visualization.    <ref type="figure">Figure 5</ref>: Qualitative results on ShapeNet-part dataset. We compare our results with PointNet++ <ref type="bibr" target="#b27">[28]</ref> and ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Point Cloud Segmentation</head><p>We evaluate our segmentation model on ShapeNetpart <ref type="bibr" target="#b42">[43]</ref> dataset. The dataset contains 16,881 shapes from 16 different categories with 50 label parts in total. The main challenge of this dataset is that all categories are highly imbalanced. There are 2048 points sampled for each shape from the dataset, where most shapes contain less than six parts. We follow the same training and testing splits provided in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>. For data augmentation, we perturb point locations with the point shuffling for better generalization.</p><p>We evaluate our segmentation model with two different inputs. One of the models is trained without feeding normals as additional features and the other model is trained with normals as additional features. The quantitative results are provided in Tab. 2, where mean IoU (Intersection-over-Union) is reported. The qualitative results are visualized in <ref type="figure">Fig. 5</ref>. Our approach with point locations only as input outperforms PointNet <ref type="bibr" target="#b25">[26]</ref>, Kd-Net <ref type="bibr" target="#b12">[13]</ref>, KCNet <ref type="bibr" target="#b31">[32]</ref>, and PCNN <ref type="bibr" target="#b2">[3]</ref>; and shows slightly worse performance comparing to PointGrid <ref type="bibr" target="#b15">[16]</ref> (volumetric method) and PointCNN <ref type="bibr" target="#b17">[18]</ref>. Meanwhile, our model achieves the best performance with the input of point locations and normals, compared with PointNet++ <ref type="bibr" target="#b27">[28]</ref>, SyncSpecCNN <ref type="bibr" target="#b43">[44]</ref>, SO-Net <ref type="bibr" target="#b16">[17]</ref>, SGPN <ref type="bibr" target="#b38">[39]</ref>, O-CNN <ref type="bibr" target="#b36">[37]</ref>, RSNet <ref type="bibr" target="#b10">[11]</ref>, and Point2Sequence <ref type="bibr" target="#b18">[19]</ref>. The more detailed quantitative results (e.g., per-category IoUs) and more visualization results are provided in Sec. E of Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Semantic Segmentation in Scenes</head><p>We also evaluate our segmentation model on two largescale indoor datasets Stanford 3D Large-Scale Indoor Spaces (S3DIS) <ref type="bibr" target="#b1">[2]</ref> and ScanNet <ref type="bibr" target="#b6">[7]</ref>. S3DIS contains 6 large-scale indoor areas with 271 rooms sampled from 3 different buildings, where each point has the semantic label that belongs to one of the 13 categories. ScanNet includes 1513 scanned indoor point clouds, where each voxel has been labeled with one of the 21 categories.</p><p>We employ the same training and testing strategies as  <ref type="figure">Figure 6</ref>: Qualitative results on S3DIS dataset. We compare our results with PointNet <ref type="bibr" target="#b25">[26]</ref> and ground truth. The auditorium is a challenging room type and appears only in Area 2. Our model produces much better segmentation result, compared with the result of PointNet.</p><p>PointNet <ref type="bibr" target="#b25">[26]</ref> on S3DIS, where we use 6-fold cross validation over all six areas. The evaluation results are reported in Tab. 2, and qualitative results are visualized in <ref type="figure">Fig. 6</ref>. Our model demonstrates better segmentation results compared with PointNet <ref type="bibr" target="#b25">[26]</ref>, MS+CU (2) <ref type="bibr" target="#b7">[8]</ref>, G+RCU <ref type="bibr" target="#b7">[8]</ref>, 3P-RNN <ref type="bibr" target="#b41">[42]</ref>, SPGraph <ref type="bibr" target="#b14">[15]</ref>, and TangentConv <ref type="bibr" target="#b34">[35]</ref>. However, our model performs slightly worse than PointCNN <ref type="bibr" target="#b17">[18]</ref> due to their non-overlapping block sampling strategy with paddings which we do not use. Meanwhile, our approach shows the best segmentation results on ScanNet <ref type="bibr" target="#b6">[7]</ref> and achieves the state-of-the-art performance, compared with PointNet <ref type="bibr" target="#b25">[26]</ref>, PointNet++ <ref type="bibr" target="#b27">[28]</ref>, TangentConv <ref type="bibr" target="#b34">[35]</ref>, and PointCNN <ref type="bibr" target="#b17">[18]</ref> according to Tab. 2. More qualitative visualization results and data preparation details on both datasets are provided in Sec. D and Sec. E, respectively, of Supplementary Material and Video. Note: * TangentConv <ref type="bibr" target="#b34">[35]</ref> OA on S3DIS Area 5 is 82.5% (as reported in their paper), which is worse compared with our OA of 85.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>The goal of our ablation study is to show the importance of the proposed technique components (in Sec. 3) in our A-CNN model. We evaluate three proposed components, such as rings without overlaps (Sec. 3.1), ordering (Sec. 3.3), and annular convolution (Sec. 3.4) on the classification task of ModelNet40 dataset as shown in Tab. 4. In the first experiment, we replace our proposed constraint-based k-NN on ring regions with ball query in <ref type="bibr" target="#b27">[28]</ref>, but keep ordering and annular convolutions on. In the second and third experiments, we turn off either annular convolutions or ordering, respectively; and keep the rest two components on. Our experiments show that the proposed ring-shaped scheme contributes the most to our model. It is because multi-level rings positively affect annular convolutions. Finally, A-CNN model with all three components (i.e., rings without overlaps, ordering, and annular convolutions) achieves the best results. We also discover that reducing overlap / redundancy in multi-scale scheme can improve existing methods. We evaluate the original PointNet++ <ref type="bibr" target="#b27">[28]</ref> with and without overlap as shown in Sec. A of Supplementary Material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose a new A-CNN framework on point clouds, which can better capture local geometric information of 3D shapes. Through extensive experiments on several benchmark datasets, our method has achieved the state-of-the-art performance on point cloud classification, part segmentation, and large-scale semantic segmentation tasks. Since our work does not solely focus on large-scale scene datasets, we will explore some new deep learning architectures to improve the current results. We will also investigate to apply the proposed framework on large-scale outdoor datasets in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: A-CNN: Annularly Convolutional Neural Networks on Point Clouds</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ball Query vs Ring-based Scheme</head><p>The comparison of multi-scale method proposed in <ref type="bibr" target="#b27">[28]</ref> and our ring-based scheme is depicted in <ref type="figure">Fig. 7</ref>. It is noted that comparing to multi-scale regions, the ring-based structure does not have overlaps (no neighboring point duplication) at the query point's neighborhood. It means that each ring contains its own unique points.  <ref type="figure">Figure 7</ref>: A schematic comparison for searching neighbors in a local region with Nq points between multi-scale approach from <ref type="bibr" target="#b27">[28]</ref> and our proposed approaches with regular and dilated rings. The number of neighboring points per region (e.g., k1 and k2) is the same between different methods. Regions in multi-scale architecture have neighboring overlaps (red points belong to different regions near the same query point q), while regular and dilated rings have the unique neighbors. We have discovered that reducing redundancy can improve the existing multi-scale approach in <ref type="bibr" target="#b27">[28]</ref>. We test redundancy issue on original PointNet++ model <ref type="bibr" target="#b27">[28]</ref> with and without overlap / redundancy. We compare the original PointNet++ multi-scale model with ball queries (with redundant points) against PointNet++ with our proposed regular rings (without redundant points). Our experiments show that the proposed multi-ring (i.e., without redundant points) outperforms the multi-scale scheme (i.e., with redundant points) on ModelNet40 according to Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head><p>We use A-CNN-3L network configuration in Tab. 5 for all experiments on point cloud classification tasks and A-CNN-4L network configuration in Tab. 5 for both part segmenta-tion and semantic segmentation tasks. We use regular rings in L 1 and dilated rings in L 2 in our A-CNN-3L architecture. Similarly, we use regular rings in L 1 and dilated rings in L 2 and L 3 in our A-CNN-4L architecture.</p><p>We use Adam optimization method with learning rate 0.001 and decay rate 0.7 in classification and decay 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Visualization</head><p>Local Feature Visualization. <ref type="figure">Fig. 8</ref> and <ref type="figure" target="#fig_8">Fig. 9</ref> visualize the magnitude of the gradient per point in the classification task on ModelNet10 and ModelNet40 datasets. Blue color represents low magnitude of the gradients and red color represents high magnitude of the gradients. The points with higher magnitudes get greater updates during training and the learning contribution of them is higher. Therefore, this feature visualization could be thought as the object saliency. For example, in ModelNet40 dataset our model considers wings and tails as important regions to classify an object as an airplane; bottle neck is important for a bottle; the flowers and leaves are important for a plant; tube or middle part (usually narrow parts) is important for a lamp; legs are important to classify an object as a stool.</p><p>Global Feature Visualization. <ref type="figure" target="#fig_0">Fig. 10</ref> and <ref type="figure" target="#fig_0">Fig. 11</ref> shows the t-SNE clustering visualization <ref type="bibr" target="#b20">[21]</ref> of the learned global shape features from the proposed A-CNN model for the shape classification tasks in ModelNet10 and Model-Net40 test splits. We reduce 1024-dim feature vectors to 2-dim features. We can see that similar shapes are well clustered together according to their semantic categories. For example, in ModelNet10 dataset the clusters of desk, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Preparation Details</head><p>S3DIS data preparation. To prepare training and testing datasets, we divide every room into blocks with a size of 1 m × 1 m × 2 m and with a stride of 0.5 m. We has sampled 4096 points from each block. The height of each block is scaled to 2 m to ensure that our constraint-based k-NN search works optimally with the provided radiuses. In total, the prepared dataset contains 23,585 blocks across all six areas. Each point is represented as a 6D vector (XY Z: normalized global point coordinates and centered at origin, RGB: colors). We do not use the relative position of the block in the room scaled between 0 and 1 as used in <ref type="bibr" target="#b25">[26]</ref>, because our model already achieves better results without using this additional information. We calculate point normals for each room by using the Point Cloud Library (PCL) library <ref type="bibr" target="#b30">[31]</ref>. The calculated normals are only used to order points in the local region. For data augmentation, we use the same data augmentation strategy as used in the point cloud segmentation on ShapeNet-part dataset which is point per-  Note: "CRF" stands for conditional random field method for final result refinement in O-CNN method. turbation with point shuffling.</p><p>ScanNet data preparation. ScanNet divides original 1513 scanned scenes in 1201 and 312 for training and testing, respectively. We sample blocks from the scenes following the same procedure as in <ref type="bibr" target="#b27">[28]</ref>, where every block has a size of 1.5 m × 1.5 m with 8192 points. We estimate point normals using the PCL library <ref type="bibr" target="#b30">[31]</ref>. Each point is represented as a 6D vector (XY Z: coordinates of the block centered at origin, N x N y N z : normals) without RGB information. For data augmentation, we use the point perturbation with point shuffling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Experimental Results</head><p>Point Cloud Segmentation. Tab. 6 and Tab. 7 show the quantitative results of part segmentation on ShapeNet-part dataset with two different inputs. Tab. 6 reports results when the input is point position only. Tab. 7 reports results when the input is point position with its normals.</p><p>For ShapeNet-part dataset, we visualize more results (besides the segmentation results shown in the paper) in <ref type="figure" target="#fig_0">Fig. 12</ref>. We compare our results with PointNet++ <ref type="bibr" target="#b27">[28]</ref>, and our A-CNN model can produce better segmentation results than PointNet++ model.</p><p>Semantic Segmentation in Scenes. For S3DIS dataset, we pick rooms from all six areas: area 1 (row 1), area 2   <ref type="figure" target="#fig_0">Figure 13</ref>: The visualization results on S3DIS dataset. We compare our model with PointNet <ref type="bibr" target="#b25">[26]</ref> and the ground truth. The challenging sample rooms have been picked from the all six areas: area 1 (row 1), area 2 (row 2) area 3 (row 3), area 4 (row 4), area 5 (row 5), and area 6 (row 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chair Floor</head><p>Wall  <ref type="figure" target="#fig_0">Figure 14</ref>: The visualization results on ScanNet dataset. We compare our model with PointNet++ <ref type="bibr" target="#b27">[28]</ref> and the ground truth. The challenging sample rooms have been picked from the ScanNet dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed annularly convolutional neural networks (A-CNN) model on point clouds to perform classification, part segmentation, and semantic segmentation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>conv 1x3 (F 1 ,F 2 ,...,F n ) N i x 3 Normals conv 1x3 (F 1 ,F 2 ,...,F n ) The architecture of A-CNN. Both classification and segmentation networks share encoder part for the feature extraction. Normals are used only to determine the order of neighboring points in the local regions (dashed arrows mean no backpropagation during training) and not used as additional features, unless it is mentioned explicitly in the experiments. N , N1, N2 (where N &gt; N1 &gt; N2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>5 in segmentation tasks. We have trained our classification model for 250 epochs, our part segmentation model for 200 epochs, and our large-scale semantic segmentation models for 50 epochs on each area of S3DIS and for 200 epochs on ScanNet. The training time of our model is faster than that of PointNet++ model, since we use ring-based neighboring search, which is more efficient and effective than ball query in PointNet++ model. For instance, the training time on the segmentation model for 200 epochs is about 19 hours on a single NVIDIA Titan Xp GPU with 12 GB GDDR5X, and PointNet++ model needs about 32 hours for the same task. The size of our trained model is 22.3 MB and the size of PointNet++ model is 22.1 MB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>The magnitude of the gradient per point in the classification task on ModelNet40 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>The t-SNE clustering visualization of the learned global shape features from the proposed A-CNN model for the shapes in ModelNet10 test split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>The t-SNE clustering visualization of the learned global shape features from the proposed A-CNN model for the shapes in ModelNet40 test split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>PointNet++<ref type="bibr" target="#b27">[28]</ref> (b) PointNet++ (diff) (c) Our (d) Our (diff) (e) Ground Truth More segmentation results on ShapeNet-part dataset. Second and fourth columns show the differences between ground truth and prediction (red points are mislabeled points) of PointNet++ and our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification results on ModelNet10 and ModelNet40 datasets. AAC is accuracy average class, OA is overall accuracy.</figDesc><table><row><cell></cell><cell cols="4">ModelNet10 ModelNet40</cell></row><row><cell></cell><cell cols="4">AAC OA AAC OA</cell></row><row><cell cols="5">different methods with additional input or more points</cell></row><row><cell>AO-CNN [38]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.5</cell></row><row><cell>O-CNN [37]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.6</cell></row><row><cell>PointNet++ [28]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.9</cell></row><row><cell>SO-Net [17]</cell><cell cols="4">95.5 95.7 90.8 93.4</cell></row><row><cell>MVCNN-MultiRes [27]</cell><cell>-</cell><cell>-</cell><cell cols="2">91.4 93.8</cell></row><row><cell>VRN Ensemble [5]</cell><cell>-</cell><cell>97.1</cell><cell>-</cell><cell>95.5</cell></row><row><cell cols="4">point cloud based methods with 1024 points</cell><cell></cell></row><row><cell>PointNet [26]</cell><cell>-</cell><cell>-</cell><cell cols="2">86.2 89.2</cell></row><row><cell>Kd-Net (depth 15) [13]</cell><cell cols="4">93.5 94.0 88.5 91.8</cell></row><row><cell>Pointwise CNN [9]</cell><cell>-</cell><cell>-</cell><cell cols="2">81.4 86.1</cell></row><row><cell>KCNet [32]</cell><cell>-</cell><cell>94.4</cell><cell>-</cell><cell>91.0</cell></row><row><cell>PointGrid [16]</cell><cell>-</cell><cell>-</cell><cell cols="2">88.9 92.0</cell></row><row><cell>PCNN [3]</cell><cell>-</cell><cell>94.9</cell><cell>-</cell><cell>92.3</cell></row><row><cell>PointCNN [18]</cell><cell>-</cell><cell>-</cell><cell cols="2">88.1 92.2</cell></row><row><cell>Point2Sequence [19]</cell><cell cols="4">95.1 95.3 90.4 92.6</cell></row><row><cell>A-CNN (our)</cell><cell cols="4">95.3 95.5 90.3 92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table (</head><label>(</label><figDesc></figDesc><table><row><cell>c) Skateboard</cell><cell>(d) Bag</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Segmentation results on ShapeNet-part, S3DIS, and Scan-Net. "mean" is mean IoU (%), OA is overall accuracy.</figDesc><table><row><cell></cell><cell>ShapeNet-part</cell><cell></cell><cell cols="2">S3DIS ScanNet</cell></row><row><cell></cell><cell cols="2">without normals with normals mean mean</cell><cell>OA</cell><cell>OA</cell></row><row><cell>PointNet [26]</cell><cell>83.7</cell><cell>-</cell><cell>78.5</cell><cell>73.9</cell></row><row><cell>PointNet++ [28]</cell><cell>-</cell><cell>85.1</cell><cell>-</cell><cell>84.5</cell></row><row><cell>SyncSpecCNN [44]</cell><cell>-</cell><cell>84.7</cell><cell>-</cell><cell>-</cell></row><row><cell>O-CNN [37]</cell><cell>-</cell><cell>85.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Kd-Net [13]</cell><cell>82.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KCNet [32]</cell><cell>84.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SO-Net [17]</cell><cell>-</cell><cell>84.9</cell><cell>-</cell><cell>-</cell></row><row><cell>SGPN [39]</cell><cell>-</cell><cell>85.8</cell><cell>-</cell><cell>-</cell></row><row><cell>MS+CU (2) [8]</cell><cell>-</cell><cell>-</cell><cell>79.2</cell><cell>-</cell></row><row><cell>G+RCU [8]</cell><cell>-</cell><cell>-</cell><cell>81.1</cell><cell>-</cell></row><row><cell>RSNet [11]</cell><cell>-</cell><cell>84.9</cell><cell>-</cell><cell>-</cell></row><row><cell>3P-RNN [42]</cell><cell>-</cell><cell>-</cell><cell>86.9</cell><cell>-</cell></row><row><cell>SPGraph [15]</cell><cell>-</cell><cell>-</cell><cell>85.5</cell><cell>-</cell></row><row><cell>TangentConv [35]</cell><cell>-</cell><cell>-</cell><cell>*</cell><cell>80.9</cell></row><row><cell>PCNN [3]</cell><cell>85.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Point2Sequence [19]</cell><cell>-</cell><cell>85.2</cell><cell>-</cell><cell>-</cell></row><row><cell>PointGrid [16]</cell><cell>86.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointCNN [18]</cell><cell>86.1</cell><cell>-</cell><cell>88.1</cell><cell>85.1</cell></row><row><cell>A-CNN (our)</cell><cell>85.9</cell><cell>86.1</cell><cell>87.3</cell><cell>85.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments on ModelNet40 dataset. AAC is accuracy average class, OA is overall accuracy.</figDesc><table><row><cell>AAC OA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experiments on redundancy on ModelNet40 dataset. AAC is accuracy average class, OA is overall accuracy.</figDesc><table><row><cell></cell><cell cols="2">AAC OA</cell></row><row><cell>PointNet++ (multi-scale / with overlap)</cell><cell>86.5</cell><cell>90.2</cell></row><row><cell cols="2">PointNet++ (multi-ring / without overlap) 87.3</cell><cell>90.6</cell></row><row><cell>A-CNN (with all components)</cell><cell>90.3</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Network configurations. Both of the models represent encoder part. A-CNN-3L model consists of three layers. A-CNN-4L model consists of four layers. For each layer, C is the number of centroids, rings is the inner and outer radiuses of a ring: [R inner , Router], k is number of neighbors, F is feature map size. For example, our A-CNN-4L model at the first layer L 1 has 512 centroids; two regular rings where first ring constrained by radiuses of 0.0 and 0.1 and the second ring has radiuses of 0.1 and 0.2; k-NN search returns 16 points in the first ring, and 48 points in the second ring; the feature map size in the first ring is equal to<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32,</ref> 64]  and in the second ring is [64, 64, 128]. Convolutional kernel size across different rings and layers is the same and equal to 1 × 3. Also, we have to double the number of centroids in each layer in model A-CNN-4L on ScanNet as the number of points in each block is twice more than that in S3DIS. The magnitude of the gradient per point in the classification task on ModelNet10 dataset. dresser, night stand, and table classes are closer and even intersect with each other, because the objects from these classes look similar. The perplexity parameters for Mod-elNet10 and ModelNet40 datasets are set as 15 and 50, respectively, to reduce spare space between clusters.</figDesc><table><row><cell></cell><cell></cell><cell>L1</cell><cell>L2</cell><cell>L3</cell><cell>L4</cell></row><row><cell></cell><cell>C</cell><cell>512</cell><cell>128</cell><cell>1</cell><cell>-</cell></row><row><cell>A-CNN-3L</cell><cell cols="2">rings [[0.0, 0.1], [0.1, 0.2]]</cell><cell>[[0.1, 0.2], [0.3, 0.4]]</cell><cell>-</cell><cell>-</cell></row><row><cell>(classification)</cell><cell>k</cell><cell>[16, 48]</cell><cell>[16, 48]</cell><cell>128</cell><cell>-</cell></row><row><cell></cell><cell>F</cell><cell cols="3">[[32,32,64], [64,64,128]] [[64,64,128], [128,128,256]] [256,512,1024]</cell><cell>-</cell></row><row><cell></cell><cell>C</cell><cell>512</cell><cell>128</cell><cell>32</cell><cell>1</cell></row><row><cell>A-CNN-4L</cell><cell cols="2">rings [[0.0, 0.1], [0.1, 0.2]]</cell><cell>[[0.1, 0.2], [0.3, 0.4]]</cell><cell>[[0.2, 0.4], [0.6, 0.8]]</cell><cell>-</cell></row><row><cell>(segmentation)</cell><cell>k</cell><cell>[16, 48]</cell><cell>[16, 48]</cell><cell>[16, 48]</cell><cell>32</cell></row><row><cell></cell><cell>F</cell><cell cols="4">[[32,32,64], [64,64,128]] [[64,64,128], [128,128,256]] [[128,128,256], [256,256,512]] [512,768,1024]</cell></row><row><cell>Note: Low</cell><cell></cell><cell></cell><cell>High</cell><cell></cell><cell></cell></row><row><cell>Toilet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Monitor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sofa</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 8:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Segmentation results on ShapeNet-part dataset (input is XYZ only). Per-category and mean IoUs (%) are reported. 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6 Kd-Net [13] 82.3 80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3 KCNet [32] 84.7 82.8 81.5 86.4 77.6 90.3 76.8 91.0 87.2 84.5 95.5 69.2 94.4 81.6 60.1 75.2 81.3 86.5 86.0 80.8 90.6 79.7 92.3 88.4 85.3 96.1 77.2 95.2 84.2 64.2 80.0 83.0 A-CNN (our) 85.9 83.9 86.7 83.5 79.5 91.3 77.0 91.5 86.0 85.0 95.5 72.6 94.9 83.8 57.8 76.6 83.0</figDesc><table><row><cell></cell><cell cols="3">mean areo bag cap</cell><cell>car</cell><cell>chair ear</cell><cell>guitar knife lamp laptop motor mug pistol rocket skate</cell><cell>table</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>phone</cell><cell>board</cell></row><row><cell># shapes</cell><cell></cell><cell>2690 76</cell><cell>55</cell><cell cols="2">898 3758 69</cell><cell>787 392 1547 451 202 184 283 66</cell><cell>152 5271</cell></row><row><cell cols="7">PointNet [26] 83.4 PCNN [3] 83.7 85.1 82.4 80.1 85.5 79.5 90.8 73.2 91.3 86.0 85.0 95.7 73.2 94.8 83.3 51.0 75.0 81.8</cell></row><row><cell>PointGrid [16]</cell><cell>86.4</cell><cell cols="5">85.7 82.5 81.8 77.9 92.1 82.4 92.7 85.8 84.2 95.3 65.2 93.4 81.7 56.9 73.5 84.6</cell></row><row><cell>PointCNN [18]</cell><cell>86.1</cell><cell>84.1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Segmentation results on ShapeNet-part dataset (input is XYZ + normals). Per-category and mean IoUs (%) are reported. 84.7 77.0 91.1 85.1 91.9 87.4 83.3 95.4 56.9 96.2 81.6 53.5 74.1 84.4 Point2Sequence [19] 85.2 82.6 81.8 87.5 77.3 90.8 77.1 91.1 86.9 83.9 95.7 70.8 94.6 79.3 58.1 75.2 82.8 A-CNN (our) 86.1 84.2 84.0 88.0 79.6 91.3 75.2 91.6 87.1 85.5 95.4 75.3 94.9 82.5 67.8 77.5 83.3</figDesc><table><row><cell></cell><cell cols="3">mean areo bag cap</cell><cell>car</cell><cell>chair ear</cell><cell>guitar knife lamp laptop motor mug pistol rocket skate</cell><cell>table</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>phone</cell><cell>board</cell></row><row><cell># shapes</cell><cell></cell><cell>2690 76</cell><cell>55</cell><cell cols="2">898 3758 69</cell><cell>787 392 1547 451 202 184 283 66</cell><cell>152 5271</cell></row><row><cell>PointNet++ [28]</cell><cell>85.1</cell><cell cols="5">82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6</cell></row><row><cell>SyncSpecCNN [44]</cell><cell>84.7</cell><cell cols="5">81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 66.7 92.7 81.6 60.6 82.9 82.1</cell></row><row><cell>SO-Net [17]</cell><cell>84.9</cell><cell cols="5">82.8 77.8 88.0 77.3 90.6 73.5 90.7 83.9 82.8 94.8 69.1 94.2 80.9 53.1 72.9 83.0</cell></row><row><cell>SGPN [39]</cell><cell>85.8</cell><cell cols="5">80.4 78.6 78.8 71.5 88.6 78.0 90.9 83.0 78.8 95.8 77.8 93.8 87.4 60.1 92.3 89.4</cell></row><row><cell>RSNet [11]</cell><cell>84.9</cell><cell cols="5">82.7 86.4 84.1 78.2 90.4 69.3 91.4 87.0 83.5 95.4 66.0 92.6 81.8 56.1 75.8 82.2</cell></row><row><cell>O-CNN (+ CRF) [37]</cell><cell>85.9</cell><cell>85.5 87.1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table Toilet</head><label>Toilet</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Sink</cell><cell>Sofa</cell><cell>Bookshelf</cell><cell>Desk</cell><cell>Bed</cell><cell>Bathtub</cell><cell>Curtain</cell></row><row><cell>Counter</cell><cell>Door</cell><cell>Window</cell><cell cols="5">Shower Curtain Refrigirator Picture Cabinet Other Furniture</cell><cell>Unannotated</cell></row><row><cell cols="2">(a) Input</cell><cell></cell><cell>(b) PointNet++ [28]</cell><cell></cell><cell>(c) Our</cell><cell></cell><cell>(d) Ground Truth</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We would like to thank the reviewers for their valuable comments. This work was partially supported by the NSF IIS-1816511, CNS-1647200, OAC-1657364, OAC-1845962, Wayne State University Subaward 4207299A of CNS-1821962, NIH 1R56AG060822-01A1, and ZJNSF LZ16F020002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shabayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cherenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01462</idno>
		<title level="m">Deep learning advances on different 3D data representations: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno>71:1-71:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local deep feature learning framework for 3D shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring spatial context for 3D semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="716" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning local shape descriptors from part correspondences with multiview convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3D segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep Kdnetworks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Surface networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2540" to="2548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PointGrid: A deep network for 3D shape understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9204" to="9214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SO-Net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Point2Sequence: Learning the shape representation of 3D point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-level semantic feature for 3D shape based on deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fast marching farthest point sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moenning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Dodgson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rusu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-10" />
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science department, Technische Universitaet Muenchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D is here: Point cloud library (PCL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The European Conference on Computer Vision</title>
		<meeting>The European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">O-CNN: Octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive O-CNN: A patch-based deep representation of 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Directionally convolutional networks for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2698" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The European Conference on Computer Vision</title>
		<meeting>The European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6584" to="6592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PU-Net: Point cloud upsampling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">acc&quot; is overall accuracy and &quot;mean&quot; is average IoU over 13 classes. acc mean ceiling floor wall beam column window door table chair sofa bookcase board clutter detail. For ScanNet dataset, we pick six challenging scenes and visualize the results of our A-CNN model, PointNet++ [28], and ground truth side by side. The visualization results are provided in Fig. 14. Our approach outperforms Point-Net++ [28] and other baseline methods</title>
	</analytic>
	<monogr>
		<title level="m">Table 8: Segmentation results on S3DIS dataset</title>
		<imprint/>
	</monogr>
	<note>such as Point-Net [26], TangentConv [35], and PointCNN [18] according to Tab. 2 in the main paper</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
