<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Depiction of the dropout technique following our Bayesian interpretation (right) compared to the standard technique in the field (left). Each square represents an RNN unit, with horizontal arrows representing time dependence (recurrent connections). Vertical arrows represent the input and output to each RNN unit. Coloured connections represent dropped-out inputs, with different colours corresponding to different dropout masks. Dashed lines correspond to standard connections with no dropout. Current techniques (naive dropout, left) use different masks at different time steps, with no dropout on the recurrent layers. The proposed technique (Variational RNN, right) uses the same dropout mask at each time step, including the recurrent layers.</p><p>suitably defined likelihood functions. We then perform approximate variational inference in these probabilistic Bayesian models (which we will refer to as Variational RNNs). Approximating the posterior distribution over the weights with a mixture of Gaussians (with one component fixed at zero and small variances) will lead to a tractable optimisation objective. Optimising this objective is identical to performing a new variant of dropout in the respective RNNs.</p><p>In the new dropout variant, we repeat the same dropout mask at each time step for both inputs, outputs, and recurrent layers (drop the same network units at each time step). This is in contrast to the existing ad hoc techniques where different dropout masks are sampled at each time step for the inputs and outputs alone (no dropout is used with the recurrent connections since the use of different masks with these connections leads to deteriorated performance). Our method and its relation to existing techniques is depicted in figure 1. When used with discrete inputs (i.e. words) we place a distribution over the word embeddings as well. Dropout in the word-based model corresponds then to randomly dropping word types in the sentence, and might be interpreted as forcing the model not to rely on single words for its task.</p><p>We next survey related literature and background material, and then formalise our approximate inference for the Variational RNN, resulting in the dropout variant proposed above. Experimental results are presented thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Research</head><p>In the past few years a considerable body of work has been collected demonstrating the negative effects of a naive application of dropout in RNNs' recurrent connections. Pachitariu and Sahani <ref type="bibr" target="#b6">[7]</ref>, working with language models, reason that noise added in the recurrent connections of an RNN leads to model instabilities. Instead, they add noise to the decoding part of the model alone. Bayer et al. <ref type="bibr" target="#b7">[8]</ref> apply a deterministic approximation of dropout (fast dropout) in RNNs. They reason that with dropout, the RNN's dynamics change dramatically, and that dropout should be applied to the "non-dynamic" parts of the model -connections feeding from the hidden layer to the output layer. Pham et al. <ref type="bibr" target="#b8">[9]</ref> assess dropout with handwriting recognition tasks. They conclude that dropout in recurrent layers disrupts the RNN's ability to model sequences, and that dropout should be applied to feed-forward connections and not to recurrent connections. The work by Zaremba, Sutskever, and Vinyals <ref type="bibr" target="#b3">[4]</ref> was developed in parallel to Pham et al. <ref type="bibr" target="#b8">[9]</ref>. Zaremba et al. <ref type="bibr" target="#b3">[4]</ref> assess the performance of dropout in RNNs on a wide series of tasks. They show that applying dropout to the non-recurrent connections alone results in improved performance, and provide (as yet unbeaten) state-of-the-art results in language modelling on the Penn Treebank. They reason that without dropout only small models were used in the past in order to avoid overfitting, whereas with the application of dropout larger models can be used, leading to improved results. This work is considered a reference implementation by many (and we compare to this as a baseline below). Bluche et al. <ref type="bibr" target="#b9">[10]</ref> extend on the previous body of work and perform exploratory analysis of the performance of dropout before, inside, and after the RNN's unit. They provide mixed results, not showing significant improvement on existing techniques. More recently, and done in parallel to this work, Moon et al. <ref type="bibr" target="#b19">[20]</ref> suggested a new variant of dropout in RNNs in the speech recognition community. They randomly drop elements in the LSTM's internal cell c t and use the same mask at every time step. This is the closest to our proposed approach (although fundamentally different to the approach we suggest, explained in §4.1), and we compare to this variant below as well.</p><p>Existing approaches are based on an empirical experimentation with different flavours of dropout, following a process of trial-and-error. These approaches have led many to believe that dropout cannot be extended to a large number of parameters within the recurrent layers, leaving them with no regularisation. In contrast to these conclusions, we show that it is possible to derive a variational inference based variant of dropout which successfully regularises such parameters, by grounding our approach in recent theoretical research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>We review necessary background in Bayesian neural networks and approximate variational inference. Building on these ideas, in the next section we propose approximate inference in the probabilistic RNN which will lead to a new variant of dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian Neural Networks</head><p>Given training inputs X = {x 1 , . . . , x N } and their corresponding outputs Y = {y 1 , . . . , y N }, in Bayesian (parametric) regression we would like to infer parameters ω of a function y = f ω (x) that are likely to have generated our outputs. What parameters are likely to have generated our data? Following the Bayesian approach we would put some prior distribution over the space of parameters, p(ω). This distribution represents our prior belief as to which parameters are likely to have generated our data. We further need to define a likelihood distribution p(y|x, ω). For classification tasks we may assume a softmax likelihood,</p><formula xml:id="formula_0">p y = d|x, ω = Categorical exp(f ω d (x))/ d exp(f ω d (x))</formula><p>or a Gaussian likelihood for regression. Given a dataset X, Y, we then look for the posterior distribution over the space of parameters: p(ω|X, Y). This distribution captures how likely various function parameters are given our observed data. With it we can predict an output for a new input point x * by integrating</p><formula xml:id="formula_1">p(y * |x * , X, Y) = p(y * |x * , ω)p(ω|X, Y)dω.<label>(1)</label></formula><p>One way to define a distribution over a parametric set of functions is to place a prior distribution over a neural network's weights, resulting in a Bayesian NN <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Given weight matrices W i and bias vectors b i for layer i, we often place standard matrix Gaussian prior distributions over the weight matrices, p(W i ) = N (0, I) and often assume a point estimate for the bias vectors for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approximate Variational Inference in Bayesian Neural Networks</head><p>We are interested in finding the distribution of weight matrices (parametrising our functions) that have generated our data. This is the posterior over the weights given our observables X, Y: p(ω|X, Y). This posterior is not tractable in general, and we may use variational inference to approximate it (as was done in <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b11">12]</ref>). We need to define an approximating variational distribution q(ω), and then minimise the KL divergence between the approximating distribution and the full posterior:</p><formula xml:id="formula_2">KL q(ω)||p(ω|X, Y) ∝ − q(ω) log p(Y|X, ω)dω + KL(q(ω)||p(ω)) = − N i=1 q(ω) log p(y i |f ω (x i ))dω + KL(q(ω)||p(ω)).<label>(2)</label></formula><p>We next extend this approximate variational inference to probabilistic RNNs, and use a q(ω) distribution that will give rise to a new variant of dropout in RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Variational Inference in Recurrent Neural Networks</head><p>In this section we will concentrate on simple RNN models for brevity of notation. Derivations for LSTM and GRU follow similarly. Given input sequence x = [x 1 , ..., x T ] of length T , a simple RNN is formed by a repeated application of a function f h . This generates a hidden state h t for time step t:</p><formula xml:id="formula_3">h t = f h (x t , h t−1 ) = σ(x t W h + h t−1 U h + b h )</formula><p>for some non-linearity σ. The model output can be defined, for example, as f y (h T ) = h T W y + b y . We view this RNN as a probabilistic model by regarding ω = {W h , U h , b h , W y , b y } as random variables (following normal prior distributions). To make the dependence on ω clear, we write f ω y for f y and similarly for f ω h . We define our probabilistic model's likelihood as above (section 3.1). The posterior over random variables ω is rather complex, and we use variational inference with approximating distribution q(ω) to approximate it.</p><p>Evaluating each sum term in eq. (2) above with our RNN model we get</p><formula xml:id="formula_4">q(ω) log p(y|f ω y (h T ))dω = q(ω) log p y f ω y f ω h (x T , h T −1 ) dω = q(ω) log p y f ω y f ω h (x T , f ω h (...f ω h (x 1 , h 0 )...)) dω</formula><p>with h 0 = 0. We approximate this with Monte Carlo (MC) integration with a single sample:</p><formula xml:id="formula_5">≈ log p y f ω y f ω h (x T , f ω h (...f ω h (x 1 , h 0 )...)) , ω ∼ q(ω)</formula><p>resulting in an unbiased estimator to each sum term.</p><p>This estimator is plugged into equation <ref type="formula" target="#formula_2">(2)</ref> to obtain our minimisation objective</p><formula xml:id="formula_6">L ≈ − N i=1 log p y i f ωi y f ωi h (x i,T , f ωi h (...f ωi h (x i,1 , h 0 )...)) + KL(q(ω)||p(ω)).<label>(3)</label></formula><p>Note that for each sequence x i we sample a new realisation</p><formula xml:id="formula_7">ω i = { W i h , U i h , b i h , W i y , b i y }, and that each symbol in the sequence x i = [x i,1 , ..., x i,T ] is passed through the function f ωi h with the same weight realisations W i h , U i h , b i h used at every time step t ≤ T .</formula><p>Following <ref type="bibr" target="#b16">[17]</ref> we define our approximating distribution to factorise over the weight matrices and their rows in ω. For every weight matrix row w k the approximating distribution is:</p><formula xml:id="formula_8">q(w k ) = pN (w k ; 0, σ 2 I) + (1 − p)N (w k ; m k , σ 2 I)</formula><p>with m k variational parameter (row vector), p given in advance (the dropout probability), and small σ 2 . We optimise over m k the variational parameters of the random weight matrices; these correspond to the RNN's weight matrices in the standard view 1 . The KL in eq. (3) can be approximated as L 2 regularisation over the variational parameters m k <ref type="bibr" target="#b16">[17]</ref>.</p><p>Evaluating the model output f ω y (·) with sample ω ∼ q(ω) corresponds to randomly zeroing (masking) rows in each weight matrix W during the forward pass -i.e. performing dropout. Our objective L is identical to that of the standard RNN. In our RNN setting with a sequence input, each weight matrix row is randomly masked once, and importantly the same mask is used through all time steps. <ref type="bibr" target="#b1">2</ref> Predictions can be approximated by either propagating the mean of each layer to the next (referred to as the standard dropout approximation), or by approximating the posterior in eq. (1) with q(ω),</p><formula xml:id="formula_9">p(y * |x * , X, Y) ≈ p(y * |x * , ω)q(ω)dω ≈ 1 K K k=1 p(y * |x * , ω k )<label>(4)</label></formula><p>with ω k ∼ q(ω), i.e. by performing dropout at test time and averaging results (MC dropout).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation and Relation to Dropout in RNNs</head><p>Implementing our approximate inference is identical to implementing dropout in RNNs with the same network units dropped at each time step, randomly dropping inputs, outputs, and recurrent connections. This is in contrast to existing techniques, where different network units would be dropped at different time steps, and no dropout would be applied to the recurrent connections ( <ref type="figure">fig. 1</ref>).</p><p>Certain RNN models such as LSTMs and GRUs use different gates within the RNN units. For example, an LSTM is defined using four gates: "input", "forget", "output", and "input modulation",</p><formula xml:id="formula_10">i = sigm h t−1 U i + x t W i f = sigm h t−1 U f + x t W f o = sigm h t−1 U o + x t W o g = tanh h t−1 U g + x t W g c t = f • c t−1 + i • g h t = o • tanh(c t )<label>(5)</label></formula><p>with</p><formula xml:id="formula_11">ω = {W i , U i , W f , U f , W o , U o , W g , U g } weight matrices and • the element-wise product.</formula><p>Here an internal state c t (also referred to as cell) is updated additively.</p><p>Alternatively, the model could be re-parametrised as in <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_12">   i f o g    =    sigm sigm sigm tanh    x t h t−1 · W<label>(6)</label></formula><p>with ω = {W}, W a matrix of dimensions 2K by 4K (K being the dimensionality of x t ). We name this parametrisation a tied-weights LSTM (compared to the untied-weights LSTM in eq. (5)).</p><p>Even though these two parametrisations result in the same deterministic model, they lead to different approximating distributions q(ω). With the first parametrisation one could use different dropout masks for different gates (even when the same input x t is used). This is because the approximating distribution is placed over the matrices rather than the inputs: we might drop certain rows in one weight matrix W applied to x t and different rows in another matrix W applied to x t . With the second parametrisations we would place a distribution over the single matrix W. This leads to a faster forward-pass, but with slightly diminished results as we will see in the experiments section.</p><p>In more concrete terms, we may write our dropout variant with the second parametrisation (eq. (6)) as</p><formula xml:id="formula_13">   i f o g    =    sigm sigm sigm tanh    x t • z x h t−1 • z h · W<label>(7)</label></formula><p>with z x , z h random masks repeated at all time steps (and similarly for the parametrisation in eq. <ref type="formula" target="#formula_10">(5)</ref>).</p><p>In comparison, Zaremba et al. <ref type="bibr" target="#b3">[4]</ref>'s dropout variant replaces z x in eq. <ref type="formula" target="#formula_13">(7)</ref> with the time-dependent z t</p><p>x which is sampled anew every time step (whereas z h is removed and the recurrent connection h t−1 is not dropped):</p><formula xml:id="formula_14">   i f o g    =    sigm sigm sigm tanh    x t • z t x h t−1 · W .<label>(8)</label></formula><p>On the other hand, Moon et al. <ref type="bibr" target="#b19">[20]</ref>'s dropout variant changes eq. (5) by adapting the internal cell</p><formula xml:id="formula_15">c t = c t • z c<label>(9)</label></formula><p>with the same mask z c used at all time steps. Note that unlike <ref type="bibr" target="#b19">[20]</ref>, by viewing dropout as an operation over the weights our technique trivially extends to RNNs and GRUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Embeddings Dropout</head><p>In datasets with continuous inputs we often apply dropout to the input layer -i.e. to the input vector itself. This is equivalent to placing a distribution over the weight matrix which follows the input and approximately integrating over it (the matrix is optimised, therefore prone to overfitting otherwise).</p><p>But for models with discrete inputs such as words (where every word is mapped to a continuous vector -a word embedding) this is seldom done. With word embeddings the input can be seen as either the word embedding itself, or, more conveniently, as a "one-hot" encoding (a vector of zeros with 1 at a single position). The product of the one-hot encoded vector with an embedding matrix W E ∈ R V ×D (where D is the embedding dimensionality and V is the number of words in the vocabulary) then gives a word embedding. Curiously, this parameter layer is the largest layer in most language applications, yet it is often not regularised. Since the embedding matrix is optimised it can lead to overfitting, and it is therefore desirable to apply dropout to the one-hot encoded vectors. This in effect is identical to dropping words at random throughout the input sentence, and can also be interpreted as encouraging the model to not "depend" on single words for its output.</p><p>Note that as before, we randomly set rows of the matrix W E ∈ R V ×D to zero. Since we repeat the same mask at each time step, we drop the same words throughout the sequence -i.e. we drop word types at random rather than word tokens (as an example, the sentence "the dog and the cat" might become "-dog and -cat" or "the -and the cat", but never "-dog and the cat"). A possible inefficiency implementing this is the requirement to sample V Bernoulli random variables, where V might be large. This can be solved by the observation that for sequences of length T , at most T embeddings could be dropped (other dropped embeddings have no effect on the model output). For T V it is therefore more efficient to first map the words to the word embeddings, and only then to zero-out word embeddings based on their word type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>We start by implementing our proposed dropout variant into the Torch implementation of Zaremba et al. <ref type="bibr" target="#b3">[4]</ref>, that has become a reference implementation for many in the field. Zaremba et al. <ref type="bibr" target="#b3">[4]</ref> have set a benchmark on the Penn Treebank that to the best of our knowledge hasn't been beaten for the past 2 years. We improve on <ref type="bibr" target="#b3">[4]</ref>'s results, and show that our dropout variant improves model performance compared to early-stopping and compared to using under-specified models. We continue to evaluate our proposed dropout variant with both LSTM and GRU models on a sentiment analysis task where labelled data is scarce. We finish by giving an in-depth analysis of the properties of the proposed method, with code and many experiments deferred to the appendix due to space constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language Modelling</head><p>We replicate the language modelling experiment of Zaremba, Sutskever, and Vinyals <ref type="bibr" target="#b3">[4]</ref>. The experiment uses the Penn Treebank, a standard benchmark in the field. This dataset is considered a small one in the language processing community, with 887, 521 tokens (words) in total, making overfitting a considerable concern. Throughout the experiments we refer to LSTMs with the dropout technique proposed following our Bayesian interpretation as Variational LSTMs, and refer to existing dropout techniques as naive dropout LSTMs (eq. (8), different masks at different steps, applied to the input and output of the LSTM alone). We refer to LSTMs with no dropout as standard LSTMs.</p><p>We implemented a Variational LSTM for both the medium model of <ref type="bibr" target="#b3">[4]</ref> (2 layers with 650 units in each layer) as well as their large model (2 layers with 1500 units in each layer). The only changes we've made to <ref type="bibr" target="#b3">[4]</ref>'s setting are 1) using our proposed dropout variant instead of naive dropout, and 2) tuning weight decay (which was chosen to be zero in <ref type="bibr" target="#b3">[4]</ref>). All other hyper-parameters are kept identical to <ref type="bibr" target="#b3">[4]</ref>: learning rate decay was not tuned for our setting and is used following <ref type="bibr" target="#b3">[4]</ref>. Dropout parameters were optimised with grid search (tying the dropout probability over the embeddings together with the one over the recurrent layers, and tying the dropout probability for the inputs and outputs together as well). These are chosen to minimise validation perplexity <ref type="bibr" target="#b2">3</ref> . We further compared to Moon et al. <ref type="bibr" target="#b19">[20]</ref> who only drop elements in the LSTM internal state using the same mask at all time steps (in addition to performing dropout on the inputs and outputs, eq. <ref type="formula" target="#formula_15">(9)</ref>). We implemented their dropout variant with each model size, and repeated the procedure above to find optimal dropout probabilities (0.3 with the medium model, and 0.5 with the large model). We had to use early stopping for the large model with <ref type="bibr" target="#b19">[20]</ref>'s variant as the model starts overfitting after 16 epochs. Moon et al. <ref type="bibr" target="#b19">[20]</ref> proposed their dropout variant within the speech recognition community, where they did not have to consider embeddings overfitting (which, as we will see below, affect the recurrent layers considerably). We therefore performed an additional experiment using <ref type="bibr" target="#b19">[20]</ref>'s variant together with our embedding dropout (referred to as Moon et al. <ref type="bibr" target="#b19">[20]</ref>+emb dropout).</p><p>Our results are given in table 1. For the variational LSTM we give results using both the tied weights model (eq. (6)- <ref type="bibr" target="#b6">(7)</ref>, Variational (tied weights)), and without weight tying (eq. (5), Variational (untied weights)). For each model we report performance using both the standard dropout approximation (averaging the weights at test time -propagating the mean of each approximating distribution as input to the next layer), and using MC dropout (obtained by performing dropout at test time 1000 times, and averaging the model outputs following eq. (4), denoted MC). For each model we report average perplexity and standard deviation (each experiment was repeated 3 times with different random seeds and the results were averaged). Model training time is given in words per second (WPS).</p><p>It is interesting that using the dropout approximation, weight tying results in lower validation error and test error than the untied weights model. But with MC dropout the untied weights model performs much better. Validation perplexity for the large model is improved from <ref type="bibr" target="#b3">[4]</ref>'s 82.2 down to 77.3 (with weight tying), or 77.9 without weight tying. Test perplexity is reduced from 78.4 down to 73.4 (with MC dropout and untied weights). To the best of our knowledge, these are currently the best single model perplexities on the Penn Treebank.</p><p>It seems that Moon et al. <ref type="bibr" target="#b19">[20]</ref> underperform even compared to <ref type="bibr" target="#b3">[4]</ref>. With no embedding dropout the large model overfits and early stopping is required (with no early stopping the model's validation perplexity goes up to 131 within 30 epochs). Adding our embedding dropout, the model performs much better, but still underperforms compared to applying dropout on the inputs and outputs alone.</p><p>Comparing our results to the non-regularised LSTM (evaluated with early stopping, giving similar performance as the early stopping experiment in <ref type="bibr" target="#b3">[4]</ref>) we see that for either model size an improvement can be obtained by using our dropout variant. Comparing the medium sized Variational model to the large one we see that a significant reduction in perplexity can be achieved by using a larger model. This cannot be done with the non-regularised LSTM, where a larger model leads to worse results. This shows that reducing the complexity of the model, a possible approach to avoid overfitting, actually leads to a worse fit when using dropout.</p><p>We also see that the tied weights model achieves very close performance to that of the untied weights one when using the dropout approximation. Assessing model run time though (on a Titan X GPU), we see that tying the weights results in a more time-efficient implementation. This is because the single matrix product is implemented as a single GPU kernel, instead of the four smaller matrix products used in the untied weights model (where four GPU kernels are called sequentially). Note though that a low level implementation should give similar run times.</p><p>We further experimented with a model averaging experiment following <ref type="bibr" target="#b3">[4]</ref>'s setting, where several large models are trained independently with their outputs averaged. We used Variational LSTMs with MC dropout following the setup above. Using 10 Variational LSTMs we improve <ref type="bibr" target="#b3">[4]</ref>'s test set perplexity from 69.5 to 68.7 -obtaining identical perplexity to <ref type="bibr" target="#b3">[4]</ref>'s experiment with 38 models.</p><p>Lastly, we report validation perplexity with reduced learning rate decay (with the medium model).</p><p>Learning rate decay is often used for regularisation by setting the optimiser to make smaller steps when the model starts overfitting (as done in <ref type="bibr" target="#b3">[4]</ref>). By removing it we can assess the regularisation effects of dropout alone. As can be seen in <ref type="figure">fig. 2</ref>, even with early stopping, Variational LSTM achieves lower perplexity than naive dropout LSTM and standard LSTM. Note though that a significantly lower perplexity for all models can be achieved with learning rate decay scheduling as seen in table 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sentiment Analysis</head><p>We next evaluate our dropout variant with both LSTM and GRU models on a sentiment analysis task, where labelled data is scarce. We use MC dropout (which we compare to the dropout approximation further in appendix B), and untied weights model parametrisations.</p><p>We use the raw Cornell film reviews corpus collected by Pang and Lee <ref type="bibr" target="#b26">[27]</ref>. The dataset is composed of 5000 film reviews. We extract consecutive segments of T words from each review for T = 200, and use the corresponding film score as the observed output y. The model is built from one embedding layer (of dimensionality 128), one LSTM layer (with 128 network units for each gate; GRU setting is built similarly), and finally a fully connected layer applied to the last output of the LSTM (resulting in a scalar output). We use the Adam optimiser <ref type="bibr" target="#b27">[28]</ref> throughout the experiments, with batch size 128, and MC dropout at test time with 10 samples.</p><p>The main results can be seen in <ref type="figure" target="#fig_3">fig. 3</ref>. We compared Variational LSTM (with our dropout variant applied with each weight layer) to standard techniques in the field. Training error is shown in <ref type="figure" target="#fig_3">fig. 3a</ref> and test error is shown in <ref type="figure" target="#fig_3">fig. 3b</ref>. Optimal dropout probabilities and weight decay were used for each model (see appendix B). It seems that the only model not to overfit is the Variational LSTM, which achieves lowest test error as well. Variational GRU test error is shown in <ref type="figure" target="#fig_4">fig. 14 (</ref>with loss plot given in appendix B). Optimal dropout probabilities and weight decay were used again for each model. Variational GRU avoids overfitting to the data and converges to the lowest test error. Early stopping in this dataset will result in smaller test error though (lowest test error is obtained by the non-regularised GRU model at the second epoch). It is interesting to note that standard techniques exhibit peculiar behaviour where test error repeatedly decreases and increases. This behaviour is not observed with the Variational GRU. Convergence plots of the loss for each model are given in appendix B.</p><p>We next explore the effects of dropping-out different parts of the model. We assessed our Variational LSTM with different combinations of dropout over the embeddings (p E = 0, 0.5) and recurrent layers (p U = 0, 0.5) on the sentiment analysis task. The convergence plots can be seen in <ref type="figure" target="#fig_4">figure 4a</ref>. It seems that without both strong embeddings regularisation and strong regularisation over the recurrent layers the model would overfit rather quickly. The behaviour when p U = 0.5 and p E = 0 is quite interesting: test error decreases and then increases before decreasing again. Also, it seems that when p U = 0 and p E = 0.5 the model becomes very erratic. <ref type="figure">Figure 2</ref>: Medium model validation perplexity for the Penn Treebank language modelling task. Learning rate decay was reduced to assess model overfitting using dropout alone. Even with early stopping, Variational LSTM achieves lower perplexity than naive dropout LSTM and standard LSTM. Lower perplexity for all models can be achieved with learning rate decay scheduling, seen in table 1.   Lastly, we tested the performance of Variational LSTM with different recurrent layer dropout probabilities, fixing the embedding dropout probability at either p E = 0 or p E = 0.5 (figs. 4b-4c). These results are rather intriguing. In this experiment all models have converged, with the loss getting near zero (not shown). Yet it seems that with no embedding dropout, a higher dropout probability within the recurrent layers leads to overfitting! This presumably happens because of the large number of parameters in the embedding layer which is not regularised. Regularising the embedding layer with dropout probability p E = 0.5 we see that a higher recurrent layer dropout probability indeed leads to increased robustness to overfitting, as expected. These results suggest that embedding dropout can be of crucial importance in some tasks.</p><p>In appendix B we assess the importance of weight decay with our dropout variant. Common practice is to remove weight decay with naive dropout. Our results suggest that weight decay plays an important role with our variant (it corresponds to our prior belief of the distribution over the weights).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a new technique for recurrent neural network regularisation. Our RNN dropout variant is theoretically motivated and its effectiveness was empirically demonstrated. In future research we aim to assess model uncertainty in Variational LSTMs <ref type="bibr" target="#b16">[17]</ref>. Together with the developments presented here, this will have important implications for modelling language ambiguity and modelling dynamics in control tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Bayesian versus ensembling interpretation of dropout</head><p>Apart from our Bayesian approximation interpretation, dropout in deep networks can also be seen as following an ensembling interpretation <ref type="bibr" target="#b5">[6]</ref>. This interpretation also leads to MC dropout at test time. But the ensembling interpretation does not determine whether the ensemble should be over the network units or the weights. For example, in an RNN this view will not lead to our dropout variant, unless the ensemble is defined to tie the weights of the network ad hoc. This is in comparison to the Bayesian approximation view where the weight tying is forced by the probabilistic interpretation of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sentiment analysis -further experiments</head><p>Sentiment analysis hyper-parameters were obtained by evaluating each model with dropout probabilities 0.25 and 0.5, and weight decays ranging from 10 −6 to 10 −4 . The optimal setting for Variational LSTM is dropout probabilities 0.25 and weight decay 10 −3 , and for naive dropout LSTM the dropout probabilities are 0.5 (no weight decay is used in reference implementations of naive dropout LSTM <ref type="bibr" target="#b3">[4]</ref>).</p><p>We assess the dropout approximation in Variational LSTMs. The dropout approximation is often used in deep networks as means of approximating the MC estimate. In the approximation we replace each weight matrix M by pM where p is the dropout probability, and perform a deterministic pass through the network (without dropping out units). This can be seen as propagating the mean of the random variables W through the network <ref type="bibr" target="#b16">[17]</ref>. The approximation has been shown to work well for deep networks <ref type="bibr" target="#b5">[6]</ref>, yet it fails with convolution layers <ref type="bibr" target="#b13">[14]</ref>. We assess the approximation empirically with our Variational LSTM model, repeating the first experiment with the approximation used at test time instead of MC dropout. The results can be seen in <ref type="figure" target="#fig_8">fig. 9</ref>. It seems that the approximation gives a good estimate to the test error, similar to the results in figure 4a.</p><p>We further tested the Variational LSTM model with different weight decays, observing the effects of different values for these. Note that weight decay is applied to all layers, including the embedding layer. In <ref type="figure" target="#fig_5">figure 6</ref> we can see that higher weight decay values result in lower test error, with significant differences for different weight decays. This suggests that weight decay still plays an important role even when using dropout (whereas common practice is to remove weight decay with naive dropout). Note also that the weight decay can be optimised (together with the dropout parameters) as part of the variational approximation. This is not done in practice in the deep learning community, where grid-search or Bayesian optimisation are often used for these instead.</p><p>Testing the Variational LSTM with different sequence lengths (with sequences of lengths T = 20, 50, 200, 400) we can see that sequence length has a strong effect on model performance as well ( <ref type="figure" target="#fig_6">fig. 7</ref>). Longer sequences result in much better performance but with the price of longer convergence time. We hypothesised that the diminished performance on shorter sequences is caused by the high dropout probability on the embeddings. But a follow-up experiment with sequence lengths 50 and 200, and different embedding dropout probabilities, shows that lower dropout probabilities result in even worse model performance (figures 8 and 5).</p><p>In <ref type="figure">fig. 10a</ref> we see how different dropout probabilities and weight decays affect GRU model performance. <ref type="figure">Figure 5</ref>: p E = 0, ..., 0.5 with fixed p U = 0.5.     <ref type="figure">Figure 10</ref>: Sentiment analysis error for Variational GRU compared to naive dropout GRU and standard GRU (with no dropout). Test error for the different models (left) and for different Variational GRU configurations (right).</p><p>We compare naive dropout LSTM to Variational LSTM with dropout probability in the recurrent layers set to zero: p U = 0 (referred to as dropout LSTM). Both models apply dropout to the input and outputs of the LSTM alone, with no dropout applied to the embeddings. Naive dropout LSTM uses different masks at different time steps though, tied across the gates, whereas dropout LSTM uses the same mask at different time steps. The test error for both models can be seen in <ref type="figure">fig. 11</ref>. It seems that without dropout over the recurrent layers and embeddings both models overfit, and in fact result in identical performance.</p><p>Next, we assess the dropout approximation in the GRU model. The approximation seems to give similar results to MC dropout in the GRU model ( <ref type="figure">fig. 12</ref>). <ref type="figure">Figure 11</ref>: Naive dropout LSTM uses different dropout masks at each time step, whereas Dropout LSTM uses the same mask at each time step. Both models apply dropout to the inputs and outputs alone, and result in identical performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 12: GRU dropout approximation</head><p>Lastly, we plot the train loss for various models from the main body of the paper. All models have converged, with a stable train loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Code</head><p>An efficient Theano <ref type="bibr" target="#b28">[29]</ref>  , u_i)) f_t = self.inner_activation(xf_t + T.dot(h_tm1 * B_U <ref type="bibr" target="#b0">[1]</ref>, u_f)) c_t = f_t * c_tm1 + i_t * self.activation(xc_t + T.dot(h_tm1 * B_U <ref type="bibr" target="#b1">[2]</ref>, u_c)) o_t = self.inner_activation(xo_t + T.dot(h_tm1 * B_U <ref type="bibr" target="#b2">[3]</ref>, u_o)) h_t = o_t * self.activation(c_t) return h_t, c_t</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) LSTM train error: variational, naive dropout, and standard LSTM.(b) LSTM test error: variational, naive dropout, and standard LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(c) GRU test error: variational, naive dropout, and standard LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Sentiment analysis error for Variational LSTM / GRU compared to naive dropout LSTM / GRU and standard LSTM / GRU (with no dropout).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Test error for Variational LSTM with various settings on the sentiment analysis task. Different dropout probabilities are used with the recurrent layer (p U ) and embedding layer (p E ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Test error for Variational LSTM with different weight decays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Variational LSTM test error for different sequence lengths (T = 20, 50, 200, 400 cut-offs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Test error for various embedding dropout probabilities, with sequence length 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Dropout approximation in Variational LSTM with different dropout probabilities. (a) Various Variational GRU model configurations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Train loss (as a function of batches) for figure 3a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>GRU train loss (as a function of batches) (figure 14) Train loss (as a function of batches) for figure 4a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>implementation of the method above into Keras [30] is as simple as: def get_output(self, train=False): X = self.get_input(train) retain_prob_W = 1. -self.p_W[0] retain_prob_U = 1. -self.p_U[0] B_W = self.srng.binomial((4, X.shape[1], self.input_dim), p=retain_prob_W, dtype=theano.config.floatX) B_U = self.srng.binomial((4, X.shape[1], self.output_dim), p=retain_prob_U, dtype=theano.config.floatX) U_i, self.U_f, self.U_o, self.U_c, B_U], truncate_gradient=self.truncate_gradient)</figDesc><table><row><cell>xi = T.dot(X * B_W[0], self.W_i) + self.b_i</cell></row><row><cell>xf = T.dot(X * B_W[1], self.W_f) + self.b_f</cell></row><row><cell>xc = T.dot(X * B_W[2], self.W_c) + self.b_c</cell></row><row><cell>xo = T.dot(X * B_W[3], self.W_o) + self.b_o</cell></row><row><cell>[outputs, memories], updates = theano.scan(</cell></row><row><cell>self._step,</cell></row><row><cell>sequences=[xi, xf, xo, xc],</cell></row><row><cell>outputs_info=[</cell></row><row><cell>T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),</cell></row><row><cell>T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)</cell></row><row><cell>],</cell></row><row><cell>non_sequences=[self.return outputs[-1]</cell></row><row><cell>def _step(self,</cell></row><row><cell>xi_t, xf_t, xo_t, xc_t,</cell></row><row><cell>h_tm1, c_tm1,</cell></row><row><cell>u_i, u_f, u_o, u_c, B_U):</cell></row><row><cell>i_t = self.inner_activation(xi_t + T.dot(h_tm1 * B_U[0]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Graves et al.<ref type="bibr" target="#b25">[26]</ref> further factorise the approximating distribution over the elements of each row, and use a Gaussian approximating distribution with each element (rather than a mixture); the approximating distribution above seems to give better performance, and has a close relation with dropout<ref type="bibr" target="#b16">[17]</ref>.<ref type="bibr" target="#b1">2</ref> In appendix A we discuss the relation of our dropout interpretation to the ensembling one.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Optimal probabilities are 0.3 and 0.5 respectively for the large model, compared [4]'s 0.6 dropout probability, and 0.2 and 0.35 respectively for the medium model, compared [4]'s 0.5 dropout probability.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc Vv</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Regularization and nonlinearities for neural language models: when are they needed?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pachitariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Korhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nutan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.0701</idno>
		<title level="m">On fast dropout and its applicability to recurrent networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICFHR. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Where to apply dropout in recurrent neural networks for handwriting recognition? In ICDAR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théodore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Miguel Hernandez-Lobato</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02158</idno>
		<title level="m">Bayesian convolutional neural networks with Bernoulli approximate variational inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Anoop Korattikara Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02142</idno>
	</analytic>
	<monogr>
		<title level="m">Representing model uncertainty in deep learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RnnDrop: A Novel Dropout for RNNs in ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoshik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchul</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU Workshop</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A practical Bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble learning in Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="215" to="238" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
	<note>Oral Presentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
