<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huang</surname></persName>
							<email>huangqiang03@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Big Data Department</orgName>
								<orgName type="institution">Baidu Inc. Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Bu</surname></persName>
							<email>bujianhui@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Big Data Department Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xie</surname></persName>
							<email>xieweijian@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Big Data Department Baidu Inc. Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwen</forename><surname>Yang</surname></persName>
							<email>yangshengwen@baidu.com</email>
							<affiliation key="aff3">
								<orgName type="department">Big Data Department Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Wu</surname></persName>
							<email>wuweijia01@baidu.com</email>
							<affiliation key="aff4">
								<orgName type="department">Big Data Department Baidu Inc. Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Liu</surname></persName>
							<email>liuliping@baidu.com</email>
							<affiliation key="aff5">
								<orgName type="department">Big Data Department Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering systems</term>
					<term>sentence matching</term>
					<term>encoding model</term>
					<term>multi-task learning</term>
					<term>semantic retrieval framework</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question Answering (QA) systems are used to provide proper responses to users' questions automatically. Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification (PI) problem. Given a question, the aim of the task is to find the most similar question from a QA knowledge base. In this paper, we propose a Multi-task Sentence Encoding Model (MSEM) for the PI problem, wherein a connected graph is employed to depict the relation between sentences, and a multi-task learning model is applied to address both the sentence matching and sentence intent classification problem. In addition, we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor (ANN) technology, which enables us to find the most similar question from all available candidates very quickly during online serving. The experiments show the superiority of our proposed method as compared with the existing sentence matching models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Question answering systems have been widely studied in both the academic and industrial community and are widely applied to various scenarios. There are full-blown applications like Amazon's Alexa, Apple's Siri, Baidu's DuerOS, Google's Assistant and Microsoft's Cortana. Generally, there are two types of question answering systems: (1) information retrievalbased (IR-based) <ref type="bibr" target="#b0">[1]</ref>, and (2) generation-based <ref type="bibr" target="#b1">[2]</ref>. In this work, we focus on building an IR-based QA system to answer the Frequently Asked Questions (FAQ). The critical part of IRbased QA system is to find the most similar question from a massive QA knowledge base, which could be further reformulated as a Paraphrase Identification (PI) problem, also known as sentence matching. In recent years, neural network models have achieved great success in sentence matching. Depends on whether to use crosssentence features or not, sentence-matching models can be classified roughly into two types: (1) encoding-based, and (2) interaction-based. It is generally accepted that the interactionbased models could get better performance than the encodingbased models on certain datasets because they have abundant interaction features. However, the leaderboards of published large datasets such as SNLI <ref type="bibr" target="#b2">[3]</ref> and MultiNLI <ref type="bibr" target="#b3">[4]</ref> encourage conducting research on the encoding-based models around the semantic representation, because the encoding-based models can learn vector representations of individual sentences, which can be further applied to other natural language processing tasks. Models in practical QA systems have two main disadvantages. Firstly, they consider the semantic sentence matching as a binary classification problem, assuming that samples are independent of one another as default. However, the paraphrase relation between sentences could be transmitted. For example, if question1 and question2 are paraphrases, and question2 and question3 are paraphrases, we can infer that question1 and question3 are also paraphrases. Secondly, because of the hard time delay constraint in the online prediction procedure of a traditional IR-based QA system, as shown in <ref type="figure" target="#fig_0">Fig.  1</ref>, existing models often play the role of a re-rank module that depends on the results from the question analysis and recall module. Thus they could only re-rank a few candidates from term-based index recall modules like Lucene, instead of retrieving the most similar question from all candidates.</p><p>In this paper, we aim to address these two challenges. The main contributions of this work are summarized as follows:</p><p>• We employ a connected graph to depict the paraphrase relation between sentences for the PI task, and propose a multi-task sentence-encoding model, which solves the paraphrase identification task and the sentence intent classification task simultaneously.</p><p>• We propose a semantic retrieval framework that integrates the encoding-based sentence matching model with the approximate nearest neighbor search technology, which allows us to find the most similar question very quickly from all available questions, instead of within only a few candidates, in the QA knowledge base.</p><p>We evaluated our proposed method on various QA datasets and the experimental results show the effectiveness and superiority of our method. First, it proves that we can achieve better performance with multi-task learning. Besides, our method can achieve state-of-the-art performance compared with existing encoding-based models and interaction-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Natural Language Sentence Matching</head><p>Natural language sentence matching (NLSM) has gone through substantial developments in recent years. It plays a central role in a large number of natural language processing tasks. For the paraphrase identification (PI) task, NLSM is utilized to determine whether two sentences are paraphrases or not.</p><p>The developments of deep neural networks and the emergence of large-scale annotated datasets have led great progress on NLSM tasks. Depending on whether the crosssentence features or attention from one sentence to another were used, two types of deep neural network models have been proposed for NLSM. The first type of models is encoding-based, where sentences are encoded into sentence vectors without any cross interaction, then the matching decision is made solely based on the two sentence vectors. Typical representatives of such methods include Stack-augmented Parser-Interpreter Neural Network (SPINN) <ref type="bibr" target="#b4">[5]</ref>, Shortcut-Stacked Sentence Encoders (SSE) <ref type="bibr" target="#b5">[6]</ref>, or Gumbel Tree-LSTM <ref type="bibr" target="#b6">[7]</ref>. The other type of methods, called interaction-based model, make use of cross interaction of small units (such as words) to express word-level or phrase-level alignments for performance improvements. The main representatives are Enhanced Sequential Inference Model (ESIM) <ref type="bibr" target="#b7">[8]</ref>, Bilateral Multi-Perspective Matching Model (BiMPM) <ref type="bibr" target="#b8">[9]</ref>, and Densely Interactive Inference Network (DIIN) model <ref type="bibr" target="#b9">[10]</ref>. Generally, the second type of methods captures more interactive features between the two input sentences, so it can achieve better performance. On the other hand, the encoding-based model is much smaller and easier to train, and the vector representations can be further used for sentence clustering, semantic search, visualization and many other tasks. The advantages of encoding-based models are much more significant to QA systems in the industry, so we focus on the research of encoding-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Approximate Nearest Neighbor</head><p>Approximate nearest neighbor (ANN) search has been a hot topic over decades and plays an important role in machine learning, computer vision and information retrieval etc. For dense real-valued vectors, such as vector representations of images or natural languages, many data structures and algorithms have been proposed to improve the retrieval efficiency of ANN search. There are four types of mainstream methods, including tree structure based <ref type="bibr" target="#b10">[11]</ref>, hashing based <ref type="bibr" target="#b11">[12]</ref>, quantization based <ref type="bibr" target="#b12">[13]</ref>, and graph based <ref type="bibr" target="#b13">[14]</ref>. In the task of image retrieval, ANN index technologies have been used to build efficient image retrieval systems.</p><p>In this paper, inspired by research on image retrieval systems, we propose a semantic retrieval framework for QA systems, which combines ANN search technology and sentence encoding technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Overall Architecture</head><p>Motivated by the fact that the questions in a large QA knowledge base are not independent, we try to utilize the paraphrase relationship among questions to facilitate modeling of question representation. Each sentence is regarded as a vertex and an edge is added between a pair of vertices if they are paraphrases. In this way we can build an undirected graph to represent the paraphrase relationship among sentences, wherein a connected sub-graph can be seen as a sentence cluster with similar intent, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. On this basis, we could train a multi-class classification model for sentence intent classification. Two sentences can be considered as paraphrases if they are classified into the same class by the model. As <ref type="figure" target="#fig_2">Fig. 3</ref> shows, we employ a multi-task learning method to simultaneously train a sentence matching model and a sentence intent classification model by sharing the sentence encoder between two tasks. Take Quora Question Pairs dataset <ref type="bibr" target="#b14">[15]</ref> as an instance: input data question1 and question2 will be encoded as sentence representation , by the sentence encoder.</p><p>On the one hand, we use a softmax layer and cross entropy loss function to train the multi-class sentence intent classification model as follows:</p><formula xml:id="formula_0">$ = − ∑ ) * log.softmax( 7 + ); &lt; )=&gt; (1) ? = − ∑ ) * log.softmax( 7 + ); &lt; )=&gt; (2) in (1) &amp; (2)</formula><p>is the class index of sentence intent, and M represents the number of classes.</p><p>On the other hand, for the convenience of integrating with approximate nearest neighbor (ANN) libraries, which only support cosine distance, Euclidean distance, Manhattan distance, Hamming distance, or Dot (Inner) Product distance, we use a simple cosine matching layer instead of a more complicated multi-layer perceptron as in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A = sigmoid. * ( ( , ) − );</head><p>(3)</p><formula xml:id="formula_1">JKLMN = −( * log A + (1 − ) * log(1 − A))<label>(4)</label></formula><p>In <ref type="formula">(3)</ref>, γ and are hyperparameters. Equation <ref type="formula" target="#formula_1">(4)</ref> is loss function of the matching layer. The overall loss function is as follows:</p><formula xml:id="formula_2">= * JKLMN + (&gt;RS) * (T U VT W ) X (5)</formula><p>where in (5) is a hyperparameter for balancing the loss of each task in the multi-task learning and 0≤ ≤1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sentence Encoder</head><p>The overall architecture of our sentence encoder is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. The encoder transforms the input sentence into a fixed-length embedding. The details of each component in the sentence encoder will be described in the subsections that follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Word Representation Layer</head><p>Word Representation Layer is consists of word embedding and character representation. For word embedding, we use pretrained GloVe word embeddings <ref type="bibr" target="#b16">[17]</ref> to represent each word as a d-dimensional vector. For character representation, we infuse randomly initialized values to max-pooling convolution layer to compute the character representation of each word. We concatenate the word embedding and character representation to get the final word representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Bidirectional Gated Recurrent Unit</head><p>We use Bidirectional Gated Recurrent Unit (BiGRU) <ref type="bibr" target="#b17">[18]</ref> to maintain the sequential information about the sentence being modeled. BiGRU is consists of a forward directional GRU and a backward directional GRU. Forward directional GRU process inputs sequence from left to right, backward directional GRU on the contrary. Let us describe how the tth hidden unit is computed: ∘ in <ref type="formula" target="#formula_3">(8) &amp; (9)</ref> denotes the element-wise product. In GRU cell, context information of tth hidden unit is carried over by the last hidden unit state ℎ LR&gt; .</p><formula xml:id="formula_3">L = sigmoid( L [ &gt; + ℎ LR&gt; [ X ) (6) L = sigmoid. L^&gt; + ℎ LR&gt;^X ; (7) ℎ L _ = tanh( L N &gt; + (ℎ LR&gt; ∘ L ) N X )<label>(8)</label></formula><formula xml:id="formula_4">ℎ L = (1 − L ) ∘ ℎ L _ + L ∘ ℎ LR&gt;<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Attention Recurrent Unit</head><p>We propose an Attention Recurrent Unit (ARU) based on the attention mechanism. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, the term "Gate" stands for the element-wise forget gate . We apply position-wise multi-head attention to compute the context representation (self-attention computed here), and use it to compute the linear transformation ℎ d with the input and the forget gate :</p><formula xml:id="formula_5">= PosMultiHead( , , )<label>(10)</label></formula><p>= sigmoid.^&gt; +^X;</p><formula xml:id="formula_6">(11) ℎ d = tanh( N &gt; + N X )<label>(12)</label></formula><p>Position-wise multi-head attention is a variant of multi-head attention <ref type="bibr" target="#b18">[19]</ref> as shown below: Mpos ∈ ℝ •×$ is a weight matrix where n is the number of words on sentence, which will be updated during training.</p><p>We use a dynamic average (DA) pooling <ref type="bibr" target="#b19">[20]</ref> to improve the sequence encoding capability further:</p><formula xml:id="formula_7">ℎ = (1 − L ) ∘ ℎ L _ + L ∘ ℎ LR&gt;<label>(15)</label></formula><p>After that, we use a highway connection <ref type="bibr" target="#b20">[21]</ref> to connect input and output:</p><formula xml:id="formula_8">= sigmoid( [ &gt; + [ X )<label>(16)</label></formula><formula xml:id="formula_9">= (1 − ) ∘ + ∘ ℎ<label>(17)</label></formula><p>Compared with GRU, the computation of the tth hidden unit is no more depending on the last hidden unit state ℎ LR&gt; , so that (10), (11), (12) could be computed in parallel on GPU effectively and (15) just needs a simple computation. In this section, we also refer to the works of Quasi-RNN <ref type="bibr" target="#b21">[22]</ref> and SRU <ref type="bibr" target="#b22">[23]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Feed-Forward Network</head><p>We use a feed-forward network same as the Transformer <ref type="bibr" target="#b18">[19]</ref>. It uses a multi-layer perceptron with two layers and uses activation function ReLU, as follows:</p><formula xml:id="formula_10">FFN( ) = (0, &gt; + &gt; ) X + X<label>(18)</label></formula><p>FFN function is applied to each output state of ARU. The term "Add" in <ref type="figure" target="#fig_3">Fig. 4</ref> represents the residual connection <ref type="bibr" target="#b23">[24]</ref> and the term "Norm" represents the layer normalization <ref type="bibr" target="#b24">[25]</ref>. The output of FFN would be incorporated with the Add &amp; Norm layer to simplify the network's optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Attentive Pooling</head><p>We perform an attentive pooling operation <ref type="bibr" target="#b25">[26]</ref> over the output of the FFN, which would convert them into a fixed-length vector. It can be formulated as follows:</p><p>= softmax. xX tanh( x&gt; 7 );</p><formula xml:id="formula_11">= 7<label>(19)</label></formula><p>In <ref type="formula" target="#formula_12">(20)</ref>, ∈ ℝ •×$ is the output of FFN, where is the number of words in the sentence and is the number of hidden units of ARU. x&gt; ∈ ℝ $×s Ž and xX ∈ ℝ s Ž ×[ are two weight matrices where K and are hyperparameters that could be set manually. After the attentive pooling layer, the output matrix ∈ ℝ [×$ consists of sentence representations with udimensional vectors. We concatenate the above vectors and feed it to a highway network <ref type="bibr" target="#b26">[27]</ref> with two layers to generate the final sentence representation vector. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the proposed semantic retrieval framework, where the encoding-based model plays a very important role and has a great impact on the overall performance. In the offline system, all questions in the FAQ set are encoded to dense realvalued vectors. Then we build an ANN vector index by using an ANN tool, such as Annoy, through which we can get the most similar vectors given a vector encoded from any new question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SEMANTIC RETRIEVAL FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Framework Overview</head><p>In the online system, we deploy the same module to encode the question input by the user. By inputting the vector to the ANN module, we can get top similar questions with a semantic matching score. Then the most similar question could be seen as synonymous to the user's question, so they might share the same answer. A more complicated rank module could be used following the ANN module to re-rank the top K candidates, such as interaction-based models, or ranking algorithms with handcrafted features. However, the rank module is less important in our proposed framework than in the traditional IR-based QA frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis</head><p>As compared with the traditional IR-based QA frameworks ( <ref type="figure" target="#fig_0">Fig. 1)</ref>, our framework is less dependent on the general question analysis tools like keyword extraction. Besides, our framework removes the traditional recall module based on text search engines, which is replaced by the new recall module based on the sentence encoding and ANN technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We conduct experiments on four sentence matching datasets, each comprising a large set of instances in the form of 〈 &gt; , X , 〉, where &gt; and X are two questions, and is the label indicating whether they are paraphrases or not. <ref type="table" target="#tab_0">Table I</ref> shows a brief description of these datasets. The overlap rate is a ratio of the number of common words between two sentences in a sample to the average number of them.</p><p>• Quora Question Pair dataset <ref type="bibr" target="#b14">[15]</ref> is an open-domain English dataset derived from Quora.com. We use the same split ratio as BiMPM <ref type="bibr" target="#b8">[9]</ref>.</p><p>• LCQMC dataset <ref type="bibr" target="#b27">[28]</ref> is an open-domain Chinese dataset collected from Baidu Knows (a popular Chinese community question answering website).</p><p>• Bank Question (BQ) dataset <ref type="bibr" target="#b28">[29]</ref> is a specific-domain Chinese dataset for sentence semantic equivalence identification (SSEI).</p><p>• Telephone customer service (TCS) dataset is a specificdomain Chinese dataset from a real-world telephone customer service scenario, where voice speeches are converted into text using the automatic speech recognition technology.</p><p>The evaluation metric is accuracy for the Quora dataset, and F1 for other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Settings of Experiments</head><p>For Quora dataset, we use the Glove-840B-300D vector as the pre-trained word embedding. The character embedding is randomly initialized with 150D and the hidden size of BiGRU is set to 300. We set =0.8 in the multi-task loss function. For the sentence intent classification task, we only keep the sentence clusters with question number greater than 3, and the remaining sentence clusters with question number less than or equal to 3 are regarded as a special "other" cluster. Dropout layer is also applied to the output of the attentive pooling layer, with a dropout rate of 0.1. An Adam optimizer <ref type="bibr" target="#b29">[30]</ref> is used to optimize all the trainable weights. The learning rate is set to 4e-4 and the batch size is set to 200. When the performance of the model is no longer improved, an SGD optimizer with a learning rate of 1e-3 is used to find a better local optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparing with other methods</head><p>We compared our model with the following models:</p><p>ESIM: Enhanced Sequential Inference Model <ref type="bibr" target="#b7">[8]</ref> is an interaction-based model for natural language inference. It uses BiLSTM to encode sentence contexts and uses the attention mechanism to calculate the information between two sentences. ESIM has shown excellent performance on the SNLI dataset.</p><p>BiMPM: Bilateral Multi-Perspective Matching model <ref type="bibr" target="#b8">[9]</ref> is an interaction-based sentence matching model with superior performance. The model uses a BiLSTM layer to learn the sentence representation, four different types of multiperspective matching layers to match two sentences, an additional BiLSTM layer to aggregate the matching results, and a two-layer feed-forward network for prediction. SSE: Shortcut-Stacked Sentence Encoder <ref type="bibr" target="#b5">[6]</ref> is an encodingbased sentence-matching model, which enhances multi-layer BiLSTM with short-cut connections. SSE has been proved to be effective in improving the performance of sentence encoder, recording state-of-the-art performance of the sentence-encoding models on Quora dataset. DIIN: Densely Interactive Inference Network <ref type="bibr" target="#b9">[10]</ref> is an interaction-based model for natural language inference (NLI). It hierarchically extracts semantic features from interaction space to achieve a high-level understanding of the sentence pair. It achieves state-of-the-art performance on SNLI dataset and Quora dataset. a. The first five rows are copied from <ref type="bibr" target="#b9">[10]</ref> and the next two rows are copied from <ref type="bibr" target="#b30">[31]</ref>.  a. The first four rows are copied from <ref type="bibr" target="#b28">[29]</ref> and the next two rows are reproduced using the SMP_toolkit <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results of Experiments</head><p>The results of experiments on four sentence matching datasets are summarized as follows:</p><p>Quora dataset: <ref type="table" target="#tab_0">Table II</ref>   BiMPM and ESIM models without any sentence interaction information, and is very close to DIIN, the state-of-the-art interaction-based model, but we don't any external knowledge in our method.</p><p>LCQMC dataset: Experimental results of LCQMC dataset compared with the existing models are shown in <ref type="table" target="#tab_0">Table III</ref>. The experimental results show that our model outperforms state-ofthe-art models.</p><p>BQ dataset: BQ dataset is a specific-domain dataset with a low average overlap rate. As shown in <ref type="table" target="#tab_0">Table IV</ref>, our model outperforms state-of-the-art models by a large margin, reaching 83.62%, recording the state-of-the-art performance.</p><p>TCS dataset: As shown in <ref type="table" target="#tab_5">Table V</ref>, experimental results show that our MSEM model achieves the best performance. This indicates that our model is also very effective in the spoken question-answering scenario.</p><p>To sum up, experimental results show that our proposed model without multi-task learning outperforms SSE, the stateof-the-art encoding-based models, across all four datasets. And the model with multi-task learning further improved performance ranging from 0.4% to 1%. Compared with existing models, our model shows great advantages on datasets with low average overlap rate, which is known to be very common in realworld question answering scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>The above experiments show the effectiveness of our proposed multi-task training strategy. In this section, we present the results of an ablation study on Quora dataset for evaluating the contribution of each component of the encoder, as shown in <ref type="table" target="#tab_0">Table VI</ref>. Note that we do the significant test for each ablation experiment using the t-test (p &lt; 0.05). We first study the contribution of the ARU component. The accuracy decreases   <ref type="bibr" target="#b18">[19]</ref>, the accuracy will drop to 88.25%. Next we compare the effect of attentive pooling vs max pooling. It turns out that the attentive pooling is better than max pooling. Then if we remove the highway network, the accuracy will drop to 88.36%. Finally when we remove the character-level embedding, the accuracy will drop to 88.26%. A possible reason might be that the character-level embedding can better handle the out-of-vocab (OOV) words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Online System Evaluation</head><p>We perform an online evaluation with a telephone customer service system. We randomly select 1138 questions from the system log and send them to a baseline system and the new system, respectively. The baseline system is similar to what shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, where the retrieval module is built on Elasticsearch and 30 candidate questions will be recalled and then ranked by the MSEM model. The new system is similar to what shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, where an ANN module based on Annoy and a sentence-encoding module based on MSEM are adopted. We manually evaluate the returned results and measure the performance with the F1 score. As shown in <ref type="table" target="#tab_0">Table VIII</ref>, the F1 score of the new system is 14.26% higher than the baseline system. Obviously, the semantic competence derived from the MSEM module plays a key role in the new system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Case Studies</head><p>We perform some case studies using the Quora test set to analyze the effectiveness of the multi-task strategy. We randomly select 200 sentences with a predicted intent of nonother and manually annotate the correctness of the predicted intent. We find that the accuracy can reach 96.5%, indicating that our model can address the intent classification task pretty well. <ref type="table" target="#tab_0">Table VII</ref> shows some examples where the MSEM model works, while the MSEM without multi-task fails. In the first example, although the text similarity between S1 and S2 is low, our model can correctly identify that they have the same intent. In the second example, S1 and S2 have high text overlap, but the model can correctly identify that they have different intents, which helps our model can better distinguish their semantics. In the third example, the model classifies S1 as "other" and S2 as "non-other", which can also help the model distinguish their semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we first propose a Multi-task Sentence Encoding Model (MSEM) which addresses both the paraphrase identification task and the sentence intent classification task simultaneously, and then further propose a general semantic retrieval framework that combines the sentence encoding model and approximate nearest neighbor search technology, which can find the most similar question very quickly from all available questions in a massive QA knowledge base. We evaluated our model on several benchmark datasets. Experimental results show that our proposed method is superior to many recent sentence-matching models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The workflow of the traditional IR-based QA systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The sentence clusters with similar intent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Overall architecture of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Sentence Encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Attention Recurrent Unit (ARU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The workflow of the semantic retrieval QA systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">EXPERIMENT DATASETS</cell><cell></cell></row><row><cell>Dataset</cell><cell>Language</cell><cell>Source</cell><cell>Scale (train/valid/test)</cell><cell>pos:neg</cell><cell>Overlap rate (pos/neg/avg)</cell></row><row><cell>Quora</cell><cell>English</cell><cell>Quora</cell><cell>384,348/10,000/10,000</cell><cell>1:1.71</cell><cell>0.622/0.445/0.511</cell></row><row><cell>LCQMC</cell><cell>Chinese</cell><cell>Baidu Knows</cell><cell>238,766/8,802/12,500</cell><cell>1.35:1</cell><cell>0.771/0.530/0.668</cell></row><row><cell>BQ</cell><cell>Chinese</cell><cell>Bank</cell><cell>100,000/10,000/10,000</cell><cell>1:1</cell><cell>0.326/0.174/0.250</cell></row><row><cell>TCS</cell><cell>Chinese</cell><cell>Telephone</cell><cell>94,993/5,000/3,000</cell><cell>1:4.31</cell><cell>0.334/0.172/0.203</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II .</head><label>II</label><figDesc>EXPERIMENTAL RESULTS ON QUORA DATASET</figDesc><table><row><cell>Models</cell><cell>Accuracy</cell></row><row><cell>Siamese-CNN</cell><cell>79.60</cell></row><row><cell>Siamese-LSTM</cell><cell>82.58</cell></row><row><cell>L.D.C</cell><cell>85.55</cell></row><row><cell>BiMPM</cell><cell>88.17</cell></row><row><cell>DIIN</cell><cell>89.06</cell></row><row><cell>ESIM</cell><cell>85.0</cell></row><row><cell>SSE</cell><cell>87.8</cell></row><row><cell>MSEM (-multi-task)</cell><cell>88.11</cell></row><row><cell>MSEM</cell><cell>88.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III .</head><label>III</label><figDesc>EXPERIMENTAL RESULTS ON LCQMC DATASET</figDesc><table><row><cell>Models</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>BiLSTM-char</cell><cell>67.4</cell><cell>91.0</cell><cell>77.5</cell></row><row><cell>BiLSTM-word</cell><cell>70.6</cell><cell>89.3</cell><cell>78.92</cell></row><row><cell>BiMPM-char</cell><cell>77.6</cell><cell>93.9</cell><cell>85.0</cell></row><row><cell>BiMPM-word</cell><cell>77.7</cell><cell>93.5</cell><cell>84.9</cell></row><row><cell>ESIM</cell><cell>76.54</cell><cell>93.58</cell><cell>84.21</cell></row><row><cell>SSE</cell><cell>78.23</cell><cell>93.57</cell><cell>85.21</cell></row><row><cell>MSEM (-multi-task)</cell><cell>78.23</cell><cell>93.69</cell><cell>85.27</cell></row><row><cell>MSEM</cell><cell>78.90</cell><cell>93.73</cell><cell>85.68</cell></row><row><cell cols="4">a. The first four rows are copied from [28] and the next two rows are reproduced using the SMP_toolkit</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[31].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV .</head><label>IV</label><figDesc>EXPERIMENTAL RESULTS ON BQ DATASET</figDesc><table><row><cell>Models</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>Text-CNN</cell><cell>67.77</cell><cell>70.64</cell><cell>69.17</cell></row><row><cell>BiLSTM</cell><cell>75.04</cell><cell>70.46</cell><cell>72.68</cell></row><row><cell>BiMPM</cell><cell>82.28</cell><cell>81.18</cell><cell>81.73</cell></row><row><cell>DIIN</cell><cell>81.58</cell><cell>81.14</cell><cell>81.36</cell></row><row><cell>SSE</cell><cell>80.16</cell><cell>80.32</cell><cell>80.24</cell></row><row><cell>ESIM</cell><cell>81.91</cell><cell>81.78</cell><cell>81.85</cell></row><row><cell>MSEM (-multi-task)</cell><cell>82.39</cell><cell>83.36</cell><cell>82.87</cell></row><row><cell>MSEM</cell><cell>82.88</cell><cell>84.36</cell><cell>83.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>shows the experimental results compared with existing models on Quora dataset. Compared with SSE, the state-of-the-art encoding-base model, our MSEM model outperforms SSE by about 1% and achieves new state-ofthe-art performance. In addition, our model outperforms</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V .</head><label>V</label><figDesc>EXPERIMENTAL RESULTS ON TCS DATASET</figDesc><table><row><cell>Models</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>BiMPM</cell><cell>86.77</cell><cell>85.11</cell><cell>85.93</cell></row><row><cell>ESIM</cell><cell>87.19</cell><cell>87.02</cell><cell>87.11</cell></row><row><cell>SSE</cell><cell>88.54</cell><cell>87.02</cell><cell>87.78</cell></row><row><cell>MSEM (-multi-task)</cell><cell>87.38</cell><cell>88.55</cell><cell>87.96</cell></row><row><cell>MSEM</cell><cell>88.63</cell><cell>89.31</cell><cell>88.97</cell></row><row><cell>TABLE VI.</cell><cell cols="2">ABLATION STUDY</cell><cell></cell></row><row><cell cols="2">Models</cell><cell></cell><cell>Accuracy</cell></row><row><cell>MSEM</cell><cell></cell><cell></cell><cell>88.86</cell></row><row><cell>MSEM -multi-task</cell><cell></cell><cell></cell><cell>88.11</cell></row><row><cell>MSEM -ARU</cell><cell></cell><cell></cell><cell>87.84</cell></row><row><cell cols="2">MSEM -ARU + multi-head attention</cell><cell></cell><cell>88.25</cell></row><row><cell cols="3">MSEM -attentive pooling + max pooling</cell><cell>88.35</cell></row><row><cell cols="2">MSEM -highway network</cell><cell></cell><cell>88.36</cell></row><row><cell cols="2">MSEM -char embedding</cell><cell></cell><cell>88.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII .</head><label>VII</label><figDesc>. O indicates the predicted label of MSEM without multi-task, W indicates the predicted label of MSEM. Cluster ID 0 indicates the special "other" cluster.</figDesc><table><row><cell>EXAMPLES</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sentences</cell><cell cols="4">Label O W Cluster ID</cell></row><row><cell>S1: How do I build my own custom made desktop computer ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>3840</cell></row><row><cell>S2: How do I build a computer ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>S1: How many medals did India won in Olympic 2016 ?</cell><cell></cell><cell></cell><cell></cell><cell>2303</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>0</cell></row><row><cell>S2: How many medals will India win in 2016 Olympics ?</cell><cell></cell><cell></cell><cell></cell><cell>2736</cell></row><row><cell>S1: Why do you believe in the afterlife ?</cell><cell></cell><cell></cell><cell></cell><cell>0</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>0</cell></row><row><cell>S2: How many medals will India win in 2016 Olympics ?</cell><cell></cell><cell></cell><cell></cell><cell>1717</cell></row></table><note>a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII .</head><label>VIII</label><figDesc>RESULTS OF ONLINE SYSTEM EVALUATION</figDesc><table><row><cell>Models</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>Baseline</cell><cell>87.88</cell><cell>64.32</cell><cell>74.28</cell></row><row><cell>Our System</cell><cell>91.56</cell><cell>79.09</cell><cell>84.87</cell></row><row><cell cols="4">from 88.86% to 87.84% after removing the ARU component. If</cell></row><row><cell cols="4">we replace the ARU component with multi-head attention</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The question answering systems: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M N</forename><surname>Allam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Haggag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Research and Reviews in Information Sciences (IJRRIS)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, Deep Learning Workshop</title>
		<meeting>the International Conference on Machine Learning, Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shortcut-Stacked Sentence Encoders for Multi-Domain Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 2nd Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to compose task-specific tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilateral Multi-Perspective Matching for Natural Language Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vldb</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="518" to="529" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor algorithm based on navigable small world graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Logvinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krylov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Csernai</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Strongly-Typed Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Small-footprint deep neural networks with highway connections for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">INTERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Quasi recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<title level="m">Training RNNs as Fast as CNNs&quot; ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LCQMC: A Large-scale Chinese Question Matching Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLING</title>
		<imprint>
			<biblScope unit="page" from="1952" to="1962" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="4946" to="4951" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
