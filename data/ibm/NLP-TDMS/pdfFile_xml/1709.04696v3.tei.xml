<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
							<email>tao.shen@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney ‡Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jing.jiang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney ‡Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
							<email>tianyizh@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="department">of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney ‡Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
							<email>shirui.pan@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney ‡Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
							<email>guodong.long@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney ‡Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
							<email>chengqi.zhang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney ‡Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely used on NLP tasks to capture the long-term and local dependencies, respectively. Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation, significantly less training time, and flexibility in modeling dependencies. We propose a novel attention mechanism in which the attention between elements from input sequence(s) is directional and multi-dimensional (i.e., feature-wise). A light-weight neural net, "Directional Self-Attention Network (DiSAN)", is then proposed to learn sentence embedding, based solely on the proposed attention without any RNN/CNN structure. DiSAN is only composed of a directional self-attention with temporal order encoded, followed by a multi-dimensional attention that compresses the sequence into a vector representation. Despite its simple form, DiSAN outperforms complicated RNN models on both prediction quality and time efficiency. It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02% on the Stanford Natural Language Inference (SNLI) dataset, and shows stateof-the-art test accuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language inference (MultiNLI), Sentences Involving Compositional Knowledge (SICK), Customer Review, MPQA, TREC question-type classification and Subjectivity (SUBJ) datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>performance on a large number of NLP tasks, including neural machine translation <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015;</ref><ref type="bibr" target="#b23">Luong, Pham, and Manning 2015)</ref>, natural language inference <ref type="bibr" target="#b22">(Liu et al. 2016)</ref>, conversation generation <ref type="bibr" target="#b35">(Shang, Lu, and Li 2015)</ref>, question answering <ref type="bibr" target="#b8">(Hermann et al. 2015;</ref><ref type="bibr" target="#b39">Sukhbaatar et al. 2015)</ref>, machine reading comprehension <ref type="bibr" target="#b34">(Seo et al. 2017)</ref>, and sentiment analysis <ref type="bibr" target="#b17">(Kokkinos and Potamianos 2017)</ref>. The attention uses a hidden layer to compute a categorical distribution over elements from the input sequence to reflect their importance weights. It allows RNN/CNN to maintain a variable-length memory, so that elements from the input sequence can be selected by their importance/relevance and merged into the output. In contrast to RNN and CNN, the attention mechanism is trained to capture the dependencies that make significant contributions to the task, regardless of the distance between the elements in the sequence. It can thus provide complementary information to the distance-aware dependencies modeled by RNN/CNN. In addition, computing attention only requires matrix multiplication, which is highly parallelizable compared to the sequential computation of RNN.</p><p>In a very recent work <ref type="bibr">(Vaswani et al. 2017)</ref>, an attention mechanism is solely used to construct a sequence to sequence (seq2seq) model that achieves a state-of-the-art quality score on the neural machine translation (NMT) task. The seq2seq model, "Transformer", has an encoder-decoder structure that is only composed of stacked attention networks, without using either recurrence or convolution. The proposed attention, "multi-head attention", projects the input sequence to multiple subspaces, then applies scaled dotproduct attention to its representation in each subspace, and lastly concatenates their output. By doing this, it can combine different attentions from multiple subspaces. This mechanism is used in Transformer to compute both the context-aware features inside the encoder/decoder and the bottleneck features between them.</p><p>The attention mechanism has more flexibility in sequence length than RNN/CNN, and is more task/data-driven when modeling dependencies. Unlike sequential models, its computation can be easily and significantly accelerated by existing distributed/parallel computing schemes. However, to the best of our knowledge, a neural net entirely based on attention has not been designed for other NLP tasks except NMT, especially those that cannot be cast into a seq2seq problem.</p><p>Compared to RNN, a disadvantage of most attention mechanisms is that the temporal order information is lost, which however might be important to the task. This explains why positional encoding is applied to the sequence before being processed by the attention in Transformer. How to model order information within an attention is still an open problem.</p><p>The goal of this paper is to develop a unified and RNN/CNN-free attention network that can be generally utilized to learn the sentence encoding model for different NLP tasks, such as natural language inference, sentiment analysis, sentence classification and semantic relatedness. We focus on the sentence encoding model because it is a basic module of most DNNs used in the NLP literature.</p><p>We propose a novel attention mechanism that differs from previous ones in that it is 1) multi-dimensional: the attention w.r.t. each pair of elements from the source(s) is a vector, where each entry is the attention computed on each feature; and 2) directional: it uses one or multiple positional masks to model the asymmetric attention between two elements. We compute feature-wise attention since each element in a sequence is usually represented by a vector, e.g., word/character embedding <ref type="bibr" target="#b13">(Kim et al. 2016)</ref>, and attention on different features can contain different information about dependency, thus to handle the variation of contexts around the same word. We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing. This design mitigates the weakness of attention in modeling order information, and takes full advantage of parallel computing.</p><p>We then build a light-weight and RNN/CNN-free neural network, "Directional Self-Attention Network (DiSAN)", for sentence encoding. This network relies entirely on the proposed attentions and does not use any RNN/CNN structure. In DiSAN, the input sequence is processed by directional (forward and backward) self-attentions to model context dependency and produce context-aware representations for all tokens. Then, a multi-dimensional attention computes a vector representation of the entire sequence, which can be passed into a classification/regression module to compute the final prediction for a particular task. Unlike Transformer, neither stacking of attention blocks nor an encoderdecoder structure is required. The simple architecture of DiSAN leads to fewer parameters, less computation and easier parallelization.</p><p>In experiments 1 , we compare DiSAN with the currently popular methods on various NLP tasks, e.g., natural language inference, sentiment analysis, sentence classification, etc. DiSAN achieves the highest test accuracy on the Stanford Natural Language Inference (SNLI) dataset among sentence-encoding models and improves the currently best result by 1.02%. It also shows the state-of-the-art performance on the Stanford Sentiment Treebank (SST), Multi-Genre natural language inference (MultiNLI), SICK, Customer Review, MPQA, SUBJ and TREC question-type classification datasets. Meanwhile, it has fewer parameters and exhibits much higher computation efficiency than the mod-els it outperforms, e.g., LSTM and tree-based models.</p><p>Annotation: 1) Lowercase denotes a vector; 2) bold lowercase denotes a sequence of vectors (stored as a matrix); and 3) uppercase denotes a matrix or a tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Encoding</head><p>In the pipeline of NLP tasks, a sentence is denoted by a sequence of discrete tokens (e.g., words or characters)</p><formula xml:id="formula_0">v = [v 1 , v 2 , . . . , v n ],</formula><p>where v i could be a one-hot vector whose dimension length equals the number of distinct tokens N . A pre-trained token embedding (e.g., word2vec <ref type="bibr" target="#b26">(Mikolov et al. 2013b)</ref> or GloVe <ref type="bibr" target="#b31">(Pennington, Socher, and Manning 2014)</ref>) is applied to v and transforms all discrete tokens to a sequence of low-dimensional dense vector representations x = [x 1 , x 2 , . . . , x n ] with x i ∈ R de . This pre-process can be written as x = W (e) v, where word embedding weight matrix W (e) ∈ R de×N and x ∈ R de×n .</p><p>Most DNN sentence-encoding models for NLP tasks take x as the input and further generate a vector representation u i for each x i by context fusion. Then a sentence encoding is obtained by mapping the sequence u = [u 1 , u 2 , . . . , u n ] to a single vector s ∈ R d , which is used as a compact encoding of the entire sentence in NLP problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention</head><p>The attention is proposed to compute an alignment score between elements from two sources. In particular, given the token embeddings of a source sequence x = [x 1 , x 2 , . . . , x n ] and the vector representation of a query q, attention computes the alignment score between x i and q by a compatibility function f (x i , q), which measures the dependency between x i and q, or the attention of q to x i . A softmax function then transforms the scores [f (x i , q)] n i=1 to a probability distribution p(z|x, q) by normalizing over all the n tokens of x. Here z is an indicator of which token in x is important to q on a specific task. That is, large p(z = i|x, q) means x i contributes important information to q. The above process can be summarized by the following equations.</p><formula xml:id="formula_1">a = [f (x i , q)] n i=1 ,</formula><p>(1) p(z|x, q) = softmax(a).</p><p>(2) Specifically,</p><formula xml:id="formula_2">p(z = i|x, q) = exp(f (x i , q)) n i=1 exp(f (x i , q))</formula><p>.</p><p>(</p><p>The output of this attention mechanism is a weighted sum of the embeddings for all tokens in x, where the weights are given by p(z|x, q). It places large weights on the tokens important to q, and can be written as the expectation of a token sampled according to its importance, i.e.,</p><formula xml:id="formula_4">s = n i=1 p(z = i|x, q)x i = E i∼p(z|x,q) (x i ),<label>(4)</label></formula><p>where s ∈ R de can be used as the sentence encoding of x. Additive attention (or multi-layer perceptron attention) <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015;</ref><ref type="bibr" target="#b35">Shang, Lu, and Li</ref>  2015) and multiplicative attention (or dot-product attention) <ref type="bibr">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b39">Sukhbaatar et al. 2015;</ref><ref type="bibr" target="#b33">Rush, Chopra, and Weston 2015)</ref> are the two most commonly used attention mechanisms. They share the same and unified form of attention introduced above, but are different in the compatibility function f (x i , q). Additive attention is associated with</p><formula xml:id="formula_5">f (x i , q) = w T σ(W (1) x i + W (2) q),<label>(5)</label></formula><p>where σ(·) is an activation function and w ∈ R de is a weight vector. Multiplicative attention uses inner product or cosine similarity for f (x i , q), i.e.,</p><formula xml:id="formula_6">f (x i , q) = W (1) x i , W (2) q .<label>(6)</label></formula><p>In practice, additive attention often outperforms multiplicative one in prediction quality, but the latter is faster and more memory-efficient due to optimized matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-Attention</head><p>Self-Attention is a special case of the attention mechanism introduced above. It replaces q with a token embedding x j from the source input itself. It relates elements at different positions from a single sequence by computing the attention between each pair of tokens, x i and x j . It is very expressive and flexible for both long-range and local dependencies, which used to be respectively modeled by RNN and CNN. Moreover, it has much faster computation speed and fewer parameters than RNN. In recent works, we have already witnessed its success across a variety of NLP tasks, such as reading comprehension <ref type="bibr" target="#b11">(Hu, Peng, and Qiu 2017)</ref> and neural machine translation <ref type="bibr">(Vaswani et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two Proposed Attention Mechanisms</head><p>In this section, we introduce two novel attention mechanisms, multi-dimensional attention in Section 3.1 (with two extensions to self-attention in Section 3.2) and directional self-attention in Section 3.3. They are the main components of DiSAN and may be of independent interest to other neural nets for other NLP problems in which an attention is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-dimensional Attention</head><p>Multi-dimensional attention is a natural extension of additive attention (or MLP attention) at the feature level. Instead of computing a single scalar score f (x i , q) for each token x i as shown in Eq.(5), multi-dimensional attention computes a feature-wise score vector for x i by replacing weight vector w in Eq.(5) with a matrix W , i.e.,</p><formula xml:id="formula_7">f (x i , q) = W T σ W (1) x i + W (2) q ,<label>(7)</label></formula><p>where f (x i , q) ∈ R de is a vector with the same length as x i , and all the weight matrices W, W (1) , W (2) ∈ R de×de . We further add two bias terms to the parts in and out activation σ(·), i.e.,</p><formula xml:id="formula_8">f (x i , q) = W T σ W (1) x i + W (2) q + b (1) + b. (8)</formula><p>We then compute a categorical distribution p(z k |x, q) over all the n tokens for each feature k ∈ [d e ]. A large p(z k = i|x, q) means that feature k of token i is important to q. We apply the same procedure Eq. <ref type="formula">(1)</ref></p><formula xml:id="formula_9">-(3) in traditional at- tention to the k th dimension of f (x i , q). In particular, for each feature k ∈ [d e ], we replace f (x i , q) with [f (x i , q)] k , and change z to z k in Eq.(1)-(3)</formula><p>. Now each feature k in each token i has an importance weight P ki p(z k = i|x, q). The output s can be written as</p><formula xml:id="formula_10">s = n i=1 P ki x ki de k=1 = E i∼p(z k |x,q) (x ki ) de k=1 . (9)</formula><p>We give an illustration of traditional attention and multidimensional attention in <ref type="figure" target="#fig_0">Figure 1</ref>. In the rest of this paper, we will ignore the subscript k which indexes feature dimension for simplification if no confusion is possible. Hence, the output s can be written as an element-wise product s = n i=1 P ·i x i Remark: The word embedding usually suffers from the polysemy in natural language. Since traditional attention computes a single importance score for each word based on the word embedding, it cannot distinguish the meanings of the same word in different contexts. Multi-dimensional attention, however, computes a score for each feature of each word, so it can select the features that can best describe the word's specific meaning in any given context, and include this information in the sentence encoding output s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two types of Multi-dimensional Self-attention</head><p>When extending multi-dimension to self-attentions, we have two variants of multi-dimensional attention. The first one, called multi-dimensional "token2token" self-attention, explores the dependency between x i and x j from the same source x, and generates context-aware coding for each element. It replaces q with x j in Eq.(8), i.e.,</p><formula xml:id="formula_11">f (x i , x j ) = W T σ W (1) x i + W (2) x j + b (1) + b. (10)</formula><p>Similar to P in vanilla multi-dimensional attention, we compute a probability matrix P j ∈ R de×n for each x j such that P j ki p(z k = i|x, x j ). The output for x j is</p><formula xml:id="formula_12">s j = n i=1 P j ·i x i (11) Figure 2: Directional self-attention (DiSA) mechanism. Here, we use l i,j to denote f (h i , h j ) in Eq. (15).</formula><p>The output of token2token self-attention for all elements from x is s = [s 1 , s 2 , . . . , s n ] ∈ R de×n . The second one, multi-dimensional "source2token" selfattention, explores the dependency between x i and the entire sequence x, and compresses the sequence x into a vector. It removes q from Eq.(8), i.e.,</p><formula xml:id="formula_13">f (x i ) = W T σ W (1) x i + b (1) + b.<label>(12)</label></formula><p>The probability matrix is defined as P ki p(z k = i|x) and is computed in the same way as P in vanilla multidimensional attention. The output s is also same, i.e.,</p><formula xml:id="formula_14">s = n i=1 P ·i x i<label>(13)</label></formula><p>We will use these two types (i.e., token2token and source2token) of multi-dimensional self-attention in different parts of our sentence encoding model, DiSAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Directional Self-Attention</head><p>Directional self-attention (DiSA) is composed of a fully connected layer whose input is the token embeddings x, a "masked" multi-dimensional token2token self-attention block to explore the dependency and temporal order, and a fusion gate to combine the output and input of the attention block. Its structure is shown in <ref type="figure">Figure 2</ref>. It can be used as either a neural net or a module to compose a large network.</p><p>In DiSA, we first transform the input sequence x = [x 1 , x 2 , . . . , x n ] to a sequence of hidden state h = [h 1 , h 2 , . . . , h n ] by a fully connected layer, i.e.,</p><formula xml:id="formula_15">h = σ h W (h) x + b (h) ,<label>(14)</label></formula><p>where x ∈ R de×n , h ∈ R d h ×n , W (h) and b (h) are the learnable parameters, and σ h (·) is an activation function. We then apply multi-dimensional token2token selfattention to h, and generate context-aware vector representations s for all elements from the input sequence. We make two modifications to Eq.(10) to reduce the number of parameters and make the attention directional.</p><p>First, we set W in Eq.(10) to a scalar c and divide the part in σ(·) by c, and we use tanh(·) for σ(·), which reduces the number of parameters. In experiments, we always set c = 5, and obtain stable output.</p><p>Second, we apply a positional mask to Eq. <ref type="formula">(10)</ref>, so the attention between two elements can be asymmetric. Given a mask M ∈ {0, −∞} n×n , we set bias b to a constant vector M ij 1 in Eq. <ref type="formula">(10)</ref>, where 1 is an all-one vector. Hence, Eq. <ref type="formula">(10)</ref> is modified to</p><formula xml:id="formula_16">f (h i , h j ) = c · tanh [W (1) h i + W (2) h j + b (1) ]/c + M ij 1. (15)</formula><p>To see why a mask can encode directional information, let us consider a case in which</p><formula xml:id="formula_17">M ij = −∞ and M ji = 0, which results in [f (h i , h j )] k = −∞ and unchanged [f (h j , h i )] k . Since the probability p(z k = i|x, x j ) is computed by softmax, [f (h i , h j )] k = −∞ leads to p(z k = i|x, x j ) = 0.</formula><p>This means that there is no attention of x j to x i on feature k. On the contrary, we have p(z k = j|x, x i ) &gt; 0, which means that attention of x i to x j exists on feature k. Therefore, prior structure knowledge such as temporal order and dependency parsing can be easily encoded by the mask, and explored in generating sentence encoding. This is an important feature of DiSA that previous attention mechanisms do not have.</p><p>For self-attention, we usually need to disable the attention of each token to itself <ref type="bibr" target="#b11">(Hu, Peng, and Qiu 2017)</ref>. This is the same as applying a diagonal-disabled (i.e., diag-disabled) mask such that</p><formula xml:id="formula_18">M diag ij = 0, i = j −∞, i = j<label>(16)</label></formula><p>Moreover, we can use masks to encode temporal order information into attention output. In this paper, we use two masks, i.e., forward mask M f w and backward mask M bw ,</p><formula xml:id="formula_19">M f w ij = 0, i &lt; j −∞, otherwise<label>(17)</label></formula><formula xml:id="formula_20">M bw ij = 0, i &gt; j −∞, otherwise<label>(18)</label></formula><p>In forward mask M f w , there is the only attention of later token j to early token i, and vice versa in backward mask. We show these three positional masks in <ref type="figure" target="#fig_1">Figure 3</ref>. Given input sequence x and a mask M , we compute f (x i , x j ) according to Eq.(15), and follow the standard procedure of multi-dimensional token2token self-attention to compute the probability matrix P j for each j ∈ [n]. Each output s j in s is computed as in Eq.(11). The final output u ∈ R d h ×n of DiSA is obtained by combining the output s and the input h of the masked multidimensional token2token self-attention block. This yields a temporal order encoded and context-aware vector representation for each element/token. The combination is accomplished by a dimension-wise fusion gate, i.e.,</p><formula xml:id="formula_21">F = sigmoid W (f 1) s + W (f 2) h + b (f )<label>(19)</label></formula><formula xml:id="formula_22">u = F h + (1 − F ) s<label>(20)</label></formula><p>where W (f 1) , W (f 2) ∈ R d h ×d h and b (f ) ∈ R d h are the learnable parameters of the fusion gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Directional Self-Attention Network</head><p>We propose a light-weight network, "Directional Self-Attention Network (DiSAN)", for sentence encoding. Its architecture is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Given an input sequence of token embedding x, DiSAN firstly applies two parameter-untied DiSA blocks with forward mask M f w Eq.(17) and M bw Eq.(18), respectively. The feed-forward procedure is given in Eq.(14)-(15) and . Their outputs are denoted by u f w , u bw ∈ R d h ×n . We concatenate them vertically as [u f w ; u bw ] ∈ R 2d h ×n , and use this concatenated output as input to a multidimensional source2token self-attention block, whose output s disan ∈ R 2d h computed by Eq.(12)-(13) is the final sentence encoding result of DiSAN.</p><p>Remark: In DiSAN, forward/backward DiSA blocks work as context fusion layers. And the multi-dimensional source2token self-attention compresses the sequence into a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first apply DiSAN to natural language inference and sentiment analysis tasks. DiSAN achieves the state-of-the-art performance and significantly better efficiency than other baseline methods on benchmark datasets for both tasks. We also conduct experiments on other NLP tasks and DiSAN also achieves state-of-the-art performance.</p><p>Training Setup: We use cross-entropy loss plus L2 regularization penalty as optimization objective. We minimize it by Adadelta <ref type="bibr" target="#b47">(Zeiler 2012</ref>) (an optimizer of mini-batch SGD) with batch size of 64. We use Adadelta rather than Adam <ref type="bibr" target="#b15">(Kingma and Ba 2015)</ref> because in our experiments, DiSAN optimized by Adadelta can achieve more stable performance than Adam optimized one. Initial learning rate is set to 0.5. All weight matrices are initialized by Glorot Initialization <ref type="bibr" target="#b6">(Glorot and Bengio 2010)</ref>, and the biases are initialized with 0. We initialize the word embedding in x by 300D GloVe 6B pre-trained vectors <ref type="bibr" target="#b31">(Pennington, Socher, and Manning 2014)</ref>. The Out-of-Vocabulary words in training set are randomly initialized by uniform distribution between (−0.05, 0.05). The word embeddings are fine-tuned during the training phrase. We use Dropout <ref type="bibr" target="#b38">(Srivastava et al. 2014</ref>) with keep probability 0.75 for language inference and 0.8 for sentiment analysis. The L2 regularization decay factors γ are 5 × 10 −5 and 10 −4 for language inference and sentiment analysis, respectively. Note that the dropout keep probability and γ varies with the scale of corresponding dataset. Hidden units number d h is set to 300. Activation functions σ(·) are ELU (exponential linear unit) (Clevert, Unterthiner, and Hochreiter 2016) if not specified. All models are implemented with TensorFlow 2 and run on sin-  gle Nvidia GTX 1080Ti graphic card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Natural Language Inference</head><p>The goal of Natural Language Inference (NLI) is to reason the semantic relationship between a premise sentence and a corresponding hypothesis sentence. The possible relationship could be entailment, neutral or contradiction. We compare different models on a widely used benchmark, Stanford Natural Language Inference (SNLI) 3 <ref type="bibr" target="#b2">(Bowman et al. 2015)</ref> dataset, which consists of 549,367/9,842/9,824 (train/dev/test) premise-hypothesis pairs with labels. Following the standard procedure in <ref type="bibr" target="#b3">Bowman et al. (2016)</ref>, we launch two sentence encoding models (e.g., DiSAN) with tied parameters for the premise sentence and hypothesis sentence, respectively. Given the output encoding s p for the premise and s h for the hypothesis, the representation of relationship is the concatenation of s p , s h , s p − s h and s p s h , which is fed into a 300D fully connected layer and then a 3-unit output layer with softmax to compute a probability distribution over the three types of relationship.</p><p>For thorough comparison, besides the neural nets proposed in previous works of NLI, we implement five extra neural net baselines to compare with DiSAN. They help us to analyze the improvement contributed by each part of DiSAN and to verify that the two attention mechanisms proposed in Section 3 can improve other networks.</p><p>• Word Embedding with additive attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Word Embedding with s2t self-attention: DiSAN with</head><p>DiSA blocks removed.</p><p>• Multi-head with s2t self-attention: Multi-head attention <ref type="bibr">(Vaswani et al. 2017</ref>) (8 heads, each has 75 hidden units) with source2token self-attention. The positional encoding 3 https://nlp.stanford.edu/projects/snli/ method used in <ref type="bibr">Vaswani et al. (2017)</ref> is applied to the input sequence to encode temporal information. We find our experiments show that multi-head attention is sensitive to hyperparameters, so we adjust keep probability of dropout from 0.7 to 0.9 with step 0.05 and report the best result. • Bi-LSTM with s2t self-attention: a multi-dimensional source2token self-attention block is applied to the output of Bi-LSTM (300D forward + 300D backward LSTMs). • DiSAN without directions: DiSAN with the forward/backward masks M f w and M bw replaced with two diag-disabled masks M diag , i.e., DiSAN without forward/backward order information.</p><p>Compared to the results from the official leaderboard of SNLI in <ref type="table" target="#tab_1">Table 1</ref>, DiSAN outperforms previous works and improves the best latest test accuracy (achieved by a memory-based NSE encoder network) by a remarkable margin of 1.02%. DiSAN surpasses the RNN/CNN based models with more complicated architecture and more parameters by large margins, e.g., +2.32% to Bi-LSTM, +1.42% to Bi-LSTM with additive attention. It even outperforms models with the assistance of a semantic parsing tree, e.g., +3.52% to Tree-based CNN, +2.42% to SPINN-PI.</p><p>In the results of the five baseline methods and DiSAN at the bottom of <ref type="table" target="#tab_1">Table 1</ref>, we demonstrate that making attention multi-dimensional (feature-wise) or directional brings substantial improvement to different neural nets. First, a comparison between the first two models shows that changing token-wise attention to multi-dimensional/feature-wise attention leads to 3.31% improvement on a word embedding based model. Also, a comparison between the third baseline and DiSAN shows that DiSAN can substantially outperform multi-head attention by 1.45%. Moreover, a comparison between the forth baseline and DiSAN shows that the DiSA block can even outperform Bi-LSTM layer in context encoding, improving test accuracy by 0.64%. A comparison between the fifth baseline and DiSAN shows that directional self-attention with forward and backward masks (with temporal order encoded) can bring 0.96% improvement.</p><p>Additional advantages of DiSAN shown in <ref type="table" target="#tab_1">Table 1</ref> are its fewer parameters and compelling time efficiency. It is ×3 faster than widely used Bi-LSTM model. Compared to other models with competitive performance, e.g., 600D Bi-LSTM encoders with intra-attention (2.8M), 300D NSE encoders (3.0M) and 600D Bi-LSTM encoders with multidimensional attention (2.88M), DiSAN only has 2.35M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sentiment Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test Accu MV-RNN <ref type="bibr" target="#b36">(Socher et al. 2013)</ref> 44.4 RNTN <ref type="bibr" target="#b36">(Socher et al. 2013)</ref> 45.7 Bi-LSTM  49.8 Tree-LSTM <ref type="bibr" target="#b40">(Tai, Socher, and Manning 2015)</ref> 51.0 CNN-non-static <ref type="bibr" target="#b14">(Kim 2014)</ref> 48.0 CNN-Tensor <ref type="bibr" target="#b19">(Lei, Barzilay, and Jaakkola 2015)</ref> 51.2 NCSL <ref type="bibr" target="#b41">(Teng, Vo, and Zhang 2016)</ref> 51.1 LR-Bi-LSTM <ref type="bibr" target="#b32">(Qian, Huang, and Zhu 2017)</ref> 50  Sentiment analysis aims to analyze the sentiment of a sentence or a paragraph, e.g., a movie or a product review. We use Stanford Sentiment Treebank (SST) 4 <ref type="bibr" target="#b36">(Socher et al. 2013)</ref> for the experiments, and only focus on the fine-grained movie review sentiment classification over five classes, i.e., very negative, negative, neutral, positive and very positive. We use the standard train/dev/test sets split with 8,544/1,101/2,210 samples. Similar to Section 5.1, we employ a single sentence encoding model to obtain a sentence representation s of a movie review, then pass it into a 300D fully connected layer. Finally, a 5-unit output layer with softmax is used to calculate a probability distribution over the five classes.</p><p>In <ref type="table" target="#tab_3">Table 2</ref>, we compare previous works with DiSAN on test accuracy. To the best of our knowledge, DiSAN improves the last best accuracy (given by CNN-Tensor) by 0.52%. Compared to tree-based models with heavy use of the prior structure, e.g., MV-RNN, RNTN and Tree-LSTM, DiSAN outperforms them by 7.32%, 6.02% and 0.72%, respectively. Additionally, DiSAN achieves better performance than CNN-based models. More recent works tend to focus on lexicon-based sentiment analysis, by exploring sentiment lexicons, negation words and intensity words. 4 https://nlp.stanford.edu/sentiment/ Nonetheless, DiSAN still outperforms these fancy models, such as NCSL (+0.62%) and LR-Bi-LSTM (+1.12%). <ref type="figure">Figure 5</ref>: Fine-grained sentiment analysis accuracy vs. sentence length. The results of LSTM, Bi-LSTM and Tree-LSTM are from <ref type="bibr" target="#b40">Tai, Socher, and Manning (2015)</ref> and the result of DiSAN is the average over five random trials.</p><p>It is also interesting to see the performance of different models on the sentences with different lengths. In <ref type="figure">Figure 5</ref>, we compare LSTM, Bi-LSTM, Tree-LSTM and DiSAN on different sentence lengths. In the range of (5, 12), the length range for most movie review sentences, DiSAN significantly outperforms others. Meanwhile, DiSAN also shows impressive performance for slightly longer sentences or paragraphs in the range of <ref type="bibr">(25,</ref><ref type="bibr">38)</ref>. DiSAN performs poorly when the sentence length ≥ 38, in which however only 3.21% of total movie review sentences lie.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on Other NLP Tasks</head><p>Multi-Genre Natural Language Inference Multi-Genre Natural Language Inference (MultiNLI) 5 <ref type="bibr" target="#b46">(Williams, Nangia, and Bowman 2017)</ref> dataset consists of 433k sentence pairs annotated with textual entailment information. This dataset is similar to SNLI, but it covers more genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. However, MultiNLI is a quite new dataset, and its leaderboard does not include a session for the sentence-encoding only model. Hence, we only compare DiSAN with the baselines provided at the official website. The results of DiSAN and two sentence-encoding models on the leaderboard are shown in    <ref type="bibr" target="#b40">(Tai, Socher, and Manning 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Classifications</head><p>The goal of sentence classification is to correctly predict the class label of a given sentence in various scenarios. We evaluate the models on four sentence classification benchmarks of various NLP tasks, such as sentiment analysis and question-type classification. They are listed as follows. 1) CR: Customer review <ref type="bibr" target="#b10">(Hu and Liu 2004)</ref> of various products <ref type="bibr">(cameras, etc.)</ref>, which is to predict whether the review is positive or negative; 2) MPQA:</p><p>Opinion polarity detection subtask of the MPQA dataset <ref type="bibr" target="#b45">(Wiebe, Wilson, and Cardie 2005)</ref>; 3) SUBJ: Subjectivity dataset (Pang and Lee 2004) whose labels indicate whether each sentence is subjective or objective; 4) TREC: TREC question-type classification dataset <ref type="bibr" target="#b20">(Li and Roth 2002)</ref>. The experimental results of DiSAN and existing methods are shown in <ref type="table" target="#tab_8">Table 5</ref>.  particular, we will focus primarily on the probability in forward/backward DiSA blocks ( <ref type="figure" target="#fig_3">Figure 6</ref>), forward/backward fusion gates F in Eq.(19) <ref type="figure" target="#fig_4">(Figure 7)</ref>, and the probability in multi-dimensional source2token self-attention block <ref type="figure" target="#fig_5">(Figure 8)</ref>. For the first two, we desire to demonstrate the dependency at token level, but attention probability in DiSAN is defined on each feature, so we average the probabilities along the feature dimension. We select two sentences from SNLI test set as examples for this case study. Sentence 1 is Families have some dogs in front of a carousel and sentence 2 is volleyball match is in progress between ladies.  <ref type="figure" target="#fig_3">Figure 6</ref> shows that1) semantically important words such as nouns and verbs usually get large attention, but stop words (am, is, are, etc.) do not; 2) globally important words, e.g., volleyball, match, ladies in sentence 1 and dog, front, carousel in sentence 2, get large attention from all other words; 3) if a word is important to only some of the other words (e.g. to constitute a phrase or sense-group), it gets large attention only from these words, e.g., attention between progress, between in sentence1, and attention between families, have in sentence 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>This also shows that directional information can help to generate context-aware word representation with temporal order encoded. For instance, for word match in sentence 1, its forward DiSA focuses more on word volleyball, while its backward attention focuses more on progress and ladies, so the representation of word match contains the essential information of the entire sentence, and simultaneously includes the positional order information.</p><p>In addition, forward and backward DiSAs can focus on different parts of a sentence. For example, the forward one in sentence 2 pays attention to the word families, whereas the backward one focuses on the word carousel. Since forward and backward attentions are computed separately, it avoids normalization over multiple significant words to weaken their weights. Note that this is a weakness of traditional attention compared to RNN, especially for long sentences. In <ref type="figure" target="#fig_4">Figure 7</ref>, we show that the gate value F in Eq.(19). The gate combines the input and output of masked self-attention. It tends to selects the input representation h instead of the output s if the corresponding weight in F is large. This shows that the gate values for meaningless words, especially stop words is small. The stop words themselves cannot contribute important information, so only their semantic relations to other words might help to understand the sentence. Hence, the gate tends to use their context features given by masked self-attention. In <ref type="figure" target="#fig_5">Figure 8</ref>, we show the two multi-dimensional source2token self-attention score vectors of the same word in the two sentences, by their heatmaps. The first pair has two sentences: one is The glass bottle is big, and another is A man is pouring a glass of tea. They share the same word is glass with different meanings. The second pair has two sentences: one is The restaurant is about to close and another is A biker is close to the fountain. It can be seen that the two attention vectors for the same words are very different due to their different meanings in different contexts. This indicates that the multi-dimensional attention vector is not redundant because it can encode more information than one single score used in traditional attention and it is able to capture subtle difference of the same word in different contexts or sentences. Additionally, it can also alleviate the weakness of the attention over long sequence, which can avoid normalization over entire sequence in traditional attention only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose two novel attention mechanisms, multi-dimensional attention and directional self-attention. The multi-dimensional attention performs a feature-wise selection over the input sequence for a specific task, and the directional self-attention uses the positional masks to produce the context-aware representations with temporal information encoded. Based on these attentions, Directional Self-Attention Network (DiSAN) is proposed for sentenceencoding without any recurrent or convolutional structure. The experiment results show that DiSAN can achieve stateof-the-art inference quality and outperform existing works (LSTM, etc.) on a wide range of NLP tasks with fewer parameters and higher time efficiency.</p><p>In future work, we will explore the approaches to using the proposed attention mechanisms on more sophisticated tasks, e.g. question answering and reading comprehension, to achieve better performance on various benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Traditional (additive/multiplicative) attention and (b) multi-dimensional attention. z i denotes alignment score f (x i , q), which is a scalar in (a) but a vector in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Three positional masks: (a) is the diag-disabled mask M diag ; (b) and (c) are forward mask M f w and backward mask M bw , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Directional self-attention network (DiSAN) single vector. The idea of using both forward and backward attentions is inspired by Bi-directional LSTM (Bi-LSTM)<ref type="bibr" target="#b7">(Graves, Jaitly, and Mohamed 2013)</ref>, in which forward and backward LSTMs are used to encode long-range dependency from different directions. In Bi-LSTM, LSTM combines the context-aware output with the input by multi-gate. The fusion gate used in DiSA shares the similar motivation. However, DiSAN has fewer parameters, simpler structure and better efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Attention probability in forward/backward DiSA blocks for the two example sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Fusion Gate F in forward/backward DiSA blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>(a) glass in pair 1 (b) close in pair 2 Two pairs of attention probability comparison of same word in difference sentence contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Experimental results for different methods on SNLI. |θ|: the number of parameters (excluding word embedding part).T(s)/epoch: average time (second) per epoch. Train Accu(%) and Test Accu(%): the accuracy on training and test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy of fine-grained sentiment analysis on Stanford Sentiment Treebank (SST) dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Note that the prediction accuracies of Matched and Mismatched test datasets are obtained by submitting our test results to Kaggle open evaluation platforms 6 : MultiNLI Matched Open Evaluation and MultiNLI Mismatched Open Evaluation.</figDesc><table><row><cell>Semantic Relatedness The task of semantic relatedness</cell></row><row><cell>aims to predict a similarity degree of a given pair of sen-</cell></row><row><cell>tences. We show an experimental comparison of different</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Experimental results of prediction accuracy for different methods on MultiNLI.</figDesc><table><row><cell cols="4">methods on Sentences Involving Compositional Knowledge</cell></row><row><cell cols="4">(SICK) 7 dataset (Marelli et al. 2014). SICK is composed</cell></row><row><cell cols="4">of 9,927 sentence pairs with 4,500/500/4,927 instances for</cell></row><row><cell cols="4">train/dev/test. The regression module on the top of DiSAN is</cell></row><row><cell cols="4">introduced by Tai, Socher, and Manning (2015). The results</cell></row><row><cell cols="4">in Table 4 show that DiSAN outperforms the models from</cell></row><row><cell cols="4">previous works in terms of Pearson's r and Spearman's ρ</cell></row><row><cell>indexes.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Pearson's r Spearman's ρ</cell><cell>MSE</cell></row><row><cell>Meaning Factory a</cell><cell>.8268</cell><cell>.7721</cell><cell>.3224</cell></row><row><cell>ECNU b</cell><cell>.8414</cell><cell>/</cell><cell>/</cell></row><row><cell>DT-RNN c</cell><cell cols="3">.7923 (.0070) .7319 (.0071) .3822 (.0137)</cell></row><row><cell>SDT-RNN c</cell><cell cols="3">.7900 (.0042) .7304 (.0042) .3848 (.0042)</cell></row><row><cell cols="4">Cons. Tree-LSTM d .8582 (.0038) .7966 (.0053) .2734 (.0108)</cell></row><row><cell cols="4">Dep. Tree-LSTM d .8676 (.0030) .8083 (.0042) .2532 (.0052)</cell></row><row><cell>DiSAN</cell><cell cols="3">.8695 (.0012) .8139 (.0012) .2879 (.0036)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Experimental results for different methods on SICK sentence relatedness dataset. The reported accuracies are the mean of five runs (standard deviations in parentheses). Cons. and Dep. represent Constituency and Dependency, respectively. a (Bjerva et al. 2014), b (Zhao, Zhu, and Lan 2014),</figDesc><table /><note>c (Socher et al. 2014), d</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>To gain a closer view of what dependencies in a sentence can be captured by DiSAN, we visualize the attention probability p(z = i|x, x j ) or alignment score by heatmaps. In 7 http://clic.cimec.unitn.it/composes/sick.html</figDesc><table><row><cell>Model</cell><cell>CR</cell><cell>MPQA</cell><cell>SUBJ</cell><cell>TREC</cell></row><row><cell>cBoW a</cell><cell>79.9</cell><cell>86.4</cell><cell>91.3</cell><cell>87.3</cell></row><row><cell>Skip-thought b</cell><cell>81.3</cell><cell>87.5</cell><cell>93.6</cell><cell>92.2</cell></row><row><cell>DCNN c</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>93.0</cell></row><row><cell>AdaSent d</cell><cell cols="4">83.6 (1.6) 90.4 (0.7) 92.2 (1.2) 91.1 (1.0)</cell></row><row><cell>SRU e</cell><cell cols="4">84.8 (1.3) 89.7 (1.1) 93.4 (0.8) 93.9 (0.6)</cell></row><row><cell cols="5">Wide CNNs e 82.2 (2.2) 88.8 (1.2) 92.9 (0.7) 93.2 (0.5)</cell></row><row><cell>DiSAN</cell><cell cols="4">84.8 (2.0) 90.1 (0.4) 94.2 (0.6) 94.2 (0.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Experimental results for different methods on various sentence classification benchmarks. The reported accuracies on CR, MPQA and SUBJ are the mean of 10-fold cross validation, the accuracies on TREC are the mean of dev accuracies of five runs. All standard deviations are in parentheses. a<ref type="bibr" target="#b25">(Mikolov et al. 2013a</ref>), b<ref type="bibr" target="#b16">(Kiros et al. 2015)</ref>,</figDesc><table><row><cell>(Kalchbrenner, Grefenstette, and Blunsom 2014), d (Zhao,</cell></row><row><cell>Lu, and Poupart 2015), e (Lei and Zhang 2017).</cell></row></table><note>c</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Codes and pre-trained models for experiments can be found at https://github.com/taoshen58/DiSAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.tensorflow.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.nyu.edu/projects/bowman/multinli/ 6 https://inclass.kaggle.com/c/multinli-matched-openevaluation and https://inclass.kaggle.com/c/multinli-mismatchedopen-evaluation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This research was funded by the Australian Government through the Australian Research Council (ARC) under grant 1) LP160100630 partnership with Australia Government Department of Health, and 2) LP150100671 partnership with Australia Research Alliance for Children and Youth (ARACY) and Global Business College Australia (GBCA).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Der Goot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="642" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Characteraware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Structural attention neural networks for improved sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Potamianos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01811</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<title level="m">Training rnns as fast as cnns</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00185</idno>
		<title level="m">When are tree structures necessary for deep learning of representations? arXiv preprint</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning natural language inference using bidirectional lstm model and inner-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09090</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linguistically regularized lstms for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Context-sensitive lexicon features for neural sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noam</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Orderembeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A broadcoverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="271" to="277" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
