<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016">2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Submitted</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cybernetics</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">X</forename><surname>Vol</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">X</forename><surname>No</surname></persName>
						</author>
						<title level="a" type="main">Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="j" type="main">WILL BE INSERTED BY THE EDITOR)</title>
						<imprint>
							<biblScope unit="volume">1</biblScope>
							<date type="published" when="2016">2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action Recognition</term>
					<term>Trajectory</term>
					<term>Color Encoding</term>
					<term>Convolutional Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (ConvNets) have recently shown promising performance in many computer vision tasks, especially image-based recognition. How to effectively apply ConvNets to sequence-based data is still an open problem. This paper proposes an effective yet simple method to represent spatio-temporal information carried in 3D skeleton sequences into three 2D images by encoding the joint trajectories and their dynamics into color distribution in the images, referred to as Joint Trajectory Maps (JTM), and adopts ConvNets to learn the discriminative features for human action recognition. Such an image-based representation enables us to fine-tune existing Con-vNets models for the classification of skeleton sequences without training the networks afresh. The three JTMs are generated in three orthogonal planes and provide complimentary information to each other. The final recognition is further improved through multiply score fusion of the three JTMs. The proposed method was evaluated on four public benchmark datasets, the large NTU RGB+D Dataset, MSRC-12 Kinect Gesture Dataset (MSRC-12), G3D Dataset and UTD Multimodal Human Action Dataset (UTD-MHAD) and achieved the state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H UMAN action recognition is an important problem in computer vision due to its wide applications in video surveillance, human computer interfaces, robotics, etc. Despite significant research efforts over the past few decades <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b20">[21]</ref>, accurate recognition of human actions from RGB video sequences is still an unsolved problem. With the advent of easy-to-use and low-cost depth sensors such as MS Kinect sensors, human action recognition from RGB-D (Red, Green, Blue and Depth) data has attracted increasing attention and many applications have been developed <ref type="bibr" target="#b21">[22]</ref> in recent years, due to the advantages of depth information over conventional RGB video, e.g. being insensitive to illumination changes and reliable to estimate body silhouette and skeleton <ref type="bibr" target="#b22">[23]</ref>. Since the first work <ref type="bibr" target="#b23">[24]</ref> reported in 2010, many methods <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[29]</ref> have been proposed using specifically hand-crafted feature descriptors extracted from depth. As the extraction of skeletons from depth maps <ref type="bibr" target="#b22">[23]</ref> has become increasingly robust, more  and more hand-designed skeleton features <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b41">[42]</ref> have been devised to capture spatial configuration, and Dynamic Time Warpings (DTWs), Fourier Temporal Pyramid (FTP) or Hidden Markov Models (HMMs) are employed to model temporal information. However, these hand-crafted features are always shallow and dataset-dependent. Recently, Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b46">[47]</ref> have also been adopted for action recognition from skeleton data. RNNs tend to overemphasize the temporal information especially when the training data is not sufficient, leading to overfitting. Up to date, it remains unclear how skeleton sequences could be effectively represented and fed to deep neural networks for recognition. For example, one can conventionally consider a skeleton sequence as a set of individual frames with some form of temporal smoothness, or as a subspace of poses or pose features, or as the output of a neural network encoder. Which one among these and other possibilities would result in the best representation in the context of action recognition is not well understood.</p><p>In this paper, we present an effective yet simple method that represent both spatial configuration and dynamics of joint trajectories into three texture images through color encoding, referred to as Joint Trajectory Maps (JTMs), as the input of ConvNets for action recognition. Such image-based representation enables us to fine-tune existing ConvNets models trained on ImageNet for classification of skeleton sequences without training the whole deep networks afresh. The three JTMs are complimentary to each other, and the final recognition accuracy is improved largely by a late score fusion method. One of the challenges in action recognition is how to properly model and use the spatio-temporal information. The commonly arXiv:1612.09401v1 [cs.CV] 30 Dec 2016</p><p>• A compact, effective yet simple image-based representation is proposed to represent the spatio-temporal information carried in the 3D skeleton sequences into three 2D images by encoding the dynamics of joint trajectories into three complementary Joint Trajectory Maps. • To overcome the drawbacks of ConvNets not being rotation-invariant, and to make the proposed method suitable for cross-view action recognition, it is proposed to rotate the skeleton data to not only mimic the multiple views but also to augment data effectively for training. • The proposed method was evaluated on four popular public benchmark datasets, namely, the large NTU RGB+D Dataset <ref type="bibr" target="#b45">[46]</ref>, MSRC-12 Kinect Gesture Dataset (MSRC-12) <ref type="bibr" target="#b47">[48]</ref>, G3D Dataset <ref type="bibr" target="#b48">[49]</ref> and UTD Multimodal Human Action Dataset (UTD-MHAD) <ref type="bibr" target="#b49">[50]</ref>, and achieved the state-of-the-art recognition results.</p><p>This paper is an extension of the works presented in <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Unlike <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> where skeletons are assumed to have been sufficiently sampled and discrete joints are drawn onto images using a pen whose size is properly set, this paper employs joint trajectories and proposes to rotate skeletons to mimic multiple views for cross-view action recognition and data augmentation. In addition, this paper adopts multiply score fusion to improve the final recognition accuracy. Extensive experiments and detailed analysis are also presented in this paper. The rest of this paper is organized as follows. An overview of related works is given in Section II. Details of the proposed method are described in Section III. Evaluation of the proposed method on four datasets and analysis of the results are reported in Section IV. Section V concludes the paper with remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>An extensive review on RGB-D based action recognition is beyond the scope of this paper. Readers are referred to <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b54">[55]</ref> for a comprehensive survey. In this section, the work related to the proposed method is briefly reviewed, including skeleton-based 3D action representation and deep learning based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Skeleton-Based 3D Action Representation</head><p>Skeleton based 3D action representation can be generally divided into three categories <ref type="bibr" target="#b53">[54]</ref>: joints, groups of joints , and joint dynamics. Joint representation captures the correlation of the body joints by extracting spatial descriptor <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b55">[56]</ref>- <ref type="bibr" target="#b58">[59]</ref>, geometric descriptor <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b59">[60]</ref> or key poses <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b60">[61]</ref>- <ref type="bibr" target="#b62">[63]</ref>. The groups of joints aim to detect the discriminative subsets of joints to differentiate actions. Methods such as <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b63">[64]</ref>- <ref type="bibr" target="#b66">[67]</ref> focus on mining the subsets of most discriminative joints or consider the correlation of predefined subsets of joints.</p><p>Joint dynamics focuses on modeling the dynamics of either subsets or all joints of a skeleton. In <ref type="bibr" target="#b31">[32]</ref> 3D trajectories of joints are projected into three 2D trajectories, and histogram of oriented displacement is calculated to describe the three 2D trajectories, with each displacement in the trajectory voting its length in the histogram of orientation angles. Chaudhry et al. <ref type="bibr" target="#b36">[37]</ref> divided the fully body skeleton into several body parts represented by joints, including the upper body, lower body, left/right arms and left/right legs. A shape context feature is computed by considering the directions of a set of equidistant points sub-sampled over the segments of each body part. A skeleton sequence is finally represented as a set of time series of features such as position, tangent and shape context feature. These time series are further divided into several temporal scales, and each individual feature series is modeled using a linear dynamic system. The estimated parameters of all series are used to describe the dynamics of the skeleton sequence. In <ref type="bibr" target="#b30">[31]</ref> a skeleton sequence is modeled as a continuous and differentiable function of the body joint locations over time. The local 3D body pose is characterized by the current joint locations and differential properties like speed and acceleration of the joints. Slama et al. <ref type="bibr" target="#b67">[68]</ref> represented each action sequence as a linear dynamic system that produces 3D joint trajectories. Autoregressive moving average model was adopted to represent the dynamics by means of observability matrix which embeds the parameters of the model. In <ref type="bibr" target="#b68">[69]</ref> the dynamic forest model was proposed and a set of autoregressive trees was adopted. Each node in the probabilistic autoregressive tree stores a multivariate normal distribution with a fixed covariance matrix, and the set of Gaussian posteriors estimated by the forest are used to calculate the forest posterior. Shao et al. <ref type="bibr" target="#b69">[70]</ref> proposed to use a class of integral invariants to describe motion trajectories by calculating the line integral of a class of kernel functions at multiple scales along the motion trajectory. In <ref type="bibr" target="#b38">[39]</ref> the authors represented the 3D coordinates of joints and their changes over time as a trajectory in the Riemannian manifold, and the action recognition is formulated as the problem of computing the similarity between the shape of trajectories. In this paper, we propose to use color to encode the dynamics of trajectories, and model the spatial-temporal information carried in a skeleton sequence through shape and textures. ConvNets are used to learn deep hierarchy features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Leaning Based Action Recognition</head><p>The exiting deep learning approaches to action recognition can be generally divided into four categories based on how an input sequence is represented and fed to a deep neural network. The first category views a video either as a set of still images <ref type="bibr" target="#b70">[71]</ref> or as a short and smooth transition between similar frames <ref type="bibr" target="#b71">[72]</ref>, and each color channel of the images is fed to one channel of a ConvNet. Although suboptimal, considering the video as a bag of static frames gives reasonable results. The second category is to represent a video as a volume and extends ConvNets to a third, temporal dimension <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref> replacing 2D filters with 3D ones. So far, this approach has produced little benefits, probably due to the lack of annotated training data. The third category is to treat a video as a sequence of images and feed the sequence to a RNN <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>. A RNN is typically considered as memory cells, which are sensitive to both short as well as long term patterns. It parses the video frames sequentially and encodes the frame-level information in their memory. However, using RNNs has not given an improvement over temporal pooling of convolutional features <ref type="bibr" target="#b70">[71]</ref> or over hand-crafted features. The last category is to represent a video in one or multiple compact images and adopt available trained ConvNet architectures for fine-tuning <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>. This approach has achieved state-of-the-art results on many RGB and depth/skeleton datasets. The proposed method falls into this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head><p>The proposed method consists of four major components, as illustrated in <ref type="figure">Fig. 2</ref>, rotation to mimic the multiple views, construction of three JTMs as the input of the ConvNets in three orthogonal planes from skeleton sequences, training the three ConvNets to learn discriminative features, and multiply score fusion for final classification. In the following sections, the four components are detailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rotation</head><p>A skeleton is often represented by a set of joints in 3D space with respect to the real-world coordinate system centered at the optical central of the RGB-D camera. By rotating the skeleton data, it can 1) mimic multi-views for crossview action recognition; 2) enlarge the data for training and overcome the drawback of ConvNets usually being not viewinvariant.</p><p>The rotation was performed with a fixed step of 15 • along the polar angle θ and azimuthal angle ψ, in the range of</p><formula xml:id="formula_0">[0 • , 45 • ] for θ and [−45 • , 45 • ] for ψ.</formula><p>The ranges of θ and ψ would cover the possible views considering that the JTMs are generated by projecting the trajectories onto the three orthogonal planes as detailed below.</p><p>Let Tr y be the transform around y axis (right-handed coordinate system) and Tr x be the transform around x axis. The coordinates (x r , y r , z r ) of a joint at (x, y, z) after rotation can be expressed as</p><formula xml:id="formula_1">[x r , y r , z r , 1] T = Tr y Tr x x, y, z, 1 T (1) where Tr y = R y (ψ) T y (ψ) 0 1 ; Tr x = R x (θ) T x (θ) 0 1 ,<label>(2)</label></formula><p>and</p><formula xml:id="formula_2">Ry(ψ) =   1 0 0 0 cos(ψ) − sin(ψ) 0 sin(ψ) cos(ψ)   Ty(ψ) =   0 z · sin(ψ) z · (1 − cos(ψ))   ; Rx(θ) =   cos(θ) 0 sin(θ) 0 1 0 − sin(θ) 0 cos(θ)   Tx(θ) =   −z · sin(θ) 0 z · (1 − cos(θ))   .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Construction of JTMs</head><p>We argue that an effective JTM should have the following properties to keep the spatial-temporal information of an action:</p><p>• The joints or group of joints should be distinct in the JTM such that the spatial information of the joints is well reserved. • The JTM should encode effectively the temporal evolution, i.e. trajectories of the joints, including the direction and speed of joint motions. • The JTM should be able to encode the difference in motion among the different joints or parts of the body to reflect how the joints are synchronized during the action. Specifically, a JTM can be recursively defined as follows</p><formula xml:id="formula_3">JT M i = JT M i−1 + f (i),<label>(3)</label></formula><p>where f (i) is a function encoding the spatial-temporal information at frame or time-stamp i. Since a JTM is accumulated over the period of an action, f (i) has to be carefully defined such that the JTM for an action sample has the required properties discussed above and the accumulation over time has little adverse impact on the spatial-temporal information that has already been encoded in the JTM. This paper proposes to use hue, saturation and brightness to encode the spatialtemporal motion patterns. 1) Joint Trajectory Maps: Assume an action H has n frames of skeletons and each skeleton consists of m joints. The skeleton sequence is denoted as</p><formula xml:id="formula_4">H = {F 1 , F 2 , ..., F n }, where F i = {P i 1 , P i 2 , .</formula><p>.., P i m } is a vector of joint coordinates of frame i, and P i j is the 3D coordinates of the j th joint in frame i. The skeleton trajectory T for an action of n frames consists of the trajectories of all joints and is defined as:</p><formula xml:id="formula_5">T = {T 1 , T 2 , · · · , T i , · · · , T n−1 },<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">T i = {t i 1 , t i 2 , ..., t i m } = F i+1 − F i , and the k th joint trajectory is t i k = P i+1 k − P i k . A simple form of function f (i) would be T i , that is, f (i) = T i = {t i 1 , t i 2 , ..., t i m }.<label>(5)</label></formula><p>The skeleton trajectory is projected to three orthogonal planes, i.e. three Cartesian planes of the real world coordinates of the camera, to form three JTMs. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the three projected trajectories of the right hand joint for action "right hand draw circle (clockwise)" in the UTD-MHAD dataset. It can be seen that the spatial information of this joint over the period of the action is well represented in the JTMs but the direction of the motion is lost.</p><p>2) Encoding Joint Motion Direction: To capture the motion direction in the JTM, it is proposed to use hue to "color" the joint trajectories over the action period. Different colormaps may be chosen. In this paper, the jet colormap, ranging from blue to red, and passing through the colors cyan, yellow, and orange, is adopted. Let the color of a joint trajectory be C, and the length of the trajectory be L, and C l , l ∈ (0, L) be the color at position l of a trajectory. For the q th trajectory T q from 1 to n − 1, a color C l , where l = q n−1 × L is assigned to location l of the joint trajectory, making the entire trajectory colored over the period of the sequence as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>Herein, a trajectory with color is denoted as C t i k and the function f (i) becomes:</p><formula xml:id="formula_7">f (i) = {C t i 1 , C t i 2 , ..., C t i m }.<label>(6)</label></formula><p>Fig <ref type="figure" target="#fig_4">. 5</ref> shows the front JTM of action "right hand draw circle (clockwise)" in the UTD-MHAD <ref type="bibr" target="#b49">[50]</ref> dataset. Sub-figure <ref type="formula">(1)</ref> is joint trajectories and sub-figure <ref type="formula" target="#formula_1">(2)</ref> is the trajectories with motion direction being encoded with hue. The color variations along the trajectories represent the motion direction. 3) Encoding Body Parts: Many actions, especially complex actions, often involve multiple body parts and these body parts move in a coordinating manner. It is important to capture such coordination in the JTMs. To distinguish different body parts, multiple colormaps are employed. Body parts can be defined at different levels of granularity. For example, each joint can be considered independently as a "part" and is assigned to one colormap, or several groups of joints can be defined and all joints in each group are assigned to the same colormap and colormaps are chosen randomly to each group. Since arms and legs often move more than other body parts, a body is divided into three parts in this paper. According to the joint configuration for Kinect V1 skeleton as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the left body part consists of left shoulder, left elbow, left wrist, left hand, left hip, left knee, left ankle and left foot, the right body part consists of right shoulder, right elbow, right wrist, right hand, right hip, right knee, right ankle and right foot and the middle part consists of head, neck, torso and hip center. The three parts are assigned to three colormaps (C1, C2, C3) respectively, where C1 is the same as C, i.e. the jet colormap, C2 is a colormap with reversely-ordered colors of C1, and C3 is a gray-scale map ranging from light gray to black. Let the trajectory encoded by multiple colormaps be M C t i k . Function f (i) can be expressed as:</p><formula xml:id="formula_8">f (i) = {M C t i 1 , M C t i 2 , ..., M C t i m }.<label>(7)</label></formula><p>The effect of encoding body parts with different colors for action "right hand draw circle (clockwise)" is illustrated in <ref type="figure" target="#fig_2">Fig. 5, sub-figure (3)</ref>. 4) Encoding Motion Magnitude: Motion magnitude is one of the important factors in human motion. For an action, large magnitude of motion is likely to carry discriminative information. This paper proposes to encode the motion magnitude of joints into saturation and brightness so that the changes in motion would result in texture in the JMTs. Such texture is expected to be beneficial for ConvNets to learn discriminative features. For joints with high motion magnitude or speed, high saturation will be assigned. Specifically, the saturation is set to range from s min to s max . Given a trajectory, its saturation S i j along the path of the trajectory could be calculated as</p><formula xml:id="formula_9">S i j = v i j max{v} × (s max − s min ) + s min<label>(8)</label></formula><p>where v i j is the speed of jth joint at the ith frame.</p><formula xml:id="formula_10">v i j = P i+1 j − P i j 2<label>(9)</label></formula><p>Let a trajectory modulated by saturation be M C s t i k , function f (i) is refined as:</p><formula xml:id="formula_11">f (i) = {M C s t i 1 , M C s t i 2 , ..., M C s t i m }<label>(10)</label></formula><p>For the sample example in <ref type="figure" target="#fig_4">Fig. 5</ref>, the encoding effect can be seen in the sub-figures <ref type="formula" target="#formula_5">(4)</ref>, where the slow motion becomes diluted (e.g. trajectory of knees and ankles) while the fast motion becomes saturated (e.g. the green part of the circle).</p><p>To further enhance the motion patterns in the JTMs, the brightness is modulated by the speed of joints. Given a trajectory t i j whose speed is v i j , its brightness B i j is computed as</p><formula xml:id="formula_12">B i j = v i j max{v} × (b max − b min ) + b min<label>(11)</label></formula><p>where b min and b max represent the range of the brightness. Let M C b t i k be the trajectory with brightness and function f (i) is then updated to:</p><formula xml:id="formula_13">f (i) = {M C b t i 1 , M C b t i 2 , ..., M C b t i m }.<label>(12)</label></formula><p>The effect of brightness modulation can be seen in sub-figure (5) of the example shown in in <ref type="figure" target="#fig_4">Fig. 5</ref>, where texture becomes apparent (e.g. the yellow parts of the circle). Finally, let M C sb t i k be the trajectory after encoding the motion magnitude into both saturation and brightness. Function f (i) can be expressed as:</p><formula xml:id="formula_14">f (i) = {M C sb t i 1 , M C sb t i 2 , ..., M C sb t i m }.<label>(13)</label></formula><p>As illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, sub-figure <ref type="formula" target="#formula_7">(6)</ref>, the motion variation enriches the texture in the final JTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ConvNets Training</head><p>After constructing the three JTMs on three orthogonal image planes, three ConvNets are fine-tuned individually, each Con-vNet is an AlexNet <ref type="bibr" target="#b78">[79]</ref>. The fine-tuning procedure is similar to the one in <ref type="bibr" target="#b78">[79]</ref>. The network weights are learned using the mini-batch stochastic gradient descent with the momentum being set to 0.9 and weight decay being set to 0.0005. All hidden weight layers use the rectification (RELU) activation function. At each iteration, a mini-batch of 256 samples is constructed by sampling 256 shuffled training samples. The images are resized to 256 × 256. The learning rate is set to 10 −2 for training from scratch and set to 10 −3 for fine-tuning with pre-trained models on ILSVRC-2012, and then it is decreased according to a fixed schedule. For each ConvNet the training undergoes 100 cycles and the learning rate decreases every 30 cycles. For all experiments, the dropout regularization ratio was set to 0.9 in order to reduce complex co-adaptations of neurons in the nets for both networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multiply Score Fusion</head><p>Given a testing skeleton sequence (sample), three JTMs are generated and fed into the three ConvNets respectively. Multiply score fusion is used to combine the outputs from the individual ConvNets. Specifically, the score vectors outputted by the three ConvNets are multiplied in an element-wise way, and the max score in the resultant vector is assigned as the probability of the test sequence. The index of this max score corresponds to the recognized class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The proposed method was evaluated on four public benchmark datasets: the large NTU RGB+D Dataset <ref type="bibr" target="#b45">[46]</ref>, MSRC-12 Kinect Gesture Dataset <ref type="bibr" target="#b47">[48]</ref>, G3D <ref type="bibr" target="#b48">[49]</ref> and UTD-MHAD <ref type="bibr" target="#b49">[50]</ref>. Experiments were conducted on the effectiveness of individual encoding scheme in the proposed method, the effectiveness of rotation, the role of fine-tuning, and the multiply score fusion compared with the max and average score fusion methods. The final recognition results were compared with the state-ofthe-art reported on the same datasets. In all experiments, the saturation and brightness range from 0% ∼ 100% (mapped to 0 ∼ 255 in the JTM images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation of Key Design Factors 1) Different Encoding Schemes:</head><p>The effectiveness of different encoding schemes (as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>) was evaluated on the G3D dataset, and the recognition accuracies are listed in <ref type="table" target="#tab_0">Table I</ref>. From <ref type="table" target="#tab_0">Table I</ref> we can see that the proposed encoding methods effectively capture spatio-temporal information. Each encoding method gradually amends more information to the JTMs for the three ConvNets to learn the discriminative features and improves the recognition. The three JTMs are complimentary to each other to improve recognition significantly through fusion.</p><p>2) Rotation: Rotation is adopted to mimic multiple views, and this simple process makes the proposed method capable of cross-view action recognition. At the same time, the rotation enlarges the training data and enables the method to work on small datasets. <ref type="table" target="#tab_0">Table II</ref> shows the comparison of the proposed method with and without rotation on the NTU RGB+D and G3D datasets. As expected, the rotation operation improves the performance of cross-view recognition largely (by almost 3.5 percentage points). 3) Fine-tuning vs. Training from Scratch: Even though the number of training samples per class is over 600 for the NTU RGB+D Dataset, fine-tuning with available models from ImageNet is still preferred in terms of recognition accuracy. <ref type="table" target="#tab_0">Table III</ref> shows the results of two settings, fine-tuning and training from scratch, on NTU RGB+D and G3D datasets. In both settings, no rotation was performed. Notice that finetuning improved the recognition by 5 percentage point on the NTU RGB+D Dataset and almost doubled the recognition accuracy on the small G3D Dataset compared to training from scratch. 4) Comparison of Three Score Fusion Methods: There are two common used late score fusion methods, namely, average score fusion method and max score fusion method. However, in this paper, we propose to adopt multiply score fusion which turns out to be more effective on the evaluated datasets. The comparison of these three score fusion methods on the four datasets for final recognition are listed in <ref type="table" target="#tab_0">Table IV</ref>. From the <ref type="table">Table we</ref> can see that on the evaluated four datasets, the multiply score fusion consistently outperformed the average and max score fusion methods. This verifies that the three JTMs are likely to be statistically independent and provide complementary information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. NTU RGB+D Dataset</head><p>To our best knowledge, NTU RGB+D Dataset is currently the largest action recognition dataset. The 3D data is captured by Kinect v2 cameras. The dataset has more than 56 thousand sequences and 4 million frames, containing 60 actions performed by 40 subjects aging between 10 and 35. It consists of front view, two side views and left, right 45 degree views. This dataset is challenging due to large intra-class and viewpoint variations.</p><p>For fair comparison and evaluation, the same protocol as that in <ref type="bibr" target="#b45">[46]</ref> was used. It has both cross-subject and crossview evaluation. In the cross-subject evaluation, samples of subjects <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> and 38 were used as training and samples of the remaining subjects were reserved for testing. In the cross-view evaluation, samples taken by cameras 2 and 3 were used as training, testing set includes the samples of camera 1. <ref type="table" target="#tab_4">Table V</ref> lists the performance of the proposed method and those reported before. From this Table we can see that our proposed method achieved the state-of-the-art results compared with both hand-crafted features and deep learning methods. The work <ref type="bibr" target="#b33">[34]</ref> focused only on single person action and could not model multi-person interactions well. Dynamic Skeletons method <ref type="bibr" target="#b37">[38]</ref> performed better than some RNN-based methods verifying the weakness of the RNNs <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b45">[46]</ref>, which only mines the short-term dynamics and tends to overemphasize the temporal information even on large training data. LSTM and its variants <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> performed better due to their ability to utilize long-term context compared to conventional RNNs, but it is still weak in exploiting spatial information. The proposed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cross subject Cross view Lie Group <ref type="bibr" target="#b33">[34]</ref> 50.08% 52.76% Dynamic Skeletons <ref type="bibr" target="#b37">[38]</ref> 60.23% 65.22% HBRNN <ref type="bibr" target="#b42">[43]</ref> 59.07% 63.97% 2 Layer RNN <ref type="bibr" target="#b45">[46]</ref> 56.29% 64.09% 2 Layer LSTM <ref type="bibr" target="#b45">[46]</ref> 60.69% 67.29% Part-aware LSTM <ref type="bibr" target="#b45">[46]</ref> 62.93% 70.27% ST-LSTM <ref type="bibr" target="#b46">[47]</ref> 65.20% 76.10% ST-LSTM+ Trust Gate <ref type="bibr" target="#b46">[47]</ref> 69.20% 77.70% Proposed Method 76.32% 81.08% method achieved the best results in both cross-subject and cross-view evaluation.</p><p>C. MSRC-12 Kinect Gesture Dataset MSRC-12 <ref type="bibr" target="#b47">[48]</ref> is a relatively large dataset for gesture/action recognition from 3D skeleton data captured by a Kinect sensor. The dataset has 594 sequences, containing 12 gestures by 30 subjects, 6244 gesture instances in total. The 12 gestures are: "lift outstretched arms", "duck", "push right", "goggles", "wind it up", "shoot", "bow", "throw", "had enough", "beat both", "change weapon" and "kick". For this dataset, crosssubjects protocol was adopted, that is, odd subjects were used for training and even subjects were for testing. <ref type="table" target="#tab_0">Table VI</ref> lists the performance of the proposed method and the results reported before. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) HGM <ref type="bibr" target="#b79">[80]</ref> 66.25% Pose-Lexicon <ref type="bibr" target="#b80">[81]</ref> 85.86% ELC-KSVD <ref type="bibr" target="#b81">[82]</ref> 90.22% Cov3DJ <ref type="bibr" target="#b56">[57]</ref> 91.70% SOS <ref type="bibr" target="#b51">[52]</ref> 94.27% Proposed Method 94.86%</p><p>The confusion matrix is shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. From the confusion matrix we can see that the proposed method distinguishes most of actions very well, but it is not very effective to distinguish "goggles" and "had enough" which shares the similar appearance of JTMs probably caused by 3D to 2D projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. G3D Dataset</head><p>Gaming 3D Dataset (G3D) <ref type="bibr" target="#b48">[49]</ref> focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: "punch right", "punch left", "kick right", "kick left", "defend", "golf swing", "tennis swing forehand", "tennis swing backhand", "tennis serve", "throw bowling ball", "aim and fire gun", "walk", "run", "jump", "climb", "crouch", "steer a car", "wave", "flap" and "clap". For this dataset, the first 4 subjects were used for training, the fifth for validation and the remaining 5 subjects were for testing as configured in <ref type="bibr" target="#b82">[83]</ref>. <ref type="table" target="#tab_0">Table VII</ref> compared the performance of the proposed method and those reported in <ref type="bibr" target="#b82">[83]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) Cov3DJ <ref type="bibr" target="#b56">[57]</ref> 71.95% ELC-KSVD <ref type="bibr" target="#b81">[82]</ref> 82.37% LRBM <ref type="bibr" target="#b82">[83]</ref> 90.50% SOS <ref type="bibr" target="#b51">[52]</ref> 95.45% Proposed Method 96.02%</p><p>The confusion matrix is shown in <ref type="figure" target="#fig_7">figure 7</ref>. From the confusion matrix we can see that the proposed method recognizes most of actions well. The proposed method outperformed LRBM. LRBM confused the actions among "tennis swing forehand" and "bowling", "golf" and "tennis swing backhand", "aim and fire gun" and "wave", "jump" and "walk", however, these actions are well distinguished by the proposed method likely because of the quality spatial information encoded in the JTMs. As for "aim and fire gun" and "wave", the proposed method could not distinguish them well without encoding the motion magnitude, but does well with the encoding of motion magnitude. However, the proposed method, confused "tennis swing forehand" and "tennis swing backhand". It's probably because the front and side projections of body shape of the two actions are too similar. <ref type="bibr" target="#b49">[50]</ref> is a multimodal action dataset, captured by one Microsoft Kinect camera and one wearable inertial sensor. This dataset contains 27 actions performed by 8 subjects (4 females and 4 males) with each subject performing each action 4 times. After removing three corrupted sequences, the dataset has 861 sequences. The actions are: "right arm swipe to the left", "right arm swipe to the right", "right hand wave", "two hand front clap", "right arm throw", "cross arms in the chest", "basketball shoot", "right hand draw x", "right hand draw circle (clockwise)", "right hand draw circle (counter clockwise)", "draw triangle", "bowling (right hand)", "front boxing", "baseball swing from right", "tennis right hand forehand swing", "arm curl (two arms)", "tennis serve", "two hand push", "right hand know on door", "right hand catch an object", "right hand pick up and throw", "jogging in place",    "walking in place", "sit to stand", "stand to sit", "forward lunge (left foot forward)" and "squat (two arms stretch out)". It covers sport actions (e.g. "bowling", "tennis serve" and "baseball swing"), hand gestures (e.g. "draw X", "draw triangle", and "draw circle"), daily activities (e.g. "knock on door", "sit to stand" and "stand to sit") and training exercises (e.g. "arm curl", "lung" and "squat"). For this dataset, crosssubjects protocol was adopted as in <ref type="bibr" target="#b49">[50]</ref>, namely, the data from the subjects numbered 1, 3, 5, 7 were used for training while subjects 2, 4, 6, 8 were used for testing. <ref type="table" target="#tab_0">Table VIII</ref> compares the performance of the proposed method and those reported in <ref type="bibr" target="#b49">[50]</ref>.  <ref type="bibr" target="#b49">[50]</ref> 79.10% Cov3DJ <ref type="bibr" target="#b56">[57]</ref> 85.58% SOS <ref type="bibr" target="#b51">[52]</ref> 86.97% Proposed Method 87.90%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. UTD-MHAD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UTD-MHAD</head><p>Please notice that the method used in <ref type="bibr" target="#b49">[50]</ref> is based on Depth and Inertial sensor data, not skeleton data alone.</p><p>The confusion matrix is shown in <ref type="figure" target="#fig_9">Fig. 8</ref>. This dataset is much more challenging compared to the previous two datasets. From the confusion matrix we can see that the proposed method can not distinguish some actions well, for example, "jog" and "walk". A probable reason is that the proposed encoding process is also a temporal normalization process. The actions "jog" and "walk" would be normalized to have similar JTMs after the encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion</head><p>In this paper, we adopt the three orthogonal planes of the natural real coordinates of the camera. One question is whether there are some orthogonal planes better than the    natural ones. Generally speaking, there possibly exist three orthogonal views which are better than the natural coordinates if the three views result in less self-occlusion among the joints for all actions. Since only very sparse 20 joints are used to represent the skeleton, the likelihood of such self-occlusion of the joints would be very small. Consequently, no particular three orthogonal views would be obviously superior to others. However, the depth camera only captures 2 1 2 D in the natural coordinates and the skeleton is estimated from the 2 1 2 D. It is likely that the natural coordinates could be slightly, but not significantly, better than other three orthogonal views.</p><p>To validate this, we conducted the following experiments on the G3D Dataset. Different three orthogonal views were generated by rotating the 3D points of joints and projecting them to the three orthogonal planes. The rotation was performed with a fixed step of 22.5 • along the polar angle θ and azimuthal angle ψ, both in the range of [−45 • , 45 • ]. Note that this range effectively covers all possible views since rotation beyond this range would result in swapping of views. Such swapping would not affect the recognition accuracy after fusion. <ref type="table" target="#tab_0">Table IX</ref> shows the recognition accuracies of different orthogonal views indicated by the values of θ and ψ. The results in <ref type="table" target="#tab_0">Table IX</ref> have shown small and insignificant variation of the recognition accuracy among the views and the natural coordinates produced the best result.</p><p>In this paper, we fuse three orthogonal image planes to improve the final accuracy. Another questions is whether adding more views will lead to better recognition. Some experiments were conducted on the G3D dataset to answer this question. Firstly, the views of the natural coordinates were fused with the views after rotating the points by the specified angles in θ and ψ. <ref type="table" target="#tab_13">Table X</ref> shows the results by fusing two pairs of three orthogonal planes, one is the natural coordinates and the other is specified by the rotation angles θ and ψ. The accuracies of all cases are almost same. We also evaluated the performance by fusing all views of the 9 coordinates including the natural ones, where θ  <ref type="table" target="#tab_0">Table XI</ref>. It can be seen that fusing views of multiple orthogonal coordinates did not improve the performance on this dataset. Similar results would be expected on other datasets for the reason explained above.</p><p>The above analysis and experiments have demonstrated that the three orthogonal views in the natural coordinates are likely to be sufficient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper addresses the problem of human action recognition by applying ConvNets to skeleton sequences. An effective method is proposed to project the joint trajectories to three orthogonal JTMs to encode the spatial-temporal information into texture patterns. The three JTMs are complementary to each other. Such image-based representation enables us to fine-tune the existing ConvNets models trained on image data for classification of skeleton sequences, without training the deep ConvNets afresh. The experimental results on the four datasets have shown the efficacy of the proposed encoding scheme. Extension of the proposed method to on-line action recognition is the focus of future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received XXX; revised XXX. This work was supported by the National Natural Science Foundation of China (grant 61571325) and Key Projects in the Tianjin Science &amp; Technology Pillar Program (grant 15ZCZD GX001900). (Corresponding author: Yonghong Hou) P. Wang and W. Li are with the Advanced Multimedia Research Lab, University of Wollongong, Wollongong, Australia. (e-mail: pw212@uowmail.edu.au; wanqing@uow.edu.au). C. Li and Y. Hou are with the School of Electronic Information Engineering, Tianjin University, Tianjin, China. (email:chuankunli@tju.edu.cn;houroy@tju.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The joint configuration for Kinect V1 skeleton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The trajectories projected onto three Cartesian planes for action "right hand draw circle (clockwise)" in UTD-MHAD [50]: (1) front plane; (2) top plane; (3) side plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>An example of colored coded joint trajectory with different colors reflecting the temporal order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Step-by-step illustration of the front JTM for action "right hand draw circle (clockwise)" from the UTD-MHAD<ref type="bibr" target="#b49">[50]</ref> dataset. (1) Joint trajectory map without encoding any motion direction and magnitude; (2) encoding joint motion direction in hue, where color variations indicate motion direction; (3) encoding body parts with different colormaps; (4) encoding motion magnitude into saturation; (5) encoding motion magnitude into brightness;<ref type="bibr" target="#b5">(6)</ref> final JTM with all encodings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The confusion matrix of the proposed method on the MSRC-12 Kinect gesture dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>d f i r e g u n w a l k j o g g i n g j u m p c l i m b c r o u c h s t e e r a c a r w a v e f l a p c l a p</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>The confusion matrix of the proposed method on the G3D Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>s w ip e le f t s w ip e r ig h t w a v e c la p t h r o w a r m c r o s s b a s k e t b a ll s h o o</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>The confusion matrix of the proposed method on the UTD-MHAD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>∈ {−22.5 • , 0 • , 22.5 • } and ψ ∈ {−22.5 • , 0 • , 22.5 • }, and all views of the 25 coordinates, where θ ∈ {−45 • , −22.5 • , 0 • , 22.5 • , 45 • } and ψ ∈ {−45 • , −22.5 • , 0 • , 22.5 • , 45 • } respectively. The results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF THE DIFFERENT ENCODING SCHEMES ON THE G3D DATASET IN TERMS OF RECOGNITION ACCURACY.</figDesc><table><row><cell>Techniques</cell><cell>Front</cell><cell>Top</cell><cell>Side</cell><cell>Fusion</cell></row><row><cell>Trajectory: t i 1 Trajectory: C t i 1 Trajectory: M C t i 1 Trajectory: M Cs t i 1 Trajectory: M C b t i 1 Trajectory: M C sb t i 1</cell><cell>65.45% 76.12% 79.98% 83.52% 84.46% 86.25%</cell><cell>72.18% 75.55% 78.25% 81.32% 84.68% 87.56%</cell><cell>73.54% 76.56% 79.40% 82.08% 85.60% 86.54%</cell><cell>80.58% 83.65% 87.68% 89.98% 93.84% 96.02%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>THE PROPOSED METHOD WITH AND WITHOUT ROTATION ON THE NTU RGB+D AND G3D DATASETS IN TERMS OF RECOGNITION ACCURACY.</figDesc><table><row><cell>Dataset</cell><cell>Without Rotation</cell><cell>With Rotation</cell></row><row><cell>NTU RGB+D (Cross Subject)</cell><cell>75.30%</cell><cell>76.32%</cell></row><row><cell>NTU RGB+D (Cross View)</cell><cell>77.67%</cell><cell>81.08%</cell></row><row><cell>G3D</cell><cell>95.12%</cell><cell>96.02%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISONS</head><label>III</label><figDesc>OF FINE-TUNING AND TRANINING FROM SCRATCH ON THE NTU RGB+D AND G3D DATASETS IN TERMS OF RECOGNITION ACCURACY.</figDesc><table><row><cell>Dataset</cell><cell>Training from Scratch</cell><cell>Fine-tuning</cell></row><row><cell>NTU RGB+D (Cross Subject)</cell><cell>72.50%</cell><cell>75.30%</cell></row><row><cell>NTU RGB+D (Cross View)</cell><cell>73.77%</cell><cell>77.67%</cell></row><row><cell>G3D</cell><cell>46.64%</cell><cell>94.65%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF THREE SCORE FUSION METHODS ON THE FOUR DATASETS IN TERMS OF RECOGNITION ACCURACY.</figDesc><table><row><cell>Dataset</cell><cell>Max</cell><cell>Average</cell><cell>Multiply</cell></row><row><cell>NTU RGB+D (Cross Subject)</cell><cell>73.56%</cell><cell>75.05%</cell><cell>76.32%</cell></row><row><cell>NTU RGB+D (Cross View)</cell><cell>78.43%</cell><cell>79.88%</cell><cell>81.08%</cell></row><row><cell>MSRC-12</cell><cell>91.70%</cell><cell>93.42%</cell><cell>94.86%</cell></row><row><cell>G3D</cell><cell>93.78%</cell><cell>94.65%</cell><cell>96.02%</cell></row><row><cell>UTD-MHAD</cell><cell>85.81%</cell><cell>86.42%</cell><cell>87.90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARATIVE</head><label>V</label><figDesc>ACCURACIES OF THE PROPOSED METHOD AND PREVIOUS METHODS ON NTU RGB+D DATASET.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF THE PROPOSED METHOD WITH THE EXISTING METHODS ON THE MSRC-12 KINECT GESTURE DATASET.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>OF THE PROPOSED METHOD WITH PREVIOUS METHODS ON THE G3D DATASET.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>OF THE PROPOSED METHOD WITH THE PREVIOUS METHODS ON UTD-MHAD DATASET.</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>ELC-KSVD [82]</cell><cell>76.19%</cell></row><row><cell>Kinect &amp; Inertial</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE IX THE</head><label>IX</label><figDesc>RECOGNITION ACCURACY (%) OF DIFFERENT ORTHOGNAL VIEWS.</figDesc><table><row><cell>ψ</cell><cell>θ</cell><cell>− 45 •</cell><cell>−22.5 •</cell><cell>0 •</cell><cell>22.5 •</cell><cell>45 •</cell></row><row><cell cols="2">−45 •</cell><cell>94.45</cell><cell>92.12</cell><cell>94.85</cell><cell>92.24</cell><cell>92.42</cell></row><row><cell cols="2">−22.5 •</cell><cell>95.40</cell><cell>94.45</cell><cell>94.45</cell><cell>92.73</cell><cell>92.24</cell></row><row><cell>0 •</cell><cell></cell><cell>94.45</cell><cell>95.05</cell><cell>95.12</cell><cell>94.85</cell><cell>94.15</cell></row><row><cell cols="2">22.5 •</cell><cell>94.85</cell><cell>94.85</cell><cell>94.45</cell><cell>94.85</cell><cell>93.45</cell></row><row><cell>45 •</cell><cell></cell><cell>92.24</cell><cell>95.00</cell><cell>94.85</cell><cell>94.15</cell><cell>94.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE X THE</head><label>X</label><figDesc>RESULTS OF FUSING THE ORIGINAL THREE ORTHOGONAL PLANES AND ROTATED THREE PLANES.</figDesc><table><row><cell>ψ</cell><cell>θ</cell><cell>− 45 •</cell><cell>−22.5 •</cell><cell>0 •</cell><cell>22.5 •</cell><cell>45 •</cell></row><row><cell cols="2">−45 •</cell><cell>94.45</cell><cell>93.45</cell><cell>94.85</cell><cell>95.15</cell><cell>95.12</cell></row><row><cell cols="2">−22.5 •</cell><cell>94.85</cell><cell>94.54</cell><cell>95.15</cell><cell>95.12</cell><cell>94.85</cell></row><row><cell>0 •</cell><cell></cell><cell>95.12</cell><cell>95.15</cell><cell>95.12</cell><cell>94.54</cell><cell>94.54</cell></row><row><cell cols="2">22.5 •</cell><cell>94.85</cell><cell>94.24</cell><cell>95.15</cell><cell>95.15</cell><cell>94.85</cell></row><row><cell>45 •</cell><cell></cell><cell>94.85</cell><cell>94.85</cell><cell>95.15</cell><cell>95.12</cell><cell>95.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XI THE</head><label>XI</label><figDesc>RESULTS FOR FUSING VIEWS OF MULTIPLE COORDINATES.</figDesc><table><row><cell>Number of Coordinates</cell><cell>Accuracy (%)</cell></row><row><cell>9</cell><cell>95.15</cell></row><row><cell>25</cell><cell>94.85</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Space-time gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition of human body motion using phase space constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="624" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An appearance-based representation of action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="307" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The recognition of human movement using temporal templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Actions sketch: A novel action representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="984" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Expandable data-driven graphical modeling of human actions based on salient postures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1499" to="1510" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Circuits and Systems for Video Technology</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning discriminative key poses for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1860" to="1870" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatio-temporal laplacian pyramid coding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="817" to="827" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient feature extraction, encoding, and classification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2593" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion part regularization: Improving action recognition via trajectory selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3698" to="3706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representations for action recognition: a genetic programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="158" to="170" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernelized multiview projection for robust action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rank pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal segment networks: towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with microsoft kinect sensor: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1318" to="1334" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Action recognition based on a bag of 3D points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing actions using depth motion maps-based histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1057" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Super normal vector for activity recognition using depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="804" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Range-sample depth feature for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="772" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Eigenjoints-based action recognition using Naive-Bayes-Nearest-Neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3D kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Histogram of oriented displacements (HOD): Describing trajectories of human joints for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1351" to="1357" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mining mid-level features for action recognition based on effective skeleton representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Digital Image Computing: Techniques and Applications (DICTA)</title>
		<meeting>International Conference on Digital Image Computing: Techniques and Applications (DICTA)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A new descriptor for multiple 3D motion trajectories recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4749" to="4754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bioinspired dynamic 3D discriminative skeletal features for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint angles similarities and HOG2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3-D human action recognition by shape analysis of motion trajectories on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1340" to="1352" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">R3DG features: Relative 3d geometry-based skeletal representations for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rolling rotations for recognizing human actions from 3D skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Latent maxmargin multitask learning with skelets for 3-D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4041" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning for skeleton based action recognition using regularized deep LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">NTU RGB+ D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatio-temporal LSTM with trust gates for 3D human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Instructing people for training gestural interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fothergill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Mentis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer-Human Interaction</title>
		<imprint>
			<publisher>ACM HCI</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">G3D: A gaming action dataset and real time action recognition evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2015 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Skeleton optical spectra based action recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human activity recognition from 3d data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3d skeleton-based human action classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="130" to="147" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">RGB-D-based action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="86" to="105" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploring the trade-off between accuracy and observational latency in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Laviola</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="420" to="436" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3D joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2466" to="2472" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spectral graph skeletons for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="417" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4513" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sequence of the most informative joints (smij): A new representation for human skeletal action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="38" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ongoing human action recognition with motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barnachon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouakaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boufama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guillou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical modeling of spatio-temporally composable human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="812" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Concurrent action detection with structural prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3136" to="3143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficient posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eweiwi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="428" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multimodal multipart learning for action recognition in depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2123" to="2129" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Accurate 3d action recognition using learning on the grassmann manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="556" to="567" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Efficient nonlinear markov models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1314" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Integral invariants for space motion trajectory matching and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2418" to="2432" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Convnets-based action recognition from depth maps through virtual cameras and pseudocoloring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Action recognition from depth maps using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Human-Machine Systems</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Annual Conference on Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A hierarchical model based on latent dirichlet allocation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2014 22nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2613" to="2618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning a pose lexicon for semantic action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Discriminative key pose extraction using extended lc-ksvd for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Digital Image Computing: Techniques and Applications (DICTA)</title>
		<meeting>International Conference on Digital Image Computing: Techniques and Applications (DICTA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">A generative restricted boltzmann machine based method for high-dimensional motion data modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="14" to="22" />
		</imprint>
	</monogr>
	<note>Computer Vision and Image Understanding</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">He is currently pursuing the PhD degree with the School of Computing and Information Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010, and received the MS degree in communication and information system from Tianjin University</title>
		<meeting><address><addrLine>Nanchang, China; Tianjin, China; Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Wollongong</orgName>
		</respStmt>
	</monogr>
	<note>S&apos;14) received the BE degree in network engineering from Nanchang University. His current research interests include computer vision and machine learning</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Wanqing Li received his PhD in electronic engineering from The University of Western Australia. He is an Associate Professor and Co-Director of Advanced Multimedia Research Lab (AMRL) of University of Wollongong, Australia. His research areas are 3D computer vision, 3D multimedia signal processing and medical image analysis</title>
		<imprint/>
	</monogr>
	<note>Dr. Li is a Senior Member of IEEE</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">2012 and received the MS degree in communication and information system from North University of China</title>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Taiyuan, China; Taiyuan, China; China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Chuankun Li received the BE degree in electronic information engineering from North University of China ; Tianjin University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently pursuing the Ph.D degree with School of electronic information engineering. His current research interests include computer vision and machine learning</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">he has been an associate professor of school of electronic and information engineering</title>
	</analytic>
	<monogr>
		<title level="m">Tianjin University. His research interests include computer vision, artificial intelligence and multimedia signal processing</title>
		<meeting><address><addrLine>Xian, China; Tianjin, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>1991, and the M.Eng. and Ph.D degrees both in communication and information system from Tianjin University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
