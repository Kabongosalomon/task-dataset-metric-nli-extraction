<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NodeNet: A Graph Regularised Neural Network for Node Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrey</forename><surname>Dabhi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robert Bosch Enigneering and Business Solutions</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<orgName type="institution">Nirma University</orgName>
								<address>
									<settlement>Bengaluru, Ahmedabad</settlement>
									<country>India, India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manojkumar</forename><surname>Parmar</surname></persName>
							<email>manojkumar.parmar@bosch.com</email>
							<affiliation key="aff1">
								<orgName type="department">Robert Bosch Enigneering and Business Solutions</orgName>
								<orgName type="institution">HEC Paris</orgName>
								<address>
									<settlement>Bengaluru, Jouy-en-Josas Cedex</settlement>
									<country>India, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NodeNet: A Graph Regularised Neural Network for Node Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world events exhibit a high degree of interdependence and connections, and hence data points generated also inherit the linkages. However, the majority of AI/ML techniques leave out the linkages among data points. The recent surge of interest in graph-based AI/ML techniques is aimed to leverage the linkages. Graph-based learning algorithms utilize the data and related information effectively to build superior models. Neural Graph Learning (NGL) is one such technique that utilizes a traditional machine learning algorithm with a modified loss function to leverage the edges in the graph structure. In this paper, we propose a model using NGL -NodeNet, to solve node classification task for citation graphs. We discuss our modifications and their relevance to the task. We further compare our results with the current state of the art and investigate reasons for the superior performance of NodeNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many machine learning tasks in the domain of computer vision and natural language processing have been revolutionized by end-to-end deep learning approaches. While deep learning can effectively capture patterns in structure data, there is an increasing number of tasks where representing data in the form of graphs is more intuitive and improves the performance significantly.</p><p>Graphs are relational in nature, in the sense that the connections amongst the nodes denote some form of relationship between them. A graph also provides mechanism to represent multiple relationships in the form of link orientation and edge-level features. More importantly graphs are ubiquitous. Complex molecular structures, citation networks, social networks, computation graphs, interaction graphs, etc. are all examples of graphs which can be used to solve different machine learning tasks more effectively. A graph based learning system can model the complex buying behaviour in a better way than the traditional models to make highly accurate recommendations. In fact as shown in <ref type="figure" target="#fig_0">Figure  1</ref> images can be considered as just a special case of graphs, where each node not lying on the edge has equal number of neighbours and all the nodes can be arranged in a planar systematic arrangement.</p><p>(a) 2D Convolution. Every pixel is considered as a node and the number of neighbours is determined by the size of filter. It takes weighted average values of green node and its neighbours.</p><p>(b) Graph Convolution. A simple operation to obtain the latent representation would be to take weighted average values of green node and its neighbours. In node classification, given a graph(s) with a set of labelled nodes the task is to learn a model which can accurately predict the label of the unlabelled nodes. The sets of labelled and unlabelled nodes may belong to the same graph or to disjoint graphs from the same dataset.</p><p>Representing data as graphs poses some very unique challenges. Graphs have varying sizes and topology. Persisting information about the identity of nodes across multiple batches is difficult. Graphs may contain loops and running exhaustive search is not practically feasible. Though majority of these challenges have been conquered by modern GCNs, they still suffer from over-smoothing and over-fitting.</p><p>In the following sections we discuss our approach to node classification and how we arrived at it. In Section 2, we discuss GCNs and GRNNs used in past. We also introduce NGL method, which is our inspiration behind NodeNet. In Section 3, we discuss our modifications to NGL. In Section 4, we present our results and our thoughts on why the modifications are relevant to this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Survey</head><p>Akin to CNNs, GCNs stack multiple graph convolutional layers to extract high-level node features, followed by fully-connected layers for classification. <ref type="figure">Figure 2</ref> shows a simple GCN with 2 convolutional layers for node classification. These networks are trained as end-to-end models without any separated steps for feature extraction or manipulation Other approach to node classification, inspired from transformer models used in natural language processing, employ graph-transformer layers and attention models. For example, Zhang et al. <ref type="bibr" target="#b1">[2]</ref> discusses Graph-BERT, inspired from BERT uses statistical and spectral approaches over samples subgraphs to generate input embedding and feed them to a graph-transformer based encoder. The output of the transformer is then used for classification. The complete architecture of Graph-BERT is shown in <ref type="figure">Figure 3</ref>. Other approaches like the one proposed by Sun et al. <ref type="bibr" target="#b2">[3]</ref>, follow a RNN like architecture which has proven to be quite good in NLP domain.</p><p>These approaches have shown very good results. However, the components have to be designed from scratch for each dataset. The approach proposed by Bui et al. <ref type="bibr" target="#b3">[4]</ref> is known as Neural Graph Learning (NGL). In NGL, a regularization term is added to the classification loss so that the neural network also learns to associate the label more strongly with certain node features which are related to a particular class label. This regularisation term, Graph Loss, is calculated using a distance metric, node-level features of neighbours and edge-level features. The mathematical formulation of the overall cost function is given in Eqn. 1.</p><formula xml:id="formula_0">C NGL (θ) = V l n=1 c(g θ (x n ), y n ) + α 1 (u,v)∈E LL w uv d(h θ (x u ), h θ (x v )) + α 2 (u,v)∈E LU w uv d(h θ (x u ), h θ (x v )) + α 3 (u,v)∈E U U w uv d(h θ (x u ), h θ (x v ))<label>(1)</label></formula><p>where E LL , E LU , and E U U are sets of labeled-labeled, labeled-unlabeled and unlabeled-unlabeled edges correspondingly, g(·) is the label predicted by the neural network, h(·) represents the latent representations of the inputs produced by the neural network, d(·) is a distance metric, and {α 1 , α 2 , α 3 } are hyperparameters. This cost function accounts for the label propogation cost and the neural network cost. Bui et al. <ref type="bibr" target="#b3">[4]</ref> vouch for using l-1 norm (Eqn. 2) or l-2 norm (Eqn. 3) distance metric for d(·). We build upon the concept of NGL to propose the architecture of NodeNet.</p><formula xml:id="formula_1">d( a, b) = Σ|a i − b i | (2) d( a, b) = | a − b|<label>(3)</label></formula><p>While NGL does improve the efficiency, it comes with an added benefit that we do not require the inforation about the neighbor nodes at the time of inference. In our opinion it could be because the neural network itself memorizes information about the edges in the graph via the loss function.</p><p>In this paper we consider 3 very popular citation network datasets to validate our modifications: Cora, Citeseer, and Pubmed. In citation networks, nodes correspond to scientific documents, edges correspond to citation links, and each node has a feature vector as well as a class label. The statistics of the 3 datasets are given in <ref type="table" target="#tab_0">Table 1</ref>. We felt that even though the current state-of-the-art techniques provide great results, they do not satisfactorily address the problems of over-fitting and over-smoothing. Majority of the GNNs designed for node classification do not contain more than 2 layers as the effect of over-smoothing is very high in deeper neural networks. From an application point-of-view, the whole graph or a sub-graph may or may not be available at the time of inference, which make conventional GNNs unsuitable for these kind of applications. NGL <ref type="bibr" target="#b3">[4]</ref> addresses these issues to a great extent, but falls short in terms of accuracy. With NodeNet we aim to cross this barrier of accuracy as well</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>When we came across the approach of Bui et al. <ref type="bibr" target="#b3">[4]</ref>, we saw an opportunity to leverage a large knowledge base of neural network architectures and combine it with the extra information extracted from the edges of the graph. We propose changes in 3 areas, pre-processing, network architecture and regularization term, to arrive at NodeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-processing</head><p>Generally term frequency-inverse document frequency (TF-IDF) <ref type="bibr" target="#b7">[8]</ref> vectors perform better in any NLP task as compared to binary count vectors. This applies to classification as well. The words and absolute word counts are not available for the documents / nodes in Cora and CiteSeer datasets. Hence, we prepare a modified TF-IDF vector using Eqn. 4.</p><p>mT F IDF i,j = IDF i,j n i (4)</p><formula xml:id="formula_2">IDF i,j = log N 1 + N ti,j + 1<label>(5)</label></formula><p>In Eqn. 4 and Eqn. 5, the values are being calculated for j th term of i th document. Additionally, n i stands for total number of terms in i th document, N stands for the total number of documents and N ti,j stands for number of documents in which the term t i,j appears. Eqn. 4 is the modified TF-IDF <ref type="bibr" target="#b8">[9]</ref> score and Eqn. 5 is the smooth inverse document frequency weight <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>Bui et al. <ref type="bibr" target="#b3">[4]</ref> propose a network architecture containing only fully-connected (dense) and activation layers for node classification on PubMed dataset. We observed that this network easily overfits the dataset. Batch Normalization is known to have a regularization effect while enabling training with a relatively higher learning rate <ref type="bibr" target="#b9">[10]</ref> and Dropout <ref type="bibr" target="#b0">[1]</ref> is a very effective way to combat over fitting. We first added the batch normalization layer, but were still able to observe effect of over fitting. Hence we also added the dropout layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularization term</head><p>We changed the distance metric from l-2 norm to cosine similarity. The reason being that it better represents the degree of similarity between 2 documents <ref type="bibr" target="#b10">[11]</ref>. Such a similarity can only be found  <ref type="figure">Figure 4</ref>: Network Architecture along a citation edge as a paper would only cite other papers which are relevant to another, and hence there will be more similarity in the unique words used.</p><formula xml:id="formula_3">cosineSim( a, b) = a · b | a| × | b|<label>(6)</label></formula><p>where cosineSim( a, b) function calculates the cosine similarity between 2 vectors, a and b.</p><p>By replacing d(·) in Eqn. 1 by the cosine similarity function, we get the following final cost function for NodeNet:</p><formula xml:id="formula_4">C NodeNet (θ) = V l n=1 c(g θ (x n ), y n ) + α 1 (u,v)∈E LL w uv cosineSim(h θ (x u ), h θ (x v )) + α 2 (u,v)∈E LU w uv cosineSim(h θ (x u ), h θ (x v )) + α 3 (u,v)∈E U U w uv cosineSim(h θ (x u ), h θ (x v ))<label>(7)</label></formula><p>4 Results</p><p>As can be seen from the results presented in <ref type="table" target="#tab_2">Table 2</ref>, NodeNet surpasses the current state-of-the-art for all the 3 data sets. We have tried to tackle the issues of over-smoothing and over-fitting by using appropriate regularization techniques.</p><p>We observed from the loss curve for PubMed data set ( <ref type="figure" target="#fig_2">Fig. 5(a)</ref>) that the neural networks has a faster and more stable convergence over the PubMed dataset which has TF-IDF vectors as compared to Cora <ref type="figure" target="#fig_2">(Fig. 5(b)</ref>) and CiteSeer data sets which have binary count vectors. Hence, we decided to transform binary count vectors into modified TF-IDF vectors. This improved the learning ability and stabilized the convergence <ref type="figure" target="#fig_2">(Fig. 5(c)</ref>), at the cost of a small decline in the accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>Using Neural Graph Learning we can leverage the advantages provided by graphs and the vast knowledge-base of neural network architectures. NodeNet is the outcome of coming together of years of knowledge from NLP domain and the advantages of representing data in form of a graph. We demonstrates in this paper that NGL can be further modified to improve it's performance for node classification. Proposed NodeNet modifies pre processing step, architecture and loss function to achieve superior accuracy results while avoiding over-fitting. The NodeNet can be further adapted to make it suitable for generic node classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Research communities and industries building applications based on node classification or working on data which can be modelled as graphs may benefit from this work. No one is put at disadvantage. If the system fails it will provide incorrect labels. If the system is integrated with other systems downstream then the evaluation of impact of failure has to be done separately. If decision are to be taken based on the output, then the impact has to be evaluated from an action-oriented perspective.</p><p>We do not believe that the proposed algorithm leverages any biases in the data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>2D Convolution vs. Graph Convolution (adapted from [1]) 3. Graph Autoencoders 4. Spatio-temporal Graph Neural Networks (STGNNs) out of which, and GCN and GRNN are of the greatest interest as they are the most suitable GNN architectures for node classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>A GCN with multiple graph convolutional layers for node classification (adapted from<ref type="bibr" target="#b0">[1]</ref>) Architecture of the Graph-BERT Model (adapted from<ref type="bibr" target="#b1">[2]</ref>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Loss curves of NodeNet for different datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Dataset statistics</cell><cell></cell></row><row><cell>Dataset</cell><cell># nodes</cell><cell># edges</cell><cell># features</cell><cell># classes</cell><cell>Type of feature</cell></row><row><cell>Cora [5]</cell><cell>2708</cell><cell>5429</cell><cell>1433</cell><cell>7</cell><cell>Binary Count Vector</cell></row><row><cell>Citeseer [6]</cell><cell>3327</cell><cell>4732</cell><cell>3703</cell><cell>6</cell><cell>Binary Count Vector</cell></row><row><cell>Pubmed [7]</cell><cell>19717</cell><cell>44338</cell><cell>500</cell><cell>3</cell><cell>TF-IDF Vector</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Comparison of accuracy against state-of-the-art algorithms 1</cell><cell></cell></row><row><cell>Dataset</cell><cell>State-of-the-art Algorithm</cell><cell>NodeNet</cell></row><row><cell>Cora</cell><cell>86.00 (DFNet-ATT [12])</cell><cell>86.80</cell></row><row><cell>Cora with additional training data</cell><cell>89.48 (SplineCNN [13])</cell><cell>-</cell></row><row><cell>Cora with TF-IDF vectors</cell><cell>-</cell><cell>85.17</cell></row><row><cell>Citeseer</cell><cell>78.70 (GCN-LPA [14])</cell><cell>80.09</cell></row><row><cell>Citeseer with TF-IDF vectors</cell><cell>-</cell><cell>78.02</cell></row><row><cell>PubMed</cell><cell>87.80 (GCN-LPA [14])</cell><cell>90.21</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As of May 31, 2020 on Papers With Code</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We want to thank Sri Krishnan V, Mohan B V and Adit Shah from Robert Bosch Engineering and Business Solutions Private Limited, India, for their valuable comments, contributions and continued support to the project. We are grateful to all experts for providing us with their valuable insights and informed opinions ensuring completeness of our study.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adagcn: Adaboosting graph convolutional networks into deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural graph learning: Training neural networks using graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramavajjala</surname></persName>
		</author>
		<idno type="DOI">10.1145/3159652.3159731</idno>
		<ptr target="https://doi.org/10.1145/3159652.3159731" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1009953814988</idno>
		<ptr target="https://doi.org/10.1023%2Fa%3A1009953814988" />
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Citeseer: An automatic citation indexing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="DOI">10.1145/276675.276685</idno>
		<ptr target="https://doi.org/10.1145/276675.276685" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Conference on Digital Libraries, DL &apos;98</title>
		<meeting>the Third ACM Conference on Digital Libraries, DL &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v29i3.2157</idno>
		<ptr target="https://www.aaai.org/ojs/index.php/aimagazine/article/view/2157" />
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m">Modern Information Retrieval</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">020139829</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511809071.007</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="100" to="123" />
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modern information retrieval: a brief overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BULLETIN OF THE IEEE COMPUTER SOCIETY TECHNICAL COMMITTEE ON DATA ENGINEERING</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="35" to="43" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dfnets: Spectral cnns for graphs with feedback-looped filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">O K</forename><surname>Asiri Suranga Wijesinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8834-dfnets-spectral-cnns-for-graphs-with-feedback-looped-filters.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6009" to="6020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
