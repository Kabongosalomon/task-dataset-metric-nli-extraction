<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Transductive Approach for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Transductive Approach for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised video object segmentation aims to separate a target object from a video sequence, given the mask in the first frame. Most of current prevailing methods utilize information from additional modules trained in other domains like optical flow and instance segmentation, and as a result they do not compete with other methods on common ground. To address this issue, we propose a simple yet strong transductive method, in which additional modules, datasets, and dedicated architectural designs are not needed. Our method takes a label propagation approach where pixel labels are passed forward based on feature similarity in an embedding space. Different from other propagation methods, ours diffuses temporal information in a holistic manner which take accounts of long-term object appearance. In addition, our method requires few additional computational overhead, and runs at a fast ∼37 fps speed. Our single model with a vanilla ResNet50 backbone achieves an overall score of 72.3% on the DAVIS 2017 validation set and 63.1% on the test set. This simple yet high performing and efficient method can serve as a solid baseline that facilitates future research. Code and models are available at https://github.com/ microsoft/transductive-vos.pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation addresses the problem of extracting object segments from a video sequence given the annotations in the starting frame. This semi-supervised setting is challenging as it requires the system to generalize to various objects, deformations, and occlusions. Nevertheless, video object segmentation has received considerable attention because of its broad practical applications in surveillance, self-driving cars, robotics, and video editing.</p><p>Despite the simplicity of the formulation, video object segmentation is closely related to many other visual problems, such as instance segmentation <ref type="bibr" target="#b18">[19]</ref>, object reidentification <ref type="bibr" target="#b12">[13]</ref>, optical flow estimation <ref type="bibr" target="#b14">[15]</ref>, and object Equal contribution. Work done when Yizhuo was an intern at MSRA.  <ref type="figure">Figure 1</ref>: A comparison of performance and speed for semi-supervised video object segmentation methods on the DAVIS 2017 validation set. Ours performs comparably to the state-of-the-art methods, while running at an online speed ( &gt; 30 fps). tracking <ref type="bibr" target="#b4">[5]</ref>. As these tasks share similar challenges with video object segmentation, previous efforts <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> attempt to transfer the modules trained for such tasks into video object segmentation pipeline. More specifically, optical flow and tracking encourage local dependencies by estimating displacements in nearby frames, while instance segmentation and object re-identification enforces global dependencies by learning invariances to large appearance changes. The integration of such modules allows a significant performance improvement in video object segmentation.</p><p>The idea of enforcing local and global dependencies has been a central topic in general semi-supervised learning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b13">14]</ref> (also known as transductive inference). The basic assumptions are: 1) nearby samples tend to have the same label and 2) samples that lie on the same manifold should should have the same label. The local and global dependencies describe a sufficiently smooth affinity distribution, so that label propagation on the unlabeled data gives reliable estimates. Prior classical approaches that realize this idea include random walk <ref type="bibr" target="#b36">[37]</ref>, graph-cut <ref type="bibr" target="#b5">[6]</ref> and spectral methods <ref type="bibr" target="#b3">[4]</ref>.</p><p>This inspires us to explore a unified approach for semisupervised video object segmentation without the integration of the modules derived from other domains. We model the local dependency through a spatial prior and a motion prior. It is based on the assumption that spatially nearby pixels are likely to have same labels and that temporally distant frames weakens the spatial continuity. On the other hand, we model the global dependency through visual appearance, which is learned by convolutional neural networks on the training data.</p><p>The inference follows the regularization framework <ref type="bibr" target="#b48">[49]</ref> which propagates labels in the constructed spatio-temporal dependency graph. While label propagation algorithms have been explored in the recent literature for video object segmentation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34]</ref>, the manner in which they learn and propagate affinity is sparse and local, i.e., learning pixel affinities either between adjacent frames or between the first frame and a distant frame. We observe that there exists much smooth unlabeled structure in a temporal volume that these methods do not exploit. This may cause failures when handling deformations and occlusions. In contrast, our label propagation approach attempts to capture all frames which span the video sequence from the first frame to the frame preceding the current frame. To limit the computational overhead, sampling is performed densely within the recent history and sparsely in the more distant history, yielding a model that accounts for object appearance variation while reducing temporal redundancy.</p><p>In its implementation, our model does not rely on any other task modules, additional datasets, nor dedicated architectural designs beyond a pretrained ResNet-50 model from the ImageNet model zoo <ref type="bibr" target="#b19">[20]</ref>. During inference, perframe prediction involves only a feed-forward pass through the base network plus an inner product with the prediction history. Thus the inference is fast and also not affected by the number of objects. Experimentally, our model runs at a frame rate of 37 per second, achieving an overall score of 72.3% on Davis 2017 validation set, as well as 63.1% on Davis 2017 test set. Our model also achieves a competitive overall score of 67.8% on the recent Youtube-VOS validation set. Our method is competitive to current prevailing methods while being substantially simpler and faster. We hope the model can serve as a simple baseline for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review related work on video object segmentation in the semi-supervised setting. For an overview of unsupervised and interactive video object segmentation, we refer readers to other papers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single frame models.</head><p>In the past few years, methods with leading performance have been based on finetuning the model on the single annotated frame and performing inference on individual test frames. These methods basically learn an objectness prior and spatial continuity without con-sidering temporal information. The convolutional neural network architecture plays an important role for finetuning on a single frame to be effective. OSVOS <ref type="bibr" target="#b6">[7]</ref> is the pioneering work in this direction. Lucid <ref type="bibr" target="#b24">[25]</ref> seeks to augment the data specifically for each video from only a single frame of ground truth. OnAVOS <ref type="bibr" target="#b41">[42]</ref> mines confident regions in the testing sequence to augment the training data. The later work of OSVOS-S <ref type="bibr" target="#b30">[31]</ref> integrates semantic information from an instance segmentation model to boost performance. PReMVOS <ref type="bibr" target="#b29">[30]</ref>, CNN-MRF <ref type="bibr" target="#b2">[3]</ref>, and DyeNet <ref type="bibr" target="#b28">[29]</ref> also build on top of single frame models.</p><p>The effectiveness of single frame models demonstrates that optimizing a domain-specific spatial smoothness term greatly enhances performance. However, finetuning via gradient descent generally takes tens of seconds per video, which can make it impractical for many applications.</p><p>Propagation-based models.</p><p>Propagation-based methods embed image pixels into a feature space and utilize pixel similarity in the feature space to guide label propagation. In methods such as VideoMatch <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>, only pixels in the first frame are used for reference in computing pixel similarity. Since no finetuning at run time is involved, propagation-based models run much faster than the aforementioned single frame models, but the lack of domainspecific finetuning leads to performance that is much worse. Later works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref> explore adding the preceding frame to the first frame as reference, which significantly improves performance and leads to greater temporal smoothness. However, this local and sparse propagation scheme suffers from the drifting problem <ref type="bibr" target="#b15">[16]</ref>.</p><p>Long-range spatio-temporal models. There are two lines of work which attempt to optimize over a dense long-range spatio-temporal volume. The first <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b20">21]</ref> builds a recurrent neural network which uses the estimate from the previous frame to predict the object segmentation in the current frame. The whole model is learned via backpropagation through time. However, such models are sensitive to estimation errors in the previous frame.</p><p>The second direction is based on graphical models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref> (i.e., Markov Random Fields) defined over the spatio-temporal domain. These works were popular prior to deep learning, and employed edge potentials defined by handcrafted features such as SIFT. The models are computationally expensive and no longer competitive to learning-based methods.</p><p>Relation to other vision problems. As the above methods suggest, video object segmentation is closely related to a variety of computer vision problems such as instance segmentation, object re-identification, and optical flow estimation and tracking. Many recent methods integrate components for these other tasks into the video object segmentation pipeline. For example, OSVOS-S <ref type="bibr" target="#b30">[31]</ref> includes a instance transduction model induction model segmentation module; PReMVOS <ref type="bibr" target="#b29">[30]</ref> and DyeNet <ref type="bibr" target="#b28">[29]</ref> incorporate a object re-identification module; CNN-MRF <ref type="bibr" target="#b2">[3]</ref>, MaskTrack <ref type="bibr" target="#b33">[34]</ref> and MaskRNN <ref type="bibr" target="#b20">[21]</ref> rely on optical flow estimation. The integration of other modules heavily depends on transfer learning from other datasets. Though performance improvement is observed, it usually involves further complications. For example, instance segmentation becomes less useful when the video encounters a new object category which is not present in instance segmentation model. Optical flow <ref type="bibr" target="#b14">[15]</ref> suffers from occlusions which would mislead label propagation.</p><formula xml:id="formula_0">1 st frame prediction of k-n frame (a) Previous Induction Model (b) Our Transduction Model k-n frame k-1 frame k frame groundtruth of 1 st frame prediction of k-1 frame prediction of k frame 1 st frame prediction of k-n frame k-n frame k-1 frame k frame groundtruth of 1 st frame prediction of k-1 frame prediction of k frame</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most relevant works.</head><p>Space-time memory network (STM) <ref type="bibr" target="#b32">[33]</ref> is a significant work and most similar with ours. Ours is developed independently from STM, while STM is published earlier than ours. The insight that exploiting dense long-term information is similar. However, the transductive framework in the proposed approach, which stems from the classical semi-supervised learning, brings theoretical foundations to video object segmentation. Moreover, in the implementation, ours is much simpler and more efficient which does not require additional datasets, and infers all objects simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In contrast to much prior work on finetuning a model on a single annotated frame or transferring knowledge from other related tasks, our approach focuses on fully exploiting the unlabeled structure in a video sequence. This enables us to build a simple model that is both strong in performance and fast in inference.</p><p>We first describe a generic semi-supervised classification framework <ref type="bibr" target="#b48">[49]</ref> and then adapt it to online video object segmentation in a manner that follows our ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Transductive Inference Framework</head><p>Let us first consider a general semi-supervised classification problem. Suppose that we have a dataset D = {(x 1 , y 1 ), (x 2 , y 2 ), (x l , y l ), x l+1 , ..., x n }, which contains l labeled data pairs and n − l unlabeled data points. The task is to infer the labels {ŷ i } n i=l+1 for the unlabeled data {x l+1 , ..., x n } based on all the observation D. Inference of the unlabeled data is formulated in prior work <ref type="bibr" target="#b48">[49]</ref> as a transductive regularization framework,</p><formula xml:id="formula_1">Q(ŷ) = n i,j w ij ||ŷ i √ d i −ŷ j d j || 2 + µ l i=1 ||ŷ i − y i || 2 ,<label>(1)</label></formula><p>where w ij encodes the similarity between data points (x i , x j ), and d i denotes the degree d i = j w ij for pixel i. The first term is a smoothness constraint that enforces similar points to have identical labels. The second term is a fitting constraint, which penalizes solutions that deviate from the initial observations. The parameter µ balances these two terms. Semi-supervised classification amounts to solve the following optimization problem,</p><formula xml:id="formula_2">y = argmin Q(y).<label>(2)</label></formula><p>It is shown <ref type="bibr" target="#b48">[49]</ref> that the above energy minimization problem can be solved by iterative algorithm as follows. Let S = D −1/2 WD −1/2 be the normalized similarity matrix constructed from w ij . Iteratively solve forŷ(k) until convergence, as 1</p><formula xml:id="formula_3">y(k + 1) = αSŷ(k) + (1 − α)y(0),<label>(3)</label></formula><p>where α = µ/(µ + 1), and y(0) = [y 1 , y 2 , ..., y n ] T is the initial observation of the label clamped with supervised labels. The typical value of α is 0.99. The power of this transduction model comes from the globalized model it builds over the dense structures in the unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Online Video Object Segmentation</head><p>Based on this general framework <ref type="bibr" target="#b48">[49]</ref>, we build a transductive model for semi-supervised video object segmentation that accounts for dense long-range interactions.</p><p>This gives rise to three challenges. First, video frames stream sequentially, so the model must work in an online fashion, where the inference of one frame should not depend on future frames. Second, the number of pixels in one video can scale into the tens of millions. A similarity matrix over all the pixels would thus be intractable to compute. Third, an effective similarity measure W needs to be learned between pixels in a video sequence.</p><p>For the algorithm to run online, it is assumed that the predictions on all prior frames have been determined when the current frame t arrives. We therefore approximate the Eqn 3 by expanding the inference procedure through time,</p><formula xml:id="formula_4">y(t + 1) = S 1:t→t+1ŷ (t).<label>(4)</label></formula><p>S 1:t→t+1 represents the similarity matrix S that is only constructed between pixels up to the t-th frame and the pixels in the t + 1-th frame. Since no labels are provided beyond the first frame, the prior term y(0) is omitted for the frame t + 1. For time t+1, the above propagation procedure is equivantly minimizing a set of smoothness terms in the spatiotemporal volume,</p><formula xml:id="formula_5">Q t+1 (ŷ) = i j w ij ||ŷ i √ d i −ŷ j d j || 2 ,<label>(5)</label></formula><p>where i indexes the pixels at the target time t + 1, j indexes the pixels in all frames prior to and including time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Label Propagation</head><p>Given the annotations on the starting frame of a video, we process the remaining frames sequentially, propagating labels to each frame based on Eqn. 4. The quality of video object segmentation heavily depends on the similarity metric S, whose core component is the the affinity matrix W. <ref type="figure">Figure 3</ref>: Sampling strategy for label propagation. We sample densely in the recent history, and more sparsely in the distant history.</p><formula xml:id="formula_6">… … … … … … … … global local − 1 − 2 − 3 − 4 − 12 − 19 − 26 − 33 − 40</formula><p>Similarity metric. In order to build a smooth classification function, the similarity metric should account for global high-level semantics and local low-level spatial continuity. Our similarity measure w ij includes an appearance term and a spatial term,</p><formula xml:id="formula_7">w ij = exp(f T i f j ) · exp(− ||loc(i) − loc(j)|| 2 σ 2 ),<label>(6)</label></formula><p>where f i , f j are the feature embeddings for pixels p i , p j through a convolutional neural network. loc(i) is the spatial location of pixel i. The spatial term is controlled by a locality parameter σ. Learning of the appearance model is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame sampling.</head><p>Computing a similarity matrix S over all the previous frames is computationally infeasible, as long videos can span hundreds of frames or more. Inspired by Temporal Segment Networks <ref type="bibr" target="#b42">[43]</ref>, we sample a small number of frames in the observance of the temporal redundancy in videos.</p><p>Specifically, as in <ref type="figure">Figure 3</ref>, we sample a total of 9 frames from the preceding 40 frames: the 4 consecutive frames before the target frame to model the local motion, and 5 more frames sparsely sampled from the remaining 36 frames to model long-term interactions. We find this sampling strategy to strike a good balance between efficiency and effectiveness. Detailed ablations about the choice of frame sampling are presented in the experiments.</p><p>A simple motion prior. Pixels that are more distant in the temporal domain have weaker spatial dependencies. To integrate this knowledge, we use a simple motion prior where a smaller σ = 8 is used when the temporal references are sampled locally and densely, and a larger σ = 21 is employed when the reference frames are distant. We find this simple motion model to be effective for finding longterm dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Architecture Optical Proposal Tracking Re-ID DyeNet <ref type="bibr" target="#b28">[29]</ref> ResNet 101 CNN-MRF <ref type="bibr" target="#b2">[3]</ref> Deeplab PReMVOS <ref type="bibr" target="#b29">[30]</ref> Deeplab-V3+ FEELVOS <ref type="bibr" target="#b40">[41]</ref> Deeplab-V3+ STM <ref type="bibr" target="#b32">[33]</ref> 2×ResNet-50 TVOS (ours)</p><p>ResNet-50 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning the appearance embedding</head><p>We learn the appearance embedding in a data-driven fashion using a 2D convolutional neural network. The embedding aims to capture both short-term and long-term variations due to motion, scale and deformations. The embedding is learned from the training data in which each frame from the video is annotated with the segmented object and the object identity.</p><p>Given a target pixel x i and we consider all pixels in the prior frames as references. Denote f i and f j the feature embeddings for pixel x i and a reference pixel x j . Then the predicted labelŷ i of x i is given bŷ</p><formula xml:id="formula_8">y i = j exp(f T i f j ) k exp(f T i f k ) · y j ,<label>(7)</label></formula><p>where the reference indexes j, k span the temporal history before the current frame. We show detailed ablations on how sampling the historical frames affects the learning quality. We optimize the embedding via a standard cross-entropy loss on all pixels in the target frame,</p><formula xml:id="formula_9">L = − i logP (ŷ i = y i |x i ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>We use a ResNet-50 to train the embedding model. The convolution stride of the third and the fourth residual blocks is set to 1 to maintain a high-resolution output. We add one additional 1 × 1 convolutional layer to project the feature to a final embedding of 256 dimensions. The embedding model produces a feature with a total stride of 8.</p><p>During training, we take the pretrained weights from the ImageNet model zoo, and finetune the model on the Davis 2017 <ref type="bibr" target="#b34">[35]</ref> training set for 240 epochs and Youtube-VOS <ref type="bibr" target="#b45">[46]</ref> for 30 epochs. We apply the standard augmentations of random flipping and random cropping of size 256 × 256 on the input images. We use a SGD solver with an initial learning rate of 0.02 and a cosine annealing scheduler. The optimization takes 16 hours on 4 Tesla P100 GPUs, with a batch size of 16, each containing 10 snippets from a video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Without Motion</head><p>With Motion <ref type="figure">Figure 4</ref>: The effect of our simple motion model. Distant frames have weaker spatial priors on the location of objects, thus reducing the drifting problem.</p><p>During tracking, we extract features at the original image resolution of 480p. The results of each video frame are predicted sequentially online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we first describe our experimental settings and datasets. Then we show detailed ablations on how the transductive approach takes advantage of unlabeled structures in the temporal sequence to significantly improve the performance. Results are conducted on various datasets to compare with the state-of-the-art. Finally, we discuss temporal stability and the relationship to optical flow. Our method is abbreviated as TVOS in the result tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We evaluate our method on the Davis 2017 <ref type="bibr" target="#b34">[35]</ref>, and Youtube-VOS <ref type="bibr" target="#b45">[46]</ref> datasets. Our model is trained on the respective training set and evaluated on the validation set. For Davis 2017, we also train our model on the combined train-val set, and submit the results on the testing set on the evaluation server.</p><p>Davis 2017 contains 150 video sequences and it involves multiple objects with drastic deformations, heavy and prolonged occlusions, and very fast motions. High-definition annotations are available for all frames in the training sequences. Youtube-VOS is the largest dataset for this task to-date, containing 4453 training sequences and 474 validation sequences. It captures a comprehensive collection of 94 daily object categories. However, the frame rate of the video is much lower than videos in Davis (5 fps compared to 24 fps).</p><p>Evaluation metrics. We use the standard evaluation metric of mean intersection over union (mIoU), averaged across objects and summed over all frames. The mIoU is evaluated on both the full objects (J measure) and only on the object boundaries (F measure). The global metric (G measure) is the average of the J and F measures. Youtube-   VOS also includes a separate measure of seen objects and unseen objects to measure the generalization ability. In Section 4.4, we provide a discussion of temporal stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Dense local and global dependencies. While most prior works focus on optimizing single frame models, the key idea of this paper is to build dense long-term models over the spatio-temporal volume. In <ref type="table" target="#tab_2">Table 2</ref>, we summarize the effect of such long-term potentials which capture both local and global dependency. Each row is an appearance embedding model trained with different reference frame sampling strategies. Each column corresponds to a tracking sampling strategy. We study the following settings: one reference frame preceding the target frame, 3 consecutive frames preceding the target frame, 9 consecutive frames preceding the target frame, uniform sampling of 9 frames in the preceding 40 frames, and our sparse sampling of 9 frames in the preceding 40 frames as in <ref type="figure">Figure 3</ref>. We find that tracking over a longer term generally improves the performance, and denser sampling near the target frame is helpful. For learning the appearance embedding, training with 9 consecutive frames produces the best results, while longer ranges do not always lead to improvements. This may be due to very long ranges covering almost the entire video reduces the variations in the dataset, which leads to worse generalization for training.</p><p>In <ref type="figure" target="#fig_2">Figure 5</ref>, we show some qualitative examples for long range tracking. Using 9 consecutive frames yields more stable predictions than using only the previous frame. Adding the spatial term smooths the object boundaries. A long range of 40 frames enables the model to re-detect objects after heavy occlusions.</p><p>Transferred representations. In the last rows of Table 2, we also test the tracking performance for models pretrained on ImageNet but without further training on the DAVIS dataset. The transferred ImageNet model obtains a mean J measure of 54.8%, which is actually better than some prior methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b9">10]</ref> trained with additional Davis data. Also, even an unsupervised pretrained model on images obtains performance competitive to network modulation <ref type="bibr" target="#b46">[47]</ref> using our transductive inference algorithm. Two recent unsupervised pretrained models on ImageNet are investigated <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b17">18]</ref>. Since no domain-specific training is involved for the appearance embedding, the evaluation of transferred representations clearly validates the effective-Methods FT J F J &amp;F Speed OnAVOS <ref type="bibr" target="#b41">[42]</ref> 61.0 66.1 63.6 0.08 DyeNet <ref type="bibr" target="#b28">[29]</ref> 67.3 71.0 69.1 0.43 CNN-MRF <ref type="bibr" target="#b2">[3]</ref> 67.2 74.2 70.7 0.03 PReMVOS <ref type="bibr" target="#b29">[30]</ref> 73.9 81.7 77.8 0.03 Modulation <ref type="bibr" target="#b46">[47]</ref> 52.  ness of dense long-term modeling. The simple motion prior. As a weak spatial prior for modeling the dependency between distant frames, our simple motion model reduces noise from the model predictions and leads to about 1% improvement. <ref type="figure">Figure 4</ref> displays two concrete examples. More complicated motion models, such as a linear motion model <ref type="bibr" target="#b0">[1]</ref>, may be even more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we first give a brief overview of the current leading methods, including those that use first-frame finetuning (CNN-MRF <ref type="bibr" target="#b2">[3]</ref>, DyeNet <ref type="bibr" target="#b28">[29]</ref>, PReMVOS <ref type="bibr" target="#b29">[30]</ref>) and those that do not (FEELVOS <ref type="bibr" target="#b40">[41]</ref>, STM <ref type="bibr" target="#b32">[33]</ref> and our TVOS). For DyeNet and PReMVOS, their sub-modules are learned on dedicated datasets such as optical flow on Flying Chairs, object proposal on MSCOCO, and object segmentation on PASCAL VOC. Since Davis is much smaller than the large-scale datasets, it remains unknown how much of the gains can be attributed to knowledge transfer or to the methods themselves. Therefore, the mentioned methods are not directly comparable with our method. FEELVOS, STM and ours are much simpler, as they do not rely on additional modules for this problem. STM additionally requires heavy pretraining on large-scale image datasets.</p><p>It is also important to note that for PreMVOS, DyeNet, CNN-MRF, they are not able to run tracking in an online fashion. They use information from future frames to stabilize prediction for the target frame. Also, instead of using the first frame from the given video for training, they use the first frames from the entire test-dev set for training. Propagation-based methods are able to track objects sequentially online.</p><p>DAVIS 2017. We summarize our results on the Davis 2017 validation set in <ref type="table" target="#tab_4">Table 3</ref>, and on the Davis 2017 testdev set in <ref type="table" target="#tab_6">Table 4</ref>. On the validation set, our method performs slightly better than STM <ref type="bibr" target="#b32">[33]</ref> under the same amount <ref type="bibr">Methods</ref> FT J F J &amp;F Speed OnAVOS <ref type="bibr" target="#b41">[42]</ref> 53.4 59.6 56.5 0.08 DyeNet <ref type="bibr" target="#b28">[29]</ref> 65.8 70.5 68.2 0.43 CNN-MRF <ref type="bibr" target="#b2">[3]</ref> 64.5 70.5 67.5 0.02 PReMVOS <ref type="bibr" target="#b29">[30]</ref> 67   We train our model on the combined training and validation set for evaluating on the test-dev set. We find that there is a large gap of distribution between the Davis 2017 test-dev and validation sets. Heavy and prolonged occlusions among objects belonging to the same category are more frequent in the test-dev, which favors methods with re-identification modules. As a result, we are 4 − 5% lower than DyeNet and CNN-MRF on the test-dev set. FEELVOS is even more negatively affected, performing 8% lower than ours in terms of mean J&amp;F . STM <ref type="bibr" target="#b32">[33]</ref> does not provide an evaluation on the test set.</p><p>Youtube-VOS. We summarize the results on youtube-VOS validation set in <ref type="table" target="#tab_7">Table 5</ref>. Ours surpasses all prior works except STM <ref type="bibr" target="#b32">[33]</ref>, which relies on heavy pretraining on a variety of segmentation datasets such as saliency detection and instance segmentation. Without pretraining, STM obtains a comparable result of 68.1%. We also test the generalization ability of our model trained on DAVIS train-val and test on Youtube-VOS val. The transferred model shows great generalization ability with an overall score of 67.4% Speed analysis. During tracking, we cache the appearance embeddings for a history up to 40 frames. Inference  per frame thus only involves a feed-forward pass of the target frame through the base network, and an additional dot product of target embeddings to prior embeddings. Computation is also constant of any number of objects. This makes our algorithm extremely fast, with a runtime of 37 frames per second on a single Titan Xp GPU. <ref type="figure">Figure 1</ref> compares current algorithms on their trade-off between speed and performance. Ours is an order of magnitude faster than prior methods, while achieving the results comparable to stateof-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussions</head><p>Temporal stability. Temporal stability is often a desirable property in video object segmentation, as sharp inconsistencies may be disruptive to downstream video analysis. However, temporal stability is typically not included as an evaluation criterion. Here, we give qualitative examples showing the difference in temporal stability between our model and the state-of-the-art PreMVOS <ref type="bibr" target="#b29">[30]</ref>.</p><p>In <ref type="figure" target="#fig_3">Figure 6</ref>, we show examples of per-frame evaluation along video sequences. Although the state-of-the-art integrates various temporal smoothing modules, such as opti-cal flow, merging and tracking, we observe the detectionbased method to be prone to noise. For example, objects are lost suddenly, or being tagged with a different identity. Our method, on the other hand, makes temporally consistent predictions.</p><p>Does our model learn optical flow? Our method learns a soft mechanism for associating pixels in the target frame with pixels in the history frames. This is similar to optical flow where hard correspondences are computed between pixels. We examine how much our learned model aligns with optical flow.</p><p>We take two adjacent two frames and calculate the optical flow from our model as ∆d i = j s ij ∆d ij , where s ij is the normalized similarity, and ∆d ij is the displacement between i, j. <ref type="figure" target="#fig_4">Figure 7</ref> shows an example visualization of the flow. Compared to the optical flow computed by FlowNet2 <ref type="bibr" target="#b22">[23]</ref>, our flow makes sense on objects that would be segmented, but is much more noisy on the background. We have further added a spatial smoothness constraint on the computed optical flow for jointly learning the embeddings, as widely used for optical flow estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. We observe that the constraint smooths the optical flow on the background, but fails to regularize the model for tracking. Adding the term consistently hurts the performance of video object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a simple approach to semi-supervised video object segmentation. Our main insight is that much more unlabeled structure in the spatio-temporal volume can be exploited for video object segmentation. Our model finds such structure via transductive inference. The approach is learned end-to-end, without the need of additional modules, additional datasets, or dedicated architectural designs. Our vanilla ResNet50 model achieves competitive performance with a compelling speed of 37 frames per second. We hope our model can serve as a solid baseline for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>We pose video object segmentation from a transductive inference perspective, where dense long-term similarity dependencies are constructed to discover structures in the spatio-temporal volume. a) Previous induction model transfers knowledge from the first frame to other frames. b) Our transduction model considers holistic dependencies in the unlabeled spatio-temporal volume for joint inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Using dense long-range dependencies improves the tracking performance. The spatial term smooths the object boundaries, while long-term dependencies up to 40 frames help to re-detect objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Per-frame IoU over time for PreMVOS and our method on a example video sequences from the DAVIS validation set. PreMVOS switches object identities frequently, while our predictions are temporally smooth. The color of each IoU curve matches its corresponding object segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>An example of optical flow computed from our model compared to FlowNet2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A brief overview of leading VOS methods with dependent modules for other related vision tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the range of temporal dependencies and the simple motion component. The mean J measure on the Davis 2017 validation set is reported. See text for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative evaluation on the Davis 2017 validation set. FT denotes methods that perform online training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Quantitative evaluation on the Davis 2017 test-dev set. FT denotes methods that perform online training.</figDesc><table><row><cell>Methods</cell><cell>Overall</cell><cell>J</cell><cell>Seen</cell><cell>F</cell><cell cols="2">Unseen J F</cell></row><row><cell>RGMP [45]</cell><cell>53.8</cell><cell cols="2">59.5</cell><cell>-</cell><cell>45.2</cell><cell>-</cell></row><row><cell>OnAVOS [42]</cell><cell>55.2</cell><cell cols="5">60.1 62.7 46.6 51.4</cell></row><row><cell>RVOS [40]</cell><cell>56.8</cell><cell cols="5">63.6 67.2 45.5 51.0</cell></row><row><cell>OSVOS [7]</cell><cell>58.8</cell><cell cols="5">59.8 60.5 54.2 60.7</cell></row><row><cell>S2S [46]</cell><cell>64.4</cell><cell cols="5">71.0 70.0 55.5 61.2</cell></row><row><cell>PreMVOS [30]</cell><cell>66.9</cell><cell cols="5">71.4 75.9 56.5 63.7</cell></row><row><cell>STM [33]+Pretrain</cell><cell>79.4</cell><cell cols="5">79.7 84.2 72.8 80.9</cell></row><row><cell>TVOS</cell><cell>67.8</cell><cell cols="5">67.1 69.4 63.0 71.6</cell></row><row><cell>TVOS (from DAVIS)</cell><cell>67.4</cell><cell cols="5">66.7 69.8 62.5 70.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Quantitative evaluation on the Youtube-VOS validation set.of training data, while surpassing other propagation-based methods which do not need fine-tuning, by 4% for mean J and 3% for mean J&amp;F . In comparison to finetuning based methods, our TVOS also outperforms DyeNet and CNN-MRF by 2% while being significantly simpler and faster.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that there exists a closed-form solution to the Eqn 2 shown in<ref type="bibr" target="#b48">[49]</ref>. However, this requires the inverse of matrix S which is often computationally demanding when S is a large matrix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-target tracking by continuous energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video snapcut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data using graph mincuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5320" to="5329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep spatiotemporal random fields for efficient video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8915" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7415" to="7424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jumpcut: non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="195" to="196" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning in gigantic image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06852</idno>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="234" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 ieee computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Instance embedding transfer to unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6526" to="6535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation with motion-based bilateral networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="207" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Premvos: Proposalgeneration, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06031</idno>
		<title level="m">Video object segmentation without temporal information</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="743" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00607</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2167" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Partially labeled classification with markov random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="945" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Motion coherent tracking using multi-label mrf optimization. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="190" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rvos: End-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5277" to="5286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09513</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<title level="m">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
