<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Gaussian Instance Segmentation in Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Hung</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Yi</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Chi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<country>Taiwan AI Labs</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Gaussian Instance Segmentation in Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D instance segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel method for instance segmentation of 3D point clouds. The proposed method is called Gaussian Instance Center Network (GICN), which can approximate the distributions of instance centers scattered in the whole scene as Gaussian center heatmaps. Based on the predicted heatmaps, a small number of center candidates can be easily selected for the subsequent predictions with efficiency, including i) predicting the instance size of each center to decide a range for extracting features, ii) generating bounding boxes for centers, and iii) producing the final instance masks. GICN is a single-stage, anchor-free, and end-to-end architecture that is easy to train and efficient to perform inference. Benefited from the center-dictated mechanism with adaptive instance size selection, our method achieves state-of-the-art performance in the task of 3D instance segmentation on ScanNet and S3DIS datasets. The GICN code is available at https://github.com/LiuShihHung/GICN</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling 3D scenes requires a compilation of vision techniques to solve the corresponding tasks at different levels, such as depth estimation, feature extraction <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35]</ref>, planar reconstruction, object detection <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b43">43]</ref>, and 3D semantic/instance segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref>. Among these tasks, 3D instance segmentation is situated at a higher level but is no less challenging. The goal is to segment out each individual object in a 3D scene and assign it a correct class label. Lower-level tasks can be incorporated as components into the pipeline of 3D instance segmentation, and therefore provide different directions for possible improvements.</p><p>In this paper, we aim to tackle 3D instance segmentation from the aspects of predicting the probability heatmaps of instance centers and sizes. We propose a center-dictated mechanism to localize each target instance in the point cloud based on the predicted probability heatmaps. Unlike most of the previous 3D instance segmentation methods that rely on box proposals or a predefined set of anchors, our new method can adapt to the context of the scene for predicting a arXiv:2007.09860v1 [cs.CV] 20 Jul 2020 small number of instance centers directly from the point cloud to produce the final instance bounding boxes and masks.</p><p>More specifically, this work addresses the problem of 3D instance segmentation by formulating a task to learn Gaussian instance segmentation. The proposed method, which is called Gaussian Instance Center Network (GICN), is trained to predict a Gaussian heatmap that characterizes the instance centers, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Such formulation and design are new and particularly beneficial in that we do not need to rely on a predefined set of anchor boxes, nor do we need to generate box proposals that entail further non-maximum suppression. Working on the center heatmap also allows a more intuitive way to visualize and evaluate the intermediate result of training, which is nontrivial for other methods with entwined architectures and implicit mechanisms. See <ref type="figure">Fig. 2</ref> for example. We can easily compare the predicted heatmap with the ground-truth heatmap and identify the issues for further improvements.</p><p>From the predicted heatmap, we can simply select a small set of center candidates to proceed. The high computational cost that hinders point-cloud processing can therefore be greatly reduced. Subsequently, GICN predicts the instance size of each center to determine a proper neighborhood for feature extraction. Based on the size-aware, adaptively extracted features, GICN can better estimate the bounding box and mask for each instance center. As a result, GICN provides a more intuitive 3D pipeline that is easy to train and efficient to perform inference. Our experiments show that the proposed method can achieve state-of-the-art performance on 3D instance segmentation benchmarks. We also conduct ablation study to verify the effectiveness of our design of GICN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>While the performance of 2D instance segmentation has been significantly advanced by a series of recent work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, research on 3D instance segmentation has not yet achieved comparable success as its 2D counterpart. Recent methods for 3D instance segmentation can be mainly characterized into two categories according to their representations: voxel based versus point-cloud based. Voxel-based methods, such as MTML <ref type="bibr" target="#b18">[19]</ref>, MASC <ref type="bibr" target="#b23">[24]</ref>, and PanopticFusion <ref type="bibr" target="#b28">[28]</ref>, are designed to work on volumetric data, where the 3D space is voxelized into voxel grids for deriving the input representation from scene geometry. On the other hand, point-cloud based methods directly take the point cloud as input and extract features from the 3D points for predicting instance segmentation, e.g., SGPN <ref type="bibr" target="#b38">[38]</ref>, 3D-BoNet <ref type="bibr" target="#b41">[41]</ref>, GSPN <ref type="bibr" target="#b42">[42]</ref>, and others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b39">39]</ref>. Furthermore, point-cloud based methods often include PointNet <ref type="bibr" target="#b34">[34]</ref> or PointNet++ <ref type="bibr" target="#b35">[35]</ref> as the backbone to extract local and global features. In this work, we also use PointNet++ to compute features of 3D points and do not go into developing new methods for 3D feature extraction. The mechanism of feature extraction is orthogonal to the gist of our method, and GICN will also benefit from further improvements in point cloud features.  <ref type="bibr" target="#b0">( 1 )</ref> to generate the Gaussian approximation heatmap. We use a center selection mechanism to choose a small number of probable candidates, which will yield the bounding boxes and the instance masks using the bounding-box prediction network ( 2 ) and the mask prediction network <ref type="bibr" target="#b2">( 3 )</ref> Previous ideas that have been shown to be effective for 2D instance segmentation can also be applied to 3D instance segmentation. For example, metric learning is one of the key building blocks in many 2D instance segmentation methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>. A common metric-learning strategy used in 2D instance segmentation is to define a pairwise loss for learning suitable pixel embeddings, such that, in the embedding space, points belonging to the same instance are drawn closer to each other. For 3D instance segmentation, similar strategy can be applied to the learning of embeddings for 3D points <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b39">39]</ref>. For instance, MTML <ref type="bibr" target="#b18">[19]</ref> builds upon <ref type="bibr" target="#b4">[5]</ref> to learn inter-instance and intra-instance relations, and it adopts a post-processing step for semantic segmentation, using mean-shift clustering <ref type="bibr" target="#b12">[13]</ref> to group the 3D points in the embedding space. ASIS <ref type="bibr" target="#b39">[39]</ref> and JSIS3D <ref type="bibr" target="#b31">[31]</ref> also use mean-shift clustering to obtain instance segmentation clusters from embeddings. Another strategy related to metric learning is to train a network that can directly estimate the instance affinity score for predicting whether two points belong to the same object instance, e.g., MASC <ref type="bibr" target="#b23">[24]</ref>.</p><p>3D instance segmentation is also related to the tasks of 3D semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">40]</ref> and 3D object detection <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b43">43]</ref>. 3D semantic segmentation is to predict semantic labels for the 3D points, but it does not separate different instances. On the other hand, 3D object detection estimates the 3D bounding box of each individual object, but it is not able to provide a detailed mask on the 3D points of the target object. Therefore, 3D instance segmentation can be considered as an integrated task of 3D object detection and semantic segmentation, although simply concatenating them together might not yield an effective model to achieve good results.</p><p>Yang et al . <ref type="bibr" target="#b41">[41]</ref> propose an anchor-free approach called 3D-BoNet, which achieves good performance in 3D instance segmentation and shows the advantage of anchor-free prediction. Our method significantly differs from theirs in the design and the methodology. 3D-BoNet predicts a fixed number of 3D bounding boxes and the corresponding instance masks, while our method works on a probability heatmap that can be used to predict an arbitrary but small number of instance centers. Moreover, our method learns to select a suitable instance size for the cluster of points that belong to the same instance center.</p><p>The recent method VoteNet <ref type="bibr" target="#b32">[32]</ref> for 3D object detection presents a Hough voting mechanism to generate new points that lie close to object centers. The votes are then aggregated into clusters from which box proposals can be derived. Despite the distinction in the tasks to be solved (instance segmentation versus object detection), our method has other fundamental differences from VoteNet. These differences also highlight the advantages and contributions of our work.: i) Our method predicts a Gaussian approximation heatmap for centers from the entire point cloud while VoteNet samples a set of seeds to vote to centers. ii) Our method can adapt to the distribution of point cloud to decide appropriate cluster sizes for inferring the bounding boxes, while VoteNet sets a fixed aggregation radius for all centers. iii) VoteNet generates a fixed number of box proposals and has to perform non-maximum suppression to get the final output bounding boxes. Our method is able to produce the 3D masks immediately from the heatmap and does not need further non-maximum suppression over bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the proposed method, Gaussian Instance Center Network (GICN), learns to carry out the task of 3D instance segmentation by first predicting the distribution of instance centers. The strategy is fundamentally different from most of the existing techniques that begin by focusing on predicting boundingbox proposals. Based on a relatively small set of selected center candidates, our method then estimates the corresponding bounding boxes and instance masks. In the following sections, we detail the three key stages to accomplish the proposed Gaussian instance segmentation: center prediction network (Sec. 3.1), boundingbox prediction network (Sec. 3.2), and mask prediction network (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Center Prediction Network</head><formula xml:id="formula_0">Φ C Let P = {p i = (x i , y i , z i )} N i=1</formula><p>be an input point cloud containing N points. The center prediction network Φ C is constructed to estimate the probability of each point p i ∈ P being the center of some relevant instance in the 3D scene.</p><p>We adopt PointNet++ <ref type="bibr" target="#b35">[35]</ref> as the backbone for feature extraction, and obtain the global feature vector and point-wise feature vectors of P. Our center prediction network Φ C has a similar architecture as PointNet++ with four additional fullyconnected layers. Note that each output unit of the last fully-connected layer of Φ C is converted to probability via sigmoid.</p><formula xml:id="formula_1">Let Q = {Q i } N i=1 be the 'heatmap' generated by Φ C , where Q i ∈ [0, 1]</formula><p>is the estimated probability of point p i being the center of a 3D object instance. For the training of Φ C , we assume that the points of each object instance are distributed as a 3D multivariate Gaussian, and we derive the continuous relaxations from the discrete instance labels as the heatmap ground truths. Specifically, for each object instance, we pick the point that is closest to the instance's centroid as the Gaussian center, and generate the ground-truth heatmap values by computing the distances from points to the center. We apply a Gaussian function to each distance and normalize the value to [0, 1]. <ref type="figure">Fig. 2</ref> shows two examples of heatmaps predicted by the trained Φ C on the new input point clouds from the validation set of ScanNet dataset <ref type="bibr" target="#b8">[9]</ref>. In comparison with the ground-truth heatmaps, we can see that Φ C is able to approximate the center distributions very well even for unknown 3D scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2: Visualization of predicted and ground-truth center heatmaps on ScanNet</head><p>Center selection mechanism. To choose center candidates from Q, we can simply sort the heatmap values {Q i } N i=1 in descending order and pick those points with high probability values as the possible centers. However, since the high-probability points of the same instance tend to form a cluster near the center, naively deciding the center candidates according to a fixed sorting order would result in repeatedly selecting points from the same instance. To overcome this issue, each time a point p i ∈ P is selected as a center candidate, the remaining  for i ∈ I − do <ref type="bibr" target="#b7">8</ref> Qi ← 0 // Excluding redundant points 9 until Q * &lt; Q θ or T &gt; T θ sorted list will be updated so that for those high-probability points belonging to the same instance as p i , their heatmap values have to be reduced to zero.</p><formula xml:id="formula_2">= {pi} N i=1 and heatmap Q 1 C ← ∅ ; T ← 0 2 L = { i} N i=1 ← ΦS(P) // Semantic label 3 repeat 4 i * ← arg max i {Qi} ; Q * ← Qi * //</formula><p>To do so, we train a coupled semantic network (in this work we use sparse convolution network <ref type="bibr" target="#b14">[15]</ref>) to predict the semantic label of each point p i and subsequently decide the representative class radius r for each object class . Therefore, whenever a point p i is chosen as a center candidate, the heatmap values of all remaining points within the corresponding radius will be set to zero.</p><p>In this way the problem of redundant selections can be largely alleviated. The representative radius for each class is defined by the average (class-wise) instance size from the training data.</p><formula xml:id="formula_3">With {Q i } N i=1</formula><p>, the process of center selection will be repeatedly carried out until the currently largest heatmap value of being an instance center is below a pre-specified threshold Q θ , or the total number T of selected points exceeds T θ , which is the default upper bound of the number of object instances in a 3D scene. We outline the steps of the center selection mechanism in Algorithm 1, where the two selection thresholds are set as Q θ = 0.4 and T θ = 64 for all our experiments. The center selection mechanism will therefore yield T ≤ T θ center candidates, denoted as C = {C t } T t=1 . We remark that the effect of the proposed center selection mechanism in Algorithm 1 is analogous to performing non-maximum suppression (NMS) in advance. The resulting center candidates would be well separated from each other by the constraint of class radii, leading to less-redundant predicted bounding boxes and instance masks for further processing. Hence, our method does not require post-processing of non-maximum suppression to remove overlapped bounding boxes or masks for instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bounding-Box Prediction Network Φ B</head><p>The center prediction network and center selection mechanism provide the set of T predicted instance centers with their 3D coordinates, i.e., In principle, to generate a proper bounding box for each center C t , we should pay more attention to a properly-sized neighborhood of C t in the point cloud.</p><formula xml:id="formula_4">C = {C t } T t=1 ∈ R T ×3 .</formula><p>To this end, the instances of different classes in the training data are divided into K groups (K = 6 in our experiments), and we average the bounding-box sizes of each group to respectively obtain K different typical instance sizes s k for k = 1, 2, · · · , K, each with different length, width, and height to approximate the predicted instances shapes. Now, to generate the bounding box for each center C t ∈ C, we feed the point cloud P and the center candidates C to the bounding-box prediction network Φ B (comprising PointNet++ as the backbone). The network first predicts the probability P s k that measures how likely the resulting bounding box of C t could have the instance size s k . For each center C t , the network Φ B processes a context within the most appropriate instance size s k * , and then uses a shared PointNet++ network to extract local features, which are combined with the global feature for the subsequent convolutional layers to predict the corresponding bounding-box vertices:</p><formula xml:id="formula_5">B t = x min t y min t z min t , (x max t y max t z max t ) , for each selected center C t ∈ C .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mask Prediction Network Φ M</head><p>Inspired by the effectiveness of the 3D-BoNet <ref type="bibr" target="#b41">[41]</ref>, we design the mask prediction network Φ M by simultaneously considering point-wise and global features to predict the instance masks based on the resulting bounding boxes in the previous stage. However, a crucial difference between 3D-BoNet and our method is that we would consider T center candidates to predict T bounding boxes, and use these bounding boxes to predict T instance masks. The number T is not fixed and can adapt to each scene, depending on how many center candidates are uncovered by the center prediction network, while 3D-BoNet always handles a fixed predefined number of bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Functions</head><p>The proposed network is trained in an end-to-end manner and optimized by a joint loss L total consisting of several loss terms:</p><formula xml:id="formula_6">L total = L center + L bound + L IoU + L mask + L size .<label>(2)</label></formula><p>Specifically, we use the focal loss <ref type="bibr" target="#b22">[23]</ref> for both L center and L mask to enhance the center prediction network Φ C and the mask prediction network Φ M by focusing more on a sparse set of hard examples during training. The center loss term L center for center prediction network Φ C is defined as</p><formula xml:id="formula_7">L center = N i=1 −α(1 − Q i,f ) γ log(Q i,f ) ,<label>(3)</label></formula><p>where α and γ are the focal loss parameters, and f symbolizes the focal setting.</p><formula xml:id="formula_8">For each point p i , we set Q i,f = Q i if G t(i) (p i | C t(i) ) &gt; σ G , where G t(i)</formula><p>is the Gaussian on the ground-truth instance center C t(i) that is closest to p i . We use σ G = 0.4 in our experiments. For the other case with G t(i) (p i | C t(i) ) ≤ σ G , we set Q i,f = 1 − Q i , which means p i is not considered to be associated with the closest ground-truth instance.</p><p>To learn feasible instance sizes and bounding boxes from the predicted centers in the bounding-box prediction network Φ B , we define the sizes loss L sizes as the cross entropy loss and the bounding-box bound loss L bound as l 1 loss. Specifically, we can express the two losses as</p><formula xml:id="formula_9">L size = T t=1 − log(S t ) ,<label>(4)</label></formula><formula xml:id="formula_10">L bound = 1 T T t=1 l smooth 1 (B t − B t ) ,<label>(5)</label></formula><p>where T is total number of predicted centers and thus also the number of predicted bounding boxes; B t contains the vertices of the predicted bounding box for center C t ; B t contains the vertices of the corresponding ground-truth bounding box. S t is the predicted sizes probability, which assumes its value from one of P s k for k = 1, 2, · · · , K, depending on the size value of the corresponding ground truth. We use the smooth l 1 loss rather than the l 2 loss to ensure training stability and convergence.</p><p>Notice that, in comparison with the multi-criteria loss employed by 3D-BoNet <ref type="bibr" target="#b41">[41]</ref> for box prediction, which needs a box association layer to decide the mapping between predicted and ground-truth bounding boxes, our bounding box loss L bound in (5) is much simpler and more intuitive. Since the proposed GICN uses center candidates to predict bounding boxes, each bounding box exactly corresponds to a predicted center. Thus, the predicted bounding box can be conveniently associated with the ground-truth bounding box for the computation of the smooth l 1 loss.</p><p>Further, we use GIoU <ref type="bibr" target="#b36">[36]</ref> instead of vanilla IoU to compute the IoU loss, which is defined as</p><formula xml:id="formula_11">L IoU = 1 T T t=1 1 − GIoU(B t , B t ) .<label>(6)</label></formula><p>Finally, as mentioned early, we use the focal loss for our mask loss term L mask to compute the loss between the ground-truth and the predicted mask probability for each instance mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In the experiments we evaluate the performance of the proposed method on two benchmark datasets. Both datasets provide 3D data in form of colored point clouds that consist of 3D coordinates and RGB color information for the 3D scenes. The two datasets are detailed as follows.</p><p>-Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset <ref type="bibr" target="#b2">[3]</ref> collects six large-scale indoor-area scans from 271 rooms in three different buildings. We take the standard k-fold cross-validation scheme to evaluate the validation performance on S3DIS dataset. -ScanNet dataset <ref type="bibr" target="#b8">[9]</ref> is an RGB-D large-scale dataset containing 1,513 scans annotated with instance-level semantic segmentation labels. We randomly take 1,201 scenes for training and 312 scenes for validation, and finally test our method on ScanNet online benchmark for final evaluation.</p><p>In the following sections we report our validation performance on S3DIS dataset and the test performance on ScanNet dataset. We also perform the ablation study on Area-5 of S3DIS dataset to investigate the effectiveness of each component in the proposed pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement the proposed GICN in PyTorch and use two Nvidia GTX1080Ti GPUs for training. The learning rate of the model is 0.002 and then decays by half the value every 20 epochs. We use Adam optimizer to train the network. Our network usually converges at the 50th epoch, which takes about one day and three days, respectively, for training with S3DIS and ScanNet datasets.</p><p>Similar to SGPN <ref type="bibr" target="#b38">[38]</ref> and 3D-BoNet <ref type="bibr" target="#b41">[41]</ref>, during training we divide the whole scene into cubes of 1m 3 volume with a sliding window of stride 0.5m. At the test time, we perform the inference on all cubes in the whole scene, and use the blockmerging algorithm as SGPN to merge each cube's result to get the final output  <ref type="bibr" target="#b39">[39]</ref> 63.6 47.5 3D-BoNet <ref type="bibr" target="#b41">[41]</ref> 65.6 47.6 3D-BEVIS <ref type="bibr" target="#b9">[10]</ref> 65.6 n/a GICN (ours) 68.5 50.8</p><p>of instance segmentation for the 3D scene. Regarding the hyperparameter T θ for the maximum number of instance centers, we set it as 64 for both training and testing. Note that, in practice, after performing the center selection mechanism we usually have only 1 to 5 centers left in a cube during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on S3DIS Dataset</head><p>We test our method on S3DIS dataset, in which each scene is partitioned into 1m 3 cubes, and the number of 3D points of each cube is uniformly sampled to produce 4,096 points for training and testing. Each point is then represented by a 9D vector (RGB, normalized XYZ in block, and normalized XYZ in room) with a label from one of the 13 classes. We use 6-fold cross validation to evaluate the performance, and the scores and qualitative results are shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_4">Fig. 4</ref>. We compare the proposed GICN with 3D-BoNet (the state-of-the-art method on S3DIS dataset), as well as 3D-BEVIS <ref type="bibr" target="#b9">[10]</ref> and ASIS <ref type="bibr" target="#b39">[39]</ref>. The metrics we use for the evaluation are mean precision (mPrec) and mean recall (mRec) with IoU threshold 0.5. We use the block-merging algorithm to merge the instances from different cubes like SGPN <ref type="bibr" target="#b38">[38]</ref>. Our proposed method outperforms the state-ofthe-art methods by at least 2.9% increase in mAP, owing to the formulation and the learning of the Gaussian heatmap that approximates the distribution of instance centers and sizes for generating instance masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation on ScanNet Dataset</head><p>We further evaluate our method on ScanNet v2 3D semantic instance segmentation dataset. Each scene is also divided into 1m 3 cubes with uniformly sampled 4,096 points for training. Our model is applied to all points during testing, and we use the block-merging algorithm <ref type="bibr" target="#b38">[38]</ref> to construct the complete segmentation of the entire 3D scene.</p><p>The evaluation is performed on 18 object classes and the average precision with an IoU threshold 0.5 (AP@50%) is used as the evaluation metric. For comparison, we show our quantitative results in <ref type="table" target="#tab_1">Table 2</ref> based on the ScanNet v2 benchmark, and the qualitative results are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. The proposed GICN achieves the state-of-the-art performance in comparison with the existing instance segmentation methods that have already been published in the literature at the time of ECCV 2020 submission. It can be seen that our method performs less well on classes of instances that resemble a vertical surface, e.g., curtain and picture, in comparison with other voxel-based methods like MTML <ref type="bibr" target="#b18">[19]</ref> and PanopticFusion <ref type="bibr" target="#b28">[28]</ref>. Such classes are harder to find centers, while other classes like toilet and bathtub have more compact structures so that their centers are easier to be identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>The ablation study is aimed to investigate the effectiveness of each key component of GICN. We expect to provide more insights into how the components may affect the performance of 3D instance segmentation. We conduct the ablation study on the Area-5 data of S3DIS dataset, which is the hardest area among all areas in S3DIS. <ref type="table" target="#tab_2">Table 3</ref> summarizes the quantitative results of our ablation study. 1. Without instance size prediction: GICN predicts the probability of the instance size for modeling each instance center and then checks which size The second column depicts the predicted masks. The third column shows the ground-truth masks. Note that the color code assigned to each instance does not have to match the ground truth. Only the structure of the mask matters group the center belongs to for subsequent bounding box prediction. For comparison, we retrain the network to directly predict the bounding box without predicting the instance size, and just extract features from points inside a fixed range. The result shows that instance size prediction improves the performance by 3.7% on mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Without focal loss:</head><p>To make GICN focus on difficult cases, we use the focal loss <ref type="bibr" target="#b22">[23]</ref> to help the network solve the imbalance problem of predicting various instances. The gap between using the focal loss and using a vanilla cross entropy loss is large. We observe an improvement of 14% on mAP. The  result shows that without the focal-loss strategy our network tends to learn merely the simpler instances. 3. Without center prediction or selection: Our center prediction network yields the center heatmap for deciding the center candidates. To validate its effect, we replace the heatmap-guided center selection by randomly choosing T θ centers as the candidates or by selecting the top T θ centers based on the heatmap values. (We set T θ = 64.) The drop of 10.3% and 8.4% on mAP shows that the predicted heatmaps and the selection mechanism provide useful center information for further prediction of bounding box and mask. 4. Without semantic radius prior: In the center selection mechanism, we use semantic class radii to choose the center. If we assume that instances of all classes have uniform size, the mAP drops 2.1% and the mRec drops 1.1% because we may redundantly select duplicate centers in one instance and miss instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussions</head><p>Computational cost. <ref type="table" target="#tab_3">Table 4</ref> summarizes the timing results of different 3D instance segmentation approaches. Experiments are done using a single Titan X GPU. The results show that our method is more efficient than other methods because we do not need additional post-processing steps like Mean Shift or NMS. Moreover, benefited from the center select mechanism, we only need to handle a small number of predicted instances and therefore is faster than 3D-BoNet <ref type="bibr" target="#b41">[41]</ref>, which predicts a larger, fixed number of bounding boxes.</p><p>Dealing with hollow objects. If the shape of an instance is hollow (e.g. bathtub), there would be no point cloud near the central region and most of its points might be far from the instance center location. In that case, we will choose the point that is nearest to the center location, and the instance size prediction mechanism described in Sec. 3.2 could estimate an appropriate size that covers most of the point cloud even if the points are not close to the instance center.</p><p>The case of two center candidates being close to each other. The distance constraint of the center selection mechanism (Step 6 of Algorithm 1) is imposed to avoid selecting more than one center candidate for the same instance. It rarely happens that the constraint eliminates all the points of a nearby instance since the semantic radius prior is derived from the training data and is therefore quite reliable in principle. For comparison, <ref type="figure" target="#fig_6">Fig. 6</ref> illustrates some example results of GICN and 3D-BoNet <ref type="bibr" target="#b41">[41]</ref> on the validation split of ScanNet v2 dataset. Our method can separate the instances well even if they are close to each other while 3D-BoNet <ref type="bibr" target="#b41">[41]</ref> fails to generate the correct masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel center-dictated size-aware 3D instance segmentation method on point clouds. The proposed method, Gaussian Instance Center Network (GICN), aims for learning the Gaussian heatmap that approximates the spatial distribution of instances. By leveraging the center prediction mechanism, GICN can extract the precise instance masks according to the information encoded from the localized centers. We demonstrate the ability of GICN by evaluating the validation and test performance on S3DIS and ScanNet datasets. GICN achieves state-of-the-art results on both benchmarks. Future work may include improving the accuracy of finding centers for those difficult semantic classes, as well as using metric learning like MTML <ref type="bibr" target="#b18">[19]</ref> to learn feature embeddings for further enhancement on visual semantic reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Generating Ground-Truth Center Heatmaps</head><p>Our method needs ground-truth center heatmaps to train the center prediction network. In <ref type="figure" target="#fig_7">Fig. 7</ref> we show the point cloud of a chair as an example to explain how we generate the ground-truth center heatmaps for training. To compute the heatmap values for an instance in a scene, we first find the point closest to the instance center, and then we apply Gaussian function to all points of the instance with respect to the chosen centroid point and get the final heatmap for the instance. We show more heatmap predictions and qualitative results on the validation split of ScanNet v2 dataset in <ref type="figure">Fig. 8</ref> and <ref type="figure">Fig. 9</ref>. Additional results on S3DIS dataset are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. From <ref type="figure">Fig. 8</ref> it can be observed that the center heatmaps are quite capable of capturing instance information, and our method is able to predict accurate center heatmaps in comparison with the ground truth. The peaks in a heatmap imply the center positions, and using the center selection mechanism mentioned in the main paper can easily single out proper centers for further prediction. In the fourth row of <ref type="figure">Fig. 8</ref> we can see that although some instances are close to each other, the predicted heatmaps can still well represent the center probability. The proposed model, Gaussian Instance Center Network (GICN), consists of three sub-networks: center prediction network Φ C , bounding-box prediction network Φ B , and mask prediction network Φ M . We describe the network architecture of GICN in this section and summarize the details is <ref type="table" target="#tab_4">Table 5</ref>. For the center prediction network, we use PointNet++ as our backbone. Following the same notation in PointNet++, we have SA(K, r, [l 1 , . . . , l d ]) as a set abstraction (SA) module that contains d 1 × 1 convolution layers for K neighbourhood regions in radius r, where l i (i = 1, . . . , d) is the number of output channels for the ith layer. FP([l 1 , . . . , l d ]) is a feature propagation (FP) module consisting of d 1 × 1 layers, where the ith layer has l i output channels.</p><p>The bounding-box prediction network takes the point cloud and the predicted centers as input, and use PointNet++ to encode the context with respect to the predicted sizes. The parameters used in the bounding-box prediction network are shown in <ref type="table" target="#tab_4">Table 5</ref>, where MLP([l 1 , . . . , l d ]) comprises multi-layer perceptron with l i output channels for the ith layer (i = 1, ..., d). The output features will be concatenated with global features and then fed into several MLPs to predict the bounding box coordinates.</p><p>In the mask prediction network, we use the convolution layer to reduce dimensions of point features and global features, and then concatenate them together. The concatenated features go through several convolution layers with the predicted bounding box information to localize the instances. Finally, these features will pass through three convolution layers listed in <ref type="table" target="#tab_4">Table 5</ref> to get N × 1 mask for each predicted instance. Conv(C, [h, w], [s 1 , s 2 ]) is a convolution layer, where [h, w] is the kernel size and [s 1 , s 2 ] denotes the stride. Note that the kernel size <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">134]</ref> represents the 128 channels of concatenated features plus the six-dimensional box information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An overview of GICN. The global and local features are extracted from the input point cloud and then passed through the center prediction network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Center selection &amp; thresholds Q θ , T θ Input: A semantic net ΦS, representative class radii {r } Output: A set of center candidates C = {Ct} T t=1 Data: Point cloud P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>The largest heatmap value 5 T ← T + 1 ; 6 I</head><label>516</label><figDesc>CT ← pi * // A chosen center candidate − ← {i | d(pi, pi * ) ≤ r * } // Filtered by the class radius 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Bounding-box prediction network Φ B . The network first predicts the instance size for each of the T selected centers, and then uses a shared PointNet++ network to extract features from the point cloud within the neighborhood of the predicted size. The extracted local features combined with the global features will go through convolutional layers to predict T bounding boxes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Results of S3DIS dataset. The first column shows the input point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results of the validation split of ScanNet v2 dataset. Different colors indicate different instances. Moe results are in Appendix B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison with 3D-BoNet from the validation split of ScanNet v2 dataset. The red circles show some examples that 3D-BoNet fails to segment but the proposed GICN successfully produces the instance masks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>An example of ground-truth center heatmap Appendix B: More Qualitative Results on ScanNet and S3DIS Datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :Fig. 9 :Fig. 10 :</head><label>8910</label><figDesc>Center heatmap prediction results from the validation split of ScanNet v2 dataset. The background is shown in gray color More qualitative results from the validation split of ScanNet v2 dataset. Different colors indicate different instances. We illustrate the background semantic in black for better visualization More qualitative results from the validation split of S3DIS dataset. Different colors indicate different instances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons on S3DIS instance segmentation (6-fold cross validation)</figDesc><table><row><cell>mPrec (%) mRec (%)</cell></row><row><cell>ASIS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ScanNet v2 instance segmentation online benchmark. The table shows AP@50% score of each semantic class. Our method achieves the best mean AP@50% performance among all existing methods published in the literature</figDesc><table><row><cell>Method</cell><cell>mean</cell><cell>bathtub</cell><cell>bed</cell><cell>bookshelf</cell><cell>cabinet</cell><cell>chair</cell><cell>counter</cell><cell>curtain</cell><cell>desk</cell><cell>door</cell><cell>other</cell><cell>picture</cell><cell>refrigerator</cell><cell>shower curtain</cell><cell>sink</cell><cell>sofa</cell><cell>table</cell><cell>toilet</cell><cell>window</cell></row><row><cell>SGPN [38]</cell><cell cols="7">14.3 20.8 39.0 16.9 6.5 27.5 2.9</cell><cell cols="12">6.9 0.0 8.7 4.3 1.4 2.7 0.0 11.2 35.1 16.8 43.8 13.8</cell></row><row><cell>3D-BEVIS [10]</cell><cell cols="4">24.8 66.7 56.6 7.6</cell><cell cols="15">3.5 39.4 2.7 3.5 9.8 9.9 3.0 2.5 9.8 37.5 12.6 60.4 18.1 85.4 17.1</cell></row><row><cell>DPC-instance [12]</cell><cell cols="19">35.5 50.0 51.7 46.7 22.8 42.2 13.3 40.5 11.1 20.5 24.1 7.5 23.3 30.6 44.5 43.9 45.7 97.4 23.9</cell></row><row><cell>3D-SIS [17]</cell><cell cols="19">38.2 100 43.2 24.5 19.0 57.7 1.3 26.3 3.3 32.0 24.0 7.5 42.2 85.7 11.7 69.9 27.1 88.3 23.5</cell></row><row><cell>MASC [24]</cell><cell cols="19">44.7 52.8 55.5 38.1 38.2 63.3 0.2 50.9 26.0 36.1 43.2 32.7 45.1 57.1 36.7 63.9 38.6 98.0 27.6</cell></row><row><cell cols="20">ResNet-backbone [22] 45.9 100 73.7 15.9 25.9 58.7 13.8 47.5 21.7 41.6 40.8 12.8 31.5 71.4 41.1 53.6 59.0 87.3 30.4</cell></row><row><cell cols="20">PanopticFusion [28] 47.8 66.7 71.2 59.5 25.9 55.0 0.0 61.3 17.5 25.0 43.4 43.7 41.1 85.7 48.5 59.1 26.7 94.4 35.9</cell></row><row><cell>3D-BoNet [41]</cell><cell cols="19">48.8 100 67.2 59.0 30.1 48.4 9.8 62.0 30.6 34.1 25.9 12.5 43.4 79.6 40.2 49.9 51.3 90.9 43.9</cell></row><row><cell>MTML [19]</cell><cell cols="19">54.9 100 80.7 58.8 32.7 64.7 0.4 81.5 18.0 41.8 36.4 18.2 44.5 100 44.2 68.8 57.1 100 39.6</cell></row><row><cell>GICN (ours)</cell><cell cols="19">63.8 100 89.5 80.0 48.0 67.6 14.4 73.7 35.4 44.7 40.0 36.5 70.0 100 56.9 83.6 59.9 100 47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on Area-5 of S3DIS dataset ( †: random selection. * : top T θ ) Center Prediction / Selection 51.2 † / 53.1 * 32.2 † / 33.7</figDesc><table><row><cell></cell><cell>mPrec</cell><cell>mRec</cell></row><row><cell>Ours (GICN)</cell><cell>61.5</cell><cell>43.2</cell></row><row><cell>w/o Instance Size Prediction</cell><cell>57.8</cell><cell>41.3</cell></row><row><cell>w/o Focal Loss</cell><cell>47.4</cell><cell>35.3</cell></row><row><cell>w/o w/o Semantic Radius Prior</cell><cell>59.4</cell><cell>42.1</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The timing results on the ScanNet v2 validation split (312 scenes)</figDesc><table><row><cell>Method</cell><cell>SGPN</cell><cell>ASIS</cell><cell>GSPN</cell></row><row><cell></cell><cell>network (GPU): 650</cell><cell>network (GPU): 650</cell><cell>network (GPU): 500</cell></row><row><cell>Time (sec)</cell><cell>group merging (CPU): 46,562</cell><cell>mean shift (CPU): 53,886</cell><cell>point sampling (GPU): 2,995</cell></row><row><cell></cell><cell>block merging (CPU): 2,221</cell><cell>block merging (CPU): 2,221</cell><cell>neighbor search (CPU): 468</cell></row><row><cell>Total Time (sec)</cell><cell>49,433</cell><cell>56,757</cell><cell>3,963</cell></row><row><cell>Method</cell><cell>3D-SIS</cell><cell>3D-BoNet</cell><cell>GICN (ours)</cell></row><row><cell>Time (sec)</cell><cell>voxelization, projection, network, etc. (GPU+CPU): 38,841</cell><cell>network (GPU): 650 SCN (GPU parallel): 208 block merging (CPU): 2,221</cell><cell>network (GPU): 467 SCN (GPU parallel): 208 block merging (CPU): 2,221</cell></row><row><cell>Total Time (sec)</cell><cell>38,841</cell><cell>2,871</cell><cell>2,688</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Detailed network architecture of GICN</figDesc><table><row><cell>Sub-network</cell><cell>Architecture</cell></row><row><cell></cell><cell>SA(1024, 0.1, [32, 32, 64])</cell></row><row><cell></cell><cell>SA(256, 0.2, [64, 64, 128])</cell></row><row><cell></cell><cell>SA(64, 0.4, [128, 128, 256])</cell></row><row><cell>Center</cell><cell>SA(None, None, [256, 256, 512])</cell></row><row><cell>prediction network</cell><cell>FP([256, 256])</cell></row><row><cell></cell><cell>FP([256, 256])</cell></row><row><cell></cell><cell>FP([256, 128])</cell></row><row><cell></cell><cell>FP([128, 128, 128])</cell></row><row><cell>Bounding-box</cell><cell>MLP([64, 128, 256]) (Before concat)</cell></row><row><cell>prediction network</cell><cell>MLP([512, 128, 6]) (After concat)</cell></row><row><cell>Mask prediction network</cell><cell>Conv(64, [1, 134], [1,1]) Conv(32, [1, 1], [1, 1]) Conv(1, [1, 1], [1, 1])</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix C: Network Architecture Details</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking task and metrics of instance segmentation on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mukuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno>abs/1909.12655</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2858" to="2866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MaskLab: instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified point-based framework for 3d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision, 3DV 2019</title>
		<meeting><address><addrLine>Québec City, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2432" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d bird&apos;s-eye-view instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition -41st DAGM German Conference, DAGM GCPR 2019</title>
		<meeting><address><addrLine>Dortmund, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dilated point convolutions: On the receptive field of point convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1907.12046</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<title level="m">Dilated point convolutions: On the receptive field of point convolutions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The estimation of the gradient of a density function, with applications in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SSAP: single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D-SIS: 3d semantic instance segmentation of RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4421" to="4430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="9018" to="9028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d instance segmentation via multi-task metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2978" to="2991" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">3d graph embedding learning with a structure-aware loss function for point cloud semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MASC: multi-scale affinity with sparse convolution for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<idno>abs/1902.04478</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SGN: sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="3516" to="3524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Affinity derivation and graph merge for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">Proceedings, Part III</title>
		<meeting>Part III<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="708" to="724" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">PanopticFusion: online volumetric semantic mapping at the level of stuff and things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kaji</surname></persName>
		</author>
		<idno>abs/1903.01177</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8837" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-convolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotný</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="89" to="105" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">JSIS3D: joint semanticinstance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mvx-net: Multimodal voxelnet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation, ICRA 2019</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7276" to="7282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SGPN: similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<idno>abs/1906.01140</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GSPN: generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3947" to="3956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
