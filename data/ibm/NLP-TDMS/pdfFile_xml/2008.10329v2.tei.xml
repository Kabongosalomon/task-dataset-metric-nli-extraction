<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascade Convolutional Neural Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of mathematics and statistics</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of mathematics and statistics</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of mathematics and statistics</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of mathematics and statistics</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascade Convolutional Neural Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Super-resolution</term>
					<term>cascade structure</term>
					<term>convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstractï¼šWith the development of the super-resolution convolutional neural network (SRCNN), deep learning technique has been applied to the field of image super-resolution communities. Many researchers focus on optimizing and improving the structure of SRCNN, which achieved well performance in speed and restoration quality for image super-resolution. However, most of these previous approaches only consider a specific scale images during the training process, while ignoring the relationship between different scale images. Motivated by this concern, in this paper, we propose a cascade convolution neural network (CSRCNN) for image super-resolution, which includes three cascaded Fast SRCNN and each Fast SRCNN can process a specific scale image. Thus, different scale images can be trained at the same time and the learned network can make full use of the information reside in different scale images. Extensive experiments have shown that our method performs better than the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single image super-resolution (SR) is an important issue in the field of computer vision research. Given a low resolution (LR) image, the purpose of single image super-resolution is to recovering a high resolution (HR) image corresponding to the LR image. Currently, single image super-resolution based methods are generally divided into two categories: sparse codingbased methods <ref type="bibr" target="#b29">(Yang and Yang., 2013;</ref><ref type="bibr" target="#b9">Farhadifard et al., 2014;</ref><ref type="bibr" target="#b14">Lin-Yuan et al., 2016;</ref><ref type="bibr" target="#b2">Ahmed et al., 2018;</ref><ref type="bibr" target="#b34">Yang et al., 2019)</ref> and deep learning-based methods <ref type="bibr" target="#b4">(Cui et al., 2014;</ref><ref type="bibr" target="#b5">Choi et al., 2018;</ref><ref type="bibr" target="#b8">Dong et al., , 2016</ref>.</p><p>Most sparse coding-based methods assume that each pair of patches in LR and HR images have similar coding coefficients in the patch space <ref type="bibr">(Yang et al., 2010;</ref><ref type="bibr" target="#b9">Farhadifard et al., 2014;</ref><ref type="bibr" target="#b0">Ahmed et al., 2016</ref><ref type="bibr" target="#b1">Ahmed et al., , 2017</ref>. Therefore, the HR image patches can be represented by the sparse coding LR image patches. Specifically, learning a pair of dictionaries and for LR and HR image patches respectively <ref type="bibr" target="#b27">(Yang et al., 2008</ref><ref type="bibr" target="#b28">(Yang et al., , 2012</ref>. The final reconstructed HR image is obtained through the learned dictionary and coding obtained by the learned dictionary <ref type="bibr" target="#b37">(Zhang et al.,2017</ref><ref type="bibr" target="#b42">Zheng et al., 2019)</ref>. At present, existing sparse codingbased methods mainly focus on how to learn better dictionaries and optimize dictionaries <ref type="bibr" target="#b40">(Zhao et al., 2018;</ref><ref type="bibr" target="#b2">Ahmed et al., 2018;</ref><ref type="bibr" target="#b34">Yang et al., 2019;</ref><ref type="bibr">)</ref>. For example, Zhao et al <ref type="bibr" target="#b40">(Zhao et al., 2018;</ref><ref type="bibr">)</ref> design a transfer robust sparse coding based on graph for image representation and Ahmed et al <ref type="bibr" target="#b2">(Ahmed et al., 2018)</ref> propose a new multiple dictionary learning strategy, which achieved well performance in speed and restoration quality for image super-resolution.</p><p>Deep learning-based methods aims to directly learn an end-to-end mapping from LR images to HR images <ref type="bibr" target="#b4">(Cui et al., 2014;</ref><ref type="bibr" target="#b5">Choi et al., 2018;</ref><ref type="bibr" target="#b8">Dong et al., , 2016</ref>.  first applied convolutional neural network to the single image super-resolution, proposed a super-resolution convolutional neural network (SRCNN) and verified its better restoration performance. However, SRCNN only learns the mapping from bicubic interpolated LR images (not the original LR images) to HR images, the calculation cost of the network will be increased quadratically <ref type="bibr" target="#b8">(Dong et al., 2016)</ref> with the size of HR image increases. In addition, SRCNN adopts a 5*5 convolution kernel to learn nonlinear mapping which obviously limited the learning ability of network in such a setting. To solve the above problems, <ref type="bibr">Kim et al (Dong et al., 2014, 2016;</ref><ref type="bibr" target="#b13">Kim et al., 2016;</ref><ref type="bibr" target="#b17">Lai et al. 2017;</ref><ref type="bibr" target="#b16">Lim et al., 2017)</ref> proposed lots of approaches such as the large super-resolution convolutional neural network (SRCNN-ex), fast super-resolution convolutional neural networks (FSRCNN), very deep super-resolution networks (VDSR), Laplacian pyramid super-resolution network (LaPSNR) and enhanced deep super-resolution network (EDSR) to improve the network structure of SRCNN. In addition, Christian et al <ref type="bibr" target="#b15">(Ledig et al., 2017)</ref> exploited generative adversarial networks (GAN) <ref type="bibr" target="#b10">(Goodfellow et al., 2014)</ref> to handle out image super-resolution problem and proposed superresolution generative adversarial network (SRGAN). GAN is different from convolutional neural networks (CNN), the main difference is that GAN aims to generate more realistic or HR images that are more consistent with human eye, while the aim of CNN is to faithfully restore the high-frequency information of images. Lots of improved networks based on GAN have been proposed in recent years . These networks have solved the above problems to some extent and achieved well restoration performance.</p><p>However, all of the models mentioned above only trained the images in a specific scale and do not consider the relationships between different image scales. For example, if we set the scale factor is 4 during the training process, FSRCNN only uses the image information contained in scale of 4, and does not make full use of the complementary information resided in different scales. if we can make full use of the information of different scale images in the training process, the quality of recovered images will be further improved.</p><p>Based on the above ideas, we design a cascaded convolutional neural networks (CSRCNN). It consists of three cascaded FSRCNN, where each FSRCNN can process a specific scale image. For each FSRCNN, we set its scaling factor is 2 which can double enlarge the size of input image. Suppose that all person images have the same weight-to-height ratio. and respectively denote the weight and height of the original HR image. /8 * /8, /4 * /4 and /2 * /2 are respectively image shapes input to three subnetworks. Since images with different scale are trained together, the learned network can make use of image information of different scales. In addition, we use L1 loss function to make the reconstructed image clearer in texture and edge in the training process. Finally, we evaluated the impact of the number of cascaded FSRCNN on our network performance in 4.3. Experiments show that the proposed cascade convolutional neural network can achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning for Image Super-Resolution</head><p>The purpose of image super-resolution is to reconstruct high resolution image from a given low resolution image one.  first applied convolutional neural network to the field of image super-resolution and proposed a super-resolution convolutional neural network (SRCNN) which achieved better performance for image restoration. Recently, deep learning technique has been widely applied to the image super-resolution communities and most of works focus on optimizing and improving the structure of SRCNN. Various network structures have been developed, such as deep network with residual learning <ref type="bibr" target="#b13">(Kim et al., 2016)</ref>, Laplace pyramid structure <ref type="bibr" target="#b17">(Lai et al., 2017)</ref>, residual block , and residual dense network <ref type="bibr" target="#b5">(Choi et al., 2018)</ref>, which obtained superior performances. Besides supervised learning, unsupervised learning <ref type="bibr" target="#b20">(Peng., 2019;</ref><ref type="bibr" target="#b21">Timofte et al., 2013)</ref> and reinforcement learning <ref type="bibr" target="#b31">(Yun et al., 2017;</ref><ref type="bibr" target="#b22">Timofte et al., 2014)</ref> are also introduced to solve the problem of image superresolution in recent years. Specifically, the literature  has a systematic description of image super resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fast Super-Resolution Convolutional Neural Networks</head><p>Fast super-resolution convolutional neural networks (FSRCNN) was proposed by <ref type="bibr" target="#b8">Dong et al (2016)</ref>, aiming to accelerate the previously proposed SRCNN. Compared with SRCNN, FSRCNN can directly learn the mapping from the original LR image to HR image by introducing the deconvolution layer at the end of the network. The calculation cost of the whole network has been reduced to a certain extent by the above methods. In addition, FSRCNN adds a shrinking and an expanding layer at the beginning and the end of the mapping layer respectively, to enhance the presentation capability of non-linear mapping <ref type="bibr" target="#b11">(He et al., 2016)</ref>. In the nonlinear layer, FSRCNN adopted a smaller filter (size=3*3). The whole network is designed as a compact hourglass-type CNN structure, which includes five parts: feature extraction, shrinking, non-linear mapping, expanding, deconvolution. The shrinking layer uses a filter of size 1*1 to reduce the LR feature dimension from to , where is the feature dimension of LR image after feature extraction and is the number of filters ( â‰¤ ). The expanding layer uses filters with a size of 1*1 to maintain consistency with the shrinking layer, which is the inverse operation of the shrinking layer. To avoid the "dead features" <ref type="bibr" target="#b39">(Zeiler and Fergus., 2014)</ref> caused by zero gradients in , the author uses <ref type="bibr" target="#b12">(He et al., 2015)</ref> as the activation function after each convolution layer. Thus, a complete FSRCNN can be represented as: We propose a cascade convolutional neural network (CSRCNN) framework in this section. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the network structure of CSRCNN. which consists of three cascaded FSRCNN <ref type="bibr" target="#b8">(Dong et al., 2016)</ref>, and each FSRCNN can process a specific scale image. In each sub FSRCNN, we set its scaling factor to 2 which can double enlarge the size of input image. Suppose that all person images have the same weight-to-height ratio. and respectively denote the weight and height of the original HR image.</p><formula xml:id="formula_0">( ) ( ) ( ) ( ) ( )</formula><p>/8 * /8, /4 * /4 and /2 * /2 are respectively image shapes input to three subnetworks. Here, we use to represent the input image of each sub FSRCNN, where = 0,1,2 represents the ID of each sub FSRCNN and the scale index of the input image. We set to represent the scale ratio of LR image to HR image, the size of input image is described as * . The scale ratios are respectively r0 = 1/8, r1 = 1/4, r2 = 1/2, and r3 = 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Function</head><p>Our network is composed of three cascaded FSRCNN. For each subnetwork, on the one hand, the output image of network will enter the next cascaded subnetwork for training. On the other hand, it will form a sub loss function with the corresponding real HR image. The loss function of the whole network is composed of three sub loss functions. 0 , 1 , 2 used in our experiments, represent the loss function of three subnetwork respectively. The loss function of the whole network is represented by = 0 + 1 + 2 .</p><p>(2) For each subnetwork, the loss function is calculated as follows: (3)</p><p>In the selection of loss function of each sub-network, we use L1 loss function to replace MSE loss function. The main reason is that L1 loss function make the reconstructed image clearer in texture and edge in the training process, while MSE loss function will lose high-frequency information in the reconstruction process, such as texture and edge <ref type="bibr" target="#b30">(Yi and Huang., 2015)</ref>, which makes the final reconstructed image have poor perception performance. After the CSRCNN, all LR images are enlarged to the uniform HR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Assessment Process</head><p>During the evaluation process, we will assign LR image to different stages of the cascaded network according to the scale ratio of LR images to HR images. When a LR image is assigned to the FSRCNN-k, it will be resized to the input shape of corresponding network * . For example, for a given test image with upscaling factor 3Ã—, we will resize the shape of the image to / * / . The resized image will be arranged into FSRCNN2. Here, each subnetwork can double enlarge the size of input image. At the end of the evaluation, all images are enlarged to the uniform HR image <ref type="bibr" target="#b19">(Pang et al., 2017)</ref>. <ref type="figure" target="#fig_2">Fig.2</ref> shows the entire evaluation process for the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Differences between FSRCNN</head><p>Our network contains three cascade FSRCNN where each FSRCNN can process a specific scale image. The loss function of each FSRCNN is L1-Loss function, while the loss function of FSRCNN <ref type="bibr" target="#b8">(Dong et al., 2016)</ref> is MSE Loss function. In addition, FSRCNN only focus on a specific scale images in the training process, while our network can train different scales image at the same time due to the use of cascade structure. During the test stage, if we want to reconstruct two different scales images (such as 2Ã—, 4Ã—), we need to train two networks separately when we adopt FSRCNN, while our methods just only need to train once to get the high resolution images with scales of 2Ã—, 4Ã— and 8Ã—.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1Datasetï¼š</head><p>Training and test datasetï¼šFollowing SRCNN and FSRCNN, we combined 91 images dataset and the general-100 dataset to train our network. In particular, general-100 dataset were proposed by <ref type="bibr" target="#b8">Dong et al (2016)</ref>, it contains 100 bmp-format images with ranges from 710Ã—704 (large) to 131 Ã— 112 (small) in size. These images are all of favorable quality with clear edges and less smooth regions. In addition, we adopted the data augmentation method <ref type="bibr" target="#b24">(Wang, Z., 2015)</ref> to make full use of the dataset which includes scaling and rotating images. For test dataset, we use the Set5 <ref type="bibr" target="#b3">(Bevilacqua et al., 2012)</ref>, Set14 <ref type="bibr" target="#b38">(Zeyde et al., 2010)</ref> and BSD200 dataset <ref type="bibr" target="#b18">(Martin et al., 2001)</ref>. All test images will be cropped according to the model structure which make the size of the model output image an integer. Training samples: Due to our network use cascade structure, different scale images can be trained at the same time. To form LR/HR sub-image pairs, we first sample the original training images by different scale factor , then crop LR images obtained in sampling process and the ground truth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>Training strategy: For deep model, the model performance will improve with the increase of data, so we combined 91 images dataset and the general-100 dataset <ref type="bibr" target="#b8">(Dong et al., 2016)</ref> in training processing. First, we use the 91-image dataset to train a network from scratch. Then, we add the General-100 dataset to network for fine-tuning when the training is saturated. With this strategy, the training converges much earlier than training with the two datasets from the beginning. The choice of learning rate: The performance of the network will be affected by the choice of different learning rates. Choosing a good learning rate is important for the performance of network. For FSRCNN, the learning rate of the convolution layers is set to be 10 âˆ’3 , the learning rate of the deconvolution layer is set to be 10 âˆ’4 , the learning rate of all the layers is reduced by half during fine -tuning. Obviously, the learning rate of convolution layer and deconvolutional layer is static in each iteration, while we adopt a dynamic method to update the learning rate of convolution layer and deconvolutional layer in our network which make our network produce better learning rate according to different iterations. We take 0 = 10 âˆ’3 , 10 âˆ’4 as the initial learning rate of the convolution layer and the deconvolution layer, and set the total number of iterations of the network as . When the number of iterations is , the learning rate of the network convolution layer and the deconvolution layer is represented as: The choice of networks initialization: Network initialization to the training of the network has a great influence, a good initialization method can largely reduce the training time of the network. Due to our network selected <ref type="bibr" target="#b12">(He et al., 2015)</ref> as the activation function, we chose the MSRA initialization <ref type="bibr" target="#b12">(He et al., 2015)</ref> which make each layer neuron input/output variance is consistent. This is a mean of zero and variance of 2 gaussian distribution, which makes our networks to converge faster. Parameter setting: Our cascade convolutional neural network consists of three subnetworks. For each sub-network, there are three sensitive variables governing the network performance which includes the LR feature dimension , the number of shrinking filters , and the mapping  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation for the number of cascaded FSRCNN</head><p>In this section, we compare the impact of the number of cascaded FSRCNN on network performance. The network composed of one FSRCNN, two cascaded FSRCNN and three cascaded FSRCNN are represented as Net1, Net2 and Net3 respectively. For Net2, the loss function of the whole network is composed of L1 and L2 in 3.2 summary. We select PSNR as evaluation indicators. Table1 shows the results visually on Set5 <ref type="bibr" target="#b3">(Bevilacqua et al., 2012)</ref>, Set14 <ref type="bibr" target="#b38">(Zeyde et al., 2010)</ref>, BSD200 dataset for 4Ã—. From table1, we observed that the PSNR value obtained by use Net3 is higher than that obtained by use Net2 and Net1 and the PSNR value obtained by use Net2 is higher than that obtained by use Net1 , which verified that use different scale image information in the training process can improve the performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-Arts</head><p>In order to verify the well reconstruction performance of our proposed CSRCNN. We compare the proposed CSRCNN with 3 state-of-the-art SR algorithms: Bicubic, SRCNN , FSRCNN <ref type="bibr" target="#b8">(Dong et al., 2016)</ref>. We carry out extensive experiments on three datasets: Set5 <ref type="bibr" target="#b3">(Bevilacqua et al., 2012)</ref>, Set14 <ref type="bibr" target="#b38">(Zeyde et al., 2010)</ref>, BSD200 dataset <ref type="bibr" target="#b18">(Martin et al., 2001)</ref>, and adopted two image quality indicators to evaluate the SR images which includes PSNR and SSIM <ref type="bibr" target="#b23">(Wang et al., 2004)</ref>. Table2 shows quantitative comparisons for 2Ã—, 3Ã— and 4Ã— SR. From Table2, we can observe that compared with the above methods, the proposed CSRCNN achieves the best results in three evaluation datasets for 2Ã—, 3Ã— and 4Ã— SR. <ref type="figure" target="#fig_4">Fig.3</ref> show the convergence curves of CSRCNN on these three evaluation datasets with upscaling factor 3. Among them, the PSNR value of the proposed CSRCNN is 0.51db, 1.04db and 0.46db higher than that of FSRNCNN in Set5 datasets for 2Ã—, 3Ã— and 4Ã— SR. We observed that the PSNR value of CSRCNN is 1.04 higher than that of FSRCNN for 3Ã— SR, one possible reason is that we preprocessed the LR image, we use bicubic interpolation when we threw the LR image into the network, such that the size of the interpolated image was half that of the real HR image.</p><p>Consider our network is made up of three cascade FSRCNN where each FSRCNN's upscaling factor is 2, we also made a comparison for multiple scaling factors 8 for FSRCNN. This comparison is shown Table3. Obviously, our network produces higher PSNR and SSIM values than FSRCNN due to the cascade design.  In <ref type="figure">Fig.4</ref>, we show visual comparisons on Set5 and Set14 datasets with upscaling factor 3. As you can see, our method accurately reconstructs the details of the image, such as texture and edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a more efficient network for mining image information in different scales, named Cascade convolutional neural network. We used a cascaded network structure to make full use of the image information reside in different scale of images. Furthermore, we adopted L1 loss function as the loss function of each subnetwork in the training process which make the reconstructed image clearer in texture and edge. Finally, we also explore the effects of the number of cascaded FSRCNN for the performance of our network. Extensive experiments show that the proposed method achieves satisfactory SR performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>the output image of each FSRCNN that is the restoration HR image by each sub FSRCNN-k. For each cascaded FSRCNN, the output image is twice the size of input image. We use 1 k I represents the real The network structure of CSRCNN. The network is composed of three cascade FSRCNN, in which the upscaling factor of each FSRCNN is 2. The size of the input of the network is a /8 * /8 LR image, which will be successively entered into three sub-FSRCNN. Each sub-FSRCNN can double enlarge the size of input image. For each sub network, the output image of the network will form a sub loss function with the corresponding real HR image, and the sum of the sub loss functions of these sub networks will eventually form the loss function of the whole network.HR image, which has the same size with the super-solved image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>LR images are assigned to different stages of the cascaded FSRCNN according to their resolution scale ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>8 * ) represents the integer part of the final result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>visual comparisons on Set5, Set14 and BSD200 with upscaling factor 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Cascade Convolutional Neural Network 3.1 Network Structure</head><label></label><figDesc></figDesc><table><row><cell>5, ,1 Conv d</cell><cell>âˆ’</cell><cell cols="2">1, , PReLU Conv s d PReLU Conv s s PReLU mConv d s 3, , 1, , âˆ’ âˆ’ âˆ’ âˆ’ âˆ’</cell></row><row><cell cols="3">PReLU DeConv âˆ’âˆ’</cell><cell>9,1, . d</cell><cell>(1)</cell></row><row><cell>3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The influence of the number of cascaded FSRCNN with upscaling factor 4, where the test data set is</figDesc><table><row><cell></cell><cell cols="2">Set5, Set14 and BSD200.</cell><cell></cell><cell></cell></row><row><cell cols="2">Test-dataset</cell><cell>Upscaling factor</cell><cell>Net1 PSNR</cell><cell>Net2 PSNR</cell><cell>Net3 PSNR</cell></row><row><cell></cell><cell>Set-5</cell><cell>Ã—4</cell><cell>30.55</cell><cell>30.82</cell><cell>31.01</cell></row><row><cell></cell><cell>Set-14</cell><cell>Ã—4</cell><cell>27.50</cell><cell>28.02</cell><cell>28.47</cell></row><row><cell></cell><cell>BSD200</cell><cell>Ã—4</cell><cell>26.92</cell><cell>27.42</cell><cell>27.68</cell></row><row><cell cols="6">the depth . According to relevant experimental conclusions of FSRCNN, we set = 56, =</cell></row><row><cell>12 and</cell><cell cols="2">= 4 for each subnetwork.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation of state-of-the-art SR algorithms average PSNR/SSIM for scale factors 2Ã—, 3Ã— and 4Ã—.</figDesc><table><row><cell>Test-dataset</cell><cell>Upscaling factor</cell><cell cols="3">Bicubic PSNR/SSIM PSNR/SSIM PSNR/SSIM SRCNN FSRCNN</cell><cell>CSRCNN (our) PSNR/SSIM</cell></row><row><cell>Set-5</cell><cell>Ã—2</cell><cell>33.66/0.9299</cell><cell>36.33/0.9521</cell><cell>36.94/0.9558</cell><cell>37.45/0.9570</cell></row><row><cell>Set-14</cell><cell>Ã—2</cell><cell>30.23/0.8677</cell><cell>32.15/0.9039</cell><cell>32.54/0.9088</cell><cell>34.34/0.9240</cell></row><row><cell>BSD200</cell><cell>Ã—2</cell><cell>29.70/0.8625</cell><cell>31.34/0.9287</cell><cell>31.73/0.9074</cell><cell>32.92/0.9122</cell></row><row><cell>Set-5</cell><cell>Ã—3</cell><cell>30.39/0.8682</cell><cell>32.45/0.9033</cell><cell>33.06/0.9140</cell><cell>34.10/0.9233</cell></row><row><cell>Set-14</cell><cell>Ã—3</cell><cell>27.54/0.7736</cell><cell>29.01/0.8145</cell><cell>29.37/0.8242</cell><cell>30.02/0.8346</cell></row><row><cell>BSD200</cell><cell>Ã—3</cell><cell>27.26/0.7638</cell><cell>28.27/0.8038</cell><cell>28.55/0.8137</cell><cell>29.78/0.8302</cell></row><row><cell>Set-5</cell><cell>Ã—4</cell><cell>28.42/0.8104</cell><cell>30.15/0.8530</cell><cell>30.55/0.8657</cell><cell>31.01/0.8702</cell></row><row><cell>Set-14</cell><cell>Ã—4</cell><cell>26.00/0.7019</cell><cell>27.21/0.7413</cell><cell>27.50/0.7535</cell><cell>28.47/0.7720</cell></row><row><cell>BSD200</cell><cell>Ã—4</cell><cell>25.97/0.6949</cell><cell>26.72/0.7291</cell><cell>26.92/0.7398</cell><cell>27.68/0.7552</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :25.74/0.715 24.30/0.614 24.50/0.581</head><label>3</label><figDesc>Quantitative evaluation of state-of-the-art SR algorithms average PSNR/SSIM for scale factors 8Ã—.</figDesc><table><row><cell>Test-dataset</cell><cell>Upscaling factor</cell><cell cols="3">Bicubic PSNR/SSIM PSNR/SSIM PSNR/SSIM SRCNN FSRCNN</cell><cell>CSRCNN (our) PSNR/SSIM</cell></row><row><cell>Set-5</cell><cell>Ã—8</cell><cell>24.39/0.657</cell><cell>25.33/0.689</cell><cell>25.41/0.682</cell></row><row><cell>Set-14</cell><cell>Ã—8</cell><cell>23.19/0.568</cell><cell>23.85/0.593</cell><cell>23.93/0.592</cell></row><row><cell>BSD200</cell><cell>Ã—8</cell><cell>23.67/0.547</cell><cell>24.13/0.565</cell><cell>24.21/0.567</cell></row></table><note>Fig.4. visual comparisons on Set5 and Set14 datasets with upscaling factor 3.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single image super-resolution by directionally structured coupled dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coupled dictionary learning in wavelet domain for Single-Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waqas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal, Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="453" to="461" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selective sparse coding based coupled dictionary learning algorithm for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waqas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Computing, Mathematics and Engineering Technologies</title>
		<meeting>of International Conference on Computing, Mathematics and Engineering Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-complexity singleimage super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of British Machine Vision Conference</title>
		<meeting>of British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep network cascade for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="347" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating the super resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image super resolution based on sparse representation via directionally structured dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Farhadifard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nazzal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Signal Processing and Communications Applications Conference</title>
		<meeting>of Signal essing and Communications Applications Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1718" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Neural Information Processing Systems</title>
		<meeting>of International Conference on Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on image-net classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate Image Super-Resolution Using Very Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation. Computer Engineering and Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Cai-Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2599" to="2613" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Super-resolution Reconstruction Using Multiconnection Deep Residual Network Combined an Improved Loss Function for Single-frame Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Anchored Neighborhood Regression for Fast Example-Based Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: From Error Visibility to Structural Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeply improved sparse coding for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Learning for Image Super-resolution: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2861" to="2873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coupled Dictionary Training for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast direct super-resolution by simple functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized Huber Loss Regression and Quantile Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Statistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="547" to="557" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action decision networks for visual tracking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2711" to="2720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crafting a toolchain for image restoration by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2443" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2443" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiple Regressions based Image Super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimal Discriminative Projection for Sparse Representation-based Classification via Bilevel Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Domain adaptive collaborative representation based classification. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="175" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimal couple projections for domain adaptive sparse representation-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5922" to="5935" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On Single Image Scale-Up Using Sparse-Representations. Curves and Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Curves and Surfaces -7th International Conference</title>
		<meeting>of Curves and Surfaces -7th International Conference<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06-24" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transfer robust sparse coding based on graph and joint distribution adaption for image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Residual Dense Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiple Kernel Coupled Projections for Domain Adaptive Dictionary Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2292" to="2304" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3096" to="3105" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
