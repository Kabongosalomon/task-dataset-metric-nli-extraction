<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SeqFace: Make full use of sequence information for face recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruirui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology</orgName>
								<orgName type="institution">Beijing University of Chemical Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">YUNSHITU Corp</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SeqFace: Make full use of sequence information for face recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>face recognition</term>
					<term>face sequences</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks (CNNs) have greatly improved the Face Recognition (FR) performance in recent years. Almost all CNNs in FR are trained on the carefully labeled datasets containing plenty of identities. However, such high-quality datasets are very expensive to collect, which restricts many researchers to achieve state-of-the-art performance. In this paper, we propose a framework, called SeqFace, for learning discriminative face features. Besides a traditional identity training dataset, the designed SeqFace can train CNNs by using an additional dataset which includes a large number of face sequences collected from videos. Moreover, the label smoothing regularization (LSR) and a new proposed discriminative sequence agent (DSA) loss are employed to enhance discrimination power of deep face features via making full use of the sequence data. Our method achieves excellent performance on Labeled Faces in the Wild (LFW), YouTube Faces (YTF), only with a single ResNet. The code and models are publicly available onlien 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In most scenario, FR is a metric learning problem since it is impossible to classify faces to known identities in the training set. Recently, deep CNNs are widely used in FR, due to their great discriminative feature learning capability. The face feature is mainly trained via two types of methods according to their loss functions in CNN models. One method uses classification loss functions, such as softmax loss <ref type="bibr" target="#b31">[Taigman et al. 2014;</ref><ref type="bibr" target="#b26">Sun et al. 2014;</ref><ref type="bibr">Wen et al. 2016a</ref>]. The other type uses metric learning loss functions, such as contrastive loss and triplet loss <ref type="bibr" target="#b10">[Hoffer and Ailon 2015;</ref><ref type="bibr" target="#b3">Chopra et al. 2005]</ref>. In many recent CNNs for FR, two types of loss functions are usually combined together for learning face features <ref type="bibr">[Wen et al. 2016b</ref>]. All these loss functions aim to maximize the interidentity variations and minimize the intra-identity variations under a certain metric space. No matter which loss functions are applied, we find that the training data share the same type, called identity data in the paper.</p><p>An identity dataset includes M faces of N identities, and each face in the dataset is clearly labeled as the image of the i-th (0 ≤ i &lt; N ) identity. Currently all public or private datasets for training deep face features, such as CASIA <ref type="bibr" target="#b39">[Yi et al. 2014</ref>], MS-Celeb-1M <ref type="bibr" target="#b7">[Guo et al. 2016]</ref> and CelebFaces <ref type="bibr" target="#b2">[Chen et al. 2014</ref>], belong to identity datasets. However, a large-scale high-quality identity dataset is very expensive to construct, since it could cost lots of effort and money. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>In our SeqFace framework, the CNN model is trained on an identity dataset and a sequence dataset, and is supervised jointly by a chief classification loss and another auxiliary loss. Different sequences can belong to the same identity in sequence data.</p><p>Identity data need two kinds of information: face image and identity annotation. Identities in most public and private datasets are celebrities, because celebrity photos are rather easily crawled and annotated from the Internet. However, a celebrity dataset might be not a satisfied training dataset, if there are obvious differences between the evaluated faces and the celebrity faces in age, race, pose, and so on.</p><p>Beside photos from the Internet, videos (movies, TVs, surveillance videos, etc.) can also provide large quantities of face images, but few works utilize these face images because labelling process of identities is relatively difficult. However, it might be necessary to collect such faces as training data in some circumstances, such as surveillance. Face detection and tracking on videos can automatically generate data with lots of face sequences, and each sequence contains several faces of one identity. We call this type of data sequence data.</p><p>A sequence dataset includes M faces of N sequences, and each face is labeled as the image of the i-th (0 ≤ i &lt; N ) sequence. Because faces in a sequence must belong to one identity (note that different sequences might belong to one identity), sequence data have potential to help to reduce the intra-identity variations. A large-scale high-quality sequence dataset can be efficiently and automatically constructed by using state-of-the-art face detection and tracking methods. Although face sequences are broadly used in video FR applications, previous works have rarely utilized these unlabelled sequence datasets as the training data to learn face features in image FR.</p><p>In this paper, we propose a framework, namely SeqFace, to learn discriminative face features on both identity data and sequence data (see <ref type="figure">Figure 1</ref>). The SeqFace framework is not objective to replace other models or loss functions, but to make full use of sequence data in the training procedure. In the SeqFace, a CNN model is jointly supervised by two loss functions. The first one is a chief classification loss, such as the softmax loss, which aims to maximize the inter-identity variations and minimize the intra-identity variations simultaneously. The second one is an auxiliary loss, such as the center loss <ref type="bibr">[Wen et al. 2016b]</ref>, which mainly encourages the intra-identity compactness. Because the traditional classification loss functions cannot deal with sequence data, label smoothing regularization (LSR) is employed to improve the chief loss. Moreover, we also propose a DSA loss as the auxiliary loss, which is superior to the center loss because it contributes to the inter-class dispersion. With the help of additional sequence data, CNN models can be trained with high feature discrimination in the SeqFace.</p><p>To summarize, our major contributions are as follows:</p><p>1. We present a framework (SeqFace) to learn discriminative face features. Besides the traditional identity data, unlabeled sequences are used as the training data in the SeqFace to enhance discrimination power of face features for the first time.</p><p>2. To make full use of sequence data, we employed the LSR to help the chief classification loss deal with sequence data. A new DSA loss function, which contributes to the intra-class compactness and the inter-class dispersion of the features, is also proposed as the auxiliary loss to train CNNs. Experiments demonstrate that the LSR and the DSA loss both boost the FR performance greatly.</p><p>3. We conduct experiments on two popular and challenging FR benchmark datasets (Labeled Faces in the Wild (LFW) and YouTube Faces (YTF) <ref type="bibr" target="#b36">[Wolf et al. 2011]</ref>) with one single ResNet-64, and the model achieves a 99.83 % verification accuracy on LFW, and a 98.12% verification accuracy on YTF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>In this section, we briefly review works of deep face recognition, and face sequences related works in FR are also introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.1">Deep Face Recognition</head><p>Deep face recognition is one of the most active field, and has achieved a series of breakthroughs in recent years thanks to the great success of CNNs <ref type="bibr" target="#b14">Krizhevsky et al. 2012;</ref><ref type="bibr" target="#b24">Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b29">Szegedy et al. 2015]</ref>. Many methods <ref type="bibr" target="#b31">[Taigman et al. 2014;</ref><ref type="bibr" target="#b26">Sun et al. 2014;</ref><ref type="bibr">Wen et al. 2016a;</ref><ref type="bibr" target="#b23">Schroff et al. 2015;</ref><ref type="bibr" target="#b2">Chen et al. 2014;</ref><ref type="bibr">Sun et al. 2015a;</ref><ref type="bibr">Sun et al. 2015b]</ref> have proven that CNNs outperform humans in FR on some benchmark data sets. FR is treated as a multi-class classification problem and CNN models are supervised by the softmax loss in many methods <ref type="bibr" target="#b31">[Taigman et al. 2014;</ref><ref type="bibr" target="#b26">Sun et al. 2014;</ref><ref type="bibr">Wen et al. 2016a</ref>]. Some metric learning loss functions, such as contrastive loss <ref type="bibr" target="#b39">[Yi et al. 2014;</ref><ref type="bibr" target="#b3">Chopra et al. 2005</ref>], triplet loss <ref type="bibr" target="#b10">[Hoffer and Ailon 2015;</ref><ref type="bibr" target="#b23">Schroff et al. 2015</ref>] and center loss <ref type="bibr">[Wen et al. 2016b]</ref>, are also applied to boost FR performance greatly. Other loss functions <ref type="bibr" target="#b4">[Deng et al. 2017;</ref><ref type="bibr" target="#b41">Zhang et al. 2017</ref>] based on metric loss also demonstrate effective performance on FR. Recently, some angular margin based methods <ref type="bibr" target="#b18">[Liu et al. 2016;</ref><ref type="bibr" target="#b19">Liu et al. 2017;</ref><ref type="bibr" target="#b5">Deng et al. 2018;</ref><ref type="bibr" target="#b33">Wang et al. 2018</ref>] are proposed and achieve outperforming performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.2">Sequences in Face Recognition</head><p>In many applications, sequences or image sets are the most natural form of input to the FR system. Video face recognition methods <ref type="bibr" target="#b38">[Yamaguchi et al. 1998;</ref><ref type="bibr" target="#b1">Cevikalp and Triggs 2010;</ref><ref type="bibr" target="#b32">Wang et al. 2017;</ref><ref type="bibr" target="#b0">Bashbaghi et al. 2017;</ref><ref type="bibr" target="#b6">Ding and Tao 2016;</ref><ref type="bibr" target="#b12">Huang et al. 2015;</ref><ref type="bibr" target="#b22">Parchami et al. 2017;</ref><ref type="bibr" target="#b25">Sohn et al. 2017</ref>] based on face sequences, or face sets, also are expected to achieve better performance than ones based on individual images. Most of these studies attempt to utilize redundant information of face sequences/sets to improve recognition performance, but not to learn discriminative features from sequence data. Recently, some approaches <ref type="bibr" target="#b6">[Ding and Tao 2016;</ref><ref type="bibr" target="#b22">Parchami et al. 2017;</ref><ref type="bibr" target="#b25">Sohn et al. 2017]</ref> aim to learn deep video features for video face recognition. In <ref type="bibr" target="#b25">[Sohn et al. 2017</ref>], large-scale unlabeled face sequences are employed as the training data, but these sequence data are only utilized to learn transformations between image and video domains. Learning discriminative face features still mainly depends on traditional large-scale identity datasets in this deep CNN approaches of video FR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SeqFace Framework</head><p>The proposed SeqFace is a framework for learning discriminative face features on identity datasets and sequence datasets simultaneously. Identity overlap between these two datasets is not allowed in the SeqFace. A CNN model (ResNet-like models in our implementation) is jointly supervised by one chief classification loss and one auxiliary loss in the SeqFace. The chief loss enlarges the inter-identity feature differences and reduces the intra-identity feature variations simultaneously, and the major target of the auxiliary loss is to reduce intra-identity(intra-sequence) variations. The final loss can be formulated as</p><formula xml:id="formula_0">L = L Chief + η · L Auxiliary ,<label>(1)</label></formula><p>where η is a parameter used to balance two loss functions.</p><p>Similar with many methods, we also treat the FR problem as a classification task to train CNNs, and CNNs are mainly supervised by a chief classification loss, such as the Softmax loss, the A-Softmax loss in SphereFace , and so on. All faces of one identity in identity data is labeled as belonging to one class in the classification loss. However, an input face in sequence data cannot belong to any class (identity) in the classification loss, because there is no identity annotation in sequence data. That is to say, though a regular classification loss can be applied to train the CNN model in the SeqFace, it only can deal with identity data and has to ignore sequence data.</p><p>We know that faces in one sequence certainly belong to one identity. Therefore, if a loss encourages the intra-sequence feature compactness, and does not penalize the inter-sequence feature compactness, it could supervise CNNs to learn discriminative face features on sequence data, and it could naturally deal with identity data too. Because this loss mainly affects the intra-sequence and intra-identity compactness, it has to be an auxiliary loss. The center loss <ref type="bibr">[Wen et al. 2016b</ref>] function is formulated as</p><formula xml:id="formula_1">LC = 1 2 K k=1 x k − cy k 2 2 ,<label>(2)</label></formula><p>where K is the number of training samples, x k denotes the feature of the k-th training sample, y k is the class(identity) label of the sample, and the cy k denotes the y k -th class(identity) center of deep features for classification problems. The center loss can deals with identity data and sequence data in the same way, and the cy k is the feature center of the y k -th identity for identity data or the feature center of the y k -th sequence for sequence data. Since the formulation only characterizes intra-identity and intra-sequence feature compactness effectively, it doesn't penalize closed feature centers of different sequences. Therefore, the center loss is a good option for the auxiliary loss in the SeqFace.</p><p>To summarize, one of the benefits of our SeqFace framework is to reduce the intra-identity variations while enlarging the inter-identity variation with CNNs supervised by a chief classification loss and another auxiliary loss. In fact, we can employ the regular softmax loss and the center loss as the chief loss and the auxiliary loss in the SeqFace. However, the softmax loss has to ignore sequence data, and the center loss only concerns the intra-identity and intra-sequence compactness. Sequence data only contribute to intra-identity compactness through the supervision of the center loss. In order to make full use of sequence information, the LSR and the DSA loss are presented in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label Smoothing Regularization</head><p>The softmax loss is applied to supervise CNNs classification, and its simplicity and probabilistic interpretation make the softmax loss widely adopted in FR issues. The softmax loss can be used as the chief loss in the SeqFace, but it has to ignore sequence data when training CNNs because of the lack of identity annotation. The softmax loss can be considered as the combination of a softmax function and a cross-entropy loss, and the cross-entropy loss is formulated as</p><formula xml:id="formula_2">LS = − C i=1 log(p(i))q(i),<label>(3)</label></formula><p>where C is the class number, p(i) ∈ [0, 1] is the predicted probability (the output of the softmax function) of the input belonging to class i, and q(i) is the ground truth distribution defined as</p><formula xml:id="formula_3">q(i) = 0 i = y 1 i = y ,<label>(4)</label></formula><p>where y is the ground truth class label of the input. Label smoothing regularization (LSR) is introduced to deal with non-ground truth inputs <ref type="bibr" target="#b30">[Szegedy et al. 2016]</ref>, and label smoothing regularization for outlier (LSRO) is then used to corporate unlabeled inputs <ref type="bibr" target="#b42">[Zheng et al. 2017]</ref> in CNNs. In the LSR, the value of q(i) can be a float value between 0 and 1 for the input which cannot be clearly labeled as any class.</p><p>In our framework, because all identities in sequence data do not exist in identity data, we define q(i) = 1/C (so C i=1 q(i) = 1) as <ref type="bibr" target="#b42">[Zheng et al. 2017</ref>] for all input faces in sequence data. Therefore, the cross-entropy loss is rewritten as</p><formula xml:id="formula_4">L = −(1 − Z)log(p(y)) − Z C C i=1 log(p(i)),<label>(5)</label></formula><p>where Z = 0 for the input face of identity data, and Z = 1 for the input face of sequence data.</p><p>The LSR can also be integrated into other softmax-like classification loss functions. In practice, a feature normalised SphereFace (L2-SphereFace for short, the same with FNorm-SphereFace in <ref type="bibr" target="#b5">[Deng et al. 2018]</ref>) is applied as the chief classification loss. An additional L2-constraint is added to the regular SphereFace , it means the input feature x k must be firstly normalized and scaled by a scalar parameter δ (δ · x k / x k 2). Therefore, the decision boundaries of the L2-SphereFace under binary classification is δ(cos mθ1 − cos θ2) = 0 for class 1, and is δ(cos θ1 − cos mθ2) = 0 for class 2. In our implementation, the parameter δ and the margin m are set to 32.0 and 4 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DSA loss</head><p>The center loss only reduces intra-class variations as the auxiliary loss, and the inter-class separability of features completely depends on the classification loss. In this section, we propose a new auxiliary loss, namely discriminative sequence agent loss (DSA Loss), which concerns the intra-class compactness and the inter-class dispersion simultaneously.</p><p>First, considering the traditional classification problem with an identity dataset, we define</p><formula xml:id="formula_5">d k,n = (x k − cn) 2 4<label>(6)</label></formula><p>as the distance between the feature x k of the k-th training sample and the feature center cn of the n-th class(identity), d k,n is actually equivalent to the Euclidean distance. Note that if x k and cn are normalized, d k,n can be re-formulated as</p><formula xml:id="formula_6">d k,n = (1 − cosθ k,n ) 2 ,<label>(7)</label></formula><p>where θ k,n denotes the angle between x k and cn, and d k,n can be regarded as the angular distance. Since our target is to reduce the distance between x k and cy k and enlarge other distances between x k and cn for all n = y k , where y k is the label of the k-th training sample, a discriminative loss can be formulated as</p><formula xml:id="formula_7">L k,n = d k,n n = y k max(α · d k,y k − d k,n + β, 0) n = y k ,<label>(8)</label></formula><p>where α ∈ [1, +∞) and β ∈ [0, +∞) are two parameters to adjust the discriminative power of the learned features. Therefore, the final loss function is</p><formula xml:id="formula_8">LD = 1 K K k=1   λ · L k,y k + (1 − λ) 1 (N − 1) · p N n=1,n =y k b(1, p) · L k,n   ,<label>(9)</label></formula><p>where the parameter λ is applied to balance the intra-class compactness and the inter-class dispersion, N is the number of identity(class) of the identity dataset. We introduce another parameter p as the probability that the n-th center is employed in computing the final loss, because N might be a huge number and it will be time-consuming if all L k,n are computed in each iteration. b(1, p) means the Bernoulli distribution with the probability p.</p><p>The gradients of LD with respect to x k and the update equation of cn, similar with that in the center loss, are computed as:</p><formula xml:id="formula_9">∂LD ∂x k = 1 K λ · x k − cy k 2 + (1 − λ) 1 (N − 1) · p · N n=1,n =y k b(1, p) · δ(L k,n &gt; 0) · (α · x k − cy k 2 − x k − cn 2 )<label>(10)</label></formula><p>and</p><formula xml:id="formula_10">∆cn = − N n=1 δ(y k = n) · x k −cn 2 1 + N n=1 δ(y k = n) ,<label>(11)</label></formula><p>where δ(condition) = 1 if the condition is satisfied, and δ(condition) = 0 if not.</p><p>Different from the center loss, our DSA loss also enforces constraints on inter-class variations. According to Equation 9, the feature x k is pulled towards the feature center cy k of its identity, and is pushed away from feature centers of other identities randomly selected in each training iteration.</p><p>Taking into account sequence data, there is a slight modification in Equation 9 to compute the final DSA loss. We assume that there xi c1 c2 c3 c4 c5 xj <ref type="figure">Figure 2</ref>: Illustration of forces on sample features of identity data and sequence data. The i-th sample is from the identity dataset, and the j-th one is from the sequence dataset. c1, c2 and c3 are feature centers of corresponding identities, and c4 and c5 are feature centers of corresponding sequences. yi = 1 and yj = 5.</p><p>are C (C is also the number of class in the chief classification loss) identities in the identity dataset and N − C sequences in the sequence dataset. There is no overlap between these two datasets as mentioned above. In Equation 9, n is selected from all C identities and N − C sequences (means N n=1,n =y k ) for samples in the identity dataset, and n is only selected from C identities (means C n=1,n =y k ) for samples in the sequence dataset. That is to say, if the k-th sample is in the identity dataset, x k should be pushed away from feature centers of other identities and all sequences, or x k is only pushed away from feature centers of identities. <ref type="figure">Figure 2</ref> illustrates two examples.</p><p>There are four parameters (λ, α, β, and p) in the DSA loss function. The parameter λ can be set to 0.5 since we concern both the intra-class compactness and the inter-class dispersion. The parameters α and β are used to adjust the discriminative power of features.</p><p>Using larger values is preferred, but it will increase the difficulty of convergence in training. According to our experiments, α = 2.0 and β = 1.0 can be applied in most applications. The parameter p is applied to select part of identities/sequences while computing L k,n , in order to reduce the computing cost. The value of the parameter p can be set flexibly based on computing resources in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">MNIST Example</head><p>We perform a toy example on the MNIST dataset <ref type="bibr" target="#b16">[Lecun and Cortes 2010]</ref> with our DSA loss. LeNet++ <ref type="bibr">[Wen et al. 2016b</ref>], a deeper and wider version of LeNet, is employed. The last hidden layer output of the model is restricted to 2-dimensions for easy visualization (see <ref type="figure" target="#fig_2">Figure 3</ref>). For comparison, we train 4 models supervised by a softmax loss, a softmax loss and a center loss, a softmax loss and a DSA loss, a softmax loss and a DSA loss (with normalized x k and cn), respectively. We set λ = 0.5, α = 2.0, β = 1.0 and p = 1.0 in the DSA loss. The loss weight values of the center/DSA loss are set to 0.04. All models are trained with the batch size of 32. The learning rate begins with 0.01, and is divided by 10 at 14K iterations. The training process is finished at 20K iterations. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the features learned with the DSA loss are more discriminative. The feature dispersion in <ref type="figure" target="#fig_2">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implement Details</head><p>In our experiments, all the face images and their landmarks are detected by MTCNN ]. The faces are aligned by similar transformation as <ref type="bibr" target="#b37">[Wu et al. 2017]</ref>, and are cropped to 144 × 144 RGB images (randomly cropped to 128 × 128 in training). Each pixel in RGB images is normalized by subtracting 127.5 then divided by 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training and Testing</head><p>Caffe <ref type="bibr" target="#b13">[Jia et al. 2014</ref>] is used to implement CNN models. Different CNN models are employed in the experiments, which will be further introduced. All weights of the auxiliary losses (η in Equation 1) are set to 0.04 in the experiments. Euclidean distances (do not normalize x k and cn in Equation 6) are applied in the DSA loss functions used in these section. At the testing stage, only features of the original image are directly extracted from the last full connected layer of CNNs, and the cosine similarity is used to measure the feature distance in the experiments. More details are presented in the corresponding sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exploration Experiment</head><p>In this section, the employed CNN is a ResNet-20 network which is similar to , and it is trained on the publicly available CASIA-WebFace dataset <ref type="bibr" target="#b39">[Yi et al. 2014</ref>] containing about 0.5M faces from 10,575 identities. All models are trained with the batch size of 32 on one Titanx GPU. The learning rate begins with 0.01, and is divided by 10 at 200K iterations. The training process is finished at 300K iterations.</p><p>To evaluate the effectiveness of sequence data, 10,575 identities in the CASIA-WebFace dataset are randomly divided into two parts: the dataset A (5,000 identities) and the dataset B(5,575 identities). Faces in the dataset B is then randomly split into 32,996 sequences. The dataset A and B are treated as the identity dataset and the sequence dataset respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Effect of the DSA loss parameters</head><p>We firstly study the effect of parameter values in the DSA loss. We train several CNN models only on the dataset A(5,000 identities) under the supervision of the DSA loss with different parameter values, and then evaluate these models on the LFW dataset. From the results shown in <ref type="figure" target="#fig_3">Figure 4</ref>, we can conclude that higher performances can be achieved if the DSA loss simultaneously concern the intra-class compactness and the inter-class dispersion of the learned features (λ = 0.5). The results also demonstrate that larger α and β might lead to more discriminative features. According to our experiments, setting α = 2.0 and β = 1.0 is preferred to balance the FR performance and the difficulty of convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effect of SeqFace</head><p>We also train 10 models (see <ref type="table" target="#tab_0">Table 1</ref>) to demonstrate the effectiveness of our SeqFace, the LSR and the DSA loss.</p><p>First, we use a regular softmax loss (Model I), a L2-SphereFace loss (Model II) to train 2 CNN models respectively. Only the dataset A is used as the training dataset. Verification accuracies demonstrate that the L2-SphereFace greatly boosts the performance.</p><p>Then, the center loss and the DSA loss are applied as the auxiliary loss to jointly supervise 2 CNN models (Model III and Model IV) with a softmax loss respectively. The reported results also demonstrate that: 1) Auxiliary loss functions have positive effect on the FR performance even without sequence training data; 2) Our DSA loss outperforms the center loss on FR issue.</p><p>Moreover, sequence data (the dataset B) are added to train 5 CNN models supervised by a LSR-based softmax loss and a center loss  (Model V), a LSR-based softmax loss and a DSA loss (Model VI), a LSR-based L2-SphereFace loss (Model VII), a LSR-based L2-SphereFace loss and a center loss (Model VIII), a LSR-based L2-SphereFace and a DSA loss (Model IX), respectively. According to results, we have following observation: 1) Sequence data can obviously improve the FR performance on the SeqFace; 2) Even one chief classification loss with the LSR can also utilize sequence data to improve the FR performance; 3) The LSR and the DSA loss greatly enhance the discriminative power of learned features.</p><p>Last, we also train a model (Model X) on total CASIA-WebFace with a L2-SphereFace loss, and it reaches 99.03% accuracy on LFW. Comparing accuracies between the Model IX and X, we can conclude that complete identity annotation is naturally preferred in training datasets, but the little gap shows that competitive performance also can be achieved by making full use of sequence information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on LFW and YTF</head><p>In this section, we evaluate the proposed SeqFace on LFW and YTF in unconstrained environments. LFW <ref type="bibr" target="#b11">[Huang et al. 2007</ref>] and YTF <ref type="bibr" target="#b36">[Wolf et al. 2011</ref>] are challenging testing benchmarks released for face verification. LFW dataset contains 13,233 faces of 5749 different identities, with large variations in pose, expression and illuminations. YTF dataset includes 3,425 videos of 1,595 identities. We follow the unrestricted with labeled outside data protocol.</p><p>To evaluate performance on YTF, the simple average feature of all faces in a video is applied to compute the final score.</p><p>A ResNet-27 model 2 (the architecture is shown in <ref type="figure">Figure 5</ref>) and a ResNet-64  are employed for evaluation. To accelerate the training process, we first train a baseline model under the supervision of the regular L2-SphereFace on the identity dataset only, and then fine-tune the baseline model by using the SeqFace.</p><p>Our models are trained with batch size of 128 on 4 Titanx GPU.</p><p>The learning rate begins with 0.01, and is divided by 10 at 300K and 600K iterations. The training is finished at 800K iterations. The model is jointly supervised by a LSR-L2-SphereFace loss and a DSA loss, and is learned on the MS-Celeb-1M and our Celeb-Seq datasets described below. In the DSA loss, λ = 0.5, α = 2.0, β = 1.0. The parameter p is set to 0.001 because of the large number of sequences in the Celeb-Seq dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Training Datasets</head><p>A refined MS-Celeb-1M (4M images and 79K identities) provided by <ref type="bibr" target="#b37">[Wu et al. 2017</ref>] is used as the identity dataset. Since there is no public sequence datasets for training deep CNNs, we construct a sequence dataset Celeb-Seq, which includes about 2.5M face images of 550K face sequences. We firstly extract about 800K face sequences by using MTCNN  and Kalman-Consensus Filter (KCF) [Olfati-Saber 2010] to detect and track video faces from 32 online TV Channels, then compute image features with the model provided by SphereFace . Lastly faces of overlap identities with MS-Celeb-1M, and nearly noisy and duplicate faces in one sequence are discarded from the dataset automatically and manually. We also remove face images belong to identities that appear in the LFW and YTF test sets. Some face sequences in the Celeb-Seq dataset are shown in <ref type="figure">Figure 6</ref>.</p><p>Removing overlap identities with MS-Celeb-MS costs us most of the time in the constructing process, because many celebrities in MS-Celeb-MS can be found from the original 800K sequences. Fortunately, constructing a satisfied sequence dataset will not be a time-consuming task in many real scenarios. For example, it is almost certain that people in an Asian street surveillance video will not appear in another European street surveillance video, and will not be found in the MS-Celeb-1M dataset too.  <ref type="figure">Figure 5</ref>: The ResNet-27 architecture for the experiments on LFW and YTF. The CNN is jointly supervised by the LSR-L2-SphereFace and the DSA loss. ID denotes identity input data, SEQ denotes sequence input data, C denotes the convolution layer, P denotes the max-pooling layer, and FC denotes the fully connected layer. The SeqFace is only a framework to make use of sequence data. We believe that new loss functions (such as ArcFace, CosFace, etc.) and a deeper ResNet with improved residual units can be employed in the SeqFace to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>A large-scale high-quality dataset for training CNNs in FR is very expensive to construct. Face features learned on publicly available datasets for researchers might not achieve satisfied performance in some circumstances, for example evaluating Asia people in surveillance videos. Though large amount of face images in the real situation can be collected, assigning labels to these images is still timeconsuming. Fortunately, a dataset containing large amount of face sequences can be efficiently constructed by using face detection and tracking methods.</p><p>In this paper, we proposed a framework named SeqFace, which can utilize sequence data to learn highly discriminative face features. A chief classification loss and another auxiliary loss are combined together to learn features on a traditional identity dataset and another sequence dataset. The LSR is employed to help the chief loss to deal with sequence input. The DSA loss was also proposed to supervise CNNs as an auxiliary loss. We achieved good results on several popular face benchmarks only with a simple ResNet model. We also believe that more competitive performance can be obtained, if recently proposed loss functions <ref type="bibr" target="#b33">[Wang et al. 2018;</ref><ref type="bibr" target="#b5">Deng et al. 2018</ref>] and CNN architectures <ref type="bibr" target="#b8">[Han et al. 2017</ref>] are employed.</p><p>As far as we know, SeqFace is the first framework to employ face sequences as training data to learn highly discriminative face fea-tures. The requirement of no identity overlap between the identity and sequence datasets might have influence on the efficiency of constructing sequence datasets, but it always happens in many situations mentioned above. In fact, we train a CNN model on MS-Celeb-1M dataset and another sequence dataset, whose face sequences are collected from surveillance videos in China. The learned model achieves good performance in surveillance applications. It is obvious that our SeqFace also has great potentiality to be applied in other similar fields, such as Person-reidentification. <ref type="figure">Figure 6</ref>: Some face sequences in the Celeb-Seq dataset. All faces are aligned and cropped to 144 × 144. Some sequences belong to the same identity (two sequences at top-right corner). Note that the numbers of faces in the sequences are different from each other. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) and Figure 3(c) demonstrates that the DSA loss can enlarge inter-class distances, and the feature centers of different classes are pushed away from each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of 2-D feature distribution for the MNIST test set. The features of samples from different classes are denoted by the points with different colors. Four CNNs are supervised by the loss functions of (a)Softmax loss. (b)Softmax loss + Center loss. (c)Softmax loss + DSA loss with Euclidean distance. (d)Softmax loss + DSA loss with angular distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Face verification accuracies on LFW achieved by the DSA losses with different parameter values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Face verification accuracy on LFW dataset</figDesc><table><row><cell>Model</cell><cell>Loss</cell><cell>Training Dataset</cell><cell>Accuracy</cell></row><row><cell>I</cell><cell>Softmax loss</cell><cell>Dataset A</cell><cell>95.12%</cell></row><row><cell>II</cell><cell>L2-SphereFace</cell><cell>Dataset A</cell><cell>97.35%</cell></row><row><cell>III</cell><cell>Softmax + Center loss</cell><cell>Dataset A</cell><cell>97.03%</cell></row><row><cell>IV</cell><cell>Softmax + DSA loss</cell><cell>Dataset A</cell><cell>97.25%</cell></row><row><cell>V</cell><cell>LSR-Softmax + Center loss</cell><cell>Dataset A + Dataset B</cell><cell>97.62%</cell></row><row><cell>VI</cell><cell>LSR-Softmax + DSA loss</cell><cell>Dataset A + Dataset B</cell><cell>98.13%</cell></row><row><cell>VII</cell><cell>LSR-L2-SphereFace</cell><cell>Dataset A + Dataset B</cell><cell>98.65%</cell></row><row><cell>VIII</cell><cell cols="2">LSR-L2-SphereFace + Center loss Dataset A + Dataset B</cell><cell>98.72%</cell></row><row><cell>IX</cell><cell>LSR-L2-SphereFace + DSA loss</cell><cell>Dataset A + Dataset B</cell><cell>98.85%</cell></row><row><cell>X</cell><cell>L2-SphereFace</cell><cell>CASIA-WebFace</cell><cell>99.03%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>reports the verification performance of several methods. To demonstrate effectiveness of the SeqFace, the performance of our baseline ResNet-27 is also reported in the table. Note that the Ar-cFace employs the improved ResNets<ref type="bibr" target="#b8">[Han et al. 2017</ref>]. The Seq-Face achieves the best accuracies on these two benchmark testing sets. It is reported in the ArcFace that a regular 50-layer ResNet achieves a 99.71% accuracy on LFW. Moreover, our ResNet-27 and ResNet-64 models achieve 99.50% and 99.67% at VR@FAR=0 on LFW.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Verification accuracies(%) of different methods on LFW and YTF. Note that ResNet models in the ArcFace used the improved residual units, and the training MS-Celeb-1M dataset used in the ArcFace contains 3.8M images and 85K identities.</figDesc><table><row><cell>Method</cell><cell>Models</cell><cell>Data</cell><cell cols="2">LFW YTF</cell></row><row><cell>DeepFace [Taigman et al. 2014]</cell><cell>3</cell><cell>4M images</cell><cell cols="2">97.35 91.4</cell></row><row><cell>DeepID2+ [Sun et al. 2015b]</cell><cell>1</cell><cell>300K images</cell><cell>98.70</cell><cell>-</cell></row><row><cell>DeepID2+ [Sun et al. 2015b]</cell><cell>25</cell><cell>300K images</cell><cell cols="2">99.47 93.2</cell></row><row><cell>FaceNet [Schroff et al. 2015]</cell><cell>1</cell><cell>200M images</cell><cell cols="2">99.65 95.1</cell></row><row><cell>Baidu [Liu et al. 2015]</cell><cell>9</cell><cell>1.2M images</cell><cell>99.77</cell><cell>-</cell></row><row><cell>Center Face [Wen et al. 2016b]</cell><cell>1</cell><cell>0.7M images</cell><cell cols="2">99.28 94.9</cell></row><row><cell>SphereFace [Liu et al. 2017]</cell><cell>1 ResNet-64</cell><cell>CASIA-Webface</cell><cell cols="2">99.42 95.0</cell></row><row><cell>CosFace [Wang et al. 2018]</cell><cell>1 ResNet-64</cell><cell>5M images</cell><cell cols="2">99.73 97.6</cell></row><row><cell>ArcFace [Deng et al. 2018]</cell><cell>1 ResNet-50</cell><cell>MS-Celeb-1M</cell><cell>99.78</cell><cell>-</cell></row><row><cell>ArcFace [Deng et al. 2018]</cell><cell>1 ResNet-100</cell><cell>MS-Celeb-1M</cell><cell>99.83</cell><cell>-</cell></row><row><cell>L2-SphereFace</cell><cell>1 ResNet-27</cell><cell>MS-Celeb-1M</cell><cell cols="2">99.55 95.7</cell></row><row><cell>SeqFace</cell><cell cols="4">1 ResNet-27 MS-Celeb-1M + Celeb-Seq 99.80 98.0</cell></row><row><cell>SeqFace</cell><cell cols="4">1 ResNet-64 MS-Celeb-1M + Celeb-Seq 99.83 98.12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ydwen/caffe-face</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic ensembles of exemplar-svms for still-to-video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bashbaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Bilodeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition based on image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2567" to="2573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (1)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Marginal loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2006" to="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Trunk-branch ensemble convolutional neural networks for video-based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence PP</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Msceleb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6307" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>SIMBAD, Springer, A. Feragen, M. Pelillo, and M. Loog</editor>
		<imprint>
			<biblScope unit="volume">9370</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A benchmark and comparative study of video-based face recognition on cox face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuer-Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="5967" to="5981" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM International Conference on Multimedia</title>
		<meeting>the 22Nd ACM International Conference on Multimedia<address><addrLine>New York, NY, USA, MM</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310abs/1506.07310</idno>
		<title level="m">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6738" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kalman-consensus filter : Optimality, stability, and performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Olfati-Saber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Decision and Control</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Proceedings of the IEEE Conference on</title>
		<meeting>the IEEE Conference on</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="7036" to="7042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video-based face recognition using ensemble of haar-like deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parchami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bashbaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCNN</title>
		<imprint>
			<biblScope unit="page" from="4625" to="4632" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556abs/1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for face recognition in unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5917" to="5925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10, 000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deepid3: Face recognition with very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>arxiv:1502.00873 abs/1502.00873</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2892" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabi-Novich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative covariance oriented representation learning for face recognition with image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5749" to="5758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Springer, B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02683</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face recognition using temporal image sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="318" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5419" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3774" to="3782" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
