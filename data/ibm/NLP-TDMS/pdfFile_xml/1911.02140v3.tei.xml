<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Parameterized Quantile Function for Distributional Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
							<email>jiang.bian@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Parameterized Quantile Function for Distributional Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Distributional Reinforcement Learning (RL) differs from traditional RL in that, rather than the expectation of total returns, it estimates distributions and has achieved state-of-the-art performance on Atari Games. The key challenge in practical distributional RL algorithms lies in how to parameterize estimated distributions so as to better approximate the true continuous distribution. Existing distributional RL algorithms parameterize either the probability side or the return value side of the distribution function, leaving the other side uniformly fixed as in C51, QR-DQN or randomly sampled as in IQN. In this paper, we propose fully parameterized quantile function that parameterizes both the quantile fraction axis (i.e., the x-axis) and the value axis (i.e., y-axis) for distributional RL. Our algorithm contains a fraction proposal network that generates a discrete set of quantile fractions and a quantile value network that gives corresponding quantile values. The two networks are jointly trained to find the best approximation of the true distribution. Experiments on 55 Atari Games show that our algorithm significantly outperforms existing distributional RL algorithms and creates a new record for the Atari Learning Environment for non-distributed agents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional reinforcement learning <ref type="bibr" target="#b9">[Jaquette et al., 1973</ref><ref type="bibr" target="#b18">, Sobel, 1982</ref><ref type="bibr" target="#b24">, White, 1988</ref><ref type="bibr" target="#b13">, Morimura et al., 2010</ref><ref type="bibr" target="#b2">, Bellemare et al., 2017</ref> differs from value-based reinforcement learning in that, instead of focusing only on the expectation of the return, distributional reinforcement learning also takes the intrinsic randomness of returns within the framework into consideration <ref type="bibr" target="#b2">[Bellemare et al., 2017</ref><ref type="bibr" target="#b6">, Dabney et al., 2018b</ref><ref type="bibr" target="#b16">,a, Rowland et al., 2018</ref>. The randomness comes from both the environment itself and agent's policy. Distributional RL algorithms characterize the total return as random variable and estimate the distribution of such random variable, while traditional Q-learning algorithms estimate only the mean (i.e., traditional value function) of such random variable.</p><p>The main challenge of distributional RL algorithm is how to parameterize and approximate the distribution. In Categorical <ref type="bibr">DQN [Bellemare et al., 2017]</ref>(C51), the possible returns are limited to a discrete set of fixed values, and the probability of each value is learned through interacting with environments. C51 out-performs all previous variants of DQN on a set of 57 Atari 2600 games in the Arcade Learning Environment (ALE) <ref type="bibr" target="#b1">[Bellemare et al., 2013]</ref>. Another approach for distributional reinforcement learning is to estimate the quantile values instead. <ref type="bibr" target="#b6">Dabney et al. [2018b]</ref> proposed QR-DQN to compute the return quantiles on fixed, uniform quantile fractions using quantile regression and minimize the quantile Huber loss <ref type="bibr" target="#b8">[Huber, 1964]</ref> between the Bellman updated distribution and current return distribution. Unlike C51, QR-DQN has no restrictions or bound for value and achieves significant improvements over C51. However, both C51 and QR-DQN approximate the distribution function or quantile function on fixed locations, either value or probability. <ref type="bibr" target="#b5">Dabney et al. [2018a]</ref> propose learning the quantile values for sampled quantile fractions rather than fixed ones with an implicit quantile value network (IQN) that maps from quantile fractions to quantile values. With sufficient network capacity and infinite number of quantiles, IQN is able to approximate the full quantile function.</p><p>However, it is impossible to have infinite quantiles in practice. With limited number of quantile fractions, efficiency and effectiveness of the samples must be reconsidered. The sampling method in IQN mainly helps training the implicit quantile value network rather than approximating the full quantile function, and thus there is no guarantee in that sampled probabilities would provide better quantile function approximation than fixed probabilities.</p><p>In this work, we extend the method in <ref type="bibr" target="#b6">Dabney et al. [2018b]</ref> and <ref type="bibr" target="#b5">Dabney et al. [2018a]</ref> and propose to fully parameterize the quantile function. By fully parameterization, we mean that unlike QR-DQN and IQN where quantile fractions are fixed or sampled and only the corresponding quantile values are parameterized, both quantile fractions and corresponding quantile values in our algorithm are parameterized. In addition to a quantile value network similar to IQN that maps quantile fractions to corresponding quantile values, we propose a fraction proposal network that generates quantile fractions for each state-action pair. The fraction proposal network is trained so that as the true distribution is approximated, the 1-Wasserstein distance between the approximated distribution and the true distribution is minimized. Given the proposed fractions generated by the fraction proposal network, we can learn the quantile value network by quantile regression. With self-adjusting fractions, we can approximate the true distribution better than with fixed or sampled fractions.</p><p>We begin with related works and backgrounds of distributional RL in Section 2. We describe our algorithm in Section 3 and provide experiment results of our algorithm on the ALE environment <ref type="bibr" target="#b1">[Bellemare et al., 2013]</ref> in Section 4. At last, we discuss the future extension of our work, and conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>We consider the standard reinforcement learning setting where agent-environment interactions are modeled as a Markov Decision Process (X , A, R, P, γ) <ref type="bibr" target="#b15">[Puterman, 1994]</ref>, where X and A denote state space and action space, P denotes the transition probability given state and action, R denotes state and action dependent reward function and γ ∈ (0, 1) denotes the reward discount factor. For a policy π, define the discounted return sum a random variable by Z π (x, a) = ∞ t=0 γ t R(x t , a t ), where x 0 = x, a 0 = a, x t ∼ P (·|x t−1 , a t−1 ) and a t ∼ π(·|x t ). The objective in reinforcement learning can be summarized as finding the optimal π * that maximizes the expectation of Z π , the action-value function Q π (x, a) = E[Z π (x, a)]. The most common approach is to find the unique fixed point of the Bellman optimality operator T <ref type="bibr" target="#b3">[Bellman, 1957]</ref>:</p><formula xml:id="formula_0">Q * (x, a) = T Q * (x, a) := E[R(x, a)] + γE P max a Q * (x , a ) .</formula><p>To update Q, which is approximated by a neural network in most deep reinforcement learning studies, Q-learning <ref type="bibr" target="#b23">[Watkins, 1989]</ref> iteratively trains the network by minimizing the squared temporal difference (TD) error defined by</p><formula xml:id="formula_1">δ 2 t = r t + γ max a ∈A Q (x t+1 , a ) − Q (x t , a t ) 2</formula><p>along the trajectory observed while the agent interacts with the environment following -greedy policy. DQN <ref type="bibr" target="#b12">[Mnih et al., 2015]</ref> uses a convolutional neural network to represent Q and achieves human-level play on the Atari-57 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributional RL</head><p>Instead of a scalar Q π (x, a), distributional RL looks into the intrinsic randomness of Z π by studying its distribution. The distributional Bellman operator for policy evaluation is</p><formula xml:id="formula_2">Z π (x, a) D = R(x, a) + γZ π (X , A ) ,</formula><p>where X ∼ P (·|x, a) and A ∼ π(·|X ), A D = B denotes that random variable A and B follow the same distribution.</p><p>Both theory and algorithms have been established for distributional RL. In theory, the distributional Bellman operator for policy evaluation is proved to be a contraction in the p-Wasserstein distance <ref type="bibr" target="#b2">[Bellemare et al., 2017]</ref>. <ref type="bibr" target="#b2">Bellemare et al. [2017]</ref> shows that C51 outperforms value-based RL, in addition <ref type="bibr" target="#b7">Hessel et al. [2018]</ref> combined C51 with enhancements such as prioritized experience replay , n-step updates <ref type="bibr" target="#b19">[Sutton, 1988]</ref>, and the dueling architecture <ref type="bibr" target="#b22">[Wang et al., 2016]</ref>, leading to the Rainbow agent, current state-of-the-art in Atari-57 for non-distributed agents, while the distributed algorithm proposed by <ref type="bibr" target="#b10">Kapturowski et al. [2018]</ref> achieves state-of-the-art performance for all agents. From an algorithmic perspective, it is impossible to represent the full space of probability distributions with a finite collection of parameters. Therefore the parameterization of quantile functions is usually the most crucial part in a general distributional RL algorithm. In C51, the true distribution is projected to a categorical distribution <ref type="bibr" target="#b2">[Bellemare et al., 2017]</ref> with fixed values for parameterization. QR-DQN fixes probabilities instead of values, and parameterizes the quantile values <ref type="bibr" target="#b5">[Dabney et al., 2018a]</ref> while IQN randomly samples the probabilities <ref type="bibr" target="#b5">[Dabney et al., 2018a]</ref>. We will introduce QR-DQN and IQN in Section 2.2, and extend from their work to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Quantile Regression for Distributional RL</head><p>In contrast to C51 which estimates probabilities for N fixed locations in return, QR-DQN <ref type="bibr" target="#b6">[Dabney et al., 2018b]</ref> estimates the respected quantile values for N fixed, uniform probabilities. In QR-DQN, the distribution of the random return is approximated by a uniform mixture of N Diracs,</p><formula xml:id="formula_3">Z θ (x, a) := 1 N N i=1 δ θi(x,a) ,</formula><p>with each θ i assigned a quantile value trained with quantile regression.</p><p>Based on QR-DQN, <ref type="bibr" target="#b5">Dabney et al. [2018a]</ref> propose using probabilities sampled from a base distribution, e.g. τ ∈ U ([0, 1]), rather than fixed probabilities. They further learn the quantile function that maps from embeddings of sampled probabilities to the corresponding quantiles, called implicit quantile value network (IQN). At the time of this writing, IQN achieves the state-or-the-art performance on Atari-57 benchmark, human-normalized mean and median of all agents that does not combine distributed RL, prioritized replay  and n-step update. <ref type="bibr" target="#b5">Dabney et al. [2018a]</ref> claimed that with enough network capacity, IQN is able to approximate to the full quantile function with infinite number of quantile fractions. However, in practice one needs to use a finite number of quantile fractions to estimate action values for decision making, e.g. 32 randomly sampled quantile fractions as in <ref type="bibr" target="#b5">Dabney et al. [2018a]</ref>. With limited fractions, a natural question arises that, how to best utilize those fractions to find the closest approximation of the true distribution?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Algorithm</head><p>We propose Fully parameterized Quantile Function (FQF) for Distributional RL. Our algorithm consists of two networks, the fraction proposal network that generates a set of quantile fractions for each state-action pair, and the quantile value network that maps probabilities to quantile values. We first describe the fully parameterized quantile function in Section 3.1, with variables on both probability axis and value axis. Then, we show how to train the fraction proposal network in Section 3.2, and how to train the quantile value network with quantile regression in Section 3.3. Finally, we present our algorithm and describe the implementation details in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fully Parameterized Quantile Function</head><p>In FQF, we estimate N adjustable quantile values for N adjustable quantile fractions to approximate the quantile function. The distribution of the return is approximated by a weighted mixture of N Diracs given by</p><formula xml:id="formula_4">Z θ,τ (x, a) := N −1 i=0 (τ i+1 − τ i )δ θi(x,a) ,<label>(1)</label></formula><p>where δ z denotes a Dirac at z ∈ R, τ 1 , ...τ N −1 represent the N-1 adjustable fractions satisfying τ i−1 &lt; τ i , with τ 0 = 0 and τ N = 1 to simplify notation. Denote quantile function <ref type="bibr" target="#b14">[Müller, 1997]</ref> </p><formula xml:id="formula_5">F −1 Z the inverse function of cumulative distribution function F Z (z) = P r(Z &lt; z). By definition we have F −1 Z (p) := inf {z ∈ R : p ≤ F Z (z)}</formula><p>where p is what we refer to as quantile fraction.</p><p>Based on the distribution in Eq.(1), denote Π θ,τ the projection operator that projects quantile function onto a staircase function supported by θ and τ , the projected quantile function is given by</p><formula xml:id="formula_6">F −1,θ,τ Z (ω) = Π θ,τ F −1 Z (ω) = θ 0 + N −1 i=0 (θ i+1 − θ i )H τi+1 (ω),</formula><p>where H is the Heaviside step function and H τ (ω) is the short for H(ω − τ ). <ref type="figure" target="#fig_0">Figure 1</ref> gives an example of such projection. For each state-action pair (x, a), we first generate the set of fractions τ using the fraction proposal network, and then obtain the quantiles values θ corresponding to τ using the quantile value network.</p><p>To measure the distortion between approximated quantile function and the true quantile function, we use the 1-Wasserstein metric given by</p><formula xml:id="formula_7">W 1 (Z, θ, τ ) = N −1 i=0 τi+1 τi F −1 Z (ω) − θ i dω.<label>(2)</label></formula><p>Unlike KL divergence used in C51 which considers only the probabilities of the outcomes, the p-Wasseretein metric takes both the probability and the distance between outcomes into consideration. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the concept of how different approximations could affect W 1 error, and shows an example of Π W1 . However, note that in practice Eq.</p><p>(2) can not be obtained without bias. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training fraction proposal Network</head><p>To achieve minimal 1-Wasserstein error, we start from fixing τ and finding the optimal corresponding quantile values θ. In QR-DQN, <ref type="bibr" target="#b5">Dabney et al. [2018a]</ref> gives an explicit form of θ to achieve the goal. We extend it to our setting: Lemma 1. <ref type="bibr" target="#b5">[Dabney et al., 2018a]</ref> For any τ 1 , ...τ N −1 ∈ [0, 1] satisfying τ i−1 &lt; τ i for i, with τ 1 = 0 and τ N = 1, and cumulative distribution function F with inverse F −1 , the set of θ minimizing Eq. <ref type="formula" target="#formula_7">(2)</ref> is given by</p><formula xml:id="formula_8">θ i = F −1 Z ( τ i + τ i+1 2 )<label>(3)</label></formula><p>We can now substitute θ i in Eq.</p><p>(2) with equation Eq.</p><p>(3) and find the optimal condition for τ to minimize W 1 (Z, τ ). For simplicity, we denoteτ i = τi+τi+1 2 .</p><p>Proposition 1. For any continuous quantile function F −1 Z that is non-decreasing, define the 1-</p><formula xml:id="formula_9">Wasserstein loss of F −1 Z and F −1,τ Z by W 1 (Z, τ ) = N −1 i=0 τi+1 τi F −1 Z (ω) − F −1 Z (τ i ) dω.<label>(4)</label></formula><p>∂W1 ∂τi is given by</p><formula xml:id="formula_10">∂W 1 ∂τ i = 2F −1 Z (τ i ) − F −1 Z (τ i ) − F −1 Z (τ i−1 ),<label>(5)</label></formula><p>∀i ∈ (0, N ).</p><formula xml:id="formula_11">Further more, ∀τ i−1 , τ i+1 ∈ [0, 1], τ i−1 &lt; τ i+1 , ∃τ i ∈ (τ i−1 , τ i+1 ) s.t. ∂W1 ∂τi = 0.</formula><p>Proof of proposition 1 is given in the appendix. While computing W 1 without bias is usually impractical, equation 5 provides us with a way to minimize W 1 without computing it. Let w 1 be the parameters of the fraction proposal network P , for an arbitrary quantile function F −1 Z , we can minimize W 1 by iteratively applying gradients descent to w 1 according to Eq.(5) and convergence is guaranteed. As the true quantile function F −1 Z is unknown to us in practice, we use the quantile value network F −1 Z,w2 with parameters w 2 for current state and action as true quantile function. The expected return, also known as action-value based on FQF is then given by</p><formula xml:id="formula_12">Q(x, a) = N −1 i=0 (τ i+1 − τ i )F −1 Z,w2 (τ i ),</formula><p>where τ 0 = 0 and τ N = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training quantile value network</head><p>With the properly chosen probabilities, we combine quantile regression and distributional Bellman update on the optimized probabilities to train the quantile function. Consider Z a random variable denoting the action-value at (x t , a t ) and Z the action-value random variable at (x t+1 , a t+1 ), the weighted temporal difference (TD) error for two probabilitiesτ i andτ j is defined by</p><formula xml:id="formula_13">δ t ij = r t + γF −1 Z ,w1 (τ i ) − F −1 Z,w1 (τ j )<label>(6)</label></formula><p>Quantile regression is used in QR-DQN and IQN to stochastically adjust the quantile estimates so as to minimize the Wasserstein distance to a target distribution. We follow QR-DQN and IQN where quantile value networks are trained by minimizing the Huber quantile regression loss <ref type="bibr" target="#b8">[Huber, 1964]</ref>, with threshold κ,</p><formula xml:id="formula_14">ρ κ τ (δ ij ) = |τ − I {δ ij &lt; 0}| L κ (δ ij ) κ , with L κ (δ ij ) = 1 2 δ 2 ij , if |δ ij | ≤ κ κ |δ ij | − 1 2 κ , otherwise</formula><p>The loss of the quantile value network is then given by</p><formula xml:id="formula_15">L(x t , a t , r t , x t+1 ) = 1 N N −1 i=0 N −1 j=0 ρ κ τj (δ t ij )<label>(7)</label></formula><p>Note that F −1 Z and its Bellman target share the same proposed quantile fractionsτ to reduce computation.</p><p>We perform joint gradient update for w 1 and w 2 , as illustrated in Algorithm 1. </p><formula xml:id="formula_16">Q(s , a ) ← N −1 i=0 (τ a i+1 − τ a i )F −1 Z ,w2 (τ a i ); a * ← argmax a Q(s , a ); // Compute L for 0 ≤ i ≤ N − 1 do for 0 ≤ j ≤ N − 1 do δ ij ← r + γF −1 Z ,w2 (τ i ) − F −1 Z,w2 (τ j ) end end L = 1 N N −1 i=0 N −1 j=0 ρ κ τj (δ ij ); // Compute ∂W 1 ∂τ i for i ∈ [1, N − 1] ∂W1 ∂τi = 2F −1 Z,w2 (τ i ) − F −1 Z,w2 (τ i ) − F −1 Z,w2 (τ i−1 ); Update w 1 with ∂W1</formula><p>∂τi ; Update w 2 with ∇L; Output: Q</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Our fraction proposal network is represented by one fully-connected MLP layer. It takes the state embedding of original IQN as input and generates fraction proposal. Recall that in Proposition 1, we require τ i−1 &lt; τ i and τ 0 = 0, τ N = 1. While it is feasible to have τ 0 = 0, τ N = 1 fixed and sort the output of τ w1 , the sort operation would make the network hard to train. A more reasonable and practical way would be to let the neural network automatically have the output sorted using cumulated softmax. Let q ∈ R N denote the output of a softmax layer, we have q i ∈ (0, 1), i ∈ [0, N − 1] and N −1</p><formula xml:id="formula_17">i=0 q i = 1. Let τ i = i−1 j=0 q j , i ∈ [0, N ]</formula><p>, then straightforwardly we have τ i &lt; τ j for ∀i &lt; j and τ 0 = 0, τ N = 1 in our fraction proposal network. Note that as W 1 is not computed, we can't directly perform gradient descent for the fraction proposal network. Instead, we use the grad_ys argument in the tensorflow operator tf.gradients to assign ∂W1 ∂τi to the optimizer. In addition, one can use entropy of q as a regularization term H(q) = − N −1 i=0 q i log q i to prevent the distribution from degenerating into a deterministic one.</p><p>We borrow the idea of implicit representations from IQN to our quantile value network. To be specific, we compute the embedding of τ , denoted by φ(τ ), with where w ij and b j are network parameters. We then compute the element-wise (Hadamard) product of state feature ψ(x) and embedding φ(τ ). Let denote element-wise product, the quantile values are given by F −1 Z (τ ) ≈ F −1 Z,w2 (ψ(x) φ(τ )). In IQN, after the set of τ is sampled from a uniform distribution, instead of using differences between τ as probabilities of the quantiles, the mean of the quantile values is used to compute action-value Q. While in expectation, Q =</p><formula xml:id="formula_18">N −1 i=0 (τ i+1 − τ i )F −1 Z ( τi+τi+1 2 ) with τ 0 = 0, τ N = 1 and Q = 1 N N i=1 F −1 Z (τ i )</formula><p>are equal, we use the former one to consist with our projection operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We test our algorithm on the Atari games from Arcade Learning Environment (ALE) Bellemare et al. <ref type="bibr">[2013]</ref>. We select the most relative algorithm to ours, IQN <ref type="bibr" target="#b5">[Dabney et al., 2018a]</ref>, as baseline, and compare FQF with QR-DQN <ref type="bibr" target="#b6">[Dabney et al., 2018b]</ref>, <ref type="bibr">C51 [Bellemare et al., 2017]</ref>, prioritized experience replay  and Rainbow <ref type="bibr" target="#b7">[Hessel et al., 2018]</ref>, the current state-of-art that combines the advantages of several RL algorithms including distributional RL. The baseline algorithm is implemented by <ref type="bibr" target="#b4">Castro et al. [2018]</ref> in the Dopamine framework, with slightly lower performance than reported in IQN. We implement FQF based on the Dopamine framework. Unfortunately, we fail to test our algorithm on Surround and Defender as Surround is not supported by the Dopamine framework and scores of Defender is unreliable in Dopamine. Following the common practice <ref type="bibr" target="#b21">[Van Hasselt et al., 2016]</ref>, we use the 30-noop evaluation settings to align with previous works. Results of FQF and IQN using sticky action for evaluation proposed by <ref type="bibr" target="#b11">Machado et al. [2018]</ref> are also provided in the appendix. In all, the algorithms are tested on 55 Atari games.</p><p>Our hyper-parameter setting is aligned with IQN for fair comparison. The number of τ for FQF is 32. The weights of the fraction proposal network are initialized so that initial probabilities are uniform as in QR-DQN, also the learning rates are relatively small compared with the quantile value network to keep the probabilities relatively stable while training. We run all agents with 200 million frames. At the training stage, we use -greedy with = 0.01. For each evaluation stage, we test the agent for 0.125 million frames with = 0.001. For each algorithm we run 3 random seeds. All experiments are performed on NVIDIA Tesla V100 16GB graphics cards.  <ref type="bibr" target="#b7">[Hessel et al., 2018</ref>] that combines C51 with prioritized replay, and n-step updates. We also set a new record on the number of games where non-distributed RL agent performs better than human. <ref type="figure" target="#fig_2">Figure 2</ref> shows the training curves of several Atari games. Even on games where FQF and IQN have similar performance such as Centipede , FQF is generally much faster thanks to self-adjusting fractions.</p><p>However, one side effect of the full parameterization in FQF is that the training speed is decreased. With same settings, FQF is roughly 20% slower than IQN due to the additional fraction proposal network. As the number of τ increases, FQF slows down significantly while IQN's training speed is not sensitive to the number of τ samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>Based on previous works of distributional RL, we propose a more general complete approximation of the return distribution. Compared with previous distributional RL algorithms, FQF focuses not only on learning the target, e.g. probabilities for C51, quantile values for QR-DQN and IQN, but also which target to learn, i.e quantile fraction. This allows FQF to learn a better approximation of the true distribution under restrictions of network capacity. Experiment result shows that FQF does achieve significant improvement.</p><p>There are some open questions we are yet unable to address in this paper. We will have some discussions here. First, does the 1-Wasserstein error converge to its minimal value when the quantile function is not fixed? We cannot guarantee convergence of the fraction proposal network in deep neural networks where we involve quantile regression and Bellman update. Second, though we empirically believe so, does the contraction mapping result for fixed probabilities given by <ref type="bibr" target="#b6">Dabney et al. [2018b]</ref> also apply on self-adjusting probabilities? Third, while FQF does provide potentially better distribution approximation with same amount of fractions, how will a better approximated distribution affect agent's policy and how will it affect the training process? More generally, how important is quantile fraction selection during training?</p><p>As for future work, we believe that studying the trained quantile fractions will provide intriguing results. Such as how sensitive are the quantile fractions to state and action, and that how the quantile fractions will evolve in a single run. Also, the combination of distributional <ref type="bibr">RL and DDPG in D4PG [Barth-Maron et al., 2018]</ref> showed that distributional RL can also be extended to continuous control settings. Extending our algorithm to continuous settings is another interesting topic. Furthermore, in our algorithm we adopted the concept of selecting the best target to learn. Can this intuition be applied to areas other than RL?</p><p>Finally, we also noticed that most of the games we fail to reach human-level performance involves complex rules that requires exploration based policies, such as Montezuma Revenge and Venture.</p><p>Integrating distributional RL will be another potential direction as in <ref type="bibr" target="#b20">[Tang and Agrawal, 2018]</ref>. In general, we believe that our algorithm can be viewed as a natural extension of existing distributional RL algorithms, and that distributional RL may integrate greatly with other algorithms to reach higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Proof for proposition 1 Proposition 1. For any continuous quantile function F −1 Z that is non-decreasing, define the 1-Wasserstein loss of F −1 Z and F −1,τ</p><formula xml:id="formula_19">Z by W 1 (Z, τ ) = N −1 i=0 τi+1 τi F −1 Z (ω) − F −1 Z (τ i ) dω.<label>(4)</label></formula><p>∂W1 ∂τi is given by</p><formula xml:id="formula_20">∂W 1 ∂τ i = 2F −1 Z (τ i ) − F −1 Z (τ i ) − F −1 Z (τ i−1 ),<label>(5)</label></formula><p>∀i ∈ (0, N ).</p><formula xml:id="formula_21">Further more, ∀τ i−1 , τ i+1 ∈ [0, 1], τ i−1 &lt; τ i+1 , ∃τ i ∈ (τ i−1 , τ i+1 ) s.t. ∂W1 ∂τi = 0. Proof. Note that F −1 Z is non-decreasing. We have ∂W 1 ∂τ i = ∂ ∂τ i ( τi τi−1 F −1 Z (ω) − F −1 Z (τ i−1 ) dω + τi+1 τi F −1 Z (ω) − F −1 Z (τ i ) dω) = ∂ ∂τ i ( τi−1 τi−1 F −1 Z (τ i−1 ) − F −1 Z (ω)dω + τî τi−1 F −1 Z (ω) − F −1 Z (τ i−1 )dω+ τi+1 τi F −1 Z (ω) − F −1 Z (τ i ) dω)) = τ i − τ i−1 4 ∂ ∂τ i F −1 Z (τ i−1 ) + F −1 Z (τ i ) − F −1 Z (τ i−1 ) − τ i − τ i−1 4 ∂ ∂τ i F −1 Z (τ i−1 )+ ∂ ∂τ i ( τi+1 τi F −1 Z (ω) − F −1 Z (τ i ) dω)) =F −1 Z (τ i ) − F −1 Z (τ i−1 ) + ∂ ∂τ i ( τi+1 τi F −1 Z (ω) − F −1 Z (τ i ) dω)) =F −1 Z (τ i ) − F −1 Z (τ i−1 ) + F −1 Z (τ i ) − F −1 Z (τ i ) =2F −1 Z (τ i ) − F −1 Z (τ i−1 ) − F −1 Z (τ i ) As F −1 Z is non-decreasing we have ∂W1 ∂τi | τi=τi−1 ≤ 0 and ∂W1 ∂τi | τi=τi+1 ≥ 0. Recall that F −1 Z is continuous, so ∃τ i ∈ (τ i−1 , τ i+1 ) s.t. ∂W1 ∂τi = 0.</formula><p>Hyper-parameter sheet  We sweep the learning rate of fraction proposal network among (0, 2.5e-5) and finally fix this learning rate as 2.5e-9. For the training of fraction proposal network, we use RMSProp optimizer. Note that though the fraction proposal network takes the state embedding of original IQN as input, we only apply gradient to our new introduced parameter and do not back-propagate the gradient to the convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximation demonstration</head><p>To demonstrate how FQF provides a better quantile function approximation, figure 3 provides plots of a toy case with different distributional RL algorithm's approximation of a known quantile function, from which we can see how quantile fraction selection affects distribution approximation.</p><p>(a) (b) <ref type="figure">Figure 3</ref>: Demonstration of quantile function approximation on a toy case. W 1 denotes 1-Wasserstein distance between the approximated function and the one obtained through MC method.</p><p>Varying number of quantile fractions <ref type="table">Table 3</ref> gives mean scores of FQF and IQN over 6 Atari games, using different number of quantile fractions, i.e. N . For IQN, the selection of N is based on the highest score of each column given in <ref type="figure" target="#fig_2">Figure 2</ref> of <ref type="bibr" target="#b5">[Dabney et al., 2018a]</ref>.</p><p>N=8 N=32 N=64 IQN 60.2 91.5 64.4 FQF 83.2 124.6 69.5 <ref type="table">Table 3</ref>: Mean scores across 6 Atari 2600 games, measured as percentages of human baseline. Scores are averages over 3 seeds.</p><p>Intuitively, the advantage of trained quantile fractions compared to random ones will be more observable at smaller N . At larger N when both trained quantile fractions and random ones are densely distributed over [0, 1], the differences between FQF and IQN becomes negligible. However from table 3 we see that even at large N , FQF performs slightly better than IQN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing proposed quantile fraction</head><p>In figure 4, we select a half-trained Kungfu Master agent with N = 8 to provide a case study of FQF. The reason why we choose a half-trained agent instead of a fully-trained agent is so that the distribution of Q is not a deterministic one. Note that theoretically the quantile function should be non-decreasing, however from the example we can see that the learned quantile function might not always follow this property, and this phenomenon further motivates a quite interesting future work that leverages the non-decreasing property as prior knowledge for quantile function learning. The figure shows how the interval between proposed quantile fractions (i.e., the output of the softmax layer that sums to 1. See Section 3.4 for details) vary during a single run. <ref type="figure">Figure 4</ref>: Interval between adjacent proposed quantile fractions for states at each time step in a single run. Different colors refer to different adjacent fractions' intervals, e.g. green curve refers to τ 2 − τ 1 . Whenever there appears an enemy behind the character, we see a spike in the fraction interval, indicating that proposed fraction is very different from that of following states without enemies. This suggests that the fraction proposal network is indeed state dependent and is able to provide different quantile fractions accordingly. To align with previous works, the scores are evaluated under 30 no-op setting. As the sticky action evaluation setting proposed by <ref type="bibr" target="#b11">Machado et al. [2018]</ref> is generally considered more meaningful in the RL community, we will add results under sticky-action evaluation setting after the conference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two approximations of the same quantile function using different set of τ with N = 6, the area of the shaded region is equal to the 1-Wasserstein error. (a) Finely-adjusted τ with minimized W 1 error. (b) Randomly chosen τ with larger W 1 error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>FQF update Parameter :N, κ Input: x, a, r, x , γ ∈ [0, 1) // Compute proposed fractions for x, a τ ← P w1 (x, a); // Compute proposed fractions for x , a for a ∈ A do τ a ← P w1 (x , a ); end // Compute greedy action</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>φ j (τ ) := ReLU n−1 i=0 cos(iπτ )w ij + b j , Performance comparison with IQN. Each training curve is averaged by 3 seeds. The training curves are smoothed with a moving average of 10 to improve readability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: hyper-parameter list</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Raw scores for a single seed across all games, starting with 30 no-op actions.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horgan</surname></persName>
		</author>
		<title level="m">Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dopamine: A Research Framework for Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Samuel Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1812.06110" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Implicit quantile networks for distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1104" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributional reinforcement learning with quantile regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177703732</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Markov decision processes with a new optimality criterion: Discrete time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stratton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaquette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="496" to="505" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recurrent experience replay in distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonparametric return distribution approximation for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuro</forename><surname>Morimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotaka</forename><surname>Hachiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="799" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<biblScope unit="volume">0471619779</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An analysis of categorical distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>abs/1511.05952</idno>
		<title level="m">Prioritized experience replay. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The variance of discounted markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="794" to="802" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploration by distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2710" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher John Cornish Hellaby</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mean, variance, and probabilistic criteria in finite markov decision processes: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dj White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note>ALE Scores GAMES RANDOM HUMAN DQN PRIOR.DUEL. QR-DQN</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
