<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitreya</forename><surname>Suin</surname></persName>
							<email>maitreyasuin21@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
							<email>kuldeeppurohit3@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper tackles the problem of motion deblurring of dynamic scenes. Although end-to-end fully convolutional designs have recently advanced the state-of-the-art in nonuniform motion deblurring, their performance-complexity trade-off is still sub-optimal. Existing approaches achieve a large receptive field by increasing the number of generic convolution layers and kernel-size, but this comes at the expense of of the increase in model size and inference speed. In this work, we propose an efficient pixel adaptive and feature attentive design for handling large blur variations across different spatial locations and process each test image adaptively. We also propose an effective content-aware global-local filtering module that significantly improves performance by considering not only global dependencies but also by dynamically exploiting neighboring pixel information. We use a patch-hierarchical attentive architecture composed of the above module that implicitly discovers the spatial variations in the blur present in the input image and in turn, performs local and global modulation of intermediate features. Extensive qualitative and quantitative comparisons with prior art on deblurring benchmarks demonstrate that our design offers significant improvements over the state-of-the-art in accuracy as well as speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion-blurred images form due to relative motion during sensor exposure and are favored by photographers and artists in many cases for aesthetic purpose, but seldom by computer vision researchers, as many standard vision tools including detectors, trackers, and feature extractors struggle to deal with blur. Blind motion deblurring is an ill-posed problem that aims to recover a sharp image from a given image degraded due to motion-induced smearing of texture and high-frequency details. Due to its diverse applications in surveillance, remote sensing, and cameras mounted * Equal contribution. Tao CVPR18</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours(a)</head><p>Zhang CVPR18</p><p>Zhang <ref type="formula" target="#formula_0">CVPR19</ref> Gao CVPR19</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kypyn ICCV2019</head><p>Ours(b) <ref type="figure">Figure 1</ref>. Comparison of different methods in terms of accuracy and inference time. Our approach outperforms all previous methods.</p><p>on hand-held and vehicle-mounted cameras, deblurring has gathered substantial attention from computer vision and image processing communities in the past two decades. Majority of traditional deblurring approaches are based on variational model, whose key component is the regularization term. The restoration quality depends on the selection of the prior, its weight, as well as tuning of other parameters involving highly non-convex optimization setups <ref type="bibr" target="#b13">[14]</ref>. Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise from various sources including moving objects, camera shake and depth variations, causing different pixels to capture different motion trajectories. Such hand-crafted priors struggle while generalizing across different types of real-world examples, where blur is far more complex than modeled <ref type="bibr" target="#b2">[3]</ref>.</p><p>Recent works based on deep convolutional neural networks (CNN) have studied the benefits of replacing the image formation model with a parametric model that can be trained to emulate the non-linear relationship between blurred-sharp image pairs. Such works <ref type="bibr" target="#b12">[13]</ref> directly regress to deblurred image intensities and overcome the limited representative capability of variational methods in describing dynamic scenes. These methods can handle combined effects of camera motion and dynamic object motion and achieve state-of-the-art results on single image deblurring task. They have reached a respectable reduction in model size, but still lack in accuracy and are not real-time.</p><p>Existing CNN-based methods have two major limitations: a) Weights of the CNN are fixed and spatially invariant which may not be optimal for different pixels in a dynamically blurred scene (e.g., sky vs. moving car pixels). This issue is generally tackled by learning a highly non-linear mapping by stacking a large number of filters. But this drastically increases the computational cost and memory consumption. b) A geometrically uniform receptive field is sub-optimal for the task of deblurring. Large image regions tend to be used to increase the receptive field even though the blur is small. This inevitably leads to a network with a large number of layers and a high computation footprint which slows down the convergence of the network.</p><p>Reaching a trade-off between the inference-speed, receptive field and the accuracy of a network is a non-trivial task (see <ref type="figure">Fig. 1</ref>). Our work focuses on the design of efficient and interpretable filtering modules that offer a better accuracyspeed trade-off as compared to simple cascade of convolutional layers. We investigate motion-dependent adaptability within a CNN to directly address the challenges in single image deblurring. Since motion blur is inherently directional and different for each image instance, a deblurring network can benefit from adapting to the blur present in each input test image. We deploy content-aware modules which adjust the filter to be applied and the receptive field at each pixel. Our analysis shows that the benefits of these dynamic modules for the deblurring task are two-fold: i) Cascade of such layers provides a large and dynamically adaptive receptive field. Directional nature of blur requires a directional receptive field, which a normal CNN cannot achieve within a small number of layers. ii) It efficiently enables spatially varying restoration, since changes in filters and features occur according to the blur in the local region. No previous work has investigated incorporating awareness of blur-variation within an end-to-end single image deblurring model.</p><p>Following the state of the art in deblurring, we adopt a multi-patch hierarchical design to directly estimate the restored sharp image. Instead of cascading along the depth, we introduce content-aware feature and filter transformation capability through a global-local attentive module and residual attention across layers to improve performance. These modules learn to exploit the similarity in the motion between different pixels within an image and are also sensitive to position-specific local context.</p><p>The efficiency of our architecture is demonstrated through a comprehensive evaluation on two benchmarks and comparisons with the state-of-the-art deblurring approaches. Our model achieves superior performance while being computationally more efficient. The major contributions of this work are:</p><p>• We propose an efficient deblurring design built on new convolutional modules that learn the transformation of features using global attention and adaptive local filters. We show that these two branches complement each other and result in superior deblurring performance. Moreover, the efficient design of attentionmodule enables us to use it throughout the network without the need for explicit downsampling.</p><p>• We further demonstrate the efficacy of learning crossattention between encode-decoder as well as different levels in our design.</p><p>• We provide extensive analysis and evaluations on dynamic scene deblurring benchmarks, demonstrating that our approach yields state-of-the-art results while being 3× faster than the nearest competitor <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Architecture</head><p>To date, the driving force behind performance improvement in deblurring has been the use of a large number of layers and larger filters which assist in increasing the "static" receptive field and the generalization capability of a CNN. However, these techniques offer suboptimal design, since network performance does not always scale with network depth, as the effective receptive field of deep CNNs is much smaller than the theoretical value (investigated in <ref type="bibr" target="#b11">[12]</ref>).</p><p>We claim that a superior alternative is a dynamic framework wherein the filtering and the receptive field change across spatial locations and also across different input images. Our experiments show that this approach is a considerably better choice due to its task-specific efficacy and utility for computationally limited environments. It delivers consistent performance across diverse magnitudes of blur.</p><p>Although previous multi-scale and scale-recurrent methods have shown good performance in removing nonuniform blur, they suffer from expensive inference time and performance bottleneck while simply increasing model depth. Instead, inspired by <ref type="bibr" target="#b25">[26]</ref> , we adopt multi-patch hierarchical structure as our base-model, which compared to multi-scale approach has the added advantage of residuallike architecture that leads to efficient learning and faster processing speed. The overall architecture of our proposed network is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We divide the network into 3 levels instead of 4 as described in <ref type="bibr" target="#b25">[26]</ref>. We found that the relative performance gain due to the inclusion of level 4 is negligible compared to the increase in inference time and number of parameters. At the bottom level input sliced into 4 non-overlapping patches for processing, and as we gradually move towards higher levels, the number of patches decrease and lower level features are adaptively fused using attention module as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The output of level 1 is the final deblurred image. Note that unlike <ref type="bibr" target="#b25">[26]</ref>, we also avoid cascading of our network along depth, as that adds severe computational burden. Instead, we advocate the use of content-aware processing modules which yield significant performance improvements over even the deepest stacked versions of original DMPHN <ref type="bibr" target="#b25">[26]</ref>. Major changes incorporated in our design are described next.</p><p>Each level of our network consists of an encoder and a decoder. Both the encoder and the decoder are made of standard convolutional layer and residual blocks where each of these residual blocks contains 1 convolution layer followed by a content-aware processing module and another convolutional layer. The content-aware processing module comprises two branches for global and local level feature processing which are dynamically fused at the end. The residual blocks of decoder and encoder are identical except for the use of cross attention in decoder. We have also designed cross-level attention for effective propagation of lower level features throughout the network. We begin with describing content-aware processing module, then proceed towards the detailed description of the two branches and finally how these branches are adaptively fused at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Content-Aware Processing Module</head><p>In contrast to high-level problems such as classification and detection <ref type="bibr" target="#b21">[22]</ref>, which can obtain large receptive field by successively down-sampling the feature map with pooling or strided convolution, restoration tasks like deblurring need finer pixel details that can not be achieved from highly downsampled features. Most of the previous deblurring ap-proaches uses standard convolutional layers for local filtering and stack those layers together to increase the receptive field. <ref type="bibr" target="#b0">[1]</ref> uses self-attention and standard convolution on parallel branch and shows that best results are obtained when both features are combined together compared to using each feature separately. Inspired by this approach, we design a content-aware "global-local" processing module which depending on the input, deploys two parallel branches to fuse global and local features. The "global" branch is made of attention module. For decoder, this includes both self and cross-encoder-decoder attention whereas for encoder only self-attention is used. For local branch we design a pixel-dependent filtering module which determines the weight and the local neighbourhood to apply the filter adaptively. We describe in detail these two branches and their adaptive fusion strategy in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attention</head><p>Following the recent success of transformer architecture <ref type="bibr" target="#b20">[21]</ref> in natural language processing domain, it has been introduced in image processing tasks as well <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref>. The main building block of this architecture is self-attention which as the name suggests calculates the response at a position in a sequence by attending to all positions within the same sequence. Given an input tensor of shape (C, H, W ) it is flattened to a matrix z ∈ R HW ×C and projected to d a and d c dimensional spaces using embedding matrices W a , W b ∈ R C×da and W c ∈ R C×dc . Embedded matrices A, B ∈ R HW ×da and C ∈ R HW ×dc are known as query, key and value, respectively. The output of the self-attention mechanism for a single head can be expressed as</p><formula xml:id="formula_0">O = softmax AB T √ d a C<label>(1)</label></formula><p>The main drawback of this approach is very high memory requirement due to the matrix multiplication AB T which requires storing a high dimensional matrix of dimension (HW, HW ) for image domain. This requires a large downsampling operation before applying attention. <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b16">[17]</ref> use a local memory block instead of global all-to-all for making it practically usable. <ref type="bibr" target="#b0">[1]</ref> uses attention only from the layer with the smallest spatial dimension until it hits memory constraints. Also, these works typically resort to smaller batch size and sometimes additionally downsampling the inputs to self-attention layers. Although self attention is implemented in recent video super-resolution work <ref type="bibr" target="#b24">[25]</ref>, to reduce memory requirement it resorts to pixelshuffling. This process is sub-optimal for spatial attention as pixels are transferred to channel domain to reduce the size. Different from others, we resort to an attention mechanism which is lightweight and fast. If we consider Eq. without the softmax and scaling factor for simplicity, we first do a (HW, d a ) × (d a , HW ) matrix multiplication and then another (HW, HW )×(HW, d c ) matrix multiplication which is responsible for the high memory requirement and has a complexity of O(d a (HW ) 2 ). Instead, if we look into this equation differently and first compute B T C which is an (d a , HW ) × (HW, d c ) matrix multiplication followed by A(B T C) which is an (HW, d a ) × (d a , d c ) matrix multiplication, this whole process becomes lightweight with a complexity of O(d a d c HW ). We suitably introduce softmax operation at two places which makes this approach intuitively different from standard self-attention but still efficiently gathers global information for each pixel. Empirically we show that it performs better than standard selfattention as discussed in ablation studies. Also due to the light-weight nature, it not only enables us to use this in all the encoder and decoder blocks across levels for selfattention but also across different layers of encoder-decoder and levels for cross attention which results in a significant increase of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Self-Attention (SA)</head><p>We start with generating a spatial attention mask M 1 describing which spatial features to emphasize or suppress for better motion understanding. Given the input feature map x ∈ R C×H×W we generate M 1 as</p><formula xml:id="formula_1">M 1 = f m1 (x; θ m1 )<label>(2)</label></formula><p>where M 1 ∈ R H×W , f m1 (·) is convolution followed by a sigmoid operation to generate a valid attention map. We generate the enhanced feature map x m1 by element-wise multiplication as</p><formula xml:id="formula_2">x m1 = x M 1<label>(3)</label></formula><p>where x m ∈ R C×H×W and M is broadcast along channel dimension accordingly. Next, we distribute these informative features to all the pixels adaptively which is similar to standard self-attention operation. Given x m , we generate three attention maps P ∈ R C2×HW , Q ∈ R C2×HW and M 2 ∈ R C using convolutional operations f p (·) ,f q (·) and f M2 (·) where globalaverage-pooling is used for the last case to get C dimensional representation. We take the first cluster of attention map Q and split it into C 2 different maps Q = {q 1 , q 2 , ..., q C2 }, q i ∈ R HW and these represent C 2 different spatial attention-weights. A single attention reflects one aspect of the blurred image. However, there are multiple pertinent properties like edges,textures etc. in the image that together helps removing the blur. Therefore, we deploy a cluster of attention maps to effectively gather C 2 different key features. Each attention map is element-wise multiplied with the input feature map x m1 to generate C 2 part feature maps as</p><formula xml:id="formula_3">x k m1 = q k x m1 , with HW i=1 q ki = 1 (k = 1, 2, ..., N ) (4) where x k m ∈ R C×HW .</formula><p>We further extract descriptive global feature by global-sum-pooling (GSP) along HW dimension to obtain k th feature representation as</p><formula xml:id="formula_4">x k m1 = GSP HW (x k m1 ) (k = 1, 2, ..., N )<label>(5)</label></formula><p>wherex k m ∈ R C . Now we havex m1 = {x 1 m1 ,x 2 m1 , ...,x C2 m1 } which are obtained from C 2 different attention-weighted average of the input x m . Each of these C 2 representations is expressed by an C-dimensional vector which is a feature descriptor for the C channels. Similar to the first step (Eq.(3)), we further enhance these C dimensional vectors by emphasizing the important featureembeddings asx</p><formula xml:id="formula_5">k m1m2 = M 2 x k m1<label>(6)</label></formula><p>where M 2 can be expressed as</p><formula xml:id="formula_6">M 2 = f m2 (x m1 ; θ m2 ) ∈ R C<label>(7)</label></formula><p>Eq.</p><p>(3) and Eq. <ref type="formula" target="#formula_5">(6)</ref> can be intuitively compared to <ref type="bibr" target="#b3">[4]</ref>, where similar gated-enhancement technique is used to refine the result by elementwise-multiplication with an attention mask that helps in propagating only the relevant information. Next we take the set of attention maps P = {p 1 , p 2 , ..., p HW } where p i ∈ R C2 is represents attention map for i th pixel. Intuitively, p i shows the relative importance of C 2 different attention-weighted average (x m1m2 ) for the current pixel and it allows the pixel to adaptively select the weighted average of all the pixels. For each output pixel j, we element-wise multiply these C 2 feature representationsx k m1m2 with the corresponding attention map p j , to get</p><formula xml:id="formula_7">y j = p j x m1m2 with C2 i=1 p ji = 1 , (j = 1, 2, ..., HW ) (8)</formula><p>where y j ∈ R C×C2 . We again apply global-averagepooling on y j along C 2 to get C dimensional feature representation for each pixel as</p><formula xml:id="formula_8">y j = GAP C2 (y j )<label>(9)</label></formula><p>whereȳ j ∈ R C represent the accumulated global feature for the j th pixel. Thus, each pixel flexibly selects features that are complementary to the current one and accumulates a global information. This whole sequence of operations can be expressed by efficient matrix-operations as</p><formula xml:id="formula_9">y att = C (A)softmax(B) T softmax(D)<label>(10)</label></formula><p>where A, B, C, D are given by</p><formula xml:id="formula_10">C = σ(f M2 (x m1 )) ∈ R C , A = σ(f M1 (x)) ∈ R C×HW B = f Q (x m1 ) ∈ R HW ×C2 , D = f P (x m1 ) ∈ R C2×HW</formula><p>This efficient and simple matrix multiplication makes this attention module very fast whereas the order of operation (first computing [(A)softmax(B) T ]) results in low memory footprint. Note that, C is broadcast along HW dimension appropriately. We utilize this attention block in both encoder and decoder at each level for self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Cross-Attention (CA)</head><p>Inspired from the use of cross-attention in <ref type="bibr" target="#b20">[21]</ref>, we implement cross encoder-decoder and cross level attention in our model. For cross encoder-decoder attention, we deploy similar attention module where the information to be attended is from different encoder layers and all the attention maps are generated by the decoder. Similarly for cross-level, the attended feature is from a lower level and the attention decisions are made by features from a higher level. We have observed that this helps in the propagation of information across layers and levels compared to simply passing the whole input or doing elementwise sum as done in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pixel-Dependent Filtering Module (PDF)</head><p>In contrast to <ref type="bibr" target="#b0">[1]</ref>, for the local branch, we use Pixel-Dependent Filtering Module to handle spatially-varying dynamic motion blur effectively. Previous works like <ref type="bibr" target="#b5">[6]</ref> generate sample-specific parameters on-the-fly using a filter generation network for image classification. <ref type="bibr" target="#b9">[10]</ref> uses input text to construct the motion-generating filter weights for video generation task. <ref type="bibr" target="#b27">[28]</ref> uses an adaptive convolutional layer where the convolution filter weights are the outputs of a separate filter-manifold network for crowd counting task. Our work is based on <ref type="bibr" target="#b18">[19]</ref> as we use a meta-layer to generate pixel dependent spatially varying kernel to implement spatially variant convolution operation. Along with that, the local pixels where the filter is to be applied, are also determined at runtime as we adjust the offsets of these filters adaptively. Given the input feature map x ∈ R C×H×W , we apply a kernel generation function to generate a spatially varying kernel V and do the convolution operation for pixel j as</p><formula xml:id="formula_11">y dyn j,c = K k=1 V j,j k W c [j k ]x[j + j k + ∆j k ]<label>(11)</label></formula><p>where y dyn j ∈ R C , K is the kernel size, j k ∈ {(−(K − 1)/2, −(K − 1)/2), ..., ((K − 1)/2, (K − 1)/2)} defines position of the convolutional kernel of dilation 1, V j,j k ∈ R K 2 ×H×W is the pixel dependent kernel generated,W c ∈ R C×C×K×K is the fixed weight and ∆j k are the learnable offsets. We set a maximum threshold ∆ max for the offsets to enforce efficient local processing which is important for low level tasks like deblurring. Note that the kernels (V ) and offsets vary from one pixel to another, but are constant for all the channels, promoting efficiency. Standard spatial convolution can be seen as a special case of the above with adapting kernel being constant V j,j k = 1 and ∆j k = 0. In contrast to <ref type="bibr" target="#b0">[1]</ref>, which simply concatenates the output of these two branches, we design attentive fusion between these two branches so that the network can adaptively adjust the importance of each branch for each pixel at runtime. Empirically we observed that it performs better than simple addition or concatenation. Also, as discussed in visualization section, it gives an insight into the specific requirement for different levels of blur. Given the original input x to this content-aware module, we generate a fusion mask as</p><formula xml:id="formula_12">M f us = sigmoid(f f us (x))<label>(12)</label></formula><p>where M f us ∈ R H×W , f f us is a single convolution layer generating single channel output. Then we fuse the two branches as</p><formula xml:id="formula_13">y GL = M f us y att + (1 − M f us ) y dyn<label>(13)</label></formula><p>The fused output y GL contains global as well as local information distributed adaptively along pixels which helps in handling spatially-varying motion blur effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Datasets: We follow the configuration of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>, which train on 2103 images from the GoPro dataset <ref type="bibr" target="#b12">[13]</ref>. For testing, we use two benchmarks: GoPro <ref type="bibr" target="#b12">[13]</ref> (1103 HD images), and HIDE <ref type="bibr" target="#b17">[18]</ref> (2025 HD images).</p><p>Training settings and implementation details: All the convolutional layers within our proposed modules contain 128 filters. The hyper-parameters for our encoder-decoder backbone are N = 3, M = 2, and P = 2, and filter size in PDF modules is 5 × 5. Following <ref type="bibr" target="#b25">[26]</ref>, we use batchsize of 6 and patch-size of 256 × 256. Adam optimizer <ref type="bibr" target="#b6">[7]</ref> was used with initial leaning rate 10 −4 , halved after every 2 × 10 5 iterations. We use PyTorch <ref type="bibr" target="#b15">[16]</ref> library and Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance comparisons</head><p>The main application of our work is efficient deblurring of general dynamic scenes. Due to the complexity of the blur present in such images, conventional image formation model based deblurring approaches struggle to per-form well. Hence, we compare with only two conventional methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> (which are selected as representative traditional methods for non-uniform deblurring, with publicly available implementations). We provide extensive comparisons with state-of-the-art learning-based methods, namely MS-CNN <ref type="bibr" target="#b12">[13]</ref>, DeblurGAN <ref type="bibr" target="#b7">[8]</ref>, DeblurGAN-v2 <ref type="bibr" target="#b8">[9]</ref>, SRN <ref type="bibr" target="#b19">[20]</ref>, and Stack(4)-DMPHN <ref type="bibr" target="#b25">[26]</ref>. We use official implementation from the authors with default parameters.</p><p>Quantitative Evaluation We show performance comparisons on two different benchmark datasets. The quantitative results on GoPro testing set and HIDE Dataset <ref type="bibr" target="#b17">[18]</ref> are listed in <ref type="table" target="#tab_0">Table 1</ref> and 2. We evaluate two variants of our model with(b) and without(a) learnable offsets as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The average PSNR and SSIM measures obtained on the GoPro test split is provided in <ref type="table" target="#tab_0">Table 1</ref>. It can be observed from the quantitative measures that our method performs better compared to previous state-of-the-art. The results shown in <ref type="figure" target="#fig_3">Figure 4</ref>. shows the large dynamic blur handling capability of our model while preserving sharpness. We further evaluate the run-time of all the methods on a single GPU with images of resolution 720 × 1280. The standarddeviation of the PSNR, SSIM, and run-time scores on the GoPro test set are 1.78, 0.018, and 0.0379, respectively. As reported in <ref type="table" target="#tab_0">Table 1</ref>, our method takes significantly less time compared to other methods.</p><p>We also evaluate our method on the recent HIDE Dataset <ref type="bibr" target="#b17">[18]</ref>. Both of GoPro and HIDE datasets contain dominant foreground object motion along with camera motion. We compare against all existing models trained on GoPro train-set for fair comparisons. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our approach outperforms all methods including <ref type="bibr" target="#b17">[18]</ref>, without requiring any human bounding box supervision. The superiority of our model is owed to the robustness of the proposed adaptive modules.</p><p>Qualitative Evaluation: Visual comparisons on different dynamic and 3D scenes are shown in Figs. 4 and 5. Visual comparisons are given in <ref type="figure" target="#fig_3">Fig. 4</ref>. We observe that the  <ref type="figure">Figure 5</ref>. Visual comparisons of deblurring results on images from the HIDE test set <ref type="bibr" target="#b17">[18]</ref>. Key blurred patches are shown in (b), while zoomed-in patches from the deblurred results are shown in (c)-(g).  <ref type="figure">Figure 6</ref>. Visualization of intermediate results on images from the GoPro test set <ref type="bibr" target="#b17">[18]</ref>.  results of prior works suffer from incomplete deblurring or artifacts. In contrast, our network is able to restore scene details more faithfully which are noticeable in the regions containing text, edges, etc. An additional advantage over <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> is that our model waives-off the requirement of parameter tuning during test phase. On both the datasets, the proposed method achieves consistently better PSNR, SSIM and visual results with lower inference-time than DMPHN <ref type="bibr" target="#b25">[26]</ref> and a comparable number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>In <ref type="table" target="#tab_2">Table 3</ref>, we analyse the effect of individual modules on our network's performance, using 1103 test images from GoPro dataset <ref type="bibr" target="#b12">[13]</ref>. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the proposed resblock contains one content-aware processing module and two standard convolutional layers. To find the optimal number of resblock in encoder and decoder we trained different versions of our network with varying number of resblocks. Although, the training performance as well as the quantitative results got better with the increase in number of blocks, beyond 3 the improvement was marginal. This led us to the choice of using 3 resblocks in each encoder and decoder and serves as a good balance between efficiency and performance as well.</p><p>As the use of local convolution and global attention together <ref type="bibr" target="#b0">[1]</ref> or replacing local convolution with attention <ref type="bibr" target="#b16">[17]</ref> is explored recently for image recognition tasks, we further analyze it for image restoration tasks like deblurring. As shown in <ref type="table" target="#tab_2">Table 3</ref>, we observe that the advantages of SA and PDF modules are complimentary and their union leads to better performance (Net4 vs Net6). For better information flow between different layers of encoder-decoder and also between different levels we used CA, where the advantage of this attentive information flow rather than simple addition can be observed by comparing the performance of Net4 and Net5 compared to Net3. We also analyze the role of both adaptive weights and the adaptive local-neighborhood for PDF module. As shown quantitatively in <ref type="table" target="#tab_2">Table 3</ref> (Net7 and Net8) and visualized in <ref type="figure" target="#fig_5">Figure 7</ref>, adaptiveness of the offsets along with the weights perform better as it satisfies the need of directional local filters. We have also showed comparisons of the convergence plots of these models in supplementary. We also try to incorporate the attention mechanism used in <ref type="bibr" target="#b0">[1]</ref> in our model for fair comparison. Due to high memory requirement, we were only able to use one attention module in the decoder in each level. The resultant PSNR was 30.52 compared to 30.76 of Net3. But, as it already occupied full GPU memory, we were unable to introduce more blocks, or cross attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization and Analysis</head><p>The first row of <ref type="figure" target="#fig_5">Fig. 7</ref> contains images from the testing datasets which suffer from complex blur due to large camera and object motion. In the subsequent rows, we visualize the output of different modules of our network and analyze the behavior change while handling different levels of blur due to camera motion, varying depth, moving objects, etc. The second row of <ref type="figure" target="#fig_5">Fig. 7</ref> shows one of the attention-maps (q i , i ∈ 1, 2, ...C 2 ) corresponding to each image. We can observe the high correlation between estimated attention weights and the dominant motion blurred regions present in the image. This adaptive ability of the network to focus on relevant parts of the image can be considered crucial to the observed performance improvement. The third and fourth rows of <ref type="figure" target="#fig_5">Fig. 7</ref> show the spatially-varying nature of filter weights and offsets. Observe that a large horizontal offset is estimated in the regions with high horizontal blur so that the filter shape can spread along the direction of motion. Although the estimated filter wights are not directly interpretable, it can be seen that the variance of the filters correlates with the magnitude of blur. We further visualize the behavior of the fusion mask which adaptively weighs the outputs of the two branches for each pixel location. As shown in <ref type="figure">Fig. 6</ref>, PDF module output is more preferred in regions with moving foreground objects or blurred edges where most of the other regions give almost equal weight to both the branches. On the other hand, homogeneous regions where the effect of blur is negligible, have shown a preference towards the attention branch. To further investigate this behavior, we have visualized the spatial mask (M 1 ). As we can observe in <ref type="figure">Fig. 6(c)</ref>, the mask suppresses these homogeneous regions even before calculating self-attention for each pixel. This shows the robustness and interpretability of our attention module while handling any type of blur. PDF Module: We synthetically blurred 25 sharp images using synthetic linear PSFs oriented in 4 different directions (0 • ,45 • ,90 • ,135 • ). For these images, we recorded the dominant direction of filter offsets estimated by our PDF module. The values obtained (11 • ,50 • ,81 • ,126 • ) show high correlation between the offset orientations and the PSF angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a new content-adaptive architecture design for the challenging task of removing spatially-varying blur in images of dynamic scenes. Efficient self-attention is utilized in all the encoder-decoder to get better representation whereas cross-attention helps in efficient feature propagation across layers and levels. Proposed dynamic filtering module shows content-awareness for local filtering. The complimentary behaviour of the two branches are shown in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="figure">Fig. 6</ref>. Different from existing deep learningbased methods for such applications, the proposed method is more interpretable which is one of its key strengths. Our experimental results demonstrated that the proposed method achieved better results than state-of-the-art methods on two benchmarks both qualitatively and quantitatively. We showed that the proposed content-adaptive approach achieves an optimal balance of memory, time and accuracy and can be applied to other image-processing tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of our proposed network. CA block represents cross attention between different levels of encoderdecoder and different levels. All the resblock contains one content aware processing module. Symbol '+' denotes elementwise summation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of our content-aware processing module. The upper and lower branch show self-attention (Sec. 3.1.1) and PDF module (Sec. 3.2). The fusion module is described in Eqs. 12 and 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparisons of deblurring results on images from the GoPro test set<ref type="bibr" target="#b12">[13]</ref>. Key blurred patches are shown in (b), while zoomed-in patches from the deblurred results are shown in (c)-(h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The second row shows one of the spatial attention map for each image. The third row shows the spatial distribution of the horizontal-offset values for the filter. Fourth row shows the variance of the predicted kernel values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparisons with existing algorithms on 1103 images from the deblurring benchmark GoPro<ref type="bibr" target="#b12">[13]</ref>.</figDesc><table><row><cell>Method</cell><cell>[24]</cell><cell>[23]</cell><cell>[5]</cell><cell>[3]</cell><cell>[13]</cell><cell>[8]</cell><cell>[20]</cell><cell>[27]</cell><cell>[2]</cell><cell>[26]</cell><cell>[9]</cell><cell cols="2">Ours(a) Ours(b)</cell></row><row><cell>PSNR (dB)</cell><cell>21</cell><cell cols="10">24.6 23.64 26.4 29.08 28.7 30.26 29.19 30.90 31.20 29.55</cell><cell>31.85</cell><cell>32.02</cell></row><row><cell>SSIM</cell><cell cols="11">0.741 0.846 0.824 0.863 0.914 0.858 0.934 0.931 0.935 0.940 0.934</cell><cell>0.948</cell><cell>0.953</cell></row><row><cell>Time (s)</cell><cell>3800</cell><cell>700</cell><cell cols="2">3600 1200</cell><cell>6</cell><cell>1</cell><cell>1.2</cell><cell>1</cell><cell>1.0</cell><cell>0.98</cell><cell>0.48</cell><cell>0.34</cell><cell>0.77</cell></row><row><cell cols="2">(a) Input Image</cell><cell cols="2">(b) Fusion M f us</cell><cell></cell><cell cols="2">(c) Mask M 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="7">Performance comparisons with existing algorithms on</cell></row><row><cell cols="6">2025 images from the deblurring benchmark HIDE [18].</cell><cell></cell></row><row><cell>Method</cell><cell>[8]</cell><cell>[9]</cell><cell>[20]</cell><cell>[18] 1</cell><cell>[26]</cell><cell>Ours</cell></row><row><cell>PSNR</cell><cell cols="6">24.51 26.61 28.36 28.89 29.09 29.98</cell></row><row><cell>SSIM</cell><cell cols="6">0.871 0.875 0.915 0.930 0.924 0.930</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Quantitative comparison of different ablations of our net-</cell></row><row><cell>work on GoPro testset.</cell><cell></cell></row><row><cell cols="2">Design SA CA CLA Kernel Of f set PSNR</cell></row><row><cell>Net1</cell><cell>30.25</cell></row><row><cell>Net2</cell><cell>30.81</cell></row><row><cell>Net3</cell><cell>30.76</cell></row><row><cell>Net4</cell><cell>30.93</cell></row><row><cell>Net5</cell><cell>31.12</cell></row><row><cell>Net6</cell><cell>31.44</cell></row><row><cell>Net7</cell><cell>31.85</cell></row><row><cell>Net8</cell><cell>32.02</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09925</idno>
		<title level="m">Attention augmented convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3848" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongjoo</forename><surname>Tae Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3160" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07064</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8878" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video generation from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1673" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blur-invariant deep learning for blind-deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><forename type="middle">Kumar</forename><surname>Tm Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE E International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE E International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5572" to="5581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pixel-adaptive convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11166" to="11175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Non-uniform deblurring for shaken images. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="168" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unnatural l0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1107" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3106" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5978" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2521" to="2529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Crowd counting via scale-adaptive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaobo</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
