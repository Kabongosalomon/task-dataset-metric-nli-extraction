<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Detecting Robotic Grasps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lenz</surname></persName>
							<email>ianlenz@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@eecs.umich.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
							<email>asaxena@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Detecting Robotic Grasps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Robotic Grasping</term>
					<term>deep learning</term>
					<term>RGB-D multi- modal data</term>
					<term>Baxter</term>
					<term>PR2</term>
					<term>3D feature learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Robotic grasping is a challenging problem involving perception, planning, and control. Some recent works <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b66">67]</ref> address the perception aspect of this problem by converting it into a detection problem in which, given a noisy, partial view of the object from a camera, the goal is to infer the top locations where a robotic gripper could be placed (see <ref type="figure">Figure 1</ref>). Unlike generic vision problems based on static images, such robotic perception problems are often used in closed loop with controllers, so there are stringent requirements on performance and computational speed. In the past, hand-designing features has been the most popular method for several robotic tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b31">32]</ref>. However, this is cumbersome and time-consuming, especially when we must incorporate new input modalities such as RGB-D cameras.</p><p>Recent methods based on deep learning <ref type="bibr" target="#b0">[1]</ref> have demonstrated state-of-the-art performance in a wide variety of tasks, including visual recognition <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b59">60]</ref>, audio recognition <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41]</ref>, and natural language processing <ref type="bibr" target="#b11">[12]</ref>. These techniques are especially powerful because they are capable of learning useful features directly from both unlabeled and labeled data, avoiding the need for hand-engineering.</p><p>However, most work in deep learning has been applied in the context of recognition. Grasping is inherently a detection problem, and previous applications of deep learning to detection have typically focused on specific vision applications such as face detection <ref type="bibr" target="#b44">[45]</ref> and pedestrian detection <ref type="bibr" target="#b56">[57]</ref>. Our goal is not only to infer a viable grasp, but to infer the optimal grasp for a given object that maximizes the chance of successfully grasping it, which differs significantly from the problem of object detection. Thus, the first major contribution of our work is to apply deep learning to the problem of robotic grasping, in a fashion which could generalize to similar detection problems.</p><p>The second major contribution of our work is to propose a new method for handling multimodal data in the context of feature learning. The use of RGB-D data, as opposed to simple 2D image data, has been shown to significantly improve grasp detection results <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b55">56]</ref>. In this work, we present a multimodal feature learning algorithm which adds a structured regularization penalty to the objective function to be optimized during learning. As opposed to previous works in deep learning, which either ignore modality information at the first layer (i.e., encourage all features to use all modalities) <ref type="bibr" target="#b58">[59]</ref> or train separate first-layer features for each modality <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b60">61]</ref>, our approach allows for a middle-ground in which each feature is encouraged to use only a subset of the input modalities, but is not forced to use only particular ones.</p><p>We also propose a two-stage cascaded detection system based on deep learning. Here, we use fewer features for the first pass, providing faster, but only approximately accurate detections. The second pass uses more features, giving more accurate detections. In our experiments, we found that the first deep network, with fewer features, was better at avoiding overfitting but less accurate. We feed the top-ranked rectangles from the first layer into the second layer, leading to robust early rejection of false positives. Unlike manually designed two-step features as in <ref type="bibr" target="#b27">[28]</ref>, our method uses deep learning, which allows us to learn detectors that not only give higher performance, but are also computationally efficient.</p><p>We test our approach on a challenging dataset, where we show that our algorithm improves both recognition and detection performance for grasping rectangle data. We also show that our two-stage approach is not only able to match the performance of a single-stage system, but, in fact, improves results while significantly reducing the computational time needed for detection.</p><p>In summary, the contributions of this paper are: <ref type="bibr">•</ref> We present a deep learning algorithm for detecting <ref type="figure">Fig. 1</ref>: Detecting robotic grasps: Left: A cluttered lab scene labeled with rectangles corresponding to robotic grasps for objects in the scene. Green lines correspond to robotic gripper plates. We use a two-stage system based on deep learning to learn features and perform detection for robotic grasping. Center: Our Baxter robot "Yogi" successfully executing a grasp detected by our algorithm. Right: The grasp detected for this case, in the RGB (top) and depth (bottom) images obtained from Kinect. robotic grasps. To the best of our knowledge, this is the first work to do so. • In order to handle multimodal inputs, we present a new way to apply structured regularization to the weights to these inputs based on multimodal group regularization. <ref type="bibr">•</ref> We present a multi-step cascaded system for detection, significantly reducing its computational cost. • Our method outperforms the state-of-the-art for rectanglebased grasp detection, as well as previous deep learning algorithms. <ref type="bibr">•</ref> We implement our algorithm on both a Baxter and a PR2 robot, and show success rates of 84% and 89%, respectively, for executing grasps on a highly varied set of objects. The rest of the paper is organized as follows: We discuss related work in Section II. We present our two-step cascaded detection system in Section III, and some additional details in Section IV. We then describe our feature learning algorithm and structured regularization method in Section V. We present our experiments in Section VI, and discuss results in Section VII. We then present experiments on both Baxter and PR2 robots in Section VIII. We present several interesting directions for future work in Section IX, then conclude in Section X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Robotic Grasping</head><p>In this section, we will focus on perception-and learningbased approaches for robotic grasping. For a more complete review of the field, we refer the reader to review papers by Bohg et al. <ref type="bibr" target="#b3">[4]</ref>, Sahbani et al. <ref type="bibr" target="#b52">[53]</ref>, Bicchi and Kumar <ref type="bibr" target="#b1">[2]</ref> and Shimoga <ref type="bibr" target="#b57">[58]</ref>.</p><p>Most works define a "grasp" as an end-effector configuration which achieves partial or complete form-or forceclosure of a given object. This is a challenging problem because it depends on the pose and configuration of the robotic gripper as well as the shape and physical properties of the object to be grasped, and typically requires a search over a large number of possible gripper configurations. Early works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref> focused on testing for form-and force-closure, and synthesizing grasps fulfilling these properties according to some hand-designed "quality score" <ref type="bibr" target="#b16">[17]</ref>. More recent works have refined these definitions <ref type="bibr" target="#b49">[50]</ref>. These works assumed full knowledge of object shape and physical properties.</p><p>Grasping Given 3D Model: Fast synthesis of grasps for known 3D models remains an active research topic <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b64">65]</ref>, with recent methods using advanced physical simulation to find optimal grasps. Gallegos et al. <ref type="bibr" target="#b17">[18]</ref> performed optimization of grasps given both a 3D model of the object to be grasped and the desired contact points for the robotic gripper. Pokorny et al. <ref type="bibr" target="#b47">[48]</ref> define spaces of graspable objects, then map new objects to these spaces to discover grasps. However, these works are only applicable when the full 3D model of the object is exactly known, which may not be the case when a robot is interacting with a new environment. We note that some of these physics-based approaches might be combined with our approach in a multi-pass system, discussed further in Sec. IX.</p><p>Sensing for Grasping: In a real-world robotic setting, a robot will not have full knowledge of the 3D model and pose of an object to be grasped, but rather only incomplete information from some set of sensors such as color or depth cameras, tactile sensors, etc. This makes the problem of grasping significantly more challenging <ref type="bibr" target="#b3">[4]</ref>, as the algorithm must use more limited and potentially noisier information to detect a good grasp. While some works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">46]</ref> simply attempt to estimate the poses of known objects and then apply full-model grasping algorithms based on these results, others avoid this assumption, functioning on novel objects which the algorithm has not seen before.</p><p>Such works often made use of other simplifying assumptions, such as assuming that objects belong to one of a set of primitive shapes <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b5">6]</ref>, or are planar <ref type="bibr" target="#b41">[42]</ref>. Other works produced impressive results for specific cases, such as grasping the corners of towels <ref type="bibr" target="#b39">[40]</ref>. While such works escape the assumption of a fully-known object model, hand-coded grasping rules have a hard time dealing with the wide range of objects seen in real-world human environments, and are difficult and time-consuming to create.</p><p>Learning for Grasping: Machine learning methods have proven effective for a wide range of perception problems <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b2">3]</ref>, allowing a perception system to learn a mapping from some feature set to various visual properties. Early work by Kamon et al. <ref type="bibr" target="#b30">[31]</ref> showed that learning approaches could also be applied to the problem of grasping from vision, introducing a learning component to grasp quality scores.</p><p>Recent works have employed richer features and learning methods, allowing robots to grasp known objects which might be partially occluded <ref type="bibr" target="#b26">[27]</ref> or in an unknown pose <ref type="bibr" target="#b12">[13]</ref> as well as fully novel objects which the system has not seen before <ref type="bibr" target="#b53">[54]</ref>. Here, we will address the latter case. Earlier work focused on detecting only a single grasping point from 2D partial-view data, using heuristic methods to determine a gripper pose based on this point. <ref type="bibr" target="#b54">[55]</ref>. The use of 3D data was shown to significantly improve these results <ref type="bibr" target="#b55">[56]</ref> thanks to giving direct physical information about the object in question. With the advent of low-cost RGB-D sensors such as the Kinect, the use of depth data for robotic grasping has become ubiquitous.</p><p>Several other works attempted to use the learning algorithm to more fully constrain the detected grasps. Ekvall and Kragic <ref type="bibr" target="#b14">[15]</ref> and Huebner and Kragic <ref type="bibr" target="#b22">[23]</ref> used shape-based approximations as bases for learning algorithms which directly gave an approach vector. Le et al. <ref type="bibr" target="#b35">[36]</ref> treated grasp detection as a ranking problem over sets of contact points in image space. Jiang et al. <ref type="bibr" target="#b27">[28]</ref> represented a grasp as a 2D oriented rectangle in image space, with two edges corresponding to the gripper plates, using surface normals to determine the grasp approach vector. These approaches allow the detection algorithm to detect more exactly the gripper pose which should be used for grasping. In this work, we will follow the rectangle-based method.</p><p>Learning-based approaches have shown impressive results in grasping novel objects, showing that learning some parameters of the detection system can outperform human tuning. However, these approaches still require a significant degree of hand-engineering in the form of designing good input features.</p><p>Other Applications with RGBD Data. Due to the availability of inexpensive depth sensors, RGB-D data has been a significant research focus in recent years for various robotics applications. For example, Jiang et al. <ref type="bibr" target="#b29">[30]</ref> consider robotic placement of objects, while Teuliere and Marchand <ref type="bibr" target="#b62">[63]</ref> used RGB-D data for visual servoing. Several works, including those of Endres et al. <ref type="bibr" target="#b15">[16]</ref> and Whelan et al. <ref type="bibr" target="#b65">[66]</ref> have extended and improved Simultaneous Localization and Mapping (SLAM) for RGB-D data. Object detection and recognition has been a major focus in research on RGB-D data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7]</ref>. Most such works use hand-engineered features such as <ref type="bibr" target="#b51">[52]</ref>. The few works that perform feature learning for RGB-D data <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b2">3]</ref> largely ignore the multimodal nature of the data, not distinguishing the color and depth channels. Here, we present a structured regularization approach which allows us to learn more robust features for RGB-D and other multimodal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning</head><p>Deep learning approaches have demonstrated the ability to learn useful features directly from data for a wide variety of tasks. Early work by Hinton and Salakhutdinov <ref type="bibr" target="#b21">[22]</ref> showed that a deep network trained on images of hand-written digits will learn features corresponding to pen-strokes. Later work using localized convolutional features <ref type="bibr" target="#b37">[38]</ref> showed that these networks learn features corresponding to object parts when trained on natural images. This demonstrates that even the basic features learned by these systems will adapt to the data given. In fact, these approaches are not restricted to the visual domain, but rather have been shown to learn useful features for a wide range of domains, such as audio <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41]</ref> and natural language data <ref type="bibr" target="#b11">[12]</ref>.</p><p>Deep Learning for Detection: However, the vast majority of work in deep learning focuses on classification problems. Only a handful of previous works have applied these methods to detection problems <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9]</ref>. For example, Osadchy et al. <ref type="bibr" target="#b44">[45]</ref> and LeCun et al. <ref type="bibr" target="#b36">[37]</ref> applied a deep energy-based model to the problem of face detection, Sermanet et al. <ref type="bibr" target="#b56">[57]</ref> applied a convolutional neural network for pedestrian detection, and Coates et al. <ref type="bibr" target="#b8">[9]</ref> used a deep learning approach to detect text in images. Girshick et al. <ref type="bibr" target="#b18">[19]</ref> used learned convolutional features over image regions for object detection, while Szegedy et al. <ref type="bibr" target="#b61">[62]</ref> used a multi-scale approach based on deep networks for the same task.</p><p>All these approaches focused on object detection and similar problems, in which the goal is to find a bounding box which tightly contains the item to be detected, and for each item, all valid bounding boxes will be similar. However, in robotic grasp detection, there may be several valid grasps for an object in different regions, making it more important to select the one with the highest chance success. In addition, orientation matters much more to robotic grasp detection, as most grasps will only be viable for a small subset of the possible gripper orientations. Our approach to grasp detection will also generalize across object classes, and even to classes never seen before by the system, as opposed to the classspecific nature of object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Deep</head><p>Learning: Recent works in deep learning have extended these methods to handle multiple modalities of input data, such as audio and video <ref type="bibr" target="#b42">[43]</ref>, text and image data <ref type="bibr" target="#b60">[61]</ref>, and even RGB-D data <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b2">3]</ref>. However, all of these approaches have fallen into two camps -either learning completely separate low-level features for each modality <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b60">61]</ref>, or simply concatenating the modalities <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b2">3]</ref>. The former approaches have proven effective for data where the basic modalities differ significantly, such as the aforementioned case of text and images, while the latter is more effective in cases where the modalities are more similar, such as RGB-D data.</p><p>For some new combinations of modalities and tasks, it may not be clear which of these approaches will give better performance. In fact, in the ideal feature set, different features Finally, the top-ranked rectangle is selected and the corresponding grasp is executed using the parameters of the detected rectangle and the surface normal at its center. Red and green lines correspond to gripper plates, blue in RGB-D features indicates masked-out pixels. may use different subsets of the modalities. In this work, we will give a structured regularization method which guides the learning algorithm to select such subsets, without imposing hard constraints on network structure. Structured Learning and Structured Regularization: Several approaches have been proposed which attempt to use a specially-designed regularization function to impose structure on a set of learned parameters without directly enforcing it. Jalali et al. <ref type="bibr" target="#b25">[26]</ref> used a group regularization function in the multitask learning setting, where one set of features is used for multiple tasks. This function applies high-order regularization separately to particular groups of parameters. Their function regularized the number of features used for each task in a set of multi-class classification tasks solved by softmax regression. Intuitively, this encodes the belief that only some subset of the input features will be useful for each task, but this set of useful features might vary between tasks.</p><p>A few works have also explored the use of structured regularization in deep learning. The Topographic ICA algorithm <ref type="bibr" target="#b23">[24]</ref> is a feature-learning approach that applies a similar penalty term to feature activations, but not to the weights themselves. Coates and Ng <ref type="bibr" target="#b7">[8]</ref> investigate the problem of selecting receptive fields, i.e., subsets of the input features to be used together in a higher-level feature. The structure of the network is learned first, then fixed before learning the parameters of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP LEARNING FOR GRASP DETECTION: SYSTEM AND MODEL</head><p>In this work, we will present an algorithm for robotic grasp detection from a single RGB-D view. Our approach will be based on machine learning, but distinguish itself from previous approaches by learning not only the weights used to rank prospective grasps, but also the features used to rank them, which were previously hand-engineered.</p><p>We will do this using deep learning methods, learning a set of RGB-D features which will be extracted from each candidate grasp, then used to score that grasp. Our approach will include a structured multimodal regularization method which improves the quality of the features learned from RGB-D data without constraining network structure.</p><p>In our system for robotic grasping, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the robot first obtains an RGB-D image of the scene containing objects to be grasped. A small deep network is used to score potential grasps in this image, and a small candidate set of the top-ranked grasps is provided to a larger deep network, which yields a single best-ranked grasp.</p><p>In this work, we will represent potential grasps using oriented rectangles in the image plane as seen on the left in <ref type="figure" target="#fig_0">Fig. 2</ref>, with one pair of parallel edges corresponding to the robotic gripper <ref type="bibr" target="#b27">[28]</ref>. Each rectangle is thus parameterized by the X and Y coordinates of its upper-left corner, its width, height, and orientation in the image plane, giving a fivedimensional search space for potential grasps. Grasps will be ranked based on features extracted from the RGB-D image region contained inside their corresponding rectangle, aligned to the gripper plates, as seen in the center of <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>To translate a rectangle such as that shown on the right in <ref type="figure" target="#fig_0">Fig. 2</ref> into a gripper pose for grasping we find the point with the minimum depth inside the central third (horizontally) of the rectangle. We then use the averaged surface normal around this point to determine the approach vector for the gripper. The orientation of the detected rectangle is translated to a rotation around this vector to orient the gripper. We use the X-Y coordinates of the rectangle center along with the depth of the closest point to determine a grasping point in the robot's coordinate frame. We compute a pre-grasp position by shifting 10 cm back from the grasping point along this approach vector and position the gripper at this point. We then approach the object along the approach vector and grasp it.</p><p>Using a standard feature learning approach such as sparse auto-encoder <ref type="bibr" target="#b20">[21]</ref>, a deep network can be trained for the problem of grasping rectangle recognition (i.e., does a given rectangle in image space correspond to a valid robotic grasp?). <ref type="figure">Fig. 3</ref>: Illustration of our two-stage detection process: Given an image of an object to grasp, a small deep network is used to exhaustively search potential rectangles, producing a small set of top-ranked rectangles. A larger deep network is then used to find the top-ranked rectangle from these candidates, producing a single optimal grasp for the given object. <ref type="figure">Fig. 4</ref>: Deep network and auto-encoder: Left: A deep network with two hidden layers, which transform the input representation, and a logistic classifier at the top layer, which uses the features from the second hidden layer to predict the probability of a grasp being feasible. Right: An auto-encoder, used for pretraining. A set of weights projects input features to a hidden layer. The same weights are then used to project these hidden unit outputs to a reconstruction of the inputs. In the sparse auto-encoder (SAE) algorithm, the hidden unit activations are also penalized.</p><p>However, in a real-world robotic setting, our system needs to perform detection (i.e., given an image containing an object, how should the robot grasp it?). This task is significantly more challenging than simple recognition.</p><p>Two-stage Cascaded Detection: In order to perform detection, one naive approach could be to consider each possible oriented rectangle in the image (perhaps discretized to some level), and evaluate each rectangle with a deep network trained for recognition. However, such near-exhaustive search of possible rectangles (based on positions, sizes, and orientations) can be quite expensive in practice for real-time robotic grasping.</p><p>Motivated by multi-step cascaded approaches in previous work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b63">64]</ref>, we instead take a two-stage approach to detection: First, we use a reduced feature set to determine a set of top candidates. Then, we use a larger, more robust feature set to rank these candidates.</p><p>However, these approaches require the design of two separate sets of features. In particular, it can be difficult to manually design a small set of first-stage features which is both quick to compute and robust enough to produce a good set of candidate detections for the second stage. Using deep learning allows us to circumvent the costly manual design of features by simply training networks of two different sizes, using the smaller for the exhaustive first pass, and the larger to re-rank the candidate detection results.</p><p>Model: To detect robotic grasps from the rectangle representation, we model the probability of a rectangle G (t) , with features x (t) ∈ R N being graspable, using a random variablê y (t) ∈ {0, 1} which indicates whether or not we predict G (t) to be graspable. We use a deep network, as shown in <ref type="figure">Fig. 4</ref>-left, with two layers of sigmoidal hidden units h <ref type="bibr" target="#b0">[1]</ref> and h <ref type="bibr" target="#b1">[2]</ref> , with K 1 and K 2 units per layer, respectively. A logistic classifier over the outputs of the second-layer hidden units then predicts P (ŷ (t) |x (t) ; Θ), so chosen because ground-truth graspability is represented as binary. Each layer will have a set of weights W [ ] mapping from its inputs to its hidden units, so the parameters of our model are Θ = {W <ref type="bibr" target="#b0">[1]</ref> , W <ref type="bibr" target="#b1">[2]</ref> , W <ref type="bibr" target="#b2">[3]</ref> }. Each hidden unit forms output by a sigmoid σ(a) = 1/(1 + exp(−a)) over its weighted input:</p><formula xml:id="formula_0">h [1](t) j = σ N i=1 x (t) i W [1] i,j h [2](t) j = σ K1 i=1 h [1](t) i W [2] i,j P (ŷ (t) = 1|x (t) ; Θ) = σ K2 i=1 h [2](t) i W [3] i (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inference and Learning</head><p>During inference, our goal is to find the single grasping rectangle with the maximum probability of being graspable for some new object. With G representing a particular grasping rectangle position, orientation, and size, we find this best rectangle as:</p><formula xml:id="formula_1">G * = arg max G P (ŷ (t) = 1|φ(G); Θ)<label>(2)</label></formula><p>Here, the function φ extracts the appropriate input representation for rectangle G.</p><p>During learning, our goal is to learn the parameters Θ that optimize the recognition accuracy of our system. Here, input data is given as a set of pairs of features x (t) ∈ R N and ground-truth labels y (t) ∈ {0, 1} for t = 1, . . . , M . As in most deep learning works, we use a two-phase learning approach.</p><p>In the first phase, we will use unsupervised feature learning to initialize the hidden-layer weights W <ref type="bibr" target="#b0">[1]</ref> and W <ref type="bibr" target="#b1">[2]</ref> . Pretraining weights this way is critical to avoid overfitting. We will use a variant of a sparse auto-encoder (SAE) <ref type="bibr" target="#b20">[21]</ref>, as illustrated in <ref type="figure">Fig. 4</ref>-right. We define g(h) as a sparsity penalty function over hidden unit activations, with λ controlling its weight. With f (W ) as a regularization function, weighted by β, andx (t) as the reconstruction of x (t) , SAE solves the following to initialize hidden-layer weights:</p><formula xml:id="formula_2">W * = arg min W M t=1 (||x (t) − x (t) || 2 2 + λ K j=1 g(h (t) j )) + βf (W ) h (t) j = σ( N i=1 x (t) i W i,j ) x (t) i = K j=1 h (t) j W i,j<label>(3)</label></formula><p>We first use this algorithm to initialize W <ref type="bibr" target="#b0">[1]</ref> to reconstruct x.</p><p>We then fix W <ref type="bibr" target="#b0">[1]</ref> and learn W <ref type="bibr" target="#b1">[2]</ref> to reconstruct h <ref type="bibr" target="#b0">[1]</ref> .</p><p>During the supervised phase of the learning algorithm, we then jointly learn classifier weights W <ref type="bibr" target="#b2">[3]</ref> and fine-tune hidden layer weights W <ref type="bibr" target="#b0">[1]</ref> and W <ref type="bibr" target="#b1">[2]</ref> for recognition. We maximize the log-likelihood of the data along with regularization penalties on hidden layer weights:</p><formula xml:id="formula_3">Θ * = arg max Θ M t=1 log P (ŷ (t) = y (t) |x (t) ; Θ) − β 1 f (W [1] ) − β 2 f (W [2] )<label>(4)</label></formula><p>Two-stage Detection Model: During inference for two-stage detection, we will first use a smaller network to produce a set of the top T rectangles with the highest probability of being graspable according to network parameters Θ 1 . We will then use a larger network with a separate set of parameters Θ 2 to re-rank these T rectangles and obtain a single best one. The only change to learning for the two-stage model is that these two sets of parameters are learned separately, using the same approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SYSTEM DETAILS</head><p>In this section, we will define the set of raw features which our system will use, forming x in the equations above, and how they are extracted from an RGB-D image. Some examples of these features are shown in <ref type="figure" target="#fig_0">Fig 2.</ref> Our algorithm uses only local information -specifically, we extract the RGB-D sub-image contained within each rectangle, and use this to generate features for that rectangle. This image is rotated so that its left and right edges correspond to the gripper plates, and then re-scaled to fit inside the network's receptive field.</p><p>From this 24x24 pixel image, seven channels' worth of features are extracted, giving 24x24x7 = 4032 input features.  The first three channels are the image in YUV color space, used because it represents image intensity and color separately. The next is simply the depth channel of the image. The last three are the X, Y, and Z components of surface normals computed based on the depth channel. These are computed after the image is aligned to the gripper so that they are always relative to the gripper plates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Pre-Processing</head><p>Whitening data is critical for deep learning approaches to work well, especially in cases such as multimodal data where the statistics of the input data may vary greatly. While PCAbased approaches have been shown to be effective <ref type="bibr" target="#b24">[25]</ref>, they are difficult to apply in cases such as ours where large portions of the data may be masked out.</p><p>Depth data, in particular, can be difficult to whiten because the range of values may be very different for different patches in the image. Thus, we first whiten each depth patch individually, subtracting the patch-wise mean and dividing by the patch-wise standard deviation, down to some minimum.</p><p>For multimodal data, the statistics of the data for each modality should match as closely as possible, to avoid learning features which are biased towards or away from using particular modes. This is particularly important when regularizing each modality separately, as in our approach. Thus, we drop mean values for each feature separately, but scale the data for each channel by dividing by the standard deviation of all its features combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preserving Aspect Ratio.</head><p>It is important for to preserve aspect ratio when feeding features into the network. This is because distorting image features may cause non-graspable rectangles to appear graspable, as shown in <ref type="figure" target="#fig_1">Fig. 5</ref>. However, padding with zeros can cause rectangles with less padding to receive higher graspability scores, as the network will have more nonzero inputs. It is important to account for this because in many cases the ideal grasp for an object might be represented by a thin rectangle which would thus contain many zero values in its receptive field from padding.</p><p>To address this problem, we scale up the magnitude of the available input for each rectangle based on the fraction of the rectangle which is masked out. In particular, we define a multiplicative scaling factor for the inputs from each modality, based on the fraction of each mode which is masked out, since each mode may have a different mask.</p><p>In the multimodal setting, we assume that the input data x is known to come from R distinct modalities, for example audio and video data, or depth and RGB data. We define the modality matrix S as an RxN binary matrix, where each element S r,i indicates membership of visible unit x i in a particular modality r, such as depth or image intensity. The scaling factor for mode r is then defined as:</p><formula xml:id="formula_4">Ψ (t) r = N i=1 S r,i / N i=1 S r,i µ (t) i , where µ (t) i is 1 if x (t)</formula><p>i is masked in, 0 otherwise. The scaling factor for case i is: ψ</p><formula xml:id="formula_5">(t) i = R r=1 S r,i Ψ (t) r .</formula><p>We could simply scale up each value of x by its corresponding scale factor when training our model, as</p><formula xml:id="formula_6">x (t) i = ψ (t) i x (t)</formula><p>i . However, since our sparse autoencoder penalizes squared error, scaling x linearly will scale the error for the corresponding cases quadratically, causing the learning algorithm to lend increased significance to cases where more data is masked out. Instead, we can use the scaled x as input to the network, but penalize reconstruction based on the original x, only scaling after the squared error has been computed:</p><formula xml:id="formula_7">W * = arg min W M t=1   N i=1 ψ (t) i (x (t) i − x (t) i ) 2 + λ K j=1 g(h (t) j )  <label>(5)</label></formula><p>We redefine the hidden units to use the scaled visible input:</p><formula xml:id="formula_8">h (t) j = σ N i=1 x (t) i W i,j<label>(6)</label></formula><p>This approach is equivalent to adding additional, potentially fractional, 'virtual' visible units to the model based on the scaling factor for each mode. In practice, we found it necessary to limit the scaling factor to a maximum of some value c, as Ψ</p><formula xml:id="formula_9">(t) r = min(Ψ (t)</formula><p>r , c). As shown in <ref type="table" target="#tab_0">Table III</ref> our mask-based scaling technique at the visible layer improves grasping results by over 25% for both metrics. As seen in <ref type="figure" target="#fig_2">Figure 6</ref>, it removes the network's inherent bias towards square rectangles, exhibiting a much wider range of aspect ratios that more closely matches that of the ground-truth data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. STRUCTURED REGULARIZATION FOR FEATURE LEARNING</head><p>A naive way of applying feature learning to multimodal data is to simply take x (as a concatenated vector) as input to the model described above, ignoring information about specific modalities, as seen on the lefthand side of <ref type="figure" target="#fig_3">Figure 7</ref>. This approach may either 1) prematurely learn features which include all modalities, which can lead to overfitting, or 2) fail to learn associations between modalities with very different underlying statistics.</p><p>Instead of concatenating multimodal input as a vector, Ngiam et al. <ref type="bibr" target="#b42">[43]</ref> proposed training a first layer representation for each modality separately, as shown in <ref type="figure" target="#fig_3">Figure 7</ref>-middle. This approach makes the assumption that the ideal low-level features for each modality are purely unimodal, while higherlayer features are purely multimodal. This approach may work better for some problems where the modalities have very different basic representations, such as the video and audio data (as used in <ref type="bibr" target="#b42">[43]</ref>), so that separate first layer features may give better performance. However, for modalities such as RGB-D data, where the input modes represent different channels of an image, learning low-level correlations can lead to more robust features -our experiments in Section VI show that simply concatenating the input modalities significantly outperforms training separate first-layer features for robotic grasp detection from RGB-D data.</p><p>For many problems, it may be difficult to tell which of these approaches will perform better, and time-consuming to tune and comparatively evaluate multiple algorithms. In addition, the ideal feature set for some problems may contain features  which use some, but not all, of the input modalities, a case which neither of these approaches are designed to handle.</p><p>To solve these problems, we propose a new algorithm for feature learning for multimodal data. Our approach incorporates a structured penalty term into the optimization problem to be solved during learning. This technique allows the model to learn correlated features between multiple input modalities, but regularizes the number of modalities used per feature (hidden unit), discouraging the model from learning weak correlations between modalities. With this regularization term, the algorithm can specify how mode-sparse or mode-dense the features should be, representing a continuum between the two extremes outlined above.</p><p>Regularization in Deep Learning: In a typical deep learning model, L 2 regularization (i.e., f (W ) = ||W || 2 2 ) or L 1 regularization (i.e., f (W ) = ||W || 1 ) are commonly used in training (e.g., as specified in Equations <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref>). These are often called a "weight cost" (or "weight decay"), and are left implicit in many works.</p><p>Applying regularization is well known to improve the generalization performance of feature learning algorithms. One might expect that a simple L 1 penalty would eliminate weak correlations in multimodal features, leading to features which use only a subset of the modes each. However, we found that in practice, a value of β large enough to cause this also degraded the quality of features for the remaining modes and lead to decreased task performance.</p><p>Multimodal Regularization: Structured regularization, such as in <ref type="bibr" target="#b25">[26]</ref>, takes a set of groups of weights, and applies some regularization function (typically high-order) separately to each group. In our structured multimodal regularization algorithm, each modality will be used as a regularization group separately for each hidden unit. For example, a group-wise pnorm would be applied as:</p><formula xml:id="formula_10">f (W ) = K j=1 R r=1 N i=1 S r,i |W p i,j | 1/p<label>(7)</label></formula><p>where S r,i is 1 if feature i belongs to group r and 0 otherwise. Using a high value of p allows us to penalize higher-valued weights from each mode to each feature more strongly than lower-valued ones. This also means that forming a high-valued weight in a group with other high-valued weights will accrue a lower additional penalty than doing so for a group with only low-valued weights. At the limit (p → ∞), this group regularization becomes equivalent to the infinity (or max) norm:</p><formula xml:id="formula_11">f (W ) = K j=1 R r=1 max i S r,i |W i,j |<label>(8)</label></formula><p>which penalizes only the maximum weight from each mode to each feature. In practice, the infinity norm is not differentiable and therefore is difficult to apply gradient-based optimization methods; in this paper, we use the log-sum-exponential as a differentiable approximation to the max norm. In experiments, this regularization function produces firstlayer weights concentrated in fewer modes per feature. However, we found that at values of β sufficient to induce the desired mode-wise sparsity patterns, penalizing the maximum also had the undesirable side-effect of causing many of the weights for other modes to saturate at their mode's maximum, suggesting that the features were overly constrained. In some cases, constraining the weights in this manner also caused the algorithm to learn duplicate (or redundant) features, in effect scaling up the feature's contribution to reconstruction to compensate for its constrained maximum. This is obviously an undesirable effect, as it reduces the effective size (or diversity) of the learned feature set.</p><p>This suggests that the max-norm may be overly constraining. A more desirable sparsity function would penalize nonzero weight maxima for each mode for each feature without additional penalty for larger values of these maxima. We can achieve this effect by applying the L 0 norm, which takes a value of 0 for an input of 0, and 1 otherwise, on top of the max-norm from above:</p><formula xml:id="formula_12">f (W ) = K j=1 R r=1 I{(max i S r,i |W i,j |) &gt; 0}<label>(9)</label></formula><p>where I is the indicator function, which takes a value of 1 if its argument is true, 0 otherwise. Again, for a gradientbased method, we used an approximation to the L 0 norm, such as log(1 + x 2 ). This regularization function now encodes <ref type="figure">Fig. 9</ref>: Example objects from the Cornell grasping dataset: <ref type="bibr" target="#b27">[28]</ref>. This dataset contains objects from a large variety of categories.</p><p>a direct penalty on the number of modes used for each weight, without further constraining the weights of modes with nonzero maxima. <ref type="figure" target="#fig_5">Figure 8</ref> shows features learned from the unsupervised stage of our group-regularized deep learning algorithm. We discuss these features, and their implications for robotic grasping, in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We used the extended version of the Cornell grasping dataset for our experiments. This dataset, along with code for this paper, is available at http://pr.cs.cornell.edu/ deepgrasping. We note that this is an updated version of the dataset used in <ref type="bibr" target="#b27">[28]</ref>, containing several more complex objects, and thus results for their algorithms will be different from those in <ref type="bibr" target="#b27">[28]</ref>. This dataset contains 1035 images of 280 graspable objects, several of which are shown in <ref type="figure">Fig. 9</ref>. Each image is annotated with several ground-truth positive and negative grasping rectangles. While the vast majority of possible rectangles for most objects will be non-graspable, the dataset contains roughly equal numbers of graspable and nongraspable rectangles. We will show that this is useful for an unsupervised learning algorithm, as it allows learning a good representation for graspable rectangles even from unlabeled data.</p><p>We performed five-fold cross-validation, and present results for splits on per image (i.e., the training set and the validation set do not share the same image) and per object (i.e., the training set and the validation set do not share any images from the same object) basis. Hyper-parameters were selected by validating performance on a separate set of 300 grasps not used in any of the cross-validation splits.</p><p>We take seven 24x24 pixel channels as described in Section IV as input, giving 4032 input features to each network. We trained a deep network with 200 hidden units each at the first and second layers using our learning algorithm as described in Sections III and V. Training this network took roughly 30 minutes. For trials involving our two-pass system, we trained a second network with 50 hidden units at each layer in the same manner. During inference we performed an exhaustive search using this network, then used the 200-unit network to re-rank the 100 highest-ranked rectangles found by the 50-unit network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>We compare our recognition results in the Cornell grasping dataset with the features from <ref type="bibr" target="#b27">[28]</ref>, as well as the combination of these features and Fast Point Feature Histogram (FPFH) features <ref type="bibr" target="#b50">[51]</ref>. We used a linear SVM for classification, which gave the best results among all other kernels. We also report chance performance, obtained by randomly selecting a label in the recognition case, and randomly assigning scores to rectangles in the detection case.</p><p>We also compare our algorithm to other deep learning approaches. We compare to a network trained only with standard L1 regularization, and a network trained in a manner similar to <ref type="bibr" target="#b42">[43]</ref>, where three separate sets of first layer features are learned for the depth channel, the combination of the Y, U, and V channels, and the combination of the X, Y, and Z surface normal components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Metrics for Detection</head><p>For detection, we compare the top-ranked rectangle for each method with the set of ground-truth rectangles for each image. We present results using two metrics, the "point" and "rectangle" metric.</p><p>For the point metric, similar to Saxena et al. <ref type="bibr" target="#b54">[55]</ref>, we compute the center point of the predicted rectangle, and consider the grasp a success if it is within some distance from at least one ground-truth rectangle center. We note that this metric ignores grasp orientation, and therefore might overestimate the performance of an algorithm for robotic applications.</p><p>For the rectangle metric, similar to Jiang et al. <ref type="bibr" target="#b27">[28]</ref>, let G be the top-ranked grasping rectangle predicted by the algorithm, and G * be a ground-truth rectangle. Any rectangles with an orientation error of more than 30 o from G are rejected. From the remaining set, we use the common bounding box evaluation metric of intersection divided by union -i.e. Area(G ∩ G * )/Area(G ∪ G * ). Since a ground-truth rectangle can define a large space of graspable rectangles (e.g., covering the entire length of a pen), we consider a prediction to be correct if it scores at least 25% by this metric. positive and negative grasping cases. Many of these features show non-zero weights to the depth channel, indicating that it learns the correlation of depths to graspability. We can see that weights to many of the modalities for these features have been eliminated by our structured regularization approach. In particular, many of these features lack weights to the U and V (3 rd and 4 th ) channels, which correspond to color, allowing the system to be more robust to different-colored objects. <ref type="figure">Figure 10</ref> shows 3D meshes for the depth channels of the four features with the strongest positive and negative correlations to valid grasps. Even without any supervised information, our algorithm was able to learn several features which correlate strongly to graspable cases and non-graspable cases. The first two positive-correlated features represent handles, or other cases with a raised region in the center, while the second two represent circular rims or handles. The negativelycorrelated features represent obviously non-graspable cases, such as ridges perpendicular to the gripper plane and "valleys" between the gripper plates. From these features, we can see that even during unsupervised feature learning, our approach is able to learn a representation useful for the task at hand, thanks purely to the fact that the data used is composed of half graspable and half non-graspable cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Learning for Robotic Grasp Detection</head><p>From <ref type="table" target="#tab_0">Table I</ref>, we see that the recognition performance is significantly improved with deep learning methods, improving 9% over the features from <ref type="bibr" target="#b27">[28]</ref> and 4.1% over those features combined with FPFH features. Both L 1 and group regularization performed similarly for recognition, but training separate first layer features decreased performance slightly. This shows that learned features, in addition to avoiding hand-design, are able to improve performance significantly over the state of the art. It demonstrates that a deep network is able to learn the concept of "graspability" in a way that generalizes to new objects it hasn't seen before. <ref type="table" target="#tab_0">Table II</ref> shows that even using any one of the three input modalities (RGB, depth, or surface normals), our algorithm is able to learn features which outperform hand-engineered ones for recognition. Depth gives the highest performance of any single-mode network. Combining depth and normal information improves results over either alone, indicating that they give non-redundant information. The highest accuracy is still obtained by using all the input modalities. This shows that combining depth and color information leads to a system which is more robust than either modality alone. This is due to the fact that some graspable cases (rims of monochromatic objects, etc.) can only be detected using depth information, while in others, the depth channel may be extremely noisy, requiring the use of color information. From this, we can see that integrating multimodal information, a major focus of this work, is important in recognizing good robotic grasps. <ref type="table" target="#tab_0">Table III</ref> shows that the performance gains from deep learning for recognition carry over to detection, as well. Once maskbased scaling has been applied, all deep learning approaches except for training separate first-layer features outperform the hand-engineered features from <ref type="bibr" target="#b27">[28]</ref> by up to 13% for the point metric and 17% for the rectangle metric, while also avoiding the need to design task-specific features. Without mask-based scaling, the system performs poorly, due to the bias illustrated in <ref type="figure" target="#fig_2">Fig. 6</ref>. Separate first-layer features also give weak detection performance, indicating that the relative scores assigned by this form of network are less robust than those learned using our structured regularization approach.</p><p>Using structured multimodal regularization also improves results over standard L 1 regularization by up to 1.8%, showing that our method also learns more robust features than standard approaches which ignore modality information. Even though using the first-pass network alone underperforms the second-  <ref type="figure">Fig. 11</ref>: Visualization of grasping scores for different grippers:</p><p>Red indicates maximum score for a grasp with left gripper plane centered at each point, blue is similar for the right plate. Best-scoring rectangle shown in green/yellow. pass network alone by up to 8.3%, integrating both in our twopass system outperforms the solo second-pass network by up to 2.4%. This shows that the two-pass system improves not only efficiency, but accuracy as well. The performance gains from multimodal regularization and the two-pass system are discussed in detail below.</p><p>Our system outperforms all baseline approaches by all metrics except for the point metric in the object-wise split case. However, we can see that the chance performance is much higher for the point metric than for the rectangle metric. This shows that the point metric can overstate performance, and the rectangle metric is a better indicator of the accuracy of a grasp detection system. Adaptability: One important advantage of our detection system is that we can flexibly specify the constraints of the gripper in our detection system. This is particularly important for a robot like Baxter, where different objects might require different gripper settings to grasp. We can constrain the detectors to handle this. <ref type="figure">Figure 11</ref> shows detection scores for systems constrained based on two different settings of Baxter's gripper, one wide and one thin. The implications of these results for other types of grippers will be discussed in Section IX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multimodal Group Regularization</head><p>Our group regularization term improves detection accuracy over simple L 1 regularization. The improvement is more significant for the object-wise split than for the image-wise split because the group regularization helps the network to avoid overfitting, which will tend to occur more when the learning algorithm is evaluated on unseen objects.   <ref type="figure" target="#fig_0">Figure 12</ref> shows typical cases where a network trained using our group regularization finds a valid grasp, but a network trained with L 1 regularization does not. In these cases, the grasp chosen by the L 1 -regularized network appears valid for some modalities -the depth channel for the sunglasses and nail polish bottle, and the RGB channels for the scissors. However, when all modalities are considered, the grasp is clearly invalid. The group-regularized network does a better job of combining information from all modalities and is more robust to noise and missing data in the depth channel, as seen in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Two-stage Detection System</head><p>Using our two-pass system enhanced both computational performance and accuracy. The number of rectangles the fullsize network needed to evaluate was reduced by roughly a factor of 1000. Meanwhile, detection performance increased by up to 2.4% as compared to a single pass with the largesize network, even though using the small network alone significantly underperforms the larger network. In most cases, the top 100 rectangles from the first pass contained the topranked rectangle from an exhaustive search using the secondstage network, and thus results were unaffected. <ref type="figure" target="#fig_8">Figure 13</ref> shows some cases where the first-stage network pruned away rectangles corresponding to weak grasps which might otherwise be chosen by the second-stage network. In these cases, the grasp chosen by the single-stage system might be feasible for a robotic gripper, but the rectangle chosen by the two-stage system represents a grasp which would clearly be successful.</p><p>The two-stage system also significantly increases the computational efficiency of our detection system. Average inference time for a MATLAB implementation of the deep network was reduced from 24.6s/image for an exhaustive search using the larger network to 13.5s/image using the two-stage system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ROBOTIC EXPERIMENTS</head><p>In order to evaluate the performance of our algorithms in the real world, we ran an extensive series of robotic experiments. To explore the generalizability and effect of the robot on the success rate of our algorithms, we performed experiments on two different robotic platforms, a Baxter Research Robot ("Yogi") and a PR2 ("Kodiak"). Baxter: The first platform used is our Baxter Research Robot, which we call "Yogi." Baxter has two arms with seven degrees of freedom each and a maximum reach of 104 cm, although we used only the left arm for these experiments. The end-effector for this arm is a two-finger parallel gripper. We augmented the gripper tips using rubber bands for additional friction. Baxter's grippers are interchangable, and we used two settings for these experiments -a "wide" setting with an open width of 8 cm and closed width of 4 cm, and a "thin" setting with an open width of 4 cm and a closed width of 0 cm (completely closed, gripper tips touching).</p><p>To detect grasps, we mounted a Kinect sensor to Yogi's head, approximately 1.75 m above the ground. angled downwards at roughly a 75 o angle towards a table in front of it. The Kinect gives RGB-D images at a resolution of 640x480 pixels. We calibrated the transformation between the Kinect's and Yogi's coordinate frames by marking four points corresponding to a set of 3D axes, and obtaining the coordinates of these points in both Kinect's and Yogi's frames.</p><p>All control for Baxter was done by specifying an endeffector position and orientation, and using the inverse kinematics provided with Baxter to determine a set of joint angles for this pose. Baxter's built-in control systems were used to drive the arm to these new joint angles.</p><p>PR2: Our second platform was our PR2 robot, "Kodiak." Similar to Baxter, PR2 has two 7-DoF arms with approximately 1 m reach, and we used only the left for these experiments. PR2's grippers open to a width of 8 cm, and are capable of closing completely from that span, so we did not need to use two settings as with Baxter. We augmented PR2's gripper friction with gaffer tape on the fingertips.</p><p>For the experiments on PR2, we used the Kinect already mounted to Kodiak's head, and used ROS's built-in functionality to obtain 3D locations from that Kinect and transform these to Kodiak's body frame for manipulation. Control was performed using the ee cart stiffness controller <ref type="bibr" target="#b4">[5]</ref> with trajectories provided by our own custom MATLAB code. Experimental Setup: For each experiment, we placed a single object within a 25 cm x 25 cm square on the table, approximately 1.2 m below the mounting point of the Kinect. This square was chosen to be well-contained within each robot's workspace, allowing objects to be reached from most approach vectors. Object positions and orientations were varied between trials, although objects were always placed in configurations in which at least one viable grasp was visible and accessible to the robot.</p><p>When using Baxter, due to the limited stroke (span from open to closed) of its gripper, we pre-selected one of the two gripper settings discussed above for each object. We constrained the search space as illustrated in <ref type="figure">Fig. 11</ref> to find grasps for that particular setting.</p><p>To detect grasps, we first took an RGB-D image from the Kinect with no objects in the scene as a background image. The depth channel of this image was used to segment objects from the scene, and to correct for the slant of the Kinect. Once an object was segmented, we used our algorithm, as described above, to obtain a single best-ranked grasping rectangle.</p><p>The search space for the first-pass network progressed in 15degree increments from 15 to 180 degrees (angles larger than 180 being mirror-images of grasps already tested), searching over 10-pixel increments across the image for the X and Y coordinates of the upper-left corner of the rectangle. For the thin gripper setting, rectangle widths and heights from 10 to 40 pixels in 10-pixel increments were searched, while for the thick setting these ranged from 40 pixels to 100 pixels in 20-pixel increments. In both cases, rectangles taller than they were wide were ignored. Once a single best-scoring grasp was detected, we translated it to a robotic grasp consisting of a grasping point and an approach vector using the rectangle's parameters and the surface normal at the rectangle's center as described above.</p><p>To execute the grasp, we first positioned the gripper at a location 10 cm back from the grasping point along the approach vector. The gripper was oriented to the approach vector, and rotated around it based on the orientation of the detected grasping rectangle.</p><p>Since Baxter's arms are highly compliant, slight imprecisions in end-effector positioning are to be expected -we found that errors of up to 2 cm were typical. Thus, we implemented a visual servoing system using its hand camera, which provides  <ref type="table" target="#tab_0">Can opener  3  100 Kinect  5  100 Colored cereal box  3  100 Plastic whale  4  75 Electric shaver  3  100  Knife  3  100 Wire bundle  3  100 White cereal box  4  50 Plastic elephant  4  100 Umbrella  4  75  Brush  3  100 Mouse  3  100 Cap-shaped bowl  3  100 Plush cat  4  75 Desk lamp  3  100  Tongs  3  100 Hot glue gun  3  67 Coffee mug  3  100 RC controller  3  67 Remote control  5  100  Towel  3</ref>   RGB images at a resolution of 320x200 pixels. We used color segmentation to separate the object from the background, and used its lateral position in image space to drive Yogi's endeffector to center the object. We did not implement visual servoing for PR2 because its gripper positioning was found to be precise to within 0.5 cm. After visual servoing was completed, we drove the gripper 14 cm forwards from its current position along the approach vector, so that the grasping point was well-contained within it. We then closed the gripper, grasping the object, and moved it 30 cm upwards. A grasp was determined to be successful if it was sufficient to lift the object and hold it for one second.</p><p>Objects to be Grasped: For our robotic experiments, we collected a diverse set of 35 objects within a size of .3 m x .3 m x .3 m and weighing at most 2.5 kg (although most were less than 1 kg) from our offices, homes, and lab. Many of them are shown in <ref type="figure" target="#fig_9">Fig. 14.</ref> Most of these objects were not present in the training dataset, and thus were completely new to the grasp detection algorithm.</p><p>Due to the physical limitations of the robots' grippers, we found that five of these objects were not graspable even when given a hand-chosen grasp. The small pair of pliers was too low to the table to grip properly. The spray paint can was too smooth for the gripper to get enough friction to lift it. The weight of the hammer was too imbalanced, causing the hammer to rotate and slip out of the gripper when grasped. Similar problems were encountered with the bicycle U-lock. The bevel spatula's handle was too close to the thinset size of Baxter's gripper, so that we could not position it precisely enough to grasp it reliably. We did not consider these objects for purposes of our experimental results, since our focus was on evaluating the performance of our grasp detection algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>: <ref type="table" target="#tab_0">Table IV</ref> shows the results of our robotic experiments on Baxter for the remaining 30 objects, a total of 100 trials. Using our algorithm, Yogi was able to successfully execute a grasp in 84% of the trials. <ref type="figure" target="#fig_1">Figure 15</ref> shows Yogi executing several of these grasps. In 8% of the trials, our algorithm detected a valid grasp which was not executed correctly by Yogi. Thus, we were able to successfully detect a good grasp in 92% of the trials. Video of some of these trials is available at http://pr.cs.cornell.edu/deepgrasping. PR2 yielded a higher success rate as seen in <ref type="table" target="#tab_5">Table V</ref>, succeeding in 89% of trials. This is largely due to the much wider span of PR2's gripper from open to closed and its ability to fully close from its widest position, as well as PR2's ability to apply a larger gripping force. Some specific instances where PR2 and Baxter's performance differed are discussed below.</p><p>For comparison purposes, we ran a small set of control experiments for 16 of the objects in the dataset. The control algorithm simply returned a fixed-size rectangle centered at the object's center of mass, as determined by depth segmentation from the background. The rectangle was aligned so that the gripper plates ran parallel to the object's principal axis. This algorithm was only successful in 31% of cases, significantly underperforming our system.</p><p>On Baxter, our algorithm sometimes detected a grasp which was not realizable by the current setting of its gripper, but might be executable by others. For example, our algorithm detected grasps across the leg of the plush cat, and the region between the handle and body of the umbrella, both too thin for the wide setting of Baxter's gripper to grasp since it has a minimum span of 4 cm. Since PR2's gripper can close completely from any position, it did not encounter these issues and thus achieved a 100% success rate for both these objects.</p><p>The XBox controller proved to be a very difficult object for either robot to grasp. From a top-down angle, there is only a small space of viable grasps with a span of less than 8 cm, but many which have either a slightly larger span (making them non-realizable by either gripper), or are subtly non-viable (e.g. grasps across the two "handles," which tend to slip off.) All viable grasps are very near to the 8 cm span of both grippers, meaning that even slight imprecision in positioning can lead to failure. Due to this, Baxter achieved a higher success rate for the XBox controller thanks to visual servoing, succeeding in 50% of cases as compared to the 25% success rate for PR2.</p><p>Our algorithm was able to consistently detect and execute valid grasps for a red cereal box, but had some failures on a white and yellow one. This is because the background for all objects in the dataset is white, leading the algorithm to learn features relating white areas at the edges of the gripper region to graspable cases. However, it was able to detect and execute correct grasps for an all-white ice cube tray, and so does not fail for all white objects. This could be remedied by extending the dataset to include cases with different background colors. Interestingly, even though the parameters of grasps detected for the white box were similar for PR2 and Baxter, PR2 was able to succeed in every case while Baxter succeeded only half the time. This is because PR2's increased gripper strength allowed it to execute grasps across corners of the box, crushing it slightly in the process.</p><p>Other failures were due to the limitations of the Kinect sensor. We were never able to properly grasp the martini glass because its glossy finish prevented Kinect from returning any depth estimates for it. Even if a valid grasp were detected using color information only, there was no way to infer a proper grasping position without depth information. Grasps for the metal bookend failed for similar reasons, but it was not as glossy as the martini glass, and gave enough returns for some to succeed.</p><p>However, our algorithm also had many noteworthy successes. It was able to consistently detect and execute grasps for a crumpled cloth towel, a complex and irregular case which bore little resemblance to any object in the dataset. It was also able to find and grasp the rims of objects such as the plastic baseball cap and coffee mug, cases where there is little visual distinction between the rim and body of the object. These objects underscore the importance of the depth channel for robotic grasping, as none of these grasps would be detectable without depth information.</p><p>Our algorithm was also able to successfully detect and execute many grasps for which the approach vector was nonvertical. The grasps shown for the coffee mug, desk lamp, cereal box, RC car controller, and toy elephant shown in <ref type="figure" target="#fig_1">Fig. 15</ref> were all executed by aligning the gripper to such an approach vector. Indeed, many of these grasps may have failed had the gripper been aligned vertically. This shows that our algorithm is not restricted to detecting top-down grasps, but rather encodes a more general notion of graspability which can be applied to grasps from many angles, albeit within the constraints of visibility from a single-view perspective. While a few failures occurred, our algorithm still achieved a high rate of accuracy for other oddly-shaped objects such as the quad-rotor casing, RC car controller, and glue gun. For objects with clearly defined handles, such as the cheese grater, kitchen tongs, can opener, and knife, our algorithm was able to detect and execute successful grasps in every trial, showing that there is a wide range of objects which it can grasp extremely consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. DISCUSSION AND FUTURE WORK</head><p>Our algorithm focuses on the problem of grasp detection for a two-fingered parallel-plate style gripper. It would be directly applicable to other grippers with fixed configurations, simply requiring new training data labeled with grasps for the gripper in question. Our system would allow even the basic features used for grasp detection to adapt to the gripper. This might be useful in cases such as jamming grippers <ref type="bibr" target="#b28">[29]</ref>, or two-fingered grippers with differently-shaped contact surfaces, which might require different features to determine a graspable area.</p><p>Our detection algorithm does not directly address the problem of 3D orientation of the gripper -this orientation is determined only after an optimal rectangle has been detected, orienting the grasp based on the object's surface normals. However, just as our approach here considers aligns a 2D feature window to the gripper, an extension of this work might align a 3D window -using voxels, rather than pixels, as its basic unit of representation for input features to the network. This would allow the system to search across the full 6-DoF 3D pose of the gripper, while still leveraging the power of feature learning.</p><p>Our system gives only a gripper pose as output, but multifingered reconfigurable hands also require a configuration of the fingers in order to grasp an object. In this case, our algorithm could be used as a heuristic to find one or more locations likely to be graspable (similar to the first pass in our two-pass system), greatly reducing the search space needed to find an optimal gripper configuration.</p><p>Our algorithm also depends only on local features to determine grasping locations. However, many household objects may have some areas which are strongly preferable to grasp over others -for example, a knife might be graspable by the blade, or a hot glue gun by the barrel, but both should actually be grasped by their respective handles. Since these regions are more likely to be labeled as graspable in the data, our system already weakly encodes this, but some may not be readily distinguishable using only local information.</p><p>Adding a term modeling the probability of each region of the image being a semantically-appropriate area to grasp the object would allow us to incorporate this information. This term could be computed once for the entire image, then added to each local detection score, keeping detection efficient.</p><p>In this work, our visual-servoing algorithm was purely heuristic, simply attempting to center the segmented object underneath the hand camera. However, in future work, a similar feature-learning approach might be applied to hand camera images of graspable and non-graspable regions, improving the visual servoing system's ability to fine-tune gripper position to ensure a good grasp.</p><p>Many robotics problems require the use of perceptual information, but can be difficult and time-consuming to engineer good features for, particularly when using RGB-D data. In future work, our approach could be extended to a wide range of such problems. Our system could easily be applied to other detection problems such as object detection or obstacle detection. However, it could also be adapted to other similar problems, such as object tracking and visual servoing.</p><p>Multimodal data has become extremely important for robotics, due both to the advent of new sensors such as the Kinect and the application of robots to more challenging tasks which require multiple modalities of information to perform well. However, it can be very difficult to design features which do a good job of integrating many modalities. While our work focuses on color, depth, and surface normals as input modes, our structured multimodal regularization algorithm might also be applied to others. This approach could improve performance while allowing roboticists to focus on other engineering challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSIONS</head><p>We presented a system for detecting robotic grasps from RGB-D data using a deep learning approach. Our method has several advantages over current state-of-the-art methods. First, using deep learning allows us to avoid hand-engineering features, learning them instead. Second, our results show that deep learning methods significantly outperform even welldesigned hand-engineered features from previous work.</p><p>We also presented a novel feature learning algorithm for multimodal data based on group regularization. In extensive experiments, we demonstrated that this algorithm produces better features for robotic grasp detection than existing deep learning approaches to multimodal data. Our experiments and results, both offline and on real robotic platforms, show that our two-stage deep learning system with group regularization is capable of robustly detecting grasps for a wide range of objects, even those previously unseen by the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Detecting and executing grasps: From left to right: Our system obtains an RGB-D image from a Kinect mounted on the robot, and searches over a large space of possible grasps, for which some candidates are shown. For each of these, it extracts a set of raw features corresponding to the color and depth images and surface normals, then uses these as inputs to a deep network which scores each rectangle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Preserving aspect ratio: Left: a pair of sunglasses with a potential grasping rectangle. Red edges indicate gripper plates. Center: image taken from the rectangle and rescaled to fit a square aspect ratio. Right: same image, padded and centered in the receptive field. Blue areas indicate masked-out padding. When rescaled, the rectangle incorrectly appears graspable. Preserving aspect ratio and padding allows the rectangle to correctly appear non-graspable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Improvement from mask-based scaling: Left: Result without mask-based scaling. Right: Result with mask-based scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Three possible models for multimodal deep learning: Left: fully dense model-all visible features are concatenated and modality information is ignored. Middle: modality-specific sparse model -separate first layer features are trained for each modality. Right: group-sparse model-a structured regularization term encourages features to use only a subset of the input modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Features corresponding to positive grasps.(b) Features corresponding to negative grasps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Features learned from grasping data: Each feature contains seven channels -from left to right, depth, Y, U, and V image channels, and X, Y, and Z surface normal components. Vertical edges correspond to gripper plates. Left: eight features with the strong positive correlations to rectangle graspability. Right: similar, but negative correlations. Group regularization eliminates many modalities from many of these features, making them more robust.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 Fig. 10 :</head><label>810</label><figDesc>shows the features learned by the unsupervised phase of our algorithm which have a high correlation to Positive Negative Learned 3D depth features: 3D meshes for depth channels of the four features with strongest positive (top) and negative(bottom) correlations to rectangle graspability. Here X and Y coordinates corresponds to positions in the deep network's receptive field, and Z coordinates corresponds to weight values to the depth channel for each location. Feature shapes clearly correspond to graspable and nongraspable structures, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 :</head><label>12</label><figDesc>Improvements from group regularization: Cases where our group regularization approach produces a viable grasp (shown in green and yellow), while a network trained only with simple L1 regularization does not (shown in blue and red). Top: RGB image, bottom: depth channel. Green and blue edges correspond to gripper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 :</head><label>13</label><figDesc>Improvements from two-stage system: Example cases where the two-stage system produces a viable grasp (shown in green and yellow), while the single-stage system does not (shown in blue and red). Top: RGB image, bottom: depth channel. Green and blue edges correspond to gripper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 :</head><label>14</label><figDesc>Robotic experiment objects: Several of the objects used in experiments, including challenging cases such as an oddly-shaped RC car controller, a cloth towel, plush cat, and white ice cube tray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Recognition results for Cornell grasping dataset.</figDesc><table><row><cell>Algorithm</cell><cell>Accuracy (%)</cell></row><row><cell>Chance</cell><cell>50</cell></row><row><cell>Jiang et al. [28]</cell><cell>84.7</cell></row><row><cell>Jiang et al. [28] + FPFH</cell><cell>89.6</cell></row><row><cell>Sparse AE, separate layer-1 feat.</cell><cell>92.8</cell></row><row><cell>Sparse AE</cell><cell>93.7</cell></row><row><cell>Sparse AE, group reg.</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Recognition results for different modalities, for a deep network pre-trained using SAE.</figDesc><table><row><cell>Modes</cell><cell>Accuracy (%)</cell></row><row><cell>Chance</cell><cell>50</cell></row><row><cell>RGB</cell><cell>90.3</cell></row><row><cell>Depth</cell><cell>92.4</cell></row><row><cell>Surf. Normals</cell><cell>90.3</cell></row><row><cell>Depth + Surf. Normals</cell><cell>92.8</cell></row><row><cell>RGB + Depth + Surf. Normals</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Detection results for point and rectangle metrics, for various learning algorithms.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Image-wise split Point Rect</cell><cell cols="2">Object-wise split Point Rect</cell></row><row><cell>Chance</cell><cell>35.9</cell><cell>6.7</cell><cell>35.9</cell><cell>6.7</cell></row><row><cell>Jiang et al. [28]</cell><cell>75.3</cell><cell>60.5</cell><cell>74.9</cell><cell>58.3</cell></row><row><cell>SAE, no mask-based scaling</cell><cell>62.1</cell><cell>39.9</cell><cell>56.2</cell><cell>35.4</cell></row><row><cell>SAE, separate layer-1 feat.</cell><cell>70.3</cell><cell>43.3</cell><cell>70.7</cell><cell>40.0</cell></row><row><cell>SAE, L 1 reg.</cell><cell>87.2</cell><cell>72.9</cell><cell>88.7</cell><cell>71.4</cell></row><row><cell>SAE, struct. reg., 1 st pass only</cell><cell>86.4</cell><cell>70.6</cell><cell>85.2</cell><cell>64.9</cell></row><row><cell>SAE, struct. reg., 2 nd pass only</cell><cell>87.5</cell><cell>73.8</cell><cell>87.6</cell><cell>73.2</cell></row><row><cell>SAE, struct. reg. two-stage</cell><cell>88.4</cell><cell>73.9</cell><cell>88.1</cell><cell>75.6</cell></row><row><cell>Wide gripper</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Thin gripper</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0 degrees</cell><cell cols="2">45 degrees</cell><cell></cell><cell>90 degrees</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Results for robotic experiments for Baxter, sorted by object category, for a total of 100 trials. Tr. indicates number of trials, Acc. indicates accuracy (in terms of success percentage.)</figDesc><table><row><cell cols="2">Kitchen tools</cell><cell>Lab tools</cell><cell>Containers</cell><cell>Toys</cell><cell>Others</cell></row><row><cell>Object</cell><cell>Tr. Acc. Object</cell><cell>Tr. Acc. Object</cell><cell>Tr. Acc. Object</cell><cell>Tr. Acc. Object</cell><cell>Tr. Acc.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Results for robotic experiments for PR2, sorted by object category, for a total of 100 trials. Tr. indicates number of trials, Acc. indicates accuracy (in terms of success percentage.)</figDesc><table><row><cell cols="2">Kitchen tools</cell><cell cols="2">Lab tools</cell><cell>Containers</cell><cell></cell><cell>Toys</cell><cell></cell><cell>Others</cell><cell></cell><cell></cell></row><row><cell>Object</cell><cell cols="2">Tr. Acc. Object</cell><cell cols="2">Tr. Acc. Object</cell><cell cols="2">Tr. Acc. Object</cell><cell cols="2">Tr. Acc. Object</cell><cell cols="2">Tr. Acc.</cell></row><row><cell>Can opener</cell><cell>3</cell><cell>100 Kinect</cell><cell>5</cell><cell>100 Colored cereal box</cell><cell>3</cell><cell>100 Plastic whale</cell><cell>4</cell><cell>75 Electric shaver</cell><cell>3</cell><cell>100</cell></row><row><cell>Knife</cell><cell>3</cell><cell>100 Wire bundle</cell><cell>3</cell><cell>100 White cereal box</cell><cell>4</cell><cell>100 Plastic elephant</cell><cell>4</cell><cell>100 Umbrella</cell><cell>4</cell><cell>100</cell></row><row><cell>Brush</cell><cell>3</cell><cell>100 Mouse</cell><cell>3</cell><cell>100 Cap-shaped bowl</cell><cell>3</cell><cell>100 Plush cat</cell><cell>4</cell><cell>100 Desk lamp</cell><cell>3</cell><cell>100</cell></row><row><cell>Tongs</cell><cell>3</cell><cell>100 Hot glue gun</cell><cell>3</cell><cell>67 Coffee mug</cell><cell>3</cell><cell>100 RC controller</cell><cell>3</cell><cell>67 Remote control</cell><cell>5</cell><cell>100</cell></row><row><cell>Towel</cell><cell>3</cell><cell>100 Quad-rotor</cell><cell>4</cell><cell>100 Ice cube tray</cell><cell>3</cell><cell>100 XBox controller</cell><cell>4</cell><cell>25 Metal bookend</cell><cell>3</cell><cell>67</cell></row><row><cell>Grater</cell><cell>3</cell><cell>100 Duct tape roll</cell><cell>4</cell><cell>100 Martini glass</cell><cell>3</cell><cell>0 Plastic frog</cell><cell>3</cell><cell>67 Glove</cell><cell>3</cell><cell>100</cell></row><row><cell>Average</cell><cell></cell><cell>100 Average</cell><cell></cell><cell>95 Average</cell><cell></cell><cell>83 Average</cell><cell></cell><cell>72 Average</cell><cell></cell><cell>95</cell></row><row><cell>Overall</cell><cell></cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Fig. 15: Robots executing grasps: Our robots grasping several objects from the experimental dataset. Top row: Baxter grasping a quad-rotor casing, coffee mug, ice cube tray, knife, and electric shaver. Middle row: Baxter grasping a desk lamp, cheese grater, umbrella, cloth towel, and hot glue gun. Bottom row: PR2 grasping a plush cat, RC car controller, cereal box, toy elephant, and glove.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Parts of this work were presented at ICLR 2013 as a workshop paper, and at RSS 2013 as a conference paper. This version includes significantly extended related work, algorithmic descriptions, and extensive robotic experiments which were not present in previous versions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Yun Jiang and Marcus Lim for useful discussions and help with baseline experiments. This research was funded in part by ARO award W911NF-12-1-0267, Microsoft Faculty Fellowship and NSF CAREER Award (Saxena), and Google Faculty Research Award (Lee).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robotic grasping and contact: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bicchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning for RGB-D Based Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISER</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Datadriven grasp synthesis -a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bakebot: Baking cookies with the pr2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bollini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS PR2 Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Manipulation of unmodeled objects using intelligent grasping schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lumia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Fuzzy Sys</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic parsing for priming object detection in rgb-d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA Workshop on Semantic Perception, Mapping and Exploration</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text detection and character recognition in scene images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object recognition and full pose registration from a single image for robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The moped framework: Object recognition and pose estimation for manipulation. IJRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Martinez</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning objectspecific grasp affordance densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Detry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baseski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Touati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Physics-based grasp planning through clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dogar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciocarlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning and evaluation of the approach vector for automatic grasp generation and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ekvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d mapping with an RGB-D camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Planning optimal grasps. ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global optimization of robotic grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Gallegos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Porta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Columbia grasp database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goldfeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciocarlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring invariances in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selection of Robot Pre-Grasps using Box-Based Shape Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topographic independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Inki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1558" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Principal Component Analysis and Whitening, chapter 6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<biblScope unit="page" from="125" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A dirty model for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic models of object geometry for grasp planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient grasping from RGBD images: Learning using a new rectangle representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moseson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning hardware agnostic grasps for a universal jamming gripper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Amend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to place new objects in a scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to grasp using visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Flash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Robust visual servoing. IJRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view rgb-d object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mechanics of form closure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakshminarayana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>ASME</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to grasp objects with multiple contact points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cloth grasp point detection based on multipleview geometric cues with application to robotic towel folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maitin-Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cusumano-Towner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visionbased computation of three-finger grasps on unknown planar objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Sanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Del Pobil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Multimodal deep learning. In ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Constructing stable force-closure grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Fall joint computer conf</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1197" to="1215" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rigid 3d geometry matching for grasping of known objects in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papazov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haddadin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="538" to="553" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning visual features to predict hand orientations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Piater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grasp moduli spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Pokorny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">On computing twofinger force-closure grasps of curved 2D objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Faverjon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">IJRR</biblScope>
			<biblScope unit="page">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From caging to grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast 3D recognition and pose using the viewpoint feature histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An overview of 3d object grasp synthesis algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sahbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>El-Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bidaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auton. Syst</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="336" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robotic grasping of novel objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robotic grasping of novel objects using vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning grasp strategies with partial shape information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robot grasp synthesis algorithms: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Shimoga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="266" />
			<date type="published" when="1996-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3D object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse, distributed, convolutional feature representations for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Direct 3d servoing using dense depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pose error robust grasping from contact wrench space metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Robust real-time visual odometry for dense RGB-D mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Johannsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Grasp evaluation with graspable feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciocarlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS Workshop on Mobile Manipulation</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
