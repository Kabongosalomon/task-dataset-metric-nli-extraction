<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2016 GATED GRAPH SEQUENCE NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
							<email>yujiali@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
							<email>zemel@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
							<email>dtarlow@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2016 GATED GRAPH SEQUENCE NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks <ref type="bibr" target="#b24">(Scarselli et al., 2009)</ref>, which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.</p><p>Published as a conference paper at ICLR 2016 to learn features on the graph that encode the partial output sequence that has already been produced (e.g., the path so far if outputting a path) and that still needs to be produced (e.g., the remaining path). We will show how the GNN framework can be adapted to these settings, leading to a novel graph-based neural network model that we call Gated Graph Sequence Neural Networks (GGS-NNs).</p><p>We illustrate aspects of this general model in experiments on bAbI tasks  and graph algorithm learning tasks that illustrate the capabilities of the model. We then present an application to the verification of computer programs. When attempting to prove properties such as memory safety (i.e., that there are no null pointer dereferences in a program), a core problem is to find mathematical descriptions of the data structures used in a program. Following <ref type="bibr" target="#b3">Brockschmidt et al. (2015)</ref>, we have phrased this as a machine learning problem where we will learn to map from a set of input graphs, representing the state of memory, to a logical description of the data structures that have been instantiated. Whereas <ref type="bibr" target="#b3">Brockschmidt et al. (2015)</ref> relied on a large amount of hand-engineering of features, we show that the system can be replaced with a GGS-NN at no cost in accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many practical applications build on graph-structured data, and thus we often want to perform machine learning tasks that take graphs as inputs. Standard approaches to the problem include engineering custom features of an input graph, graph kernels <ref type="bibr" target="#b14">(Kashima et al., 2003;</ref><ref type="bibr" target="#b25">Shervashidze et al., 2011)</ref>, and methods that define graph features in terms of random walks on graphs <ref type="bibr" target="#b20">(Perozzi et al., 2014)</ref>. More closely related to our goal in this work are methods that learn features on graphs, including Graph Neural Networks <ref type="bibr" target="#b10">(Gori et al., 2005;</ref><ref type="bibr" target="#b24">Scarselli et al., 2009)</ref>, spectral networks <ref type="bibr" target="#b4">(Bruna et al., 2013)</ref> and recent work on learning graph fingerprints for classification tasks on graph representations of chemical molecules <ref type="bibr" target="#b8">(Duvenaud et al., 2015)</ref>.</p><p>Our main contribution is an extension of Graph Neural Networks that outputs sequences. Previous work on feature learning for graph-structured inputs has focused on models that produce single outputs such as graph-level classifications, but many problems with graph inputs require outputting sequences. Examples include paths on a graph, enumerations of graph nodes with desirable properties, or sequences of global classifications mixed with, for example, a start and end node. We are not aware of existing graph feature learning work suitable for this problem. Our motivating application comes from program verification and requires outputting logical formulas, which we formulate as a sequential output problem. A secondary contribution is highlighting that Graph Neural Networks (and further extensions we develop here) are a broadly useful class of neural network model that is applicable to many problems currently facing the field.</p><p>There are two settings for feature learning on graphs: (1) learning a representation of the input graph, and (2) learning representations of the internal state during the process of producing a sequence of outputs. Here, (1) is mostly achieved by previous work on Graph Neural Networks <ref type="bibr" target="#b24">(Scarselli et al., 2009)</ref>; we make several minor adaptations of this framework, including changing it to use modern practices around Recurrent Neural Networks. (2) is important because we desire outputs from graphstructured problems that are not solely individual classifications. In these cases, the challenge is how GNNs map graphs to outputs via two steps. First, there is a propagation step that computes node representations for each node; second, an output model o v = g(h v , l v ) maps from node representations and corresponding labels to an output o v for each v ∈ V. In the notation for g, we leave the dependence on parameters implicit, and we will continue to do this throughout. The system is differentiable from end-to-end, so all parameters are learned jointly using gradient-based optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PROPAGATION MODEL</head><p>Here, an iterative procedure propagates node representations. Initial node representations h <ref type="bibr">(1)</ref> v are set to arbitrary values, then each node representation is updated following the recurrence below until convergence, where t denotes the timestep:</p><formula xml:id="formula_0">h (t) v = f * (l v , l CO(v) , l NBR(v) , h (t−1)</formula><p>NBR(v) ). Several variants are discussed in <ref type="bibr" target="#b24">Scarselli et al. (2009)</ref> including positional graph forms, node-specific updates, and alternative representations of neighborhoods. Concretely, <ref type="bibr" target="#b24">Scarselli et al. (2009)</ref> suggest decomposing f * (·) to be a sum of per-edge terms:</p><formula xml:id="formula_1">f * (l v , l CO(v) , l NBR(v) , h (t) NBR(v) ) = v ∈IN(v) f (l v , l (v ,v) , l v , h (t−1) v ) + v ∈OUT(v) f (l v , l (v,v ) , l v , h (t−1) v ),</formula><p>where f (·) is either a linear function of h v or a neural network. The parameters of f depends on the configuration of labels, e.g. in the following linear case, A and b are learnable parameters, f (l v , l <ref type="bibr">(v ,v)</ref> , l v , h (t) v ) = A <ref type="bibr">(lv,l (v ,v)</ref> ,l v ) h (t−1) v + b <ref type="bibr">(lv,l (v ,v)</ref> ,l v ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">OUTPUT MODEL AND LEARNING</head><p>The output model is defined per node and is a differentiable function g(h v , l v ) that maps to an output. This is generally a linear or neural network mapping. <ref type="bibr" target="#b24">Scarselli et al. (2009)</ref> focus on outputs that are independent per node, which are implemented by mapping the final node representations h <ref type="bibr">(T )</ref> v , to an output o v = g(h <ref type="bibr">(T )</ref> v , l v ) for each node v ∈ V. To handle graph-level classifications, they suggest to create a dummy "super node" that is connected to all other nodes by a special type of edge. Thus, graph-level regression or classification can be handled in the same manner as node-level regression or classification.</p><p>Learning is done via the Almeida-Pineda algorithm <ref type="bibr" target="#b0">(Almeida, 1990;</ref><ref type="bibr" target="#b21">Pineda, 1987)</ref>, which works by running the propagation to convergence, and then computing gradients based upon the converged solution. This has the advantage of not needing to store intermediate states in order to compute gradients. The disadvantage is that parameters must be constrained so that the propagation step is a contraction map. This is needed to ensure convergence, but it may limit the expressivity of the model. When f (·) is a neural network, this is encouraged using a penalty term on the 1-norm of the network's Jacobian. See Appendix A for an example that gives the intuition that contraction maps have trouble propagating information across a long range in a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GATED GRAPH NEURAL NETWORKS</head><p>We now describe Gated Graph Neural Networks (GG-NNs), our adaptation of GNNs that is suitable for non-sequential outputs. We will describe sequential outputs in the next section. The biggest modification of GNNs is that we use Gated Recurrent Units  and unroll the recurrence for a fixed number of steps T and use backpropagation through time in order to compute gradients. This requires more memory than the Almeida-Pineda algorithm, but it removes the need to constrain parameters to ensure convergence. We also extend the underlying representations and output model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NODE ANNOTATIONS</head><p>In GNNs, there is no point in initializing node representations because the contraction map constraint ensures that the fixed point is independent of the initializations. This is no longer the case with GG-NNs, which lets us incorporate node labels as additional inputs. To distinguish these node labels used as inputs from the ones introduced before, we call them node annotations, and use vector x to denote these annotations.</p><p>To illustrate how the node annotations are used, consider an example task of training a graph neural network to predict whether node t can be reached from node s on a given graph. For this task, there are two problem-related special nodes, s and t. To mark these nodes as special, we give them an initial annotation. The first node s gets the annotation x s = [1, 0] , and the second node t gets the annotation x t = [0, 1] . All other nodes v have their initial annotation set to x v = [0, 0] . Intuitively, this marks s as the first input argument and t as the second input argument. We then initialize the node state vectors h (1) v using these label vectors by copying x v into the first dimensions and padding with extra 0's to allow hidden states that are larger than the annotation size.</p><p>In the reachability example, it is easy for the propagation model to learn to propagate the node annotation for s to all nodes reachable from s, for example by setting the propagation matrix associated with forward edges to have a 1 in position (0,0). This will cause the first dimension of node representation to be copied along forward edges. With this setting of parameters, the propagation step will cause all nodes reachable from s to have their first bit of node representation set to 1. The output step classifier can then easily tell whether node t is reachable from s by looking whether some node has nonzero entries in the first two dimensions of its representation vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PROPAGATION MODEL</head><p>The basic recurrence of the propagation model is The matrix A ∈ R D|V|×2D|V| determines how nodes in the graph communicate with each other. The sparsity structure and parameter tying in A is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The sparsity structure corresponds to the edges of the graph, and the parameters in each submatrix are determined by the edge type and direction. A v: ∈ R D|V|×2D are the two columns of blocks in A (out) and A (in) corresponding to node v. Eq. 1 is the initialization step, which copies node annotations into the first components of the hidden state and pads the rest with zeros. Eq. 2 is the step that passes information between different nodes of the graph via incoming and outgoing edges with parameters dependent on the edge type and direction. a (t) v ∈ R 2D contains activations from edges in both directions. The remaining are GRU-like updates that incorporate information from the other nodes and from the previous timestep to update each node's hidden state. z and r are the update and reset gates, σ(x) = 1/(1 + e −x ) is the logistic sigmoid function, and is element-wise multiplication. We initially experimented with a vanilla recurrent neural network-style update, but in preliminary experiments we found this GRU-like propagation step to be more effective.</p><formula xml:id="formula_2">h (1) v = [x v , 0]<label>(1)</label></formula><formula xml:id="formula_3">a (t) v = A v: h (t−1) 1 . . . h (t−1) |V| + b (2) z t v = σ W z a (t) v + U z h (t−1) v<label>(3)</label></formula><formula xml:id="formula_4">r t v = σ W r a (t) v + U r h (t−1) v (4) h (t) v = tanh Wa (t) v + U r t v h (t−1) v (5) h (t) v = (1 − z t v ) h (t−1) v + z t v h (t) v . (6) 3 4 1 2 h (t 1) 2 h (t 1) 1 h (t 1) 3 h (t 1) 4 h (t) 4 h (t) 3 h (t) 2 h (t) 1 B C C B 1 2 3 4 1 2 3 4 Outgoing Edges B' C' C' B' 1 2 3 4 Incoming Edges | {z } | {z } (a) (b) (c) A = A (out) , A (in)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OUTPUT MODELS</head><p>There are several types of one-step outputs that we would like to produce in different situations. First, GG-NNs support node selection tasks by making o v = g(h (T ) v , x v ) for each node v ∈ V output node scores and applying a softmax over node scores. Second, for graph-level outputs, we define a graph level representation vector as</p><formula xml:id="formula_5">h G = tanh v∈V σ i(h (T ) v , x v ) tanh j(h (T ) v , x v ) ,<label>(7)</label></formula><p>where σ(i(h</p><p>v , x v )) acts as a soft attention mechanism that decides which nodes are relevant to the current graph-level task. i and j are neural networks that take the concatenation of h (T ) v and x v as input and outputs real-valued vectors. The tanh functions can also be replaced with the identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GATED GRAPH SEQUENCE NEURAL NETWORKS</head><p>Here we describe Gated Graph Sequence Neural Networks (GGS-NNs), in which several GG-NNs operate in sequence to produce an output sequence o (1) . . . o <ref type="bibr">(K)</ref> .</p><p>For the k th output step, we denote the matrix of node annotations as X</p><formula xml:id="formula_7">(k) = [x (k) 1 ; . . . ; x (k) |V| ] ∈ R |V|×L V . We use two GG-NNs F (k) o and F (k) X : F (k) o</formula><p>for predicting o (k) from X (k) , and F (k) X for predicting X (k+1) from X (k) . X (k+1) can be seen as the states carried over from step k to k + 1.</p><formula xml:id="formula_8">Both F (k) o and F (k)</formula><p>X contain a propagation model and an output model. In the propagation models, we denote the matrix of node vectors at the t th propagation step of the k th output step as H <ref type="bibr">(k,t)</ref> </p><formula xml:id="formula_9">= [h (k,t) 1 ; . . . ; h (k,t)</formula><p>|V| ] ∈ R |V|×D . As before, in step k, we set H (k,1) by 0-extending X (k) per node. An overview of the model is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Alternatively, F X can share a single propagation model, and just have separate output models. This simpler variant is faster to train and evaluate, and X are different, this variant may not work as well. We introduce a node annotation output model for predicting X (k+1) from H (k,T ) . The prediction is done for each node independently using a neural network j(h</p><formula xml:id="formula_10">X (1) H (1,1) o (1) X (2) H (2,1) o (2) X (3) . . . Init F ( 1 ) o F (1) X Init F ( 2 ) o F (2) X</formula><formula xml:id="formula_11">(k,T ) v , x (k) v ) that takes the concatenation of h (k,T ) v and x (k)</formula><p>v as input and outputs a vector of real-valued scores:</p><formula xml:id="formula_12">x (k+1) v = σ j(h (k,T ) v , x (k) v ) .<label>(8)</label></formula><p>There are two settings for training GGS-NNs: specifying all intermediate annotations X <ref type="bibr">(k)</ref> , or training the full model end-to-end given only X (1) , graphs and target sequences. The former can improve performance when we have domain knowledge about specific intermediate information that should be represented in the internal state of nodes, while the latter is more general. We describe both.</p><p>Sequence outputs with observed annotations Consider the task of making a sequence of predictions for a graph, where each prediction is only about a part of the graph. In order to ensure we predict an output for each part of the graph exactly once, it suffices to have one bit per node, indicating whether the node has been "explained" so far. In some settings, a small number of annotations are sufficient to capture the state of the output procedure. When this is the case, we may want to directly input this information into the model via labels indicating target intermediate annotations. In some cases, these annotations may be sufficient, in that we can define a model where the GG-NNs are rendered conditionally independent given the annotations.</p><p>In this case, at training time, given the annotations X (k) the sequence prediction task decomposes into single step prediction tasks and can be trained as separate GG-NNs. At test time, predicted annotations from one step will be used as input to the next step. This is analogous to training directed graphical models when data is fully observed.</p><p>Sequence outputs with latent annotations More generally, when intermediate node annotations X (k) are not available during training, we treat them as hidden units in the network, and train the whole model jointly by backpropagating through the whole sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPLANATORY APPLICATIONS</head><p>In this section we present example applications that concretely illustrate the use of GGS-NNs. We focus on a selection of bAbI artificial intelligence (AI) tasks  and two graph algorithm learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BABI TASKS</head><p>The bAbI tasks are meant to test reasoning capabilities that AI systems should be capable of. In the bAbI suite, there are 20 tasks that test basic forms of reasoning like deduction, induction, counting, and path-finding.</p><p>We have defined a basic transformation procedure that maps bAbI tasks to GG-NNs or GGS-NNs. We use the --symbolic option from the released bAbI code to get stories that just involve sequences of relations between entities, which are then converted into a graph. Each entity is mapped to a node, and each relation is mapped to an edge with edge label given by the relation. The full story is consumed and mapped to a single graph. Questions are marked by eval in the data and are comprised of a question type (e.g., has fear), and some argument (e.g., one or more nodes). The arguments are converted into initial node annotations, with the i-th bit of the i-th argument node's annotation vector set to 1. For example, if the eval line is eval E &gt; A true, then E gets initial annotation x</p><p>(1)</p><formula xml:id="formula_13">E = [1, 0] , A gets x (1) A = [0, 1]</formula><p>, and for all other nodes v, x</p><p>(1) v = [0, 0] . Question type is 1 (for '&gt;') and output is class 1 (for 'true'). Some tasks have multiple question types, for example Task 4 which has 4 question types: e, s, w, n. For such tasks we simply train a separate GG-NN for each task. We do not use the strong supervision labels or give the GGS-NNs any intermediate annotations in any experiments.</p><p>While simple, this transformation does not preserve all information about the story (e.g., it discards temporal order of the inputs), and it does not easily handle ternary and higher order relations (e.g., Yesterday John went to the garden is not easily mapped to a simple edge). We also emphasize that it is a non-trivial task to map general natural language to symbolic form, 1 so we could not directly apply this approach to arbitrary natural language. Relaxing these restrictions is left for future work.</p><p>However, even with this simple transformation, there are a variety of bAbI tasks that can be formulated, including Task 19 (Path Finding), which is arguably the hardest task. We provide baselines to show that the symbolic representation does not help RNNs or LSTMs significantly, and show that GGS-NNs solve the problem with a small number of training instances. We also develop two new bAbI-like tasks that involve outputting sequences on graphs: shortest paths, and a simple form of Eulerian circuits (on random connected 2-regular graphs). The point of these experiments is to illustrate the capabilities of GGS-NNs across a variety of problems.</p><p>Example 1. As an example, below is an instance from the symbolic dataset for bAbI task 15, Basic Deduction.</p><formula xml:id="formula_14">D is A B is E A has_fear F G is F E has_fear H F has_fear A H has_fear A C is H eval B has_fear H eval G has_fear A eval C has_fear A eval D has_fear F</formula><p>Here the first 8 lines describe the facts, the GG-NN will use these facts to build a graph. Capital letters are nodes, is and has fear are interpreted as edge labels or edge types. The last 4 lines are 4 questions asked for this input data. has fear in these lines are interpreted as a question type. For this task, in each question only one node is special, e.g. the B in eval B has fear, and we assign a single value 1 to the annotation vector for this special node and 0 to all the other nodes.</p><p>For RNN and LSTM the data is converted into token sequences like below:</p><p>n6 e1 n1 eol n6 e1 n5 eol n1 e1 n2 eol n4 e1 n5 eol n3 e1 n4 eol n3 e1 n5 eol n6 e1 n4 eol q1 n6 n2 ans 1</p><p>where n&lt;id&gt; are nodes, e&lt;id&gt; are edges, q&lt;id&gt; are question types, extra tokens eol (end-ofline) and ans (answer) are added to give the RNN &amp; LSTM access to the complete information available in the dataset. The final number is the class label.</p><p>Example 2. As a second example, below is an instance from the symbolic dataset for bAbI task 19, Path Finding. for all bAbI tasks that contain more than one questions in one example, the predictions for different questions were evaluated independently. As there is randomness in the dataset generation process, we generated 10 such datasets for each task, and report the mean and standard deviation of the evaluation performance across the 10 datasets.  X share a single propagation model. For shortest path and Eulerian circuit tasks, we used D = 20. All models are trained long enough with Adam <ref type="bibr" target="#b15">(Kingma &amp; Ba, 2014)</ref>, and the validation set is used to choose the best model to evaluate and avoid models that are overfitting.</p><formula xml:id="formula_15">E s A B n C E w F B w E eval path B A w,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">SINGLE STEP OUTPUTS</head><p>We choose four bAbI tasks that are suited to the restrictions described above and require single step outputs: 4 (Two Argument Relations), 15 (Basic Deduction), 16 (Basic Induction), and 18 (Size Reasoning). For Task 4, 15 and 16, a node selection GG-NN is used. For Task 18 we used a graphlevel classification version. All the GGNN networks contain less than 600 parameters 2 .</p><p>As baselines, we train RNN and LSTM models on the symbolic data in raw sequence form. The RNNs and LSTMs use 50 dimensional embeddings and 50 dimensional hidden layers; they predict a single output at the end of the sequences and the output is treated as a classification problem, the loss is cross entropy. The RNNs and LSTMs contain around 5k and 30k parameters, respectively. In <ref type="table">Table 2</ref>, we further break down performance of the baselines for task 4 as the amount of training data varies. While both the RNN and LSTM are able to solve the task almost perfectly, the GG-NN reaches 100% accuracy with much less data.  <ref type="table">Table 2</ref>: Performance breakdown of RNN and LSTM on bAbI task 4 as the amount of training data changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">SEQUENTIAL OUTPUTS</head><p>The bAbI Task 19 (Path Finding) is arguably the hardest task among all bAbI tasks (see e.g., <ref type="bibr" target="#b29">(Sukhbaatar et al., 2015)</ref>, which reports an accuracy of less than 20% for all methods that do not use the strong supervision). We apply a GGS-NN to this problem, again on the symbolic form of the data (so results are not comparable to those in <ref type="bibr" target="#b29">(Sukhbaatar et al., 2015)</ref>). An extra 'end' class is added to the end of each output sequence; at test time the network will keep making predictions until it predicts the 'end' class.</p><p>The results for this task are given in <ref type="table" target="#tab_5">Table 3</ref>. Both RNN and LSTM fail on this task. However, with only 50 training examples, our GGS-NNs achieve much better test accuracy than RNN and LSTM.  We further developed two new bAbI-like tasks based on algorithmic problems on graphs: Shortest Paths, and Eulerian Circuits. For the first, we generate random graphs and produce a story that lists all edges in the graphs. Questions come from choosing two random nodes A and B and asking for the shortest path (expressed as a sequence of nodes) that connects the two chosen nodes. We constrain the data generation to only produce questions where there is a unique shortest path from A to B of length at least 2. For Eulerian circuits, we generate a random two-regular connected graph and a separate random distractor graph. The question gives two nodes A and B to start the circuit, then the question is to return the Eulerian circuit (again expressed as a sequence of nodes) on the given subgraph that starts by going from A to B. Results are shown in the To prove that this program indeed concatenates the two lists a and b and that all pointer dereferences are valid, we need to (mathematically) characterize the program's heap in each iteration of the loop. For this, we use separation logic <ref type="bibr" target="#b19">(O'Hearn et al., 2001;</ref><ref type="bibr" target="#b23">Reynolds, 2002)</ref>, which uses inductive predicates to describe abstract data structures. For example, a list segment is defined as ls(x, y) ≡ x = y ∨ ∃v, n.ls(n, y) * x → {val : v, next : n}, where x → {val : v, next : n} means that x points to a memory region that contains a structure with val and next fields whose values are in turn v and n. The * connective is a conjunction as ∧ in Boolean logic, but additionally requires that its operators refer to "separate" parts of the heap. Thus, ls(cur, NULL) implies that cur is either NULL, or that it points to two values v, n on the heap, where n is described by ls again. The formula ∃t.ls(a, cur) * ls(cur, NULL) * ls(b, t) is an invariant of the loop (i.e., it holds when entering the loop, and after every iteration). Using it, we can prove that no program run will fail due to dereferencing an unallocated memory address (this property is called memory safety) and that the function indeed concatenates two lists using a Hoare-style verification scheme <ref type="bibr" target="#b13">(Hoare, 1969)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LEARNING GRAPH ALGORITHMS</head><p>The hardest part of this process is coming up with formulas that describe data structures, and this is where we propose to use machine learning. Given a program, we run it a few times and extract the state of memory (represented as a graph; see below) at relevant program locations, and then predict a separation logic formula. Static program analysis tools (e.g., <ref type="bibr" target="#b22">(Piskac et al., 2014)</ref>) can check whether a candidate formula is sufficient to prove the desired properties (e.g., memory safety).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">FORMALIZATION</head><p>Representing Heap State as a Graph As inputs we consider directed, possibly cyclic graphs representing the heap of a program. These graphs can be automatically constructed from a program's memory state. Each graph node v corresponds to an address in memory at which a sequence of pointers v 0 , . . . , v k is stored (we ignore non-pointer values in this work). Graph edges reflect these pointer values, i.e., v has edges labeled with 0, . . . , k that point to nodes v 0 , . . . , v k , respectively. A subset of nodes are labeled as corresponding to program variables.</p><p>An example input graph is displayed as "Input" in <ref type="figure" target="#fig_4">Fig. 3</ref>. In it, the node id (i.e., memory address) is displayed in the node. Edge labels correspond to specific fields in the program, e.g., 0 in our example corresponds to the next pointer in our example function from the previous section. For binary trees there are two more types of pointers left and right pointing to the left and right children of a tree node.</p><p>Output Representation Our aim is to mathematically describe the shape of the heap. In our model, we restrict ourselves to a syntactically restricted version of separation logic, in which formulas are of the form ∃x 1 , . . . , x n .a 1 * . . . * a m , where each atomic formula a i is either ls(x, y) (a list from x to y), tree(x) (a binary tree starting in x), or none(x) (no data structure at x). Existential quantifiers are used to give names to heap nodes which are needed to describe a shape, but not labeled by a program variable. For example, to describe a "panhandle list" (a list that ends in a cycle), the first list element on the cycle needs to be named. In separation logic, this can be expressed as ∃t.ls(x, t) * ls(t, t).</p><p>Data We can generate synthetic (labeled) datasets for this problem. For this, we fix a set of predicates such as ls and tree (extensions could consider doubly-linked list segments, multi-trees, . . .) together with their inductive definitions. Then we enumerate separation logic formulas instantiating our predicates using a given set of program variables. Finally, for each formula, we enumerate heap graphs satisfying that formula. The result is a dataset consisting of pairs of heap graphs and associated formulas that are used by our learning procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">FORMULATION AS GGS-NNS</head><p>It is easy to obtain the node annotations for the intermediate prediction steps from the data generation process. So we train a variant of GGS-NN with observed annotations (observed at training time; not test time) to infer formulas from heap graphs. Note that it is also possible to use an unobserved GGS- NN variant and do end-to-end learning. The procedure breaks down the production of a separation logic formula into a sequence of steps. We first decide whether to declare existential variables, and if so, choose which node corresponds to the variable. Once we have declared existentials, we iterate over all variable names and produce a separation logic formula describing the data structure rooted at the node corresponding to the current variable.</p><p>The full algorithm for predicting separation logic formula appears below, as Alg. 1. We use three explicit node annotations, namely is-named (heap node labeled by program variable or declared existentially quantified variable), active (cf. algorithm) and is-explained (heap node is part of data structure already predicted). Initial node labels can be directly computed from the input graph: "isnamed" is on for nodes labeled by program variables, "active" and "is-explained" are always off (done in line 2). The commented lines in the algorithm are implemented using a GG-NN, i.e., Alg. 1 is an instance of our GGS-NN model. An illustration of the beginning of a run of the algorithm is shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, where each step is related to one line of the algorithm.</p><p>Algorithm 1 Separation logic formula prediction procedure Input: Heap graph G with named program variables 1: X ← compute initial labels from G 2: H ← initialize node vectors by 0-extending X 3: while ∃ quantifier needed do Graph-level Classification ( †) X ← turn on "is-named" for v in X 7:</p><p>print "∃t." 8: end while 9: for node v with label "is-named" in X do X have separate propagation models. For all the GG-NN components in the GGS-NN pipeline, we unrolled the propagation process for 10 time steps. The GGS-NNs associated with step ( †) (deciding wheter more existentially quantified variable need to be declared) and ( ‡) (identify which node need to be declared as existentially quantified) uses D = 16 dimensional node representations. For all other GGS-NN components, D = 8 is used. Adam <ref type="bibr" target="#b15">(Kingma &amp; Ba, 2014)</ref> is used for optimization, the models are trained on minibatches of 20 graphs, and optimized until training error is very low. For the graph-level classification tasks, we also artificially balanced classes to have even number of examples from each class in each minibatch. All the GGS-NN components contain less than 5k parameters and no overfitting is observed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">BATCH PREDICTION DETAILS</head><p>In practice, a set of heap graphs will be given as input and a single output formula is expected to describe and be consistent with all the input graphs. The different heap graphs can be snapshots of the heap state at different points in the program execution process, or different runs of the same program with different inputs. We call this the "batch prediction" setup contrasting with the single graph prediction described in the main paper.</p><p>To make batch predictions, we run one GGS-NN for each graph simultaneously. For each prediction step, the outputs of all the GGS-NNs at that step across the batch of graphs are aggregated.</p><p>For node selection outputs, the common named variables link nodes on different graphs togeter, which is the key for aggregating predictions in a batch. We compute the score for a particular named variable t as o t = g o g Vg(t) , where V g (t) maps variable name t to a node in graph g, and o g Vg(t) is the output score for named variable t in graph g. When applying a softmax over all names using o t as scores, this is equivalent to a model that computes p(toselect = t) = g p g (toselect = V g (t)).</p><p>For graph-level classification outputs, we add up scores of a particular class across the batch of graphs, or equivalently compute p(class = k) = g p g (class = k). Node annotation outputs are updated for each graph independently as different graphs have completely different set of nodes. However, when the algorithm tries to update the annotation for one named variable, the nodes associated with that variable in all graphs are updated. During training, all labels for intermediate steps are available to us from the data generation process, so the training process again can be decomposed to single output single graph training.</p><p>A more complex scenario allowing for nested data structures (e.g., list of lists) was discussed in <ref type="bibr" target="#b3">Brockschmidt et al. (2015)</ref>. We have also successfully extended the GGS-NN model to this case. More details on this can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">EXPERIMENTS.</head><p>For this paper, we produced a dataset of 327 formulas that involves three program variables, with 498 graphs per formula, yielding around 160,000 formula/heap graph combinations. To evaluate, we split the data into training, validation and test sets using a 6:2:2 split on the formulas (i.e., the formulas in the test set were not in the training set). We measure correctness by whether the formula predicted at test time is logically equivalent to the ground truth; equivalence is approximated by canonicalizing names and order of the formulas and then comparing for exact equality.</p><p>We compared our GGS-NN-based model with a method we developed earlier <ref type="bibr" target="#b3">(Brockschmidt et al., 2015)</ref>. The earlier approach treats each prediction step as standard classification, and requires complex, manual, problem-specific feature engineering, to achieve an accuracy of 89.11%. In contrast, our new model was trained with no feature engineering and very little domain knowledge and achieved an accuracy of 89.96%.</p><p>An example heap graph and the corresponding separation logic formula found by our GGS-NN model is shown in <ref type="figure">Fig. 4</ref>. This example also involves nested data structures and the batching extension developed in the previous section.</p><p>We have also successfully used our new model in a program verification framework, supplying needed program invariants to a theorem prover to prove correctness of a collection of list-manipulating algorithms such as insertion sort. The following <ref type="table">Table 4</ref> lists a set of benchmark list manipulation programs and the separation logic formula invariants found by the GGS-NN model, which were successfully used in a verification framework to prove the correctness of corresponding programs. <ref type="figure">Figure 4</ref>: A heap graph example that contains two named variables arg1 and arg2, and one isolated NULL node (node 1). All the edges to NULL are not shown here for clarity. The numbers on edges indicate different edge types. Our GGS-NN model successfully finds the right formula ls(arg1, NULL, λt 1 → ls(t 1 , NULL, )) * tree(arg2, λt 2 → ∃e 1 .ls(t 2 , e 1 , ) * ls(e 1 , e 1 , )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program</head><p>Invariant Found  <ref type="table">Table 4</ref>: Example list manipulation programs and the separation logic formula invariants the GGS-NN model founds from a set of input graphs. The " =" parts are produced by a deterministic procedure that goes through all the named program variables in all graphs and checks for inequality.</p><p>A further extension of the current pipeline has been shown to be able to successfully prove more sophisticated programs like sorting programs and various other list-manipulating programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>The most closely related work is GNNs, which we have discussed at length above. <ref type="bibr" target="#b18">Micheli (2009)</ref> proposed another closely related model that differs from GNNs mainly in the output model. GNNs have been applied in several domains <ref type="bibr" target="#b10">(Gori et al., 2005;</ref><ref type="bibr" target="#b6">Di Massa et al., 2006;</ref><ref type="bibr" target="#b24">Scarselli et al., 2009;</ref><ref type="bibr" target="#b31">Uwents et al., 2011)</ref>, but they do not appear to be in widespread use in the ICLR community. Part of our aim here is to publicize GNNs as a useful and interesting neural network variant.</p><p>An analogy can be drawn between our adaptation from GNNs to GG-NNs, to the work of <ref type="bibr" target="#b7">Domke (2011)</ref> and <ref type="bibr" target="#b28">Stoyanov et al. (2011)</ref> in the structured prediction setting. There belief propagation (which must be run to near convergence to get good gradients) is replaced with truncated belief propagation updates, and then the model is trained so that the truncated iteration produce good results after a fixed number of iterations. Similarly, Recursive Neural Networks <ref type="bibr" target="#b9">(Goller &amp; Kuchler, 1996;</ref><ref type="bibr" target="#b26">Socher et al., 2011)</ref> being extended to Tree LSTMs <ref type="bibr" target="#b30">(Tai et al., 2015)</ref> is analogous to our using of GRU updates in GG-NNs instead of the standard GNN recurrence with the aim of improving the long-term propagation of information across a graph structure.</p><p>The general idea expressed in this paper of assembling problem-specific neural networks as a composition of learned components has a long history, dating back at least to the work of Hinton (1988) on assembling neural networks according to a family tree structure in order to predict relations between people. Similar ideas appear in <ref type="bibr" target="#b11">Hammer &amp; Jain (2004)</ref> and <ref type="bibr" target="#b2">Bottou (2014)</ref>.</p><p>Graph kernels <ref type="bibr" target="#b25">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b14">Kashima et al., 2003)</ref> can be used for a variety of kernelbased learning tasks with graph-structured inputs, but we are not aware of work that learns the kernels and outputs sequences. <ref type="bibr" target="#b20">Perozzi et al. (2014)</ref> convert graphs into sequences by following random walks on the graph then learns node embeddings using sequence-based methods. <ref type="bibr" target="#b27">Sperduti &amp; Starita (1997)</ref> map graphs to graph vectors then classify using an output neural network. There are several models that make use of similar propagation of node representations on a graph structure. <ref type="bibr" target="#b4">Bruna et al. (2013)</ref> generalize convolutions to graph structures. The difference between their work and GNNs is analogous to the difference between convolutional and recurrent networks. <ref type="bibr" target="#b8">Duvenaud et al. (2015)</ref> also consider convolutional like operations on graphs, building a learnable, differentiable variant of a successful graph feature. <ref type="bibr" target="#b17">Lusci et al. (2013)</ref> converts an arbitrary undirected graph to a number of different DAGs with different orientations and then propagates node representations inwards towards each root, training an ensemble of models. In all of the above, the focus is on one-step problems.</p><p>GNNs and our extensions have many of the same desirable properties of pointer networks <ref type="bibr" target="#b32">(Vinyals et al., 2015)</ref>; when using node selection output layers, nodes from the input can be chosen as outputs.</p><p>There are two main differences: first, in GNNs the graph structure is explicit, which makes the models less general but may provide stronger generalization ability; second, pointer networks require that each node has properties (e.g., a location in space), while GNNs can represent nodes that are defined only by their position in the graph, which makes them more general along a different dimension.</p><p>GGS-NNs are related to soft alignment and attentional models (e.g., <ref type="bibr" target="#b1">Bahdanau et al. (2014)</ref>; <ref type="bibr" target="#b16">Kumar et al. (2015)</ref>; <ref type="bibr" target="#b29">Sukhbaatar et al. (2015)</ref>) in two respects: first, the graph representation in Eq. 7 uses context to focus attention on which nodes are important to the current decision; second, node annotations in the program verification example keep track of which nodes have been explained so far, which gives an explicit mechanism for making sure that each node in the input has been used over the sequence of producing an output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B is E E has_fear H eval B has_fear</head><p>What is being learned? It is instructive to consider what is being learned by the GG-NNs. To do so, we can draw analogy between how the bAbI task 15 would be solved via a logical formulation. As an example, consider the subset of lines needed to answer one example on the right.</p><p>To do logical reasoning, we would need not only a logical encoding of the facts present in the story but also the background world knowledge encoded as inference rules such as is(x, y) ∧ has-fear(y, z) =⇒ has-fear(x, z).</p><p>Our encoding of the tasks simplifies the parsing of the story into graph form, but it does not provide any of the background knowledge. The GG-NN model can be seen as learning this, with results stored in the neural network weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results in the paper show that GGS-NNs have desirable inductive biases across a range of problems that have some intrinsic graph structure to them, and we believe there to be many more cases where GGS-NNs will be useful. There are, however, some limitations that need to be overcome to make them apply even more broadly. Two limitations that we mentioned previously are that the bAbI task translation does not incorporate temporal order of inputs or ternary and higher order relations. We can imagine several possibilities for lifting these restrictions, such as concatenating a series of GG-NNs, where there is one GG-NNs for each edge, and representing higher order relations as factor graphs. A more significant challenge is how to handle less structured input representations. For example, in the bAbI tasks it would be desirable not to use the symbolic form of the inputs. One possible approach is to incorporate less structured inputs, and latent vectors, in our GGS-NNs. However, experimentation is needed to find the best way of addressing these issues.</p><p>The current GGS-NNs formulation specifies a question only after all the facts have been consumed. This implies that the network must try to derive all consequences of the seen facts and store all pertinent information to a node within its node representation. This is likely not ideal; it would be preferable to develop methods that take the question as an initial input, and then dynamically derive the facts needed to answer the question.</p><p>We are optimistic about the further applications of GGS-NNs. We are particularly interested in continuing to develop end-to-end learnable systems that can learn about semantic properties of programs, that can learn more complicated graph algorithms, and in applying these ideas to problems that require reasoning over knowledge bases and databases. More generally, we consider these graph neural networks as representing a step towards a model that can combine structured representations with the powerful algorithms of deep learning, with the aim of taking advantage of known structure while learning and inferring how to reason with and extend these representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A CONTRACTION MAP EXAMPLE</head><p>Consider a linear 1-hidden unit cycle-structured GNN with N nodes {1, . . . , N }. For simplicity we ignored all edge labels and node labels, equivalently this is a simple example with L V = 1 and L E = 1. At each timestep t we update hidden states h 1 , . . . , h N as</p><formula xml:id="formula_17">h (t) i = m i · h (t−1) i−1 + b i ,<label>(10)</label></formula><p>for each i, where m i and b i are parameters of the propagation model. We use the convention that h j cycles around and refers to h N +j when j ≤ 0. Let</p><formula xml:id="formula_18">h (t) = [h (t) 1 , . . . , h (t) N ] , M =       0 0 0 . . . m 1 m 2 0 0 0 0 m 3 0 0 . . . . . . 0 0 m N 0      <label>(11)</label></formula><p>and b = [b 1 , . . . , b N ] . We can write the joint update for all i as</p><formula xml:id="formula_19">h (t) = Mh (t−1) + b = T (h (t−1) )<label>(12)</label></formula><p>Restrict the update to define a contraction mapping in the Euclidean metric. This means that there is some ρ &lt; 1 such that for any h, h ,</p><formula xml:id="formula_20">||T (h) − T (h )|| &lt; ρ||h − h ||,<label>(13)</label></formula><p>or in other words,</p><formula xml:id="formula_21">||M(h − h )|| &lt; ρ||h − h ||.<label>(14)</label></formula><p>We can immediately see that this implies that |m i | &lt; ρ for each i by letting h be the elementary vector that is all zero except for a 1 in position i − 1 and letting h be the all zeros vector.</p><p>Expanding Eq. 10, we get</p><formula xml:id="formula_22">h (t) i = m i · (m i−1 h (t−2) i−1 + b i−1 ) + b i = m i m i−1 h (t−2) i−1 + m i b i−1 + b i = m i m i−1 (m i−2 h (t−3) i−2 + b i−2 ) + m i b i−1 + b i = m i m i−1 m i−2 h (t−3) i−2 + m i m i−1 b i−2 + m i b i−1 + b i .<label>(15)</label></formula><p>In the GNN model, node label l i controls which values of m i and b i are used during the propagation. Looking at this expansion and noting that m i &lt; ρ for all i, we see that information about labels of nodes δ away will decay at a rate of 1 ρ δ . Thus, at least in this simple case, the restriction that T be a contraction means that it is not able to maintain long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 NONLINEAR CASE</head><p>The same analysis can be applied to a nonlinear update, i.e.</p><formula xml:id="formula_23">h (t) i = σ m i · h (t−1) i−1 + b i ,<label>(16)</label></formula><p>where σ is any nonlinear function. Then</p><formula xml:id="formula_24">T (h) = σ (Mh + b). Let T (h) = [T 1 (h), ..., T N (h)] , where T i (h (t−1) ) = h (t)</formula><p>i . The contraction map definition Eq. 13 implies that each entry of the Jacobian matrix of T is bounded by ρ, i.e.</p><formula xml:id="formula_25">∂T i ∂h j &lt; ρ, ∀i, ∀j.<label>(17)</label></formula><p>To see this, consider two vectors h and h , where h k = h k , ∀k = j and h j + ∆ = h j . The definition in Eq. 13 implies that for all i,</p><formula xml:id="formula_26">||T i (h) − T i (h )|| ≤ ||T (h) − T (h )|| &lt; ρ|∆|.<label>(18)</label></formula><p>Therefore</p><formula xml:id="formula_27">T i (h 1 , ..., h j−1 , h j , h j+1 , ..., h N ) − T i (h 1 , ..., h j−1 , h j + ∆, h j+1 , ..., h N ) ∆ &lt; ρ,<label>(19)</label></formula><p>where the left hand side is ∂Ti ∂hj by definition as ∆ → 0.</p><formula xml:id="formula_28">When j = i − 1, ∂T i ∂h i−1 &lt; ρ.<label>(20)</label></formula><p>Also, because of the special cycle graph structure, for all other js we have ∂Ti ∂hj = 0. Applying this to the update at timestep t, we get ∂h</p><formula xml:id="formula_29">(t) i ∂h (t−1) i−1 &lt; ρ.<label>(21)</label></formula><p>Now let's see how a change in h</p><p>(1)</p><formula xml:id="formula_30">1 could affect h (t)</formula><p>t . Using the chain rule and the special graph structure, we have</p><formula xml:id="formula_31">∂h (t) t ∂h (1) 1 = ∂h (t) t ∂h (t−1) t−1 · ∂h (t−1) t−1 ∂h (t−2) t−2 · · · ∂h (2) 2 ∂h (1) 1 = ∂h (t) t ∂h (t−1) t−1 · ∂h (t−1) t−1 ∂h (t−2) t−2 · · · ∂h (2) 2 ∂h (1) 1 &lt; ρ · ρ · · · ρ = ρ t−1 .<label>(22)</label></formula><p>As ρ &lt; 1, this derivative will approach 0 exponentially fast as t grows. Intuitively, this means that the impact one node has on another node far away will decay exponetially, therefore making it difficult to model long range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B WHY ARE RNN AND LSTM SO BAD ON THE SEQUENCE PREDICTION TASKS?</head><p>RNN and LSTM performance on the sequence prediction tasks, i.e. bAbI task 19, shortest path and Eulerian circuit, are very poor compared to single output tasks. The Eulerian circuit task is the one that RNN and LSTM fail most dramatically. A typical training example for this task looks like the following,</p><p>3 connected-to 7 7 connected-to 3 1 connected-to 2 2 connected-to 1 5 connected-to 7 7 connected-to 5 0 connected-to 4 4 connected-to 0 1 connected-to 0 0 connected-to 1 8 connected-to 6 6 connected-to 8 3 connected-to 6 6 connected-to 3 5 connected-to 8 8 connected-to 5 4 connected-to 2 2 connected-to 4 eval eulerian-circuit 5 7 5,7,3,6,8</p><p>This describes a graph with two cycles 3-7-5-8-6 and 1-2-4-0, where 3-7-5-8-6 is the target cycle and 1-2-4-0 is a smaller distractor graph. All edges are presented twice in both directions for symmetry. The task is to find the cycle that starts with the given two nodes and in the direction from the first to the second. The distractor graph is added to increase the difficulty of this task, this also makes the output cycle not strictly "Eulerian".</p><p>For RNN and LSTM the above training example is further transformed into a sequence of tokens, n4 e1 n8 eol n8 e1 n4 eol n2 e1 n3 eol n3 e1 n2 eol n6 e1 n8 eol n8 e1 n6 eol n1 e1 n5 eol n5 e1 n1 eol n2 e1 n1 eol n1 e1 n2 eol n9 e1 n7 eol n7 e1 n9 eol n4 e1 n7 eol n7 e1 n4 eol n6 e1 n9 eol n9 e1 n6 eol n5 e1 n3 eol n3 e1 n5 eol q1 n6 n8 ans 6 8 4 7 9</p><p>Note the node IDs here are different from the ones in the original symbolic data. The RNN and LSTM read through the whole sequence, and start to predict the first output when reading the ans token. Then for each prediction step, the ans token is fed as the input and the target node ID (treated as a class label) is expected as the output. In this current setup, the output of each prediction step is not fed as the input for the next. Our GGS-NN model uses the same setup, where the output of one step is not used as input to the next, only the predicted node annotations X (k) carry over from one step to the next, so the comparison is still fair for RNN and LSTM. Changing both our method and the baselines to make use of previous predictions is left as future work.</p><p>From this example we can see that the sequences the RNN and LSTM have to handle is quite long, close to 80 tokens before the predictions are made. Some predictions really depend on long range memory, for example the first edge (3-7) and first a few tokens (n4 e1 n8) in the sequence are needed to make prediction in the third prediction step (3 in the original symbolic data, and 4 in the tokenized RNN data). Keeping long range memory in RNNs is challenging, LSTMs do better than RNNs but still can't completely solve the problem. Another challenge about this task is the output sequence does not appear in the same order as in the input sequence. In fact, the data has no sequential nature at all, even when the edges are randomly permutated, the target output sequence should not change. The same applies for bAbI task 19 and the shortest path task. GGS-NNs are good at handling this type of "static" data, while RNN and LSTM are not. However future work is needed to determine how best to apply GGS-NNs to temporal sequential data which RNN and LSTM are good at. This is one limitation of the GGS-NNs model which we discussed in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C NESTED PREDICTION DETAILS</head><p>Data structures like list of lists are nested data structures, in which the val pointer of each node in a data structure points to another data structure. Such data structures can be represented in separation logic by allowing predicates to be nested. For example, a list of lists can be represented as ls(x, y, λt → ls(t, NULL)), where λt → ls(t, NULL) is a lambda expression and says that for each node in the list from x to y, its val pointer t satisfies ls(t, NULL). So there is a list from x to y, where each node in that list points to another list. A simple list without nested structures can be represented as ls(x, y, ) where represents an empty predicate. Note that unlike the non-nested case where the val pointer always points to NULL, we have to consider the val pointers here in order to describe and handle nested data structures.</p><p>To make our GGS-NNs able to predict nested formulas, we adapt Alg. 1 to Alg. 2. Where an outer loop goes through each named variable once and generate a nested predicate with the node associated with that variable as the active node. The nested prediction procedure handles prediction similarly as in Alg. 1. Before calling the nested prediction procedure recursively, the node annotation update in line 32 not only annotates nodes in the current structure as "is-explained", but also annotates nodes linked to via the "val" pointer from all nodes in the current structure as "active". For the list of lists example, after predicting "ls(x, y,", the annotation step annotates all nodes in the list from x to y as</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Example graph. Color denotes edge types. (b) Unrolled one timestep. (c) Parameter tying and sparsity in recurrent matrix. Letters denote edge types with B corresponding to the reverse edge of type B. B and B denote distinct parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of GGS-NN models. in many cases can achieve similar performance level as the full model. But in cases where the desired propagation behavior for F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>v</head><label></label><figDesc>set to D = 4, D = 5, D = 6, D = 3 and D = 6 respectively. For all the GGS-NNs in this section we used the simpler variant in which F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the first 8 steps to predict a separation logic formula from a memory state. Label is-named signified by variable near node, active by double border, is-explained by white fill.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>HX</head><label></label><figDesc>← initialize node vectors, turn on "active" label for v in X ← update node annotations in X Node Annotation (♠) 19: end for6.3 MODEL SETUP DETAILSWe use the full GGS-NN model where F(k) o and F (k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Traverse1</head><label></label><figDesc>ls(lst, curr) * ls(curr, NULL) Traverse2 curr = NULL * lst = NULL * ls(lst, curr) * ls(curr, NULL) Concat a = NULL * a = b * b = curr * curr = NULL * ls(curr, NULL) * ls(a, curr) * ls(b, NULL) Copy ls(curr, NULL) * ls(lst, curr) * ls(cp, NULL) Dispose ls(lst, NULL) Insert curr = NULL * curr = elt * elt = NULL * elt = lst * lst = NULL * ls(elt, NULL) * ls(lst, curr) * ls(curr, NULL) Remove curr = NULL * lst = NULL * ls(lst, curr) * ls(curr, NULL)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>s</head><label></label><figDesc>Here the first 4 lines describe edges, s, n, w, e (e does not appear in this example) are all different edge types. The last line is a path question, the answer is a sequence of directions w,s, as the path going from B to A is to first go west to E then go south to A. The s, n, w, e in the question lines are treated as output classes.More Training Details. For all tasks in this section, we generate 1000 training examples and 1000 test examples, 50 of the training examples are used for validation. When evaluating model performance,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>For all explanatory tasks, we start by training different models on only 50 training examples, and gradually increase the number of training examples to 100, 250, 500, and 950 (50 of the training examples are reserved for validation) until the model's test accuracy reaches 95% or above, a success by bAbI standard<ref type="bibr" target="#b33">Weston et al. (2015)</ref>. For each method, we report the minimum number of training examples it needs to reach 95% accuracy along with the accuracy it reaches with that amount of training examples. In all these cases, we unrolled the propagation process for 5 steps. For bAbI task 4, 15, 16, 18, 19, we used GG-NN with the size of node vectors h</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Test results appear in Table 1. For all tasks GG-NN achieves perfect test accuracy using only 50 training examples, while the RNN/LSTM baselines either use more training examples (Task 4) or fail to solve the tasks (Task 15, 16 and 18).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>97.3±1.9 (250) 97.4±2.0 (250) 100.0±0.0 (50) bAbI Task 15 48.6±1.9 (950) 50.3±1.3 (950) 100.0±0.0 (50) bAbI Task 16 33.0±1.9 (950) 37.5±0.9 (950) 100.0±0.0 (50) bAbI Task 18 88.9±0.9 (950) 88.9±0.8 (950) 100.0±0.0 (50) Accuracy in percentage of different models for different tasks. Number in parentheses is number of training examples required to reach shown accuracy.</figDesc><table><row><cell>Task</cell><cell>RNN</cell><cell></cell><cell>LSTM</cell><cell>GG-NN</cell><cell></cell></row><row><cell>bAbI Task 4 #Training Examples</cell><cell>50</cell><cell>100</cell><cell>250</cell><cell>500</cell><cell>950</cell></row><row><cell>RNN</cell><cell cols="5">76.7±3.8 90.2±4.0 97.3±1.9 98.4±1.3 99.7±0.4</cell></row><row><cell>LSTM</cell><cell cols="5">73.5±5.2 86.4±3.8 97.4±2.0 99.2±0.8 99.6±0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Accuracy in percentage of different models for different tasks. The number in parentheses is number of training examples required to reach that level of accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>RNN and LSTM fail on both tasks, but GGS-NNs learns to make perfect predictions using only 50 training examples.</figDesc><table><row><cell cols="2">node * concat(node * a, node * b) {</cell></row><row><cell>if (a == NULL) return b;</cell><cell></cell></row><row><cell>node * cur = a;</cell><cell></cell></row><row><cell>while (cur.next != NULL)</cell><cell></cell></row><row><cell>cur = cur-&gt;next;</cell><cell></cell></row><row><cell>cur-&gt;next = b;</cell><cell></cell></row><row><cell>return a;</cell><cell>}</cell></row></table><note>6 PROGRAM VERIFICATION WITH GGS-NNS Our work on GGS-NNs is motivated by a practical application in program verification. A crucial step in automatic program verification is the inference of program invariants, which approximate the set of program states reachable in an execution. Finding invariants about data structures is an open problem. As an example, consider the simple C function on the right.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although the bAbI data is quite templatic, so it is straightforward to hand-code a parser that will work for the bAbI data; the symbolic option removes the need for this.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For bAbI task 4, we treated 'e', 's', 'w', 'n' as 4 question types and trained one GG-NN for each question type, so strictly speaking for bAbI task 4 our GG-NN model has 4 times the number of parameters of a single GG-NN model. In our experiments we used a GG-NN with 271 parameters for each question type which means 1084 parameters in total.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Siddharth Krishna, Alex Gaunt, Emine Yilmaz, Milad Shokouhi, and Pushmeet Kohli for useful conversations and Douwe Kiela for comments on an earlier draft of the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 2 Nested separation logic formula prediction procedure 1: procedure OUTERLOOP(G) Graph G with named program variables 2:</p><p>X ← compute initial labels from G if pred = ls then 23:</p><p>end ← pick list end node Node Selection (♥)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A learning rule for asynchronous perceptrons with feedback in a combinatorial environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial neural networks</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From machine learning to machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to decipher the heap for program verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Constructive Machine Learning at the International Conference on Machine Learning (CMLICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dzmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison between recursive neural networks and graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabriele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Maggini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="778" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parameter learning with truncated message-passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2937" to="2943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1509.09292</idno>
		<title level="m">Convolutional networks on graphs for learning molecular fingerprints</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference onNeural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural methods for non-standard data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brijnesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks (ESANN)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representing part-whole hierarchies in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Annual Conference of the Cognitive Science Society</title>
		<meeting>the Tenth Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An axiomatic basis for computer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Antony</forename><surname>Hoare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="576" to="580" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lusci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Pollastri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural network for graphs: A contextual constructive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local reasoning about programs that alter data structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O&amp;apos;hearn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Workshop on Computer Science Logic (CSL&apos;01)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalization of back-propagation to recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">J</forename><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">2229</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GRASShopper -complete heap verification with mixed specifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruzica</forename><surname>Piskac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Zufferey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20st International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS&apos;14)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="124" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Separation logic: A logic for shared mutable data structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th IEEE Symposium on Logic in Computer Science (LICS&apos;02)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="55" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonina</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ropson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="725" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sainbayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural networks for relational learning: an experimental comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Uwents</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabriele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="315" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaitly</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03134</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Navdeep. Pointer networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: a set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and all nodes the val pointer points to from the list as &quot;active&quot;. This knowledge is not hard coded into the algorithm, the annotation model can learn this behavior from data</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
