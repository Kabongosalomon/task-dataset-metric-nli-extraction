<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2018-04-23">2018. April 23-27. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
							<email>at.luu@i2r.a-star.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking</title>
					</analytic>
					<monogr>
						<title level="m">WWW 2018: The 2018 Web Conference</title>
						<meeting> <address><addrLine>Lyon, France; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">11</biblScope>
							<date type="published" when="2018-04-23">2018. April 23-27. 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3178876.3186154</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative Filtering</term>
					<term>Recommender Systems</term>
					<term>Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a new neural architecture for collaborative ranking with implicit feedback. Our model, LRML (Latent Relational Metric Learning) is a novel metric learning approach for recommendation. More specifically, instead of simple push-pull mechanisms between user and item pairs, we propose to learn latent relations that describe each user item interaction. This helps to alleviate the potential geometric inflexibility of existing metric learing approaches. This enables not only better performance but also a greater extent of modeling capability, allowing our model to scale to a larger number of interactions. In order to do so, we employ a augmented memory module and learn to attend over these memory blocks to construct latent relations. The memory-based attention module is controlled by the user-item interaction, making the learned relation vector specific to each user-item pair. Hence, this can be interpreted as learning an exclusive and optimal relational translation for each user-item interaction. The proposed architecture demonstrates the state-of-the-art performance across multiple recommendation benchmarks. LRML outperforms other metric learning models by 6% − 7.5% in terms of Hits@10 and nDCG@10 on large datasets such as Netflix and MovieLens20M. Moreover, qualitative studies also demonstrate evidence that our proposed model is able to infer and encode explicit sentiment, temporal and attribute information despite being only trained on implicit feedback. As such, this ascertains the ability of LRML to uncover hidden relational structure within implicit datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The modern age is a world of information overload. The explosion of information, also referred to as the era of big data, is a huge motivator for the research and development of practical recommender systems. Generally, the key problem that these systems are aiming to solve is the inevitable conundrum of 'too much content, too little time' that is commonly faced by users. After all, there are easily million of movies, thousands of songs and hundreds of books to choose from at any given time. An effective recommender system ameliorates this problem by delivering the most relevant content to the user.</p><p>Our work is targeted at recommender systems that operate on implicit data (e.g., clicks, likes, bookmarks) and are known as collaborative filtering (CF) systems <ref type="bibr" target="#b26">[27]</ref>. In this setting, Matrix Factorization (MF) remains as one of the most popular baselines which has inspired a considerable number of variations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref>. The general idea of MF is as follows: Users and items are represented as a matrix and subsequently factorized into latent components which can also be interpreted as modeling the relationships between users and items using the inner product. As such, this allows missing values to be inferred which provides an approximate solution to the recommendation problem.</p><p>Recently, Hseih et al. <ref type="bibr" target="#b8">[9]</ref> revealed the potential implications pertaining to the usage of inner product to model user-item relationships. Their argument is constructed upon the fact that inner product violates the triangle inequality which is essential to model the fine-grained preferences of users. Instead, the authors proposed a metric-based learning scheme that minimizes the distance between user and item vectors (p and q) of positive interactions. Simultaneously, this also learns user-user similarity and item-item similarity in vector space. As evidence to their assertions, their proposed algorithm, the collaborative metric learning (CML) algorithm <ref type="bibr" target="#b8">[9]</ref> demonstrates highly competitive performance on many benchmark datasets.</p><p>Despite the success of CML, it faces several weaknesses. Firstly, the scoring function of CML is clearly geometrically restrictive. Given a user-item interaction, CML tries to fit the pair into the same point in vector space. Considering the many-to-many nature of the collaborative ranking problem, enforcing a good fit in vector space can be really challenging from a geometric perspective especially since the optimal point of each user and item is now a single point in vector space. Intuitively, this tries to fit a user and all his interacted items onto the same point, i.e., geometrically congestive and inflexible. While it is possible to learn user-user and item-item similarity clusters, this comes at the expense of precision and accuracy in ranking problems especially pertaining to large datasets whereby there can be millions of interactions. Secondly and by taking a more theoretically grounded angle, CML is an ill-posed algebraic system <ref type="bibr" target="#b35">[36]</ref> which further reinforces and aggravates the problem of geometric inflexibility. A proof and more details are described in the related work section.</p><p>In this work, we propose a flexible and adaptive metric learning algorithm for collaborative filtering and ranking. Our model, LRML (Latent Relational Metric Learning) learns adaptive relation vectors between user and item interactions, finding an optimal translation vector between each interaction pair. Needless to say, our work is highly inspired by recent advances in NLP which include the highly celebrated word embeddings <ref type="bibr" target="#b17">[18]</ref> and knowledge graph embeddings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32]</ref> which popularized the concept of semantic translation in vector space. In our proposed approach, we assume that there exist a latent relational structure within the implicit interaction data and therefore, we aim to model the latent relationships between users and items by inducing relation vectors.</p><p>Overall, our key intuition can be described as follows: For each user and item interaction, we learn a vector r that explains this relationship, i.e., the relation vector r connects the user vector to the item vector. Ideally, this vector r should capture the hidden semantics between each implicit interaction and is learned over an auxiliary memory module via a neural attention mechanism. The auxiliary memory module can be interpreted as a memory store of concepts in which, upon linear combination, constructs a relation vector. The content addressing of this memory module is user and item dependent, which ensures sufficient flexibility in geometric space. Apart from the clear benefits of an interpretable attention module, LRML can also be considered as an improvement to the CML algorithm <ref type="bibr" target="#b8">[9]</ref>. Our approach solves the geometric inflexibility problem by means of adaptive (user-item specific) translations in vector space. This allows for a greater extent of flexibility and modeling capability in metric space which enables our model to scale to larger datasets with easily millions of interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>Motivated by the success of deep learning, both generally and in the field of recommender systems, our ideas are materialized in the form of a neural network architecture that leverages the recent advancements of neural attention mechanisms and augmented memory modules <ref type="bibr" target="#b27">[28]</ref>. Overall, the prime contributions of this paper are:</p><p>• We present LRML (Latent Relational Metric Learning), a novel, end-to-end neural network architecture for collaborative filtering and ranking on implicit interaction data. For the first time, we adopt user and item specific latent relation vectors to model the relationship between user-item interactions. • We propose a novel Latent Relational Attentive Memory (LRAM) module in order to generate the latent relation vectors. The LRAM module provides improvements in terms of flexibility and modeling capability of the algorithm. Moreover, the neural attention also gives greater insight and interpretability of the model. • We evaluate our proposed LRML on ten publicly available benchmark datasets. This includes large, web-scale datasets like Netflix Prize and MovieLens20M. Our proposed approach demonstrates highly competitive results on all datasets, outperforming not only CML but many other strong baselines such as NeuMF <ref type="bibr" target="#b6">[7]</ref>. Moreover, on large datasets, we obtain 6% − 7.5% gain in performance over CML and other models.</p><p>• We performed extensive qualitative analysis. Upon inspection of the attention weights, our proposed LRML is capable of inferring explicit information such as ratings (e.g., 1-5 stars), temporal and item attribute information despite being only trained on implicit binary data. This ascertains the capability of LRML in unraveling hidden latent structure within seemingly non-relational datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Our work is concerned with collaborative filtering 1 with implicit feedback. We first formulate the problem and discuss the existing algorithms that are aimed at solving this problem. Then, we elaborate on the potential weaknesses of the collaborative metric learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Implicit Collaborative Filtering</head><p>The task of implicit collaborative filtering is concerned with learning via implicit interaction data, e.g., clicks, bookmarks, likes, etc. Let P be the set of all users and Q be the set of all items. The problem of implicit CF can be described as follows:</p><formula xml:id="formula_0">y ui = 1 , if interaction &lt;user,item&gt; exists 0 , otherwise<label>(1)</label></formula><p>where Y ∈ R |P |×|Q | is the user-item interaction matrix. Implicit CF models the interaction of users and items and on that note, it is good to bear in mind that a value of 0 does not necessarily imply negative feedback. In most cases, the user is unaware of the existence of the item which forms the cornerstone of the recommendation problem, i.e., estimating the scores of the unobserved entries in Y. Across the past decade, Matrix Factorization (MF) techniques are highly popular algorithms for collaborative filtering and have spurred on a huge number of variations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Since MF does not belong to the core focus of our work, we omit the technical descriptions of MF for the sake of brevity and refer interested readers to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Collaborative Metric Learning (CML)</head><p>CML <ref type="bibr" target="#b8">[9]</ref> is a recently incepted algorithm for CF and has, despite its simplicity, demonstrated highly competitive performance on several benchmarks. The key intuition is that CML operates in metric space, i.e., it minimizes the distance between each user-item interaction in Euclidean space. The scoring function of CML is defined as:</p><formula xml:id="formula_1">s(p, q) = ∥ p − q ∥ 2</formula><p>adverse repercussions when the dataset is large or dense since CML tries to force all of a user's item interactions onto the same point. Secondly and by taking a more theoretically grounded angle, we show that CML is an ill-posed algebraic system <ref type="bibr" target="#b35">[36]</ref> which further reinforces and aggravates the problem of geometric inflexibility.</p><p>The following proof elaborates on this issue. can be considered as an ill-posed algebraic system when there is a large number of interactions.</p><p>Proof. Let d be the dimensions of vectors p and q. From an algebraic perspective, each user-item interaction can be regarded as the equation p − q = 0. By considering p i − q i , where i is the index of the vectors p and q, the number of equations for each interaction is d. Let N be the total number of interactions, the total number of equations is therefore N × d. On the other hand, the number of free variables is only (|P | + |Q |) × d. Since N ≫ d(|P | + |Q |) in most settings, CML is an ill-posed algebraic system. □</p><p>Since it is not uncommon for implicit recommendation datasets to contain millions of interactions while having significantly much lesser unique items and users, we can consider, from a mathematical perspective, that CML proposes an ill-posed algebraic system. This introduces instability when training and optimizing the objective function of CML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Translating in Vector Space</head><p>Our proposed approach, LRML, ameliorates the flaws of CML by means of adaptive translation. Since our adaptive translation is learned as a weighted representation (over an augmented memory via neural attention), this introduces an extremely large number of possibilities for the user and item vectors to be translated in vector space. More specifically, the attention vector (learned via a softmax function) learns a continious weighted representation of the augmented memory. As such, this significantly expands the flexibility of the metric learning algorithm. In LRML, the user vector is now adaptively translated based on the target item (and vice versa). As such, this allows LRML to avoid the above-mentioned flaws of CML, and enables more precise and fine-grained fitting in vector space.</p><p>Translating in vector space takes inspiration from NLP and in particular, reasoning over semantics (knowledge graphs). In this area, a highly seminal work by Bordes et al. (TransE) <ref type="bibr" target="#b1">[2]</ref> proposed translations in vector space to model the relationships between entities in a knowledge graph. Word embeddings <ref type="bibr" target="#b17">[18]</ref> are also known to exhibit semantic translation in vector space whereby the relationships between two words can be explained by a relation vector. The domain of CF that models users and items, and represents them as an interaction matrix is highly related to graph and network embeddings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>To the best of our knowledge, our work is the first work that extends the 2D structure of user-item CF into a 3D structure by assuming a latent relational (3D) structure. Intuitively, this can be also interpreted as inducing a latent knowledge graph from the user-item interaction graph. <ref type="figure" target="#fig_1">Figure 1</ref> depicts the key difference between LRML and CMLwhile CML tries to place user and item into the same spot in vector space, LRML learns to fit user and item with adaptive, trainable latent vectors. More specifically, LRML learns an optimal translation between each user-item interaction. Recall in Section 2.2, we have previously established that CML suffers from instability (due to being an ill-posed algebraic system) along with geometric inflexibility, i.e., the push-pull effects from too many interactions. In order to alleviate this weakness, our proposed approach adopts attentive and adaptive user-item specific translations that benefit from the vast number of possibilities of learning weighted (linearly combined) representations.</p><p>Finally, we note that another translation-based recommendation model, TransRec was recently proposed by He et al. <ref type="bibr" target="#b5">[6]</ref> in which the authors proposed to use translations to model sequential data. While TransRec also utilizes the translation principle, LRML is a completely different model. Firstly, TransRec learns translations for sequential recommendation, e..g, the 2nd item a user interacts with is represented by a translation of the first. Secondly, the overall goals of LRML is different, i.e., LRML utilizes translations for flexible and adaptive metric learning. Thirdly, LRML uses neural attention to learn latent relations, which is also an feature that is absent in TransRec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Deep Learning</head><p>In this section, we provide some preliminaries about deep learning for recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Deep Learning for Recommendation.</head><p>In the recent years we can easily observe the emerging numbers of neural network models that have been designed for a diverse range of recommendation tasks. Notably, Recurrent neural networks <ref type="bibr" target="#b37">[38]</ref> and convolutional neural networks <ref type="bibr" target="#b29">[30]</ref> have been exploited for sequence aware recommendations. There is also an emerging line of work focusing on representation learning using reviews, e.g., Deep Co-operative Networks (DeepCoNN) <ref type="bibr" target="#b42">[43]</ref>. A recent work, the Multi-Pointer Co-Attention Networks <ref type="bibr" target="#b34">[35]</ref> is the state-of-the-art review-based CF model that uses pointer-based attention for representation learning. Autoencoder based models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref> have also been proposed for CF. In the more closely related domain of collaborative filtering on implicit feedback, Neural Matrix Factorization (NeuMF) <ref type="bibr" target="#b6">[7]</ref> is a recent state-of-the-art deep learning model that learns the interaction function between user and item using deep neural networks. NeuMF is a combined framework that concatenates the inner-product-based MF with a multi-layered perceptron (MLP). A comprehensive review can be found at <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Neural</head><p>Attention. Our work borrows inspiration from the recent advances in deep learning. Specifically, LRML uses a neural attention mechanism over an augmented memory module to generate latent vectors. Neural attention mechanisms are popular in the fields of computer vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref> and NLP <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref> and are known to improve performance and interpretability of deep learning models. The key idea of attention is to learn a weighted representation across multiple samples (or embeddings), reducing noise and selecting more informative features for the final prediction. Attention operates using a softmax function, which converts the attention vector into a probability distribution. Subsequently, this vector is then used to learn a weighted sum of a sequence of vectors.</p><p>Notably, attention mechanisms have been also recently adopted for collaborative filtering problems particularly for content-based recommendations such as multimedia recommendation <ref type="bibr" target="#b2">[3]</ref>. However, the novelty of our model lies in the difference whereby our model adopts neural attention to generate latent relation vectors over an augmented memory module. This is fundamentally different from content-based attention models which learn to attend over features and learn to predict. While the key idea of attentive selection is similar, the goal of our model is to find hidden relational structure by leveraging attention mechanisms. Moreover, the inner mechanism of our proposed LRAM is highly reminiscent of end-toend memory networks <ref type="bibr" target="#b27">[28]</ref> and key-value memory networks <ref type="bibr" target="#b18">[19]</ref> which are the competitive models for question answering, machine comprehension and aspect-aware sentiment analysis <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR PROPOSED MODEL</head><p>In this section, we introduce LRML, our novel deep learning recommendation architecture. The overall model architecture is described in <ref type="figure" target="#fig_2">Figure 2</ref>. LRML aims to model user and item pairs using relation vectors. This is what we refer to as the translational principle, i.e., p + r ≈ q. Note that the relation vector r is what separates our model from simple metric learning approaches like CML which operate via p ≈ q. Let us begin with a simple high-level overview of our model:</p><p>(1) Users and items are converted to dense vector representations using an Embedding Layer (a look-up layer). p and q are the user and item vectors respectively. (2) Given p and q, a relation vector r is generated using a neural attention mechanism over an augmented memory matrix M. The relation vector, r , is a weighted representation over a trainable LRAM module. r is dependent on user and item, and is learned to best explain the relationship between user and item.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>LRML accepts a user-item pair &lt; user,item &gt; as an input. Inputs of users and items are represented as vectors encoded via one-hot encoding corresponding to a unique index key belonging to each user and item. At the embedding layer, this one-hot encoded vector is converted into a low-dimensional real-valued dense vector representation. In order to do so, this one hot vector is multiplied with the embedding matrices P ∈ R d ×|U | and Q ∈ R d ×|I | which store the user and item embeddings respectively. d is the dimensionality of the user and item embeddings while |U | and |I | are the total number of users and items respectively. The output of this layer is a pair of embeddings &lt; ì p, ì q &gt; which are the user and item embeddings respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LRAM -Latent Relational Attentive Memory Module</head><p>One of the primary goals of our LRML model is to induce latent relations between user-item pairs. However, explicit semantic relations between user-item pairs are not available in traditional CF. As such, we introduce the Latent Relational Attentive Memory (LRAM) module. The LRAM module is a centralized memory store in which latent relations are built upon. The memory matrix of the LRAM module is represented as M ∈ R N ×d where d is the dimensionality of the user-item embeddings and N is a user-specified hyperparameter that controls the expressiveness and capacity of the LRAM module. In matrix M, we refer to each row slice m i ∈ R d as a memory slice. The input to LRAM is a user-item pair &lt; p, q &gt;. The LRAM module returns the vector r of equal dimensionality as p and q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Joint</head><p>User-Item Embedding. Given the user-item pair, &lt; ì p, ì q &gt;, the LRAM module first applies the following steps to learn a joint embedding of users and items:</p><formula xml:id="formula_3">s = p ⊙ q<label>(3)</label></formula><p>where ⊙ is simply the Hadamard product (or element-wise multiplication). The generated vector s ∈ R d is of the same dimension of p and q. Note that while other functions such as the multi-layered perceptron MLP(p, q) are also viable, we found that a simple Hadamard product performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>User-Item Key Addressing. Next, using the joint user-item embedding, we aim to learn an attention vector a. The attention vector is learned from K ∈ R N ×d which we refer to as the key matrix. Each element of the attention vector a can be defined as:</p><formula xml:id="formula_4">a i = s T k i<label>(4)</label></formula><p>where k i ∈ K ∈ R N ×d and the generated vector a ∈ R d is of the same dimensions of p, q and s. In order to normalize a to a probability distribution, we can simply use the Softmax function:</p><formula xml:id="formula_5">So f tmax(a i ) = e a i j e a j .<label>(5)</label></formula><p>Since our attention mechanism utilizes the softmax function, it ensures that our network is end-to-end differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Generating Latent Relations via</head><p>Memory-based Attention. Finally, in order to generate the latent relation vector r , we use the attention vector a to generate a weighted representation of M, i.e., adaptively selecting relevant pieces of information from the memory matrix M.</p><formula xml:id="formula_6">r = i a i m i<label>(6)</label></formula><p>The output of the LRAM module is a user and item specific latent relation vector r . The latent relation vector is a weighted representation of M. Intuitively, the memory matrix M can be interpreted as a store of conceptual building blocks that can be used to describe the relationships between users and items. The mechanism design of the LRAM module is inspired by Memory Networks and can also be interpreted as neural attentions which give our model improved interpretability. Note that the LRAM module is part of LRML and is trained end-to-end. Finally, the total number of parameters added by the LRAM module is merely 2 × N × d parameters and since typically we set N &lt; 100 in our experiments, the parameter cost incurred by the LRAM module is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization and Learning</head><p>In this section, we introduce the final layer of the network, the objective function and the regularization employed in our training scheme. LRML is end-to-end differentiable since it utilizes soft attention over the LRAM module. As such, we are able to simply train it via stochastic gradient descent (SGD) methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Relational Modeling Layer.</head><p>For each user-item pair p and q, the scoring function is defined as:</p><formula xml:id="formula_7">s(p, q) = || p + r − q || 2 2 (7)</formula><p>where r is the latent relation vector constructed from the LRAM module and ||.|| 2 2 is essentially the L2 norm of the vector p + r − q. Intuitively, this score function penalizes any deviation of (p+r) from the vector q.</p><p>3.3.2 Objective Function. LRML adopts the pairwise ranking loss or hinge loss for optimization. For each positive user-item pair &lt; p, q &gt;, we sample a corrupted pair which we denote as &lt; p ′ , q ′ &gt;.</p><p>Similar to the positive example, the corrupted pair of user and item goes through the same user and item embedding layer respectively. The pairwise ranking / hinge loss is defined as follows:</p><formula xml:id="formula_8">L = (p,q)∈∆ (p ′ ,q ′ ) ∆ max(0, s(p, q) + λ − s(p ′ , q ′ ))<label>(8)</label></formula><p>where ∆ is the set of all user-item pairs, λ is the margin that separates the golden pairs and corrupted samples. max(0, x) is also known as the relu function. Note that we use the same (generated) latent relation vector for the negative example. This is motivated by our early empirical results whereby performance was much better over generating a separate relation vector for the negative example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.3</head><p>Regularization. Finally, we apply regularization by normalizing all user and item embeddings to be constrained within the Euclidean ball. At the end of each mini-batch, we apply a constraint of ∥p * ∥ 2 ≤ 1 and ∥q * ∥ 2 ≤ 1 for regularization and preventing overfitting. In order to enforce this, we can manually project all embeddings to the unit ball either at the beginning or after each training iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERFORMANCE EVALUATION</head><p>In this section, we evaluate our proposed LRML against other stateof-the-art algorithms. Our experimental evaluation is designed to answer several research questions (RQs).</p><p>• RQ1: Does LRML outperform other baselines and state-ofthe-art methods for collaborative ranking? • RQ2: How does the relative performance of LRML and CML differ across different dataset sizes? • RQ3: What is the scalability and runtime of LRML compared to other baselines? • RQ4: What is the LRAM module learning? Are we able to derive qualitative insights about the inner workings of LRML? • RQ5: What do the relation vectors represent? Are they meaningful?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In the spirit of experimental rigor, we conduct our evaluation across a wide spectrum of datasets.</p><p>• Netflix Prize -Since the entire Netflix Prize dataset is extremely large, we construct a subset of the famous Netflix Prize dataset. Specifially, we only considered movie-item ratings from the year 2005 and filtered users who had less than 100 interactions.</p><p>• MovieLens -A widely adopted benchmark dataset 2 for collaborative filtering in the application domain of recommending movies to users. Specifically, we use two configurations of this benchmark dataset, namely MovieLens1M and Movie-Lens20M <ref type="bibr" target="#b4">[5]</ref>. • IMDb -A movie recommendation dataset obtained from IMDb that was introduced in <ref type="bibr" target="#b3">[4]</ref>. • LastFM -This dataset contains social networking, tagging, and music artist listening information from Last.fm online music system 3 . • Books -This is a book recommendation dataset that was used in <ref type="bibr" target="#b43">[44]</ref>. • Delicious -This dataset contains social networking, bookmarking, and tagging information from a set of 2K users from Delicious Social Bookmarking System 4 . This dataset, along with the lastFM dataset, originated from the Hetrec 2011 workshop 5 . • Meetup -An event-based social network <ref type="bibr" target="#b5">6</ref> . We use the datasets provided by <ref type="bibr" target="#b21">[22]</ref> which include event-user pairs from NYC. • Twitter -This is a check-in dataset constructed by <ref type="bibr" target="#b39">[40]</ref> which contains users and their check-ins. There are two subsets of this dataset, namely Twitter (WW) and Twitter (USA). In total, we evaluate our proposed algorithm on ten different datasets with diverse sizes and interaction densities, i.e., the percentage of non-zero values in the user-item interaction matrix. For all datasets, with the exception of the Netflix Prize dataset, we ensured that each user has at least 20 interactions. The statistics of all datasets are reported in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Interactions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>In this section, we introduce the key baselines for comparison against our proposed LRML.</p><p>• Bayesian Personalized Ranking (BPR) <ref type="bibr" target="#b24">[25]</ref> is a strong CF baseline that minimizes i j,</p><formula xml:id="formula_9">k − log σ (p T i q j − p T i q k ) + λ v ∥u i ∥ 2 + λ q ∥q j ∥ 2 , where (p i , q j )</formula><p>is a positive interaction and (p i , q k ) is a negative sample. • Matrix Factorization (MF) is a standard baseline for CF that models the relationship between user and item using inner products. We use the generalized version from <ref type="bibr" target="#b6">[7]</ref> which scores user item pairs with s(p, q) = σ (h T (p ⊙ q)). • Multi-layered Perceptron (MLP) is the baseline neural architecture proposed in <ref type="bibr" target="#b6">[7]</ref> in which the authors proposed to use multiple layers of nonlinearities to model the relationships between users and items. • Neural Matrix Factorization (NeuMF) <ref type="bibr" target="#b6">[7]</ref> is the state-ofthe-art unified framework combining MF with MLP. NeuMF concatenates the output of MF and MLP, and uses a regression layer to predict the user item rating. Note that NeuMF uses separate embedding representations of users and items for MF and MLP. • Collaborative Metric Learning (CML) <ref type="bibr" target="#b8">[9]</ref> can be considered as the baseline of our model which does not include relational translations between user and item vectors. Since CML and NeuMF have surpassed many other baselines such as WMF <ref type="bibr" target="#b9">[10]</ref>, eALS <ref type="bibr" target="#b7">[8]</ref> and Factorization Machines <ref type="bibr" target="#b23">[24]</ref>, we do not further report them. Additionally, for fair comparison and due to scalability issues, we do not use WARP <ref type="bibr" target="#b36">[37]</ref> for both CML and LRML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Protocol and Metrics</head><p>Our evaluation protocol follows He et al. <ref type="bibr" target="#b6">[7]</ref> very closely. Similarly, we adopt the leave-one-out evaluation protocol, i.e., the testing set comprises the last item of all users. If there are no timestamps available in the dataset (e.g., Delicious and LastFM), then the test sample is randomly sampled. A single item from each user is also sampled to form the development set. Since it is too time consuming to rank all items for every user, we randomly sampled 100 items that have no interactions with the target user and ranked the test item with respect to these 100 items. This is in concert with many works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref>. Since our problem is essentially formulated as learning-to-rank, we judge the performance of our model based on the popular and widely adopted standard metrics used in information retrieval and recommender systems: normalized discounted cumulative gain (nDCG@10) <ref type="bibr" target="#b10">[11]</ref> and Hit Ratio (H@10). Intuitively, the nDCG@10 metric is a position-aware ranking metric while H@10 metric simply considers whether the ground truth is ranked amongst the top 10 items. For more detailed explanations, we refer readers to <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>We implemented all models in TensorFlow 7 on a Linux machine. For tuning the hyperparameters, we select the model that performs best on the development set based on the nDCG metric and report the result of that model on the test set. Model parameters are saved every 50 epochs. All models are trained until convergence, i.e., if the performance (nDCG metric) on the development set does not improve after 50 epochs. Models are trained for a maximum of 500 epochs. For large datasets like MovieLens20M and Netflix Prize, we stop the training at 100 epochs. The dimensionality of user and  <ref type="table">Table 2</ref>: Experimental results on ten benchmark datasets. Best performance is in boldface and second best is underlined. LRML achieves best performance on all datasets, outperforming many strong neural baselines. Improvement is much larger on large datasets such as Netflix Prize or MovieLens20M.</p><p>item embeddings d is tuned amongst {20, 50, 100}. The number of batches B is tuned amongst {10, 100, 1000}. The minimum number of batches for NetflixPrize and MovieLens20M is 100 in order to fit into the GPU RAM. We optimize all models using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref>. The learning rate for all models are tuned amongst {0.01, 0.005, 0.001}. For models that minimize the hinge loss, the margin λ is tuned amongst {0.1, 0.2, 0.25, 0.5}. For NeuMF and MLP models, we follow the configuration and architecture proposed in He et al. <ref type="bibr" target="#b6">[7]</ref>, i.e., 3 fully-connected layers with a pyramid architecture. However, for fair comparison of all models, we do not use pretrained MLP and MF models in the NeuMF model since this effectively acts as an ensemble classifier. For LRML, the number of memory slices in M is tuned amongst N = {5, 10, 20, 25, 50, 100}. For simplicity, each training instance is paired with only a single negative sample. All embeddings and parameters are normally initialized with a standard deviation of 0.01. For most datasets and baselines, we found that the following hyperparameters work well: learning rate= 0.001, number of batches B = 10 and λ = 0.2. A larger embedding size always performs better, i.e., d = 100. The size of LRAM is dataset dependent. We found that setting N = 20 works well for most datasets (performance does not degrade going beyond 50 but does not improve either). However, we found that setting N = 100 works better on large datasets such as Netflix Prize and MovieLens20M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental Results</head><p>The empirical results of our proposed model and baselines on 10 benchmark datasets are reported in <ref type="table">Table 2</ref>. Our proposed LRML performs extremely competitively on all datasets and obtain the best performance on both nDCG@10 and H@10 metrics on all datasets. This answers RQ1, showing that our proposed LRML is capable of effective collaborative ranking. Moreover, the ranking of many of the competitor baselines is fluctuating across datasets as we see the second best performance is scattered amongst different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Comparison against CML.</head><p>In general, LRML outperforms CML on all datasets on both H@10 and nDCG@10 metric. We would like to draw the reader's attention to the two datasets, namely Netflix Prize and MovieLens20M datasets in which LRML obtained a clear margin in performance gain over the competitor models. This ascertains our earlier claim about the flaws of CML (not being able to scale to large datasets) and empirically proves the advantages of our proposed approach. Specifically, LRML outperforms CML by performance gains about 7.5% on MovieLens20M and about 6% on Netflix Prize on the nDCG@10 metric. The performance gains on the hit ratio (H@10) metric is also similarly high. When the dataset is smaller, the performance gains are less distinct. For example, the performance gain in MovieLens20M is much larger than in Movie-Lens1M. The performance gains on smaller datasets range from a marginal 1%−2%, (e.g., Books and Delicious) to reasonably large, e.g., 3% − 4% on the Meetup or Twitter (WW) datasets. As such, the concluding findings pertaining to the comparison of LRML and CML can be drawn as follows: On large datasets, the performance gain of LRML over CML is large. However, on smaller datasets, LRML at least performs equally well or sometimes reasonably better. This answers RQ2 on the effect of dataset size on relative performance of LRML and CML. Our experimental evidence shows that our proposed LRML is effective and ascertains our usage of adaptive translations in metric learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Comparison against Other Baselines.</head><p>Pertaining to the performance of the other baselines, we found that the performance of MF and BPR is extremely competitive, i.e., both MF and BPR outperform CML on several datasets. The performance of MLP, on the other hand, seem to perform reasonably well only on Movie-Lens20M and performs horribly on most datasets. Note that we also tried a non-pyramid architecture but that did not improve the performance. The performance of the model NeuMF (that combines MLPs with MF) is often better than vanilla MLP but falls short of MF in most cases. Notably, NeuMF performs reasonably well on MovieLens20M, Netflix Prize and MovieLens1M. This could possibly mean that the usage of dual embedding spaces (one for MF and one for MLP) might be overfitting on the smaller datasets. <ref type="figure" target="#fig_3">Figure 3</ref> reports the runtime (seconds taken to run a single epoch) of all models on Netflix Prize and MovieLens20M. We make several observations. First, the difference in runtime between LRML and CML is quite insignificant, i.e., LRML only spends ≈ 10s − 15s extra per epoch which is only a 5% − 10% increase in runtime on both datasets. On the other hand, it is still faster than models such as NeuMF and MLP. Notably, this is also contributed by the fact that MLP and NeuMF are point-wise models which do not pair negative samples with positive samples during training. Next, we also compare the runtime of LRML with different N (LRAM size) values and found that there is only minimal observable difference in runtime with N = 50 or N = 100. This was probably made insignificant by the highly optimized GPU operations and also due to the fact that the size of the matrix-vector operations in LRAM is relatively small. To answer RQ3, we have shown that LRML only incurs a slight computation cost over CML. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Comparison on Runtime.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND ANALYSIS</head><p>In this section, we derive qualitative insights regarding our proposed model. This section describes the discoveries that we have made while trying to understand and gain some intuition behind the performance of LRML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RQ4: What is the LRAM module learning?</head><p>A key advantage to neural attention mechanisms is an improved interpretability since we are able to visualize the weighted importance of each memory slice with respect to any given attribute value v. This helps us to understand how the model is learning. Specifically, we investigate the attributes of explicit rating information and explicit temporal information, and show empirically via visualisation that the LRAM model learns to encode these attributes. Note that both attributes are not provided to our model at training time. In this experiment, the following steps were taken:</p><p>(1) First, we categorized all user-item pairs (p, q) according to the target attribute value v.</p><p>(2) Using (p, q) as an input, we generated the attention vector a for each user-item pair. Recall that this attention vector 8 is a probability distribution that depicts how much the model is looking at each memory slice of the LRAM module. (3) For each attribute class c i ∈ v, we take the mean attention vector for all user-item pairs in the category. (4) We visualise the mean attribute vector of each attribute class to observe the correlation between attribute class and which memory slice LRML is looking at.</p><p>5.1.1 LRAM Encodes Explicit Rating Information. On datasets like MovieLens1M, explicit ratings (1-5 stars) exist but are not provided to LRML. Surprisingly, we empirically discovered that, despite being only trained on implicit interactions, explicit rating information is actually being encoded in LRAM. <ref type="figure" target="#fig_4">Figure 4</ref> shows the mean attention vector (i.e., a) for each rating class <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref>. The color scale represents the strength of the attention weights and each column of <ref type="figure" target="#fig_4">Figure 4</ref> represents the mean attention vector for each rating class. As such, we are able to observe patterns and trends across different ratings by looking at the rows (from left to right). For example, the mean attention vector of rating=1 is the first vertical slice in <ref type="figure" target="#fig_4">Figure 4</ref> and the intensity denoted by the color scale represents the attention weights.</p><p>Clearly, we observe that there is a pattern between the explicit rating score and the memory slice in which LRML is looking at. We observe that slices M2-M4 are mostly associated with bad ratings (1-2 stars) while having a high attention weight over M6, M7 and M9 signifies a good rating (4-5 stars). Moreover, there is a correlation between how much the model looks at M6, M7 and M9 and the explicit rating score. As such, it seems we are able to infer explicit rating scores solely based on how much our model is looking at each memory slice.</p><p>We believe this can be explained as follows: The goal of LRML is to find a latent relational structure between the user and item interactions. As such, while LRML is trying to assign relations between users and items via neural attention, it has learned to identify and model explicit rating sentiment from the implicit structure of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">LRAM encodes temporal information.</head><p>The second discovery is that the LRAM module actually encodes temporal information. Similar to ratings, timestamps are available on the MovieLens1M dataset but are not used to train the model. To facilitate clear visualization, we binned the timestamps into 10 separate bins in ascending order. <ref type="figure" target="#fig_5">Figure 5</ref> shows the visualized attention weights of LRAM with respect to time. Similar to explicit rating scores, we notice that certain memory slices model the chronological order of user-item interactions. On M8, we see that the intensity of the attention weights increase along with time, i.e., by viewing the row M8 from left to right, we can observe an increasing attention weight on M8 based on the intensity scale. Moreover, the converse is true for M6 which when observed from left to right, it decreases in intensity instead. In short, there is a clear pattern in which we can quite safely ascertain that LRAM has learned to encode temporal information. Once again, it is worthy to note that LRML was not given any temporal information to begin with. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RQ5: What do the relation vectors represent? Are they meaningful?</head><p>For each user-item pair in the test set, we generated the latent relation vector r . Next, we computed the cosine similarity between the relation vectors of all user-item pairs and selected the user-item pairs in which the cosine similarity between their relation vectors is the highest. Intuitively, this is to investigate if similar user-item pairs might have similar relation vectors. In order to characterize user-item pairs, we selected attributes that are available in the MovieLens1M dataset. The user attributes provided include Age, Job and Gender while only category and movie title were provided for the items. Once again, note that these attributes were not provided to our model during training. For each user-item pair, we computed the attribute matches with respect to the user-item pair with the closest relation vector, e.g., if &lt;User1,Item1&gt; and &lt;User2,Item2&gt; have the most similar relation vector, we compute the matches between each attribute of both user-item pairs. For example, we check for matches between User1 and User2 within the list of attributes such as user age, user gender and user job and item category. Ideally, the model should learn a similar relation vector for similar user-item pairs. In order to determine if the result is significant, we computed the probability of a match by random chance taking into consideration the distribution of attributes. <ref type="table">Table  3</ref> reports the results of this experiment.  <ref type="table">Table 3</ref>: Matches between user-item attributes of user-item pairs with the closest relation vector. Relation vectors encode user-item attributes without being trained on them.</p><p>We observe that the percentage of getting an attribute match is often higher than that of random chance which might signal that similar user-item pairs have similar relation vectors. In particular, the item category (movie genre) has the most prominent improvement over random chance (7.32%) individually while a considerable percentage of user-item pairs (15.07%) have an exact match of item category and job. This is 9.51% more than random chance. Additionally, we also found that (by manual inspection) there is a prominent number of job-category matches such as (programmer, thriller) and (technician/engineer, thriller). This is intuitive since engineers and programmers can be considered as semantically related professions.</p><p>Overall, we believe that, the user and movie attributes characterize the behavior of users and therefore, there might be a hidden structure within simple implicit interaction data. By imposing and inducing architectural bias, our model learns to capture this finegrained behavior even from simple implicit feedback data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed LRML (Latent Relational Metric Learning), a novel attention-based memory-augmented neural architecture that models the relationship between users and items in metric space using latent relation vectors. LRML demonstrates the stateof-the-art performance on 10 publicly available benchmark datasets for implicit collaborative ranking. Empirical results show that relative improvement is significantly greater when the dataset is large, e.g., Netflix Prize and MovieLens20M, which is due to the geometric inflexibility of the CML algorithm. Additionally, LRML leverages the hidden and latent relational structure in the implicit user-item interaction matrix. Via qualitative analysis of the attention weights, we discovered that explicit rating information, temporal information and even item attributes are encoded within the LRAM module and relation vectors even when these information are not provided during training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 2 . 1 .</head><label>21</label><figDesc>The objective function of CML: s(p, q) = ∥p − q∥ 2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Geometric Comparisons of Latent Relational Metric Learning (LRML) and CML (Collaborative Metric Learning) for Modeling User-Item Relationships in Metric Space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of our proposed LRML architecture, an end-to-end differentiable neural architecture. LRML is characterized by its key-addressed LRAM module which learns user-item specific relation vectors. The size of the memory N=6 slices in this example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Runtime (seconds/epoch) of all models on Netflix Prize and MovieLens20M. Experiments were run with batch size of 100 on a Nvidia P100 GPU. LRML only incurs a small computational cost over CML. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Attention weights over LRAM for user-item pairs of different ratings on MovieLens1M. LRML is able to model explicit rating information despite being only trained on implicit data. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Attention weights over LRAM for user-item pairs for different time bins on MovieLens1M. A clear trend is found in M6 which shows that the LRAM module encodes temporal information even when no such information is provided during training. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Our model optimizes for ∥ p + r − q ∥ ≈ 0 using pairwise ranking (hinge loss) and negative sampling.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pairwise Hinge Loss</cell></row><row><cell>Relation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Modeling Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Relation Layer</cell><cell></cell><cell>Translation Layer</cell></row><row><cell></cell><cell></cell><cell cols="2">| ! + # − &amp; |</cell><cell></cell><cell>| !′ + # − &amp;′ |</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Latent</cell><cell></cell></row><row><cell>LRAM</cell><cell></cell><cell></cell><cell></cell><cell>Relation Vector #</cell><cell>Negative Sampling</cell></row><row><cell cols="2">Memory Slices</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>User</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Vector !′</cell></row><row><cell>M1</cell><cell>M2</cell><cell>M3</cell><cell>M4</cell><cell>M5</cell><cell>M6</cell></row><row><cell>Softmax</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Item Vector &amp;′</cell></row><row><cell>K1</cell><cell>K2</cell><cell>K3</cell><cell>K4</cell><cell>K5</cell><cell>K6</cell></row><row><cell>User-Item Keys</cell><cell></cell><cell></cell><cell cols="2">Inner Product</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Joint Embedding</cell><cell></cell><cell></cell></row><row><cell cols="2">User Vector !</cell><cell></cell><cell></cell><cell cols="2">Item Vector &amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Hadamard</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Product</cell><cell></cell><cell></cell></row><row><cell cols="3">User Embedding Layer</cell><cell cols="3">Item Embedding Layer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of all datasets used in our experimental evaluation.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">#Users #Items % Density</cell></row><row><cell>Netflix Prize</cell><cell>44M</cell><cell>75K</cell><cell>13K</cell><cell>4.5</cell></row><row><cell>MovieLens20M</cell><cell>16M</cell><cell>53K</cell><cell>27K</cell><cell>1.1</cell></row><row><cell>MovieLens1M</cell><cell>1M</cell><cell>6K</cell><cell>4K</cell><cell>4.2</cell></row><row><cell>IMDb</cell><cell>117K</cell><cell>0.8K</cell><cell>114K</cell><cell>0.13</cell></row><row><cell>LastFM</cell><cell>92K</cell><cell>1.9K</cell><cell>175K</cell><cell>0.28</cell></row><row><cell>Books</cell><cell>285K</cell><cell>7.4K</cell><cell>291K</cell><cell>0.01</cell></row><row><cell>Delicious</cell><cell>43K</cell><cell>1.7K</cell><cell>69K</cell><cell>0.36</cell></row><row><cell>Meetup</cell><cell>11K</cell><cell>2.6K</cell><cell>16K</cell><cell>0.26</cell></row><row><cell>Twitter (WW)</cell><cell>18K</cell><cell>4K</cell><cell>36K</cell><cell>0.01</cell></row><row><cell>Twitter (USA)</cell><cell>171K</cell><cell>4K</cell><cell>36K</cell><cell>0.12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b1">(2)</ref> where p, q are the user and item vectors respectively. CML learns via a pairwise hinge loss, which is reminiscent of the Bayesian Personalized Ranking (BPR)<ref type="bibr" target="#b24">[25]</ref>. CML obeys the triangle inequality which, according to the authors, is a prerequisite for fine-grained fitting of users and items in vector space.CML, however, is not without flaws. As mentioned, the scoring function of CML is geometrically restrictive since the objective function tries to fit each user-item pair into the same point in vector space. Unfortunately, this intrinsic geometric inflexibility causes<ref type="bibr" target="#b0">1</ref> In this paper, we use the terms collaborative filtering and collaborative ranking interchangeably.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://grouplens.org/datasets/movielens/ 3 http://last.fm 4 http://www.delicious.com 5 https://grouplens.org/datasets/hetrec-2011/ 6 https://www.meetup.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://www.tensorflow.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">It is good to note that, at initialization, this attention vector looks at all memory slices almost equally irregardless of v.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head><p>The authors would like to thank anonymous reviewers of WWW 2018 for their time and effort in reviewing this paper. We also thank Matt Yang and Kang KyungPhil for feedback on some typo errors in previous preprint versions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Generic Coordinate Descent Framework for Learning from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Immanuel</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052694</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052694" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attentive Collaborative Filtering: Multimedia Recommendation with Item-and Component-Level Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080797</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080797" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-07" />
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623758</idno>
		<ptr target="https://doi.org/10.1145/2623330.2623758" />
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="0202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The MovieLens Datasets: History and Context. TiiS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2827872</idno>
		<idno>19:1-19:19</idno>
		<ptr target="https://doi.org/10.1145/2827872" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translation-based Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/3109859.3109882</idno>
		<ptr target="https://doi.org/10.1145/3109859.3109882" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems (RecSys &apos;17)</title>
		<meeting>the Eleventh ACM Conference on Recommender Systems (RecSys &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">International World Wide Web Conferences Steering Committee, Republic and Canton of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052569</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052569" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web (WWW &apos;17)</title>
		<meeting>the 26th International Conference on World Wide Web (WWW &apos;17)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>Neural Collaborative Filtering</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast Matrix Factorization for Online Recommendation with Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911489</idno>
		<ptr target="https://doi.org/10.1145/2911451.2911489" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-17" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collaborative Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052639</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052639" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="0201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Implicit Feedback Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2008.22</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2008.22" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th IEEE International Conference on Data Mining (ICDM 2008)</title>
		<meeting>the 8th IEEE International Conference on Data Mining (ICDM 2008)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12-15" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582418</idno>
		<ptr target="https://doi.org/10.1145/582415.582418" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<idno type="DOI">10.1145/1401890.1401944</idno>
		<ptr target="https://doi.org/10.1145/1401890.1401944" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08-24" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collaborative Variational Autoencoder for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>She</surname></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3098077</idno>
		<ptr target="https://doi.org/10.1145/3097983.3098077" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-13" />
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-01-25" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-Dimensional Network Embedding with Hierarchical Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM &apos;18)</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining (WSDM &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<title level="m">Key-value memory networks for directly reading documents</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent Models of Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepWalk: online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623732</idno>
		<ptr target="https://doi.org/10.1145/2623330.2623732" />
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-24" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A general graph-based model for recommendation in event-based social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Anh Nguyen</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjie</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2015.7113315</idno>
		<ptr target="https://doi.org/10.1109/ICDE.2015.7113315" />
	</analytic>
	<monogr>
		<title level="m">31st IEEE International Conference on Data Engineering</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04-13" />
			<biblScope unit="page" from="567" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NeuPL: Attention-based Semantic Matching and Pair-Linking for Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">C</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132847.3132963</idno>
		<ptr target="https://doi.org/10.1145/3132847.3132963" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-06" />
			<biblScope unit="page" from="1667" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorization Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2010.127</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2010.127" />
	</analytic>
	<monogr>
		<title level="m">The 10th IEEE International Conference on Data Mining</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>ICDM 2010</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI 2009, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-18" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Badrul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedl</surname></persName>
		</author>
		<idno type="DOI">10.1145/371920.372071</idno>
		<ptr target="https://doi.org/10.1145/371920.372071" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International World Wide Web Conference</title>
		<meeting>the Tenth International World Wide Web Conference<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-05-01" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2736277.2741093</idno>
		<ptr target="https://doi.org/10.1145/2736277.2741093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-18" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM &apos;18)</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining (WSDM &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1712.05403</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-Parametric Estimation of Multiple Embeddings for Link Prediction on Dynamic Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="1243" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dyadic Memory Networks for Aspect-based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00102</idno>
		<ptr target="http://arxiv.org/abs/1801.00102" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-Pointer Co-Attention Networks for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09251</idno>
		<ptr target="http://arxiv.org/abs/1801.09251" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Solutions of ill-posed problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Y</forename><surname>Arsenin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of Computation</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="page" from="491" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-010-5198-3</idno>
		<ptr target="https://doi.org/10.1007/s10994-010-5198-3" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent Recommender Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">How</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017<address><addrLine>Cambridge, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-06" />
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Who, where, when and what: discover spatio-temporal topics for twitter users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2487576</idno>
		<ptr target="https://doi.org/10.1145/2487575.2487576" />
	</analytic>
	<monogr>
		<title level="m">The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-11" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07435</idno>
		<title level="m">Deep learning based recommender system: A survey and new perspectives</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AutoSVD++: An Efficient Hybrid Collaborative Filtering Model via Contractive Auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080689</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080689" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="957" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint Deep Modeling of Users and Items Using Reviews for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017<address><addrLine>Cambridge, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-06" />
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving recommendation lists through topic diversification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cai-Nicolas</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">M</forename><surname>Mcnee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Lausen</surname></persName>
		</author>
		<idno type="DOI">10.1145/1060745.1060754</idno>
		<ptr target="https://doi.org/10.1145/1060745.1060754" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on World Wide Web, WWW 2005</title>
		<meeting>the 14th international conference on World Wide Web, WWW 2005<address><addrLine>Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-05-10" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
