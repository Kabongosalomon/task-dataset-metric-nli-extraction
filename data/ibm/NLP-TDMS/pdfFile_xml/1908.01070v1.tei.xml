<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaloss: Adaptive Loss Function for Landmark Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Teixeira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Digital Services</orgName>
								<orgName type="department" key="dep2">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birgi</forename><surname>Tamersoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Digital Services</orgName>
								<orgName type="department" key="dep2">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Digital Services</orgName>
								<orgName type="department" key="dep2">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Kapoor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Digital Services</orgName>
								<orgName type="department" key="dep2">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaloss: Adaptive Loss Function for Landmark Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Landmark localization is a challenging problem in computer vision with a multitude of applications. Recent deeplearning based methods have shown improved results by regressing likelihood maps (i.e. heatmaps) instead of regressing the coordinates directly. However, setting the precision of these regression targets during the training has been a cumbersome process since it creates a trade-off between trainability vs. localization accuracy. Using precise targets introduces a significant sampling bias and hence makes the training more difficult, whereas using imprecise targets results in inaccurate landmark detectors. In this paper, we introduce "Adaloss", an objective function that adapts itself during the training by updating the target precision based on the training statistics. This approach does not require setting problem-specific parameters and shows improved stability in training and better localization accuracy in inference. We demonstrate the effectiveness of our proposed method in three very different applications of landmark localization: 1) the challenging task of precisely detecting catheter tips in medical X-ray images 1 , 2) localizing surgical instruments in endoscopic images 1 , and 3) localizing facial features on in-the-wild images where we show state-of-the-art results on the 300-W benchmark dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Landmark localization is a fundamental problem in computer vision with far-reaching impact on a myriad of applications in multiple domains. Whether the goal is to analyze the attentiveness of a driver <ref type="bibr" target="#b0">[1]</ref>, interpret sign language <ref type="bibr" target="#b1">[2]</ref>, plan aortic valve surgeries <ref type="bibr" target="#b2">[3]</ref>, estimate body pose for humancomputer interaction <ref type="bibr" target="#b3">[4]</ref>, or detect surgical instruments in endoscopic procedures <ref type="bibr" target="#b4">[5]</ref>, the first step usually involves precise detection of relevant semantic keypoints in the given input images.</p><p>Similar to the other fundamental computer vision problems such as object classification and segmentation, land- (top-right) Facial feature localization, where some landmarks can be defined very precisely (e.g. eye corners), but others cannot be defined as precisely (e.g. chin landmarks). (bottom) Surgical instrument detection in endoscopic procedures, where the inherent occlusions and the viewpoint make it a challenging task. mark localization has also seen a tremendous progress in recent years due to the availability of large datasets (e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>), improvements in training deep neural networks (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>), and availability of more computational resources.</p><p>Most existing deep-learning based methods formulate the landmark localization problem as a structured output regression problem where the landmark locations are either represented as a set of image coordinates <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, or as a set of likelihood maps (i.e. heatmaps) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. While the former representation certainly provides a compact output, it has several undesirable properties such as: 1) Figure 2: Evolution of Adaloss during training. In the early stages, Adaloss uses a large variance for the the target heatmaps. As the training progresses, Adaloss gradually decreases the target variance until reaching the optimal value for the given application.</p><p>forcing the network to learn the arbitrary mapping between the appearance of a landmark and its image coordinates, 2) outputting a single target location for each landmark instead of a number of candidate locations, and 3) forcing the design of explicit mechanisms for handling missing landmarks. As a result, these networks are usually harder to train and their output is less usable as a pre-processing step of more complex vision tasks. On the other hand, training deep networks to regress likelihood maps as landmark locations does not suffer from these limitations.</p><p>A ubiquitous way of defining these target likelihood maps in training is creating multi-variate Gaussian distributions centered at the target landmark locations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. The main concern here is appropriately setting the variance of these Gaussian distributions. Ideally, one would use the smallest possible variance (practically a delta function) while generating these target heatmaps so that the resulting detectors will be as precise and as accurate as possible. However, this does not work in practice. First, there is an inherited uncertainty and inconsistency in the human annotations <ref type="bibr" target="#b24">[25]</ref> that needs to be considered during the training. Furthermore, training deep networks with highly sparse output distributions is difficult. This may be attributed to the significant sample bias in the outputs. With such bias during training, the networks quickly degenerate to all empty outputs and then over the course of the training they attempt to recover peaks around the target locations of various samples. Existing methods try to address these problems by application-dependent and add-hoc solutions, such as using very low learning rates <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref>, arbitrarily weighting the target heatmaps <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, or inevitably sacrificing the inference accuracy by using Gaussian distributions with larger variances.</p><p>In this paper, we introduce a novel, applicationindependent method for landmark localization, which addresses the aforementioned limitations. Our approach, which we refer to as "Adaloss", indirectly adapts the objective function throughout the training by updating the training targets (see <ref type="figure">Figure 2</ref>). Adaloss begins the training by using targets with large variances, and then iteratively adjusts the target variances based on landmark-specific training statistics. Effectively, Adaloss provides an implicit "curriculum" for training.</p><p>Besides being easy to develop and integrate into existing training pipelines, Adaloss also increases the robustness of the training in applications with highly sparse target distributions, is less sensitive to initial learning rate, and requires fewer iterations to converge. It also generates more stable networks <ref type="bibr" target="#b28">[29]</ref> and is capable of producing highly accurate landmark detectors in multiple domains. We demonstrate the effectiveness of our method in three very different applications of landmark localization: 1) detecting catheter tips in X-ray images where a single instance of a single landmark needs to be localized very precisely, 2) facial feature localization where single instances of multiple landmarks need to be localized in varying precision, where we show state-of-the-art results, and 3) detecting surgical instruments in endoscopy images where multiple instances of multiple landmarks need to be localized very precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Availability of large-scale datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> and powerful GPUs allowed deep neural network based methods to outperform former methods on landmarks detection. Toshev et al. were the first to reach state of the art results with a DNN on the FLIC dataset <ref type="bibr" target="#b8">[9]</ref> with DeepPose <ref type="bibr" target="#b29">[30]</ref>. Their approach directly regress the coordinates of the body joints. Later on, Thompson et al. <ref type="bibr" target="#b15">[16]</ref> proposed to focus on regressing heatmaps centered in the body joint locations instead of directly predicting the coordinates. This method outperformed direct coordinate regression method especially on challenging images. The intuition here was that directly regressing coordinates adds unnecessary learning complexity by trying to map RGB to XY coordinates. They also proposed to mix deep convolutional networks with graphical models by jointly training a Markov Random Field to model the correlation between the body keypoints.</p><p>Wei et al. <ref type="bibr" target="#b20">[21]</ref> proposed to use deeper convolutional networks for directly regressing the heatmaps, by using multiple encoding-decoding networks iteratively, with intermediate supervision. This method was then improved by Newell et al. <ref type="bibr" target="#b16">[17]</ref> who extended the networks by adding skip-connections between the convolution and deconvolution modules to better combine the features across multiple scales. Their proposed stacked hourglass architecture is as of today one of the most common architectures for landmark localization networks. Dong et al. <ref type="bibr" target="#b30">[31]</ref> recently proposed a new method for improving temporal consistency for detecting facial landmarks in videos by augmenting the training loss with a registration loss based on optical flows. Honari et al. <ref type="bibr" target="#b17">[18]</ref> investigated the situation where precise annotations are only provided for a small subset. They proposed a multitasking framework which can leverage from imprecise annotations such as regression and classification annotation to improve precise annotation such as landmarks. Finally, Dong et al. <ref type="bibr" target="#b18">[19]</ref> focused on augmenting the training data by introducing a new Generative Adversarial Network (GAN) <ref type="bibr" target="#b31">[32]</ref> to generate a pool of style-aggregated face images, and showed state-of-the-art results on the challenging 300-W face dataset.</p><p>While most recent works focused on improving model architectures and data augmentation, our approach aims to propose a better objective function that may be used in any heatmap regression setup and does not require problem dependent parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adaloss</head><p>Adaloss, like many other deep-learning based landmark localization methods in the literature, represents the landmark locations as heatmaps. These heatmaps are created by placing multi-variate Gaussian distributions at the ground truth landmark locations. However, unlike existing methods, Adaloss does not use fixed target heatmaps throughout the training but instead updates them as the training progresses.</p><p>Adaloss begins the training with less precise (i.e. large variance) targets and iteratively makes them more precise (i.e. decrease the variance) during the training. This may be interpreted as progressively altering the difficulty of the task on hand and hence can be considered as a curriculum method. In the beginning, the network tries to solve an easier problem, and once that easy problem is solved, it moves on to another more difficult problem, but uses what is learned so far as a strong initialization.</p><p>However, to facilitate convergence during training, the curriculum must be appropriately defined. Here, we propose to use a simple additive update equation to set the variance for the target of each landmark, l after each epoch, t (note that, for keeping the notation simple, we are using "standard deviation" and σ, instead of "variance" and Σ in our explanations, but depending on the application the target distributions may be one, two, or three dimensional):</p><formula xml:id="formula_0">σ l t+1 = σ l t + ∆σ l t (1)</formula><p>where, ∆σ l t is the change in the standard deviation for the target of landmark l in epoch t. Note that for multi-variate tasks, we simply employ isotropic targets, where each diagonal element of the target variance is defined as in Equation <ref type="bibr" target="#b0">1</ref>.</p><p>A naive approach would be to set ∆σ l t = −k as a constant decay parameter, linearly decreasing the standard deviation after each epoch. Even though this naive approach may work for some problems it would not generalize well to all problems. For some, the constant decay would be very fast and hence would affect the trainability and for others, it would be very slow making the learning very inefficient. This is also the case when training for regressing multiple landmarks at once. Each of these individual landmark regression problems may have a varying level of difficulty and hence a constant decay parameter would not work for all.</p><p>Thus ∆σ l t should instead be set according to what has been observed in the training so far. Motivated by how AdaDelta <ref type="bibr" target="#b33">[33]</ref> method sets the learning rate, we compute ∆σ l t based on the previous losses in a fixed window of size w. Note that, unlike AdaDelta, we use the actual loss values within this training window, and hence store the complete loss history instead of a running average. The history of the losses is defined as:</p><formula xml:id="formula_1">H l t = [L l t−i ], 0 ≤ i &lt; w<label>(2)</label></formula><p>To characterize the evolution of the loss function, we compute the loss variance V l t which is set to be the variance of the values in the loss history vector H l t :</p><formula xml:id="formula_2">V l t = 1 w w−1 i=0 (L l t−i − E[H l t ]) 2<label>(3)</label></formula><p>If the loss variance is decreasing, it means that the loss function has not changed significantly during the last w epochs. This suggests that the current landmark localization task (with the current standard deviation values) has converged or is close to convergence. In this case, it would help to decrease the standard deviation and attempt to solve a more difficult task. To this end, we compute the ratio of the previous variance V l t−1 with the current variance V l t and use it as our standard deviation decay:</p><formula xml:id="formula_3">∆σ l t = 1 − V l t−1 V l t<label>(4)</label></formula><p>For numerical stability, we also introduce a regularizer ρ and finally, putting it all together, we obtain the following update equation for the target standard deviations:</p><formula xml:id="formula_4">σ l t+1 = σ l t + ρ · 1 − V l t−1 V l t<label>(5)</label></formula><p>where we set σ l 0 to be the quarter of the target resolution. This provides a good starting point as it addresses the sample bias problem associated with standard heaptmap regression methods.</p><p>Note that in Equation 5 an increase in the loss variance would cause a corresponding increase in the target standard deviation. On one hand this may help escaping from a local minimum, but on the other hand it could possibly lead to divergence with an overly relaxed regression task. Thus, in practice, we take into consideration the training loss in determining whether to apply a non-increasing update restriction rule or not.</p><p>This especially plays an important role while training multiple landmarks. Since the landmarks may be of varying difficulty, σ l for one landmark may reduce faster than others. Without the non-increasing update rule, we could have situations where the training is stuck in an oscillatory behavior. However, the network might need to compromise on the precision of some landmarks during the optimization process. In this case, the network may choose to focus on a particular set of landmarks to the detriment of the others, for which the loss may stagnate or increase. It might thus be useful to allow Adaloss to take a step back and increase the standard deviation for a given landmark when the loss variance increases together with the training loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluated our method on three very different landmark localization problems: catheter tip detection in X-ray images (single instance of a single landmark that needs to be localized very precisely), facial feature localization (single instances of multiple landmarks where the achievable precision is landmark-dependent), and surgical instrument </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Technical details</head><p>For all experiments, we used popular network architectures from respective domains and ensured the initial parameters are set appropriately when comparing with and without Adaloss. The models were trained and validated using Py-Torch <ref type="bibr" target="#b34">[34]</ref>. All our experiments used the same following Adaloss parameters, w = 3, ρ = 0.9 and σ 0 set to a quarter of the image resolution.</p><p>For the catheter tip detection task, we used a U-Net architecture <ref type="bibr" target="#b9">[10]</ref> with 9 convolutional layers with a fixed kernel size of 5 using Batch Normalization <ref type="bibr" target="#b10">[11]</ref> and Rectified Linear Unit (ReLU) <ref type="bibr" target="#b12">[13]</ref>. We trained the network for 100 epochs.</p><p>For the face landmark detection task, we used a Dense U-Net architecture <ref type="bibr" target="#b35">[35]</ref> with 23 convolutional layers with a fixed kernel size of 11 using Batch Normalization and ReLU. We trained the network for 150 epochs.</p><p>For the endoscopy instruments detection, we used a U-Net architecture <ref type="bibr" target="#b9">[10]</ref> with 9 convolutional layers using Batch Normalization and Leaky ReLU. We trained the network for 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Single instance of a single landmark</head><p>Our first experiment is conducted on a medical dataset of 3300 fluoroscopy scans (X-ray images) annotated with a single landmark which marks the tip of a catheter 2 . We refer to this dataset as "Cathdet". 3000 images are used for training and 300 are used for testing. The output of the network is a heatmap with resolution of 256x256.</p><p>We first examined the benefits of Adaloss in terms of training stability. As setting the learning rate also known <ref type="figure">Figure 4</ref>: Evolution of the Euclidian distance on the CathDet validation set for multiple fixed values of sigma and Adaloss. Left figure is using the Adam optimizer with a learning rate of 10 −3 and on the right using a learning rate of 10 −4 . While the models with fixed sigmas can only train with the lower learning rate, models trained with Adaloss trains faster and better with higher learning rate.</p><p>to have a significant impact on training stability, a natural comparison to our method is to use an adaptive learning rate method such as Adadelta <ref type="bibr" target="#b33">[33]</ref>. However, as illustrated in <ref type="figure" target="#fig_1">figure Figure 3</ref>, only adapting the learning rate was not sufficient for handling problems with significant sample bias.</p><p>In addition to using a smaller learning rate, another possibility could be to increase sigma. However, finding the appropriate value for sigma highly depends on the problem and while using higher sigma certainly helps to stabilize the training, it also increase tolerance to errors in prediction and ends up altering the final accuracy.</p><p>We compared these approaches to Adaloss and trained multiple models with different learning rates and different values for sigma ( <ref type="figure">Figure 4</ref>). Here, when using the Adam optimizer with a learning rate of 10 −3 , models with fixed sigmas are not able to train, even with high values such as σ = 40. When decreasing the learning rate by a factor of 10, models with fixed sigmas no longer diverge, at the cost of slower training and lower accuracy (except for the one with σ = 5 which still could not train). In comparison, models trained with Adaloss can train with both learning rates, and converges faster and better with the higher learning rate. The best non-adaptive method here is using σ = 40. The network which is trained with σ = 60 starts with a better accuracy, but the error starts to increase after a certain number of epochs, due to the excessive tolerance. <ref type="table" target="#tab_0">Table 1</ref> shows best results on testing set for all values of sigmas and Adaloss after 150 epochs. Model trained with Adaloss shows the lowest 1.19 Euclidian distance.</p><p>Finally, we investigated the evolution of the standard deviation across the training ( <ref type="figure" target="#fig_4">Figure 6</ref>). After 60 epochs, the model converges and the standard deviation stops decreas-   ing. This shows that Adaloss has found the optimal value for sigma for this problem. Here, the value of the final standard deviation is about 5 pixels. We found this value to be representative of the variance in the ground truth annotations of the tip of the catheter. The variance here is very small as the target defines a precise point on the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring the kernel smoothness</head><p>Recently, Ruderman et al. <ref type="bibr" target="#b28">[29]</ref> explored how filter smoothness is an important factor for determining the stability of convolutional networks to deformation. They presented a method for measuring filter smoothness using Normalized Total Variation (NTV) and show how this metric correlates with the ability of the network to learn the deformation bias. We computed this metric on best networks with and without Adaloss and reported the NTV scores in <ref type="table" target="#tab_1">Table 2</ref>. Model trained with Adaloss presents a much smaller NTV (2.80 vs 3.03), indicating a potential impact of using Adaloss on filter smoothness.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Single instances of multiple landmarks</head><p>We conducted our second experiment on the 300-W dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>. This dataset is an aggregation of five face datasets annotated with 68 landmarks: LFPW <ref type="bibr" target="#b38">[38]</ref>, AFW <ref type="bibr" target="#b6">[7]</ref>, HELEN <ref type="bibr" target="#b39">[39]</ref>, XM2VTS and IBUG. It is often used as a benchmark dataset for evaluating facial feature localization methods <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b43">42]</ref>. We followed the same splitting as in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b44">43]</ref>, and used all training samples from LFPW and HELEN and the whole AFW dataset for training. Testing samples from LFPW and HELEN are used as the "common" testing set, while IBUG testing set is considered as "challenging". This splitting results in a training set of 3148 images, a common testing set of 554 images and a challenging set of 135 images. The union of the common and the challenging set is used as the "full" testing set.</p><p>When working on multi-landmarks problems, the difficulty resides in the difference between the landmarks. For a given set of landmarks, some of them might have well defined features and a small variance in the annotation (e.g. eye landmarks) while some others might have varying features (this is the case for the mouth, which could be either open or closed), or may have a large variance in the annotation as for the jaw which defines a broad region of the face.</p><p>Adaloss addresses this issue by using different standard deviations for different landmarks. Here, the update rule is the same as for the single landmark case, except that the loss is computed per landmark, meaning that at a given step in the training, one landmark can see its standard deviation decrease while the other ones might not. To further increase the impact of having varying target distributions, we normalize each target heatmap with its mean. <ref type="figure" target="#fig_6">Figure 7</ref> shows how Adaloss evolves during training. We aggregated the evolution of the standard deviation for different regions. Jaw and eyebrows are difficult landmarks: jaw landmarks suffer from significant variance in the annotations and eyebrows landmarks are very sensitive to face orientation. It is interesting to see that the standard deviation decreased slowly for these landmarks and kept a relatively high standard deviation at convergence. On the other hand, the standard deviation decreased faster for the mouth landmarks, and even faster and lower for the eyes and nose landmarks, which are less subject to variance in annotations.</p><p>These observations suggest that Adaloss was indeed able to adapt the standard deviation for the landmarks with regard to their difficulty. We trained our Dense U-Net end-to-end minimizing the Mean Squared Error (MSE) using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with an initial learning rate of 10 −4 . We augmented the training set using rotations from −30 to +30 degrees. We used the ground truth bounding box to prepare the input for our network at a resolution of 256x256. <ref type="table" target="#tab_3">Table 3</ref> compares the normalized mean errors (NME) with recent state-of-theart methods. Following these works, we use the inter-ocular distance (IOD) to normalize the mean error. Our network trained with Adaloss shows the best results on all testing sets. On the common test set, our approach outperforms the previous best method with a NME of 2.76 vs 3.28. On the challenging set, our method reached a NME of 5.61 vs 6.60. Overall NME on the full test is 3.31 vs 3.98. We also compare our approach with the widely used Dlib <ref type="bibr" target="#b45">[44]</ref>, which was also initialized using the provided ground truth bounding box from 300W. Again, our approach presents better results on all testing sets, and especially on the challenging one (5.61 vs <ref type="bibr">19.39)</ref>, showing the robustness of our approach. <ref type="figure" target="#fig_7">Figure 8</ref> shows qualitative results from our method compared to <ref type="bibr" target="#b18">[19]</ref> on difficult samples from the challenging set. Results from <ref type="bibr" target="#b18">[19]</ref> were generated using provided code and models. Our method was able to better address the occlusion issues and learned a better model of the face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multiple instances of multiple landmarks</head><p>For our final experiment, we used the MICCAI 2015 challenge EndoVis dataset <ref type="bibr" target="#b50">[48]</ref>, a high resolution (576 x 720) intra-operative dataset of laparoscopic images with annotated instruments <ref type="bibr" target="#b51">[49]</ref>.</p><p>In this experiment, we focused on only two landmarks: "Right Clasper" and "Left Clasper", which correspond to the extremities of the tool <ref type="figure" target="#fig_8">(Figure 9</ref>). These landmarks are the most important to track as they define the parts of the tool that are interacting with the tissues. Here again, landmarks are represented with Gaussian heatmaps, but in this case, there can be multiple Gaussians in each target heatmap, since multiple instances of a particular tool type may appear in the frames simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Common Challenging Full LBF <ref type="bibr" target="#b46">[45]</ref> 4   When detecting multiple instances of a landmark on a given image, it is not possible to use argmax anymore to get the final coordinates. We first applied a median filter with a kernel dimension of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b6">7]</ref> followed by non-maxima suppression (NMS) using a window size of 10 pixels. Finally, the centers of the remaining clusters of points correspond to the localization of the detected landmarks. The EndoVis dataset provides images of two different tools: "Curved Scissor" and "Needle Driver". Current stateof-the-art method for detecting instruments in endoscopic images uses a multi-stage approach. It firsts locates joints and association between joints using a detection network, then it refines these joints through a regression network and then retrieves the final location using maximum bipartite graph matching <ref type="bibr" target="#b51">[49]</ref>. While this approach performs landmark detection on multiple instruments with no distinction, it is often desirable to distinguish one instrument from another.</p><p>We trained our network end-to-end minimizing the Mean Squared Error (MSE) using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with an initial learning rate of 10 −3 . We computed Precision, Recall and Euclidian distance of our predictions on the testing set of 910 images. This testing set is composed of images with both curved scissors and needle drivers, for which there may be multiple instances in the same frame. Results can be seen in <ref type="table" target="#tab_5">Table 4</ref>. Our model performs a left clasper precision of 90.50% with a recall of 95.56% on the needle driver. In comparison, the multi-stage approach from <ref type="bibr" target="#b51">[49]</ref> performs a left clasper precision of 86.65% with a recall of 86.28% on both tools with no distinction.</p><p>One important thing to note is that the curved scissor is not present in the training set. Our end-to-end network was thus trained to detect only needle drivers. To investigate the robustness of our network and its ability to distinguish tools, we evaluated our approach on two subsets, one composed of 308 images with needle driver only, and other composed of 301 samples where each image contains both needle drivers and curved scissors. On the first subset, our model detected the claspers with a mean precision of 96.56% across all landmarks. On the second subset, the mean precision dropped to 92.16%. While this decrease in precision is expected due to the presence of unseen tools, it is interesting to see that the mean precision is still over 90%, showing that our model was able to learn discriminative features to distinguish different instruments. Qualitative results from these subsets can be found in <ref type="figure" target="#fig_0">Figure 10</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a novel, application-independent approach, "Adaloss", for training deep neural networks for landmark localization. Our method systematically increases the problem difficulty during training and as a result implicitly creates a training curriculum. It addresses the sample bias problem in training, which is arguably the main limitation of the existing heatmap regression methods. We demonstrated the effectiveness of our method on three challenging domains: detecting catheter tips in X-ray images, localizing surgical instruments in endoscopy images, and facial feature localization on in-the-wild images, where we significantly advance the state-of-the-art. While progressively updating the objective function for landmark regression appears to be effective, the viability of such a method has not been explored for other regression or classification tasks and is yet open to discussion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>From one task to another, landmark localization has far reaching impact on multiple domains. (top-left) Detection of the tip of a catheter in a medical X-ray image, where the tolerance for practical usefulness is about 1 pixel at 256 x 256 resolution and annotations are very precise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Evolution of the euclidian distance on the CathDet validation set using Adadelta with and without Adaloss. detection in endoscopy images (multiple instances of multiple landmarks that needs to be localized with minimal false-positives).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>σ = 5 σ</head><label>5</label><figDesc>= 25 σ = 40 σ = 60 Adaloss Dist. 87.05 19.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Example of a challenging sample from the validation set. In the early training (left image), the proposed method generates targets with a large variance. At convergence (middle image), target variance becomes much smaller and precisely defines the tip of the catheter. Right image presents the ground truth catheter tip position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Evolution of the standard deviation during the training of the catheter tip detection model. At convergence, sigma stops decreasing, showing that it has reached the optimal value (Here around 5 pixels). This final value may represent the variance in the annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Evolution of the standard deviation for landmarks in different regions. Well defined regions such as eyes and nose decrease fast while locations subject to occlusions or variance in annotations decrease slower.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Example of difficult samples from the challenging dataset. Our method is more robust to occlusions and better captures local features even at low resolution, or in bad lighting conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Example of multiple instances of a Needle Driver from the testing set. Landmarks (A) define the Right Clasper and landmarks (B) the Left Clasper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Results from the proposed approach on both EndoVis testing set. Left and center images show results from the testing set without Scissors. Right image shows a result from the full testing set where the Curved Scissor was not misinterpreted as a Needle Driver.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Euclidian distance on CathDet testing set for multi- ple values of sigma compared to Adaloss.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Normalized Total Variation (NTV) score with and without Adaloss. 'all' represents the aggregate NTV on all the layers, and 'last' the NTV on the final convolutional layer. Model trained using Adaloss has smoother filters, indicating potential improvement in network stability.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Normalized mean errors (NME) on the 300-W dataset. Proposed approach shows best results on all testing sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>/ 98.86 / 7.23 96.59 / 94.26 / 6.38 With Scissors 98.02 / 97.10 / 7.21 97.37 / 87.23 / 6.56 Full set 95.56 / 90.50 / 6.83 96.38 / 81.20 / 6.61</figDesc><table><row><cell></cell><cell>Left Clasper</cell><cell>Right Clasper</cell></row><row><cell>No Scissor</cell><cell>93.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results of the Adaloss model on different testing set of EndoVis: Recall (%) / Precision (%) / Euclidian distance (px).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This feature is based on research and is not commercially available. Due to regulatory reasons its future availability cannot be guaranteed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Due to compliance reasons, this dataset is not publicly available.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facial expression analysis for predicting unsafe driving behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Jabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Pontikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Nass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hand landmarks detection and localization in color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Grzejszczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kawulok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Galuszka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="16363" to="16387" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic aorta segmentation and valve landmark detection in c-arm ct: application to aortic valve implantation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nottling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kempfert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brokmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<idno>2012. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Medical Imaging</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient human pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2821" to="2840" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Articulated multiinstrument 2-d pose estimation using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kurmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Lin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1276" to="1287" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stefanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Comput</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanggeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<idno>2017. 1</idno>
		<title level="m">Facial landmark detection with tweaked convolutional neural networks. T-PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation. CoRR, abs/1406.2984</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1708.01101</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multicontext attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<idno>abs/1803.09894</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical study into annotator agreement, ground truth estimation, and algorithm evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garcarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Landmark localisation in radiographs using weighted heatmap displacement voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisang</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In MICCAI Workshop on Comp. Methods and Clinical Apps. in Musculoskeletal Imaging</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attaining human-level performance with atlas location autocontext for anatomical landmark detection in 3d ct data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison Q O&amp;apos;</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antanas</forename><surname>Kascenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wyeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Clunie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carrie</forename><surname>Sansom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evelina</forename><surname>Šeduikytė</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Muir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learned deformation stability in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avraham</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4659</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervision-by-Registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">H-denseunet: Hybrid densely connected unet for liver and liver tumor segmentation from CT volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<idno>abs/1709.07330</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A semi-automatic methodology for facial landmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMFG</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Vuong Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012 -12th</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment with a single CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face alignment via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1233" to="1245" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<title level="m">Proceedings, Part VI</title>
		<meeting>Part VI<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recurrent 3d-2d dual learning for large-pose facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="1642" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Endoscopic vision challenge: Instrument segmentation and tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Articulated multi-instrument 2-d pose estimation using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kurmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Lin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1276" to="1287" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
