<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Part-aware Prototype Network for Few-shot Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyi</forename><surname>Zhang</surname></persName>
							<email>zhangxy9@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
							<email>zhangsy1@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Part-aware Prototype Network for Few-shot Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot semantic segmentation aims to learn to segment new object classes with only a few annotated examples, which has a wide range of real-world applications. Most existing methods either focus on the restrictive setting of one-way few-shot segmentation or suffer from incomplete coverage of object regions. In this paper, we propose a novel few-shot semantic segmentation framework based on the prototype representation. Our key idea is to decompose the holistic class representation into a set of part-aware prototypes, capable of capturing diverse and fine-grained object features. In addition, we propose to leverage unlabeled data to enrich our part-aware prototypes, resulting in better modeling of intra-class variations of semantic objects. We develop a novel graph neural network model to generate and enhance the proposed part-aware prototypes based on labeled and unlabeled images. Extensive experimental evaluations on two benchmarks show that our method outperforms the prior art with a sizable margin. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a core task in modern computer vision with many potential applications ranging from autonomous navigation <ref type="bibr" target="#b3">[4]</ref> to medical image understanding <ref type="bibr" target="#b5">[6]</ref>. A particular challenge in deploying segmentation algorithms in real-world applications is to adapt to novel object classes efficiently in dynamic environments. Despite the remarkable success achieved by deep convolutional networks in semantic segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">38]</ref>, a notorious disadvantage of those supervised approaches is that they typically require thousands of pixelwise labeled images, which are very costly to obtain. While much effort has been made to alleviate such burden on data annotation, such as weak supervision <ref type="bibr" target="#b14">[15]</ref>, most of them still rely on collecting large-sized datasets.</p><p>A promising strategy, inspired by human visual recognition <ref type="bibr" target="#b24">[25]</ref>, is to enable the algorithm to learn to segment a new object class with only a few annotated examples. Such a learning task, termed as few-shot semantic segmentation, has attracted much attention recently <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21]</ref>. Most of those initial attempts adopt the metric-based meta-learning framework <ref type="bibr" target="#b31">[32]</ref>, in which they first match learned features from support and query images, and then decode the matching scores into final segmentation.</p><p>However, the existing matching-based methods often suffer from several drawbacks due to the challenging nature of semantic segmentation. First, some prior works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref> solely focus on the task of one-way few-shot segmentation. Their approaches employ dense pair-wise feature matching and specific decoding networks to generate segmentation, and hence it is non-trivial or computationally expensive to generalize to the multi-way setting. Second, other prototype-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> typically use a holistic representation for each semantic class, which is difficult to cope with diverse appearance in objects with different parts, poses or subcategories. More importantly, all those methods represent a semantic class based on a small support set, which is restrictive for capturing rich and fine-grained feature variations required for segmentation tasks.</p><p>In this work, we propose a novel prototype-based few-shot learning framework of semantic segmentation to tackle the aforementioned limitations. Our main idea is to enrich the prototype representations of semantic classes in two directions. First, we decompose the commonly used holistic prototype representation into a small set of part-aware prototypes, which is capable of capturing diverse and fine-grained object features and yields better spatial coverage in semantic object regions. Moreover, inspired by the prior work in image classification <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1]</ref>, we incorporate a set of unlabeled images into our support set so that our part-aware prototypes can be learned from both labeled and unlabeled data source. This enables us to go beyond the restricted small support set and to better model the intra-class variation in object features. We refer to this new problem setting as semi-supervised few-shot semantic segmentation. Based on our new prototypes, we also design a simple and yet flexible matching strategy, which can be applied to either one-way or multi-way setting.</p><p>Specifically, we develop a deep neural network for the task of few-shot semantic segmentation, which consists of three main modules: an embedding network, a prototypes generation network and a part-aware mask generation network. Given a few-shot segmentation task, our embedding network module first computes a 2D conv feature map for each image. Taking as input all the feature maps, the prototype generation module extracts a set of part-aware representations of semantic classes from both labeled and unlabeled support images. To achieve this, we first cluster object features into a set of prototype candidates and then use a graph attention network to refine those prototypes using all the support data. Finally, the part-aware mask generation network fuses the score maps generated by matching the part-aware prototypes to the query image and predicts the semantic segments. We train our deep network using the metalearning strategy with an augmented loss <ref type="bibr" target="#b33">[34]</ref> that exploits the original semantic classes for efficient network learning.</p><p>We conduct extensive experiments evaluation on the PASCAL-5 i <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref> and COCO-20 i dataset <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref> to validate our few-shot semantic segmentation strat-egy. The results show that our part-aware prototype learning outperforms the state of the art with a large margin. We also include the detailed ablation study in order to provide a better understanding of our method.</p><p>The main contribution of this work can be summarized as the following:</p><p>-We develop a flexible prototype-based method for few-shot semantic segmentation, achieving superior performances in one-way and multi-way setting. -We propose a part-aware prototype representation for semantic classes, capable of encoding fine-grained object features for better segmentation. -To better capture the intra-class variation, we leverage unlabeled data for semi-supervised prototype learning with a graph attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot Classification</head><p>Few-shot learning aims to learn a new concept representation from only a few annotated examples. Most of existing works can be categorized into metriclearning based <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>, optimization-learning based <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>, and graph neural network <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> based methods. Our work is inspired by the metric-learning based methods. In particular, Oriol et al. <ref type="bibr" target="#b31">[32]</ref> propose to encode an input into an embedded feature and to perform a weighted nearest neighbor matching for classification. The prototypical network <ref type="bibr" target="#b27">[28]</ref> aims to learn a metric space in which an input is classified according to its distance to class prototypes. Our work is in line with the prototypical network, but we adopt this idea in more challenging segmentation tasks, enjoying a simple design and yet high performance.</p><p>There have been several recent attempts aiming to improve the few-shot learning by incorporating a set of unlabeled data, referred to as semi-supervised few-shot learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref>. Ren et al. <ref type="bibr" target="#b23">[24]</ref> first try to leverage unlabeled data to refine the prototypes by Soft K-means. Ayyad et al. <ref type="bibr" target="#b0">[1]</ref> introduced a consistency loss both in local and global for utilizing unlabeled data effectively. These methods are initially proposed for solving semi-supervised problems in few-shot classification regime and hence it is non-trivial to extend them to few-shot segmentation directly. We are the first to leverage unlabeled data in the challenging few-shot segmentation task for capturing the large intra-class variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-shot Semantic Segmentation</head><p>Few-shot semantic segmentation aims to segment semantic objects in an image with only a few annotated examples, and attracted much attention recently. The existing works can be largely grouped into two types: parametric matchingbased methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref> and prototype-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>. A recent exception, MetaSegNet <ref type="bibr" target="#b28">[29]</ref>, adopts the optimization-based few-shot learning strategy and formulates few-shot segmentation as a pixel classification problem.</p><p>In the parametric-matching based methods, Shaban et al. <ref type="bibr" target="#b2">[3]</ref> first develop a weight imprinting mechanism to generate the classification weight for few-shot segmentation. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> propose to concatenate the holistic objects representation with query features in each spatial position and introduce a dense comparison module to estimate their prediction.The subsequent method, proposed by Zhang et al. <ref type="bibr" target="#b34">[35]</ref>, attends to foreground features for each query feature with a graph attention mechanism. These methods however mainly focus on the restrictive one-way few-shot setting and it is computationally expensive to generalize them to the multi-way setting.</p><p>Prototype-based methods conduct pixel-wise matching on query images with holistic prototypes of semantic classes. Wang et al. <ref type="bibr" target="#b32">[33]</ref> propose to learn classspecific prototype representation by introducing the prototypes alignment regularization between support and query images. Siam et al. <ref type="bibr" target="#b26">[27]</ref> adopt a novel multi-resolution prototype imprinting scheme for few-shot segmentation. All these prototype-based methods are limited by their holistic representations. To tackle this issue, we propose to decompose object representation into a small set of part-level features for modeling diverse object features at a fine-grained level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Neural Networks</head><p>Our work is related to learning deep networks on graph-structured data. The Graph Neural Networks are first proposed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref> which learn a feature representation via a recurrent message passing process. Graph convolutional networks are a natural generalization of convolutional neural networks to non-Euclidean graphs. Kipf et al. <ref type="bibr" target="#b13">[14]</ref> introduce learning polynomials of the graph laplacian instead of computing eigenvectors to alleviate the computational bottleneck, and validated its effectiveness on semi-supervised learning. Velic et al. <ref type="bibr" target="#b30">[31]</ref> incorporate the attention mechanism into the graph neural network to augment node representation with their contextual information. Garcia et al. <ref type="bibr" target="#b8">[9]</ref> firstly introduce the graph neural network into the few-shot image classification. By contrast, our work employ graph neural network to learn a set of prototypes for the task of semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Setting</head><p>We consider the problem of few-shot semantic segmentation, which aims to learn to segment semantic objects from only a few annotated training images per class. To this end, we adopt a meta-learning strategy <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3]</ref> that builds a meta learner M to solve a family of few-shot semantic segmentation tasks T = {T } sampled from an underlying task distribution P T .</p><p>Formally, each few-shot segmentation task T (also called an episode) is composed of a set of support data S with ground-truth masks and a set of query images Q. In our semi-supervised few-shot semantic segmentation setting, the support data S = {S l , S u } where S l and S u are the annotated image-label pairs and unlabeled images, respectively. More specifically, for the C-way K-shot setting, the annotated support data consists of K image-label pairs from each class, denoted as C T is the subset of class sets for the task T and</p><formula xml:id="formula_0">S l = {(I l c,k , Y l c,k )} c∈C T k=1,...,K , where {Y l c,k } are pixel-wise annotations,</formula><formula xml:id="formula_1">|C T | = C. The unlabeled support images S u = {I u i } Nu i=1</formula><p>are randomly sampled from the semantic class set C with their class labels removed during training and inference. Similarly, the query set</p><formula xml:id="formula_2">Q = {(I q j , Y q j )} Nq j=1</formula><p>, contains N q images from the class set C T whose ground-truth annotations {Y q j } are provided during training but unknown in test. The meta learner M aims to learn a functional mapping from the support set S and a query image I q to its segmentation Y q for all the tasks. To achieve this, we construct a training set of segmentation tasks D tr = {(S n , Q n )} |D tr | n=1 with a class set C tr , and train the meta learner episodically on the tasks in D tr . After the meta-training, the model M encodes the knowledge on how to perform segmentation on different semantic classes across tasks. We finally evaluate the learned model in a test set of tasks D te = {(S m , Q m )} |D te | m=1 whose class set C te is non-overlapped with C tr .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>In this work, we adopt a prototype-based few-shot learning framework to build a meta learner M for semantic segmentation. The main idea of our method is to capture the intra-class variation and fine-grained features of semantic classes by a new prototype representation. Specifically, we propose to decompose the commonly-used holistic representations of support objects into a set of partaware prototypes for each class, and additionally utilize unlabeled data to enrich their representations. To this end, we develop a deep graph network, as our meta learner, to encode such a new representation and to segment the query images. Our network consists of three main networks: an embedding network that computes convolutional feature maps for the images within a task (in Sec. 4.1); a prototypes generation network that extracts a set of part-aware prototypes from the labeled and unlabeled support images (in Sec. 4.2); and a part-aware mask generation network that generates the final semantic segmentation of the query images (in Sec. 4.3).</p><p>To train our meta model, we adopt a hybrid loss and introduce an auxiliary semantic branch that exploits the original semantic classes for efficient learning (in Sec. <ref type="bibr">4.4)</ref>. We refer to our deep model as the Part-aware Prototype Network (PPNet). An overview of our framework is illustrated in <ref type="figure" target="#fig_0">Fig.1</ref> and we will introduce the model details in the remaining of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embedding Network</head><p>Given a task (or episode), the first module of our PPNet is an embedding network that extracts the convolutional feature maps of all images in the task. Following prior work <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, we adopt ResNet <ref type="bibr" target="#b11">[12]</ref> as our embedding network, and introduce the dilated convolution to enlarge the receptive field and preserve more spatial details. Formally, we denote the embedding network as f em , and compute the feature maps of images in a task T as F = f em (I), ∀I ∈ S ∪ Q. Here F ∈ R H f ×W f ×n ch , n ch is the number of feature channels, and (H f , W f ) is the height and width of the feature map. We also resize the annotation mask Y into the same spatial size as the feature map, denoted as</p><formula xml:id="formula_3">M ∈ R H f ×W f .</formula><p>In the C-way K-shot setting, we reshape and group all the image features in the labeled support set S l into C + 1 subsets: F l = {F l k , k = 0, 1, · · · , C}, where 0 indicates background class and F l k contains all the features f ∈ R n ch annotated with semantic class k. Similarly, we denote all the features in the unlabeled support set S u as F u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prototypes Generation Network</head><p>Our second module, the prototypes generation network, aims to generate a set of discriminative part-aware prototypes for each class. For notation clarity, here we focus on a single semantic class k. The module takes the image feature set F l k and F u as input, and outputs the set of prototypes P k .</p><p>To this end, we introduce a graph neural network defined on the features, which computes the prototypes in two steps according to the different nature of the labeled and unlabeled support sets. Specifically, the network first extracts part-aware prototypes directly from the labeled support data F l k and then refines their representation by making use of the unlabeled data F u . As a result, the prototypes generation network consists of two submodules: a Part Generation Module and a Part Refinement Module, which are described in detail as following.</p><p>Part Generation with Labeled Data We first introduce the part generation module, which builds a set of part-aware prototypes from the labeled support set in order to capture fine-grained part-level variation in object regions.</p><p>Specifically, we denote the number of prototypes per class as N p and the pro-</p><formula xml:id="formula_4">totype set P k = {p i } Np i=1 , p i ∈ R n ch .</formula><p>To define our prototypes, we first compute a data partition G = {G 1 , G 2 , · · · , G Np } on the feature set F l k using the K-means clustering and then generate an initial set of prototypesP k = {p i } Np i=1 with an average pooling layer as follows,</p><formula xml:id="formula_5">p i = 1 |G i | j∈Gi f j , f j ∈ F l k<label>(1)</label></formula><p>We further incorporate a global context of the semantic class into the part-aware prototypes by augmenting each initial prototype with a context vector, which is estimated from other prototypes in the same class based on the attention mechanism <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_6">p i =p i + λ p Np j=1∧j =i µ ijpj , µ ij = d(p i ,p j ) j =i d(p i ,p j )<label>(2)</label></formula><p>where λ p is a scaling parameter and µ ij is the attention weight calculated with a similarity metric d, e.g., cosine similarity.</p><p>Part Refinement with Unlabeled Data: The second submodule, the part refinement module, aims to capture the intra-class variation of each semantic class by enriching the prototypes on additional unlabeled support images. However, exploiting the unlabeled data is challenging due to the fact that the set of unannotated image features F u is much more noisy and in general has a much larger volume than the labeled set F l k . We tackle the above two problems by a grouping and pruning process, which yields a smaller and more relevant set of features R u k for class k. Based on R u k , we then design a graph attention network to smooth the unlabeled features and to refine the part-aware prototypes by aggregating those features. Concretely, our refinement process includes the following three steps:</p><p>Step-1: Relevant feature generation. We first compute a region-level feature representation of unlabeled images by utilizing the idea of superpixel generation. Concretely, we apply SLIC <ref type="bibr" target="#b12">[13]</ref> to all the unlabeled images and generate a set of groupings on F u . Denoting the groupings as R = {R 1 , R 2 , · · · , R Nr }, we use the average pooling to produce a pool of region-level features R u = {r i } Nr i=1 . We then select a set of relevant features for class k as follows:</p><formula xml:id="formula_7">R u k = {r j : r j ∈ R u ∧ ∃p i ∈ P k , d(p i , r j ) &gt; σ}<label>(3)</label></formula><p>where d(·, ·) is a cosine similarity function between prototype and feature, and σ is a threshold that determines the relevance of the features.</p><p>Step-2: Unlabeled feature augmentation. With the selected unlabeled features, the second step aims to enhance those region-level representations by incorporating contextual information in the unlabeled feature set. This allows us to encode both local and global cues of a semantic class.</p><p>Specifically, we build a fully-connected graph on the feature set R u k and use the following message passing function to compute the updateR u</p><formula xml:id="formula_8">k = {r i } |R u k | i=1</formula><p>:</p><formula xml:id="formula_9">r i = r i + h   1 Z u i |R u k | j=1∧j =i d(r i , r j )Wr j  <label>(4)</label></formula><p>wherer i represents the updated representation at node i, h is an element-wise activate function (e.g., ReLU). d is a similarity function encoding the relations between two feature vectors r i and r j , and Z u i is a normalization factor for node i. W ∈ R n ch ×n ch is the weight matrix defining a linear mapping to encode the message from node j.</p><p>Step-3: Part-aware prototype refinement. Given the augmented unlabeled features, we refine the original part-aware prototypes with an attention strategy similar to the labeled one. We use the part-aware prototypes P k as attention query to choose similar unlabeled features inR u k and aggregate them into P k :</p><formula xml:id="formula_10">p r i = p i + λ r |R u k | j=1 φ ijrj , φ ij = d(p i ,r j ) j d(p i ,r j )<label>(5)</label></formula><p>where λ r is a scaling parameter and φ ij is the attention weight. The final refined prototype set for class k is denoted as P r k = {p r 1 , p r 2 , · · · , p r Np }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Part-aware Mask Generation Network</head><p>Given the part-aware prototypes {P r k } C k=0 of every semantic class and background in each task, we introduce a simple and yet flexible matching strategy to generate the semantic mask prediction on a query image I q . We denote its conv feature map as F q and its feature column at location (m, n) as f q m,n . Specifically, we first generate a similarity score map for each part-aware prototype performing the prototype-pixel matching as follows,</p><formula xml:id="formula_11">S k,j (m, n) = d(f q m,n , p r j ), p r j ∈ P r k , S k,j ∈ R H f ×W f<label>(6)</label></formula><p>where d is the cosine similarity function and S k,j (m, n) is the score at location (m, n). We then fuse together all the score maps from the class k by max-pooling and generate the output segmentation scores by concatenating score maps from all the classes:</p><formula xml:id="formula_12">S q k = MaxPool({S k,j } Np j=1 ),Ŷ q = {S q k } C k=0<label>(7)</label></formula><p>where indicates concatenation. To generate the final segmentation, we upsample the score outputŶ q by bilinear interpolation and choose the class label with the highest score at each pixel location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Training with Semantic Regularization</head><p>To estimate the parameters of proposed model, we train our PPNet in the metalearning framework. Specifically, we adopt the standard cross-entropy loss to train our entire network on all the tasks in the training set D tr . Inspired by <ref type="bibr" target="#b32">[33]</ref>, we compute the cross-entropy loss on both support and query images. The loss for each task can be written as:</p><formula xml:id="formula_13">L meta = L CE (Ŷ q , Y q ) + L CE (Ŷ l , Y l )<label>(8)</label></formula><p>where L CE is the cross-entropy function, Y l ,Ŷ l are the ground-truth and prediction mask for support image. We note that while our full model is not strictly differentiable w.r.t the embedding network thanks to the prototype clustering and candidate region selection, we are able to compute an approximate gradient by fixing the clustering and selection outcomes. This approximation works well empirically largely due to a well pre-trained embedding network. In order to learn better visual representation for few-shot segmentation, we introduce another semantic branch <ref type="bibr" target="#b33">[34]</ref> for computing a semantic loss defined on the global semantic class space C tr (in contrast to C classes in individual tasks). To achieve this, we augment the network with an Atrous Spatial Pyramid Pooling module (ASPP) decoder to predict mask scoresŶ q sem ,Ŷ l sem of support and query image respectively in the global class space C tr , and compute the semantic loss as below,</p><formula xml:id="formula_14">L sem = L CE (Ŷ q sem , Y q sem ) + L CE (Ŷ l sem , Y l sem )<label>(9)</label></formula><p>Here Y q sem , Y l sem are ground-truth masks defined over shared class space C tr . The overall training loss for each task is:</p><formula xml:id="formula_15">L f ull = L meta + βL sem<label>(10)</label></formula><p>where β is hyper-parameter to balance the weight of task loss and semantic loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our method on the task of few-shot semantic segmentation by conducting a set of experiments on two datasets, including PASCAL-5 i <ref type="bibr" target="#b2">[3]</ref> and COCO-20 i <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>. In each dataset, we compare our approach with the state-ofthe-art methods in terms of prediction accuracy. Below we first introduce the implementation details and experimental configuration in Sec. 5.1. Then we present our experimental analysis on PASCAL-5 i dataset in Sec. 5.2, followed by our results on the COCO-20 i dataset in Sec. 5.3. We report comparisons of quantitative results and analysis on each dataset. Finally, we conduct a series of ablation studies to evaluate the importance of each component of the model in Sec. 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Configuration</head><p>Network Details: We adopt ResNet <ref type="bibr" target="#b11">[12]</ref>, initialized with weights pre-trained on ILSVRC <ref type="bibr" target="#b24">[25]</ref>, as feature extractor to compute the convolutional feature maps. In last two res-blocks, the strides of max-pooling are set as 1 and dilated convolutions are taken with dilation rate 2, 4 respectively. The last ReLU operation is removed for generating the prototypes. The input images are resized into a fixed resolution <ref type="bibr">[417,</ref><ref type="bibr">417]</ref> and horizontal random flipping is used for data augmentation. For the part-aware prototypes network, the typical hyper-parameter of the parts is N p = 5. In the part refinement module, we first generate N r =100 candidate regions on unlabeled data, and select the relevant regions for each semantic class by setting similarity threshold σ as 0. In addition, λ p in Eq. 2 and λ r in Eq. 5 are set to 0.8 and 0.2 respectively, which control the proportion of parts and unlabeled information passed.</p><p>Training Setting: For the meta-training phase, the model is trained with the SGD optimizer, initial learning rate 5e-4, weight decay 1e-4 and momentum 0.9. We train 24k iterations in total, and decay the learning rate 10 times in 10k, 20k iteration respectively. The weight β of semantic loss L sem is set as 0.5. At the testing phase, we average the mean-IoU of 5-runs <ref type="bibr" target="#b32">[33]</ref> with different random seeds in each fold with each run containing 1000 tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline and Evaluation Metrics:</head><p>We adopt ResNet-50 <ref type="bibr" target="#b11">[12]</ref> as feature extractor in PANet <ref type="bibr" target="#b32">[33]</ref> to be our baseline model, denoted as PANet*. Following previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>, mean-IoU and binary-IoU are adopted for model evaluation. Mean-IoU measures the averaged Intersection-over-Union (IoU) of all the classes. Binary-IoU 2 is calculated by treating all object classes as the foreground and averaging the IoU of foreground and background. In our experiments, we mainly focus on mean-IoU metrics for evaluation since it reflects the model generalization ability more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on PASCAL-5 i</head><p>Dataset: The PASCAL-5 i is introduced in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>, which is created from PAS-CAL VOC 2012 dataset with SBD <ref type="bibr" target="#b10">[11]</ref> augmentation. Specifically, the 20 classes <ref type="table">Table 1</ref>. Mean-IoU of 1-way on PASCAL-5 i . * denotes the results implemented by ourselves. MS denotes the model evaluated with multi-scale inputs. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. Red numbers denote the averaged mean-IoU over 4 folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>MS Backbone 1-shot 5-shot #params fold-1 fold-2 fold-3 fold-4 mean fold-1 fold-2 fold-3 fold-4 mean OSLSM <ref type="bibr" target="#b2">[3]</ref> x <ref type="formula" target="#formula_5">VGG16</ref>   in PASCAL VOC is split into 4-folds evenly, each containing 5 categories. Models are trained on three folds and evaluated on the rest using cross-validation.</p><p>Quantitative Results: We compare the performance of our PPNet with the previous state-of-the-art methods. The detail results of 1-way setting are reported in Tab.1. With the ResNet-50 as the embedding network, our model achieves 61.96% mean-IoU in 5-shot setting, which outperforms the state-ofthe-art method with a sizable margin of 2.71%. The performance can be further improved to 52.84%(1-shot) and 62.97%(5-shot) by refining the part prototypes with the unlabeled data. Compared with the PANet <ref type="bibr" target="#b32">[33]</ref>, our method achieves the considerable improvement in both 1-shot and 5-shot setting, which demonstrates the part-aware prototypes can provide a superior representation than the holistic prototype. Moreover, our method achieves the state-of-the-art performance at 65.10% in 5-shot setting relied on the ResNet-101 backbone 3 .</p><p>To investigate the effectiveness of our method on multi-way setting, a 2-way experiment is conducted and the results are reported in Tab.2. Our method can outperform the previous works both in with/without unlabeled data with a large margin. Our model can achieve 51.65% and 61.30% for 1-shot and 5-shot setting, which has 3.32% and 3.30% performance gain compared with PANet*, respectively. The quantitative results indicate our PPNet can achieve the state-of-the-art in a more challenging setting.</p><p>Visualization Analysis: To better understand our part-aware prototype framework, we visualize the responding region of our part prototypes and the prediction results in <ref type="figure" target="#fig_2">Fig.3</ref>. The response heatmaps are presented in the column 4-8 of the (a). For example, given a support image(horse or cat), the part prototypes are corresponding to different body parts, which is capable of modeling one semantic class at a fine-grained level. Moreover, our model can cope with the large appearance and scale variation between support and query images, which is illustrated in (b) and (c). Compared with the PANet*, our method can enhance the modeling capability with the part prototypes, and has a significant improvement on the segmentation prediction by utilizing the unlabeled images to better model the intra-class variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on COCO-20 i</head><p>Dataset: COCO-20 i <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref> is another more challenging benchmark built from MSCOCO <ref type="bibr" target="#b16">[17]</ref>. Similar to PASCAl-5 i , MS-COCO dataset is divided into 4-folds with 20 categories in each fold. There are two splits of MSCOCO: we refer the data partition in <ref type="bibr" target="#b32">[33]</ref> as split-A while the split in <ref type="bibr" target="#b19">[20]</ref> as split-B. We mainly focus on split-A and also report the performance on split-B. Models are trained on three folds and evaluated on the rest with a cross-validation strategy.  Quantitative Results: We report the performance of our method on this more challenging benchmark in Tab.3. Compared with the recent works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>, our method can achieve the state-of-the-art performance in different splits that use the same type of embedding networks by a sizable margin. Compared with the baseline PANet*, the same performance improvement trends are shown in both setting. In split-B <ref type="bibr" target="#b19">[20]</ref>, our model is superior to FWB <ref type="bibr" target="#b19">[20]</ref> nearly in every fold, except for fold-4 in 1-shot, achieving 29.03% in 1-shot and 38.53% in 5-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>In this subsection, we conduct several experiments to evaluate the effectiveness of our model components on COCO-20 i split-A 1-way 1-shot setting.</p><p>Part-aware Prototypes (PAP): As in Tab. 4, by decomposing the holistic object representation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36]</ref> into a small set of part-level representations, the averaged mean-IoU is improved from 22.95% to 23.35%. We further demonstrate the effectiveness of the global context used for augmenting part prototype(in Eq.2). The performance can achieve continuous improvement to 25.02%, which suggests that global semantic is important for part-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Branch (SEM):</head><p>We also conduct experiments to validate the semantic branch <ref type="bibr" target="#b33">[34]</ref>. It is evident that the semantic branch is able to improve the convergence and the final performance significantly, which indicates that the full PPNet exploits the semantic information efficiently. Unlabel Data (UD): We also investigate the graph attention network for the exploitation of the unlabeled data. As discussed in the method, we propose to utilize the graph attention network to refine the part prototypes. We compare the performance of the full PPNet with the PPNet without the GNN module used in step-2. The performance demonstrates the effectiveness of the GNN, and our full PPNet can achieve 27.16% in terms of averaged mean-IoU over 4 folds.</p><p>Hyper-parameters N p , N u and β: We conduct several ablation studies to explore the influence of the hyper-parameters of our PPNet. We first investigate the part number N p on 'baseline+PAP' model and plot the performance curve in <ref type="figure" target="#fig_3">Fig.4(a)</ref>. In our experiments, the highest performance is achieved when N p is 5 and 20 (red line) over 4 folds, and we set N p =5 for computation efficiency. In our semi-supervised few-shot segmentation task, we also investigate the influence of the unlabeled image number N u . In <ref type="figure" target="#fig_3">Fig.4(b)</ref>, we can achieve the highest averaged mean-IoU over 4 folds (red line) with our full PPNet when N u =6. In addition, we also investigate the weight β for semantic loss L sem in our final model during training stage. As shown in <ref type="figure" target="#fig_3">Fig.4(c)</ref>, the optimal value is β=0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we presented a flexible prototype-based method for few-shot semantic segmentation. Our approach is able to capture diverse appearances of each semantic class. To achieve this, we proposed a part-aware prototypes representation to encode the fine-grained object features. In addition, we leveraged unlabeled data to capture the intra-class variations of the prototypes, where we introduce the first framework of semi-supervised few-shot semantic segmentation. We developed a novel graph neural network model to generate and enhance the part-aware prototypes based on support images with and without pixel-wise annotations. We evaluated our method on several few-shot segmentation benchmarks, in which our approach outperforms the prior works with a large margin, achieving the state-of-the-art performance.</p><p>1 Binary-IoU for PASCAL-5 i</p><p>As in Tab.1, our model achieves 70.90%(1-shot) and 77.45%(5-shot) with gain of 1.0% and 6.95% respectively in terms of binary-IoU, compared with PGNet, which validates the superiority of our method when distinguishing complex background. <ref type="table">Table 1</ref>. Averaged binary-IoU over 4 folds of 1-way setting on PASCAL-5 i . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation on Multi-Way Setting</head><p>To demonstrate the model's versatility, we perform more experiments of 2-way and 5-way setting on COCO-20 i . As shown in Tab.3, the results show that our model outperforms the baseline model PANet* by a sizeable margin both with/without unlabeled data. Even in the more challenging 5-way 1-shot setting, our model still improves mean-IoU consistently in each fold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">More Investigation of Graph Attention Network</head><p>We compare the effectiveness of the graph attention network in Sec.4.2 with a non-parametric graph attention network for utilizing unlabeled data. Here the non-parametric graph attention network means the network takes cosine distance as similarity function d in Equ.4, 5, and removes the linear mapping weight W in Equ.4. As in Tab.4, the performance will drop from 27.16% to 26.32%, which suggests that our graph attention network encode meta-knowledge of message propagation, and are effective for capturing informative features from unlabeled data.</p><p>4 More Qualitative Visualization for PASCAL-5 i and COCO-20 i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visualization for PASCAL-5 i</head><p>As in <ref type="figure" target="#fig_0">Fig.1</ref>, our model can cope with the large appearance and scale variations between support and query images by utilizing the unlabeled data, both in 1-way 1-shot and 2-way 1-shot setting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visualization for COCO-20 i</head><p>We also provide more visualization results of 1-way 1-shot setting for COCO-20 i as in <ref type="figure" target="#fig_1">Fig.2</ref>. Our part-aware prototype network is still capable of modeling one semantic class at a fine-grained level and further coping with variations between support and query images in this more challenging benchmark. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Model Overview: For each task T , Embedding Network first aims to prepare convolutional feature maps of support, unlabeled and query images. Prototypes Generation Network then generates a set of part-aware prototypes by taking support and unlabeled image features as input. It consists of two submodules: Part Generation Module and Part Refinement Module (see below for details). Finally, the Part-aware Mask Generation Network performs segmentation on query features based on a set of part-aware prototypes. In addition, Semantic Branch generates mask predictions over the global semantic class space C tr .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Part Generation Module aims to generate the initial part-aware prototypes on support images and further incorporate with their global context of the same semantic class. Part Refinement Module further improves part-aware prototypes representation with unlabeled images features by a graph attention network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative Visualization of 1-way 1-shot setting on PASCAL-5 i . (a) demonstrates the part-aware prototypes response heatmaps. The bright region denotes a high similarity between prototypes and query images. (b) and (c) show the capabilities of our model in coping with appearance and scale variation by utilizing unlabeled data.Red masks denote prediction results of our models. Blue and green masks denote the ground-truth of support and query images. See suppl. for more visualization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Ablation studies of Np parts, Nu unlabeled data and weight β for semantic loss on COCO-20 i split-A 1-way 1-shot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 1 .</head><label>1</label><figDesc>Qualitative Visualization of 1-way 1-shot and 2-way 1-shot on PASCAL-5 i . (a) demonstrates the prediction results in the appearance variation scenario while (b) shows the prediction in scale variation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative Visualization of 1-way 1-shot on COCO-20 i split-A. (a) shows the part prototypes prediction heatmaps. The prediction results in the appearance variation and scale variation scenario are demonstrated in (b),(c) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>33.60 55.30 40.90 33.50 40.80 35.90 58.10 42.70 39.10 43.90 272.6M co-FCN [22] x VGG16 31.67 50.60 44.90 32.40 41.10 37.50 50.00 44.10 33.90 41.40 34.20M SG-one [37] x VGG16 40.20 58.40 48.40 38.40 46.30 41.90 58.60 48.60 39.40 47.10 19.</figDesc><table><row><cell></cell><cell></cell><cell>00M</cell></row><row><cell>AMP [27]</cell><cell cols="2">x VGG16 36.80 51.60 46.90 36.00 42.80 44.60 58.00 53.30 42.10 49.50 15.8M</cell></row><row><cell>PANet [33]</cell><cell cols="2">x VGG16 42.30 58.00 51.10 41.20 48.10 51.80 64.60 59.80 46.50 55.70 14.7M</cell></row><row><cell cols="2">PANet* [33] x</cell><cell>RN50 44.03 57.52 50.84 44.03 49.10 55.31 67.22 61.28 53.21 59.26 23.5M</cell></row><row><cell cols="2">PGNet* [35] x</cell><cell>RN50 53.10 63.60 47.60 47.70 53.00 56.30 66.10 48.00 53.20 55.90 32.5M</cell></row><row><cell>FWB[20]</cell><cell cols="2">x RN101 51.30 64.49 56.71 52.24 56.19 54.84 67.38 62.16 55.30 59.92 43.0M</cell></row><row><cell>CANet [36]</cell><cell></cell><cell>RN50 52.50 65.90 51.30 51.90 55.40 55.50 67.80 51.90 53.20 57.10 36.35M</cell></row><row><cell>PGNet [35]</cell><cell></cell><cell>RN50 56.00 66.90 50.60 50.40 56.00 57.70 68.70 52.90 54.60 58.50 32.5M</cell></row><row><cell cols="2">Ours(w/o S u ) x our x</cell><cell>RN50 47.83 58.75 53.80 45.63 51.50 58.39 67.83 64.88 56.73 61.96 23.5M RN50 48.58 60.58 55.71 46.47 52.84 58.85 68.28 66.77 57.98 62.97 31.5M</cell></row><row><cell>Ours</cell><cell cols="2">x RN101 52.71 62.82 57.38 47.74 55.16 60.25 70.00 69.41 60.72 65.10 50.5M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mean-IoU of 2-way on PSACAL-5 i . * denotes our implementation. Red numbers denote the averaged mean-IoU over 4 folds. RN50 42.82 56.28 48.72 45.53 48.33 54.65 64.80 57.61 54.94 58.00 Ours(w/o S u ) RN50 45.63 58.00 51.65 45.69 50.24 55.34 66.38 63.79 56.85 60.59 Ours RN50 47.36 58.34 52.71 48.18 51.65 55.54 67.26 64.36 58.02 61.30</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell cols="10">1-shot fold-1 fold-2 fold-3 fold-4 mean fold-1 fold-2 fold-3 fold-4 mean 5-shot</cell></row><row><cell>MetaSegNet [29]</cell><cell>RN9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">41.9 41.75 46.31 43.63 43.30</cell></row><row><cell>PANet[33]</cell><cell>VGG16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.10</cell></row><row><cell>PANet*[33]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Mean-IoU results of 1-way on COCO-20 i split-A. Red numbers denote the averaged mean-IoU over 4 folds. * is our implementation 22.58 21.50 16.20 22.95 45.85 29.15 30.59 29.59 33.80 Ours(w/o S u ) A RN50 34.53 25.44 24.33 18.57 25.71 48.30 30.90 35.65 30.20 36.24 Ours A RN50 36.48 26.53 25.99 19.65 27.16 48.88 31.36 36.02 30.64 36.73 FWB [20] B RN101 16.98 17.78 20.96 28.85 21.19 19.13 21.46 23.39 30.08 23.05 Ours B RN50 28.09 30.84 29.49 27.70 29.03 38.97 40.81 37.07 37.28 38.53</figDesc><table><row><cell>Methods</cell><cell cols="2">Split Backbone</cell><cell>1-shot fold-1 fold-2 fold-3 fold-4 mean fold-1 fold-2 fold-3 fold-4 mean 5-shot</cell></row><row><cell>PANet [33]</cell><cell>A</cell><cell cols="2">VGG16 28.70 21.20 19.10 14.80 20.90 39.43 28.30 28.20 22.70 29.70</cell></row><row><cell>PANet* [33]</cell><cell>A</cell><cell cols="2">RN50 31.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation Studies of 1-way 1-shot on COCO-20 i split-A in every fold. Red numbers denote the averaged mean-IoU over 4 folds.</figDesc><table><row><cell>1-shot</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>GFLOPs results</figDesc><table><row><cell>Method</cell><cell>Cg</cell><cell cols="2">Cq Average of N queries</cell></row><row><cell>PANet*</cell><cell cols="2">68.47 68.49</cell><cell>68.47/N +68.49</cell></row><row><cell cols="3">Ours (w/o unlabeled) 69.39 68.57</cell><cell>69.39/N +68.57</cell></row><row><cell>Ours</cell><cell cols="3">479.39 68.57 479.39/N +68.57</cell></row><row><cell cols="4">3 More Quantitative Results on COCO-20 i</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Mean-IoU results of 2-way 1-shot and 5-way 1-shot on COCO-20 i split-A. Red numbers denote the averaged mean-IoU over 4 folds.VGG16  29.88 21.13 20.46 15.37 21.71 24.94 19.85 19.28 14.11 19.55 PANet* [34] RN50 31.86 21.47 21.31 16.43 22.76 27.20 21.50 19.66 15.35 20.93 PPNet(w/o S u ) RN50 33.87 23.98 22.75 17.59 24.55 29.12 22.29 21.10 16.37 22.22 PPNet RN50 34.20 24.21 23.39 19.06 25.22 30.84 23.03 21.32 17.93 23.28</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>2-way, 1-shot fold-1 fold-2 fold-3 fold-4 mean fold-1 fold-2 fold-3 fold-4 mean 5-way, 1-shot</cell></row><row><cell>PANet [34]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies of 1-way 1-shot on COCO-20 i split-A in every fold. Red numbers denote the averaged mean-IoU over 4 folds. 22.58 21.50 16.20 22.95 (w/o params) 36.34 23.59 26.39 18.97 26.32 PPNet 36.48 26.53 25.99 19.65 27.16</figDesc><table><row><cell>1-shot</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is avilable at: https://github.com/Xiangyi1996/PPNet-PyTorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We report Binary-IoU in supplementary material for a clear comparison with the previous works.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We note that our 1-shot performance is affected by the limited representation power of the prototypes learned from a single support image while prior methods<ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> employ a complex Convnet decoder to exploit additional spatial smoothness prior.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Both authors contributed equally to the work. This work was supported by Shanghai NSF Grant (No. 18ZR1425100)</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this material, we firstly report the binary-IoU for a clear comparison with previous works in PASCAL-5 i 1-way setting, and then detail the model complexity of our methods. We futher demonstrate the versatility of our model on multi-way setting, and effectiveness of graph attention network for utilizing unlabeled data, as a supplement to Sec.5.3. Finally, we provide more qualitative visualization results for PASCAL-5 i and COCO-20 i .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayyad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<title level="m">Semi-supervised few-shot learning with local and global consistency. arXiv preprint arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L I E B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference(BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic instance segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep representations of medical images using siamese cnns with application to content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Machine Learning for Health Workshop(NIPS workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference(BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning(ICML)</title>
		<meeting>the 34th International Conference on Machine Learning(ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision(ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast slic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://github.com/Algy/fast-slic" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems(NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision(ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision(ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision(ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Few-shot segmentation propagation with guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<title level="m">Conditional networks for few-shot semantic segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<title level="m">Optimization as a model for few-shot learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Meta-learning for semi-supervised few-shot classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adaptive masked weight imprinting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems(NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Differentiable meta-learning model for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems(NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems(NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<title level="m">Panet: Few-shot image semantic segmentation with prototype alignment. arXiv preprint arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A dual attention network with semantic embedding for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence(AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence(AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision(ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision(ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition(CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">PPNet(RN-101) . Instead, we decompose the computation cost into two parts: prototype generation cost, C g and the inference cost on a query image, C q . Below we report model cost (GFLOPs) of the experiment in Tab. 2. While our method uses more FLOPs in prototype generation due to extra unlabeled data, our inference cost is similar to the PANet*. In each task, as the prototype generation needs to be computed only once, the inference cost will dominate the average computation cost for sufficient number of queries</title>
		<idno type="arXiv">arXiv:2007.06309v2[cs.CV]12</idno>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
	<note>Both authors contributed equally to the work. This work was supported by Shanghai NSF Grant (No. 18ZR1425100</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
