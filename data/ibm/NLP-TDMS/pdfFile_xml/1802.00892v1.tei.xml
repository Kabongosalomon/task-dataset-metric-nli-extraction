<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zheng</surname></persName>
							<email>zhengshiliang0@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
							<email>rxia@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning techniques have achieved success in aspect-based sentiment analysis in recent years. However, there are two important issues that still remain to be further studied, i.e., 1) how to efficiently represent the target especially when the target contains multiple words; 2) how to utilize the interaction between target and left/right contexts to capture the most important words in them. In this paper, we propose an approach, called left-centerright separated neural network with rotatory attention (LCR-Rot), to better address the two problems. Our approach has two characteristics: 1) it has three separated LSTMs, i.e., left, center and right LSTMs, corresponding to three parts of a review (left context, target phrase and right context); 2) it has a rotatory attention mechanism which models the relation between target and left/right contexts. The target2context attention is used to capture the most indicative sentiment words in left/right contexts. Subsequently, the context2target attention is used to capture the most important word in the target. This leads to a two-side representation of the target: left-aware target and right-aware target. We compare our approach on three benchmark datasets with ten related methods proposed recently. The results show that our approach significantly outperforms the state-of-the-art techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-based sentiment analysis is a fine-grained classification task in sentiment analysis, identifying sentiment polarity of a sentence expressed toward a target <ref type="bibr" target="#b9">[Pang and Lee, 2008;</ref><ref type="bibr" target="#b7">Liu, 2012;</ref><ref type="bibr" target="#b9">Pontiki et al., 2014]</ref>. In the early studies, methods for the aspect-based sentiment classification task were similar as that used in standard sentiment classification task. Researchers normally designed a set of features (such as bag-of-words, sentiment lexicons, and linguistic features) to train a statistical learning algorithm for sentiment classification <ref type="bibr" target="#b6">[Kiritchenko et al., 2014;</ref><ref type="bibr">Wagner et al., 2014</ref>; * The corresponding author of this paper. <ref type="bibr" target="#b12">Vo and Zhang, 2015]</ref>. However, such kind of feature engineering work was labor-intensive and almost reached its performance bottleneck. In recently years, more and more researchers have adopted more advanced deep learning algorithms. By taking advantage of the powerful representation ability, well-designed neural networks can automatically generate meaningful low-dimensional representations for the targets and their contexts, and obtained the state-ofthe-art results in aspect-based sentiment classification task <ref type="bibr" target="#b1">[Dong et al., 2014;</ref><ref type="bibr" target="#b13">Wang et al., 2016;</ref><ref type="bibr" target="#b12">Tang et al., 2016a;</ref><ref type="bibr" target="#b12">2016b]</ref>.</p><p>As we have mentioned, aspect-based sentiment classification differs from traditional sentiment classification in that the former is target-related. <ref type="bibr" target="#b3">Jiang et al. [2011]</ref> pointed out that 40% of the classification errors are caused by ignoring the target information in twitter sentiment classification. The sentiment polarity of a sentence is strongly related to its target in aspect-based sentiment analysis. Taking the following sentence Example 1: "I am pleased with the life of battery, but the windows 8 operating system is so bad." for example, the target set is {the life of battery, windows 8 operating system}. As far the target the life of battery is considered, the expected sentiment is positive; by contrast, as far as the target windows 8 operating system is considered, the correct sentiment should be negative. That is, in one review sentence, the sentiment toward different targets could be opposite. Along with the deepening of research work, incorporating the target information into the model gradually becomes a consensus in aspect-based sentiment classification in recent years.</p><p>However, the previous way of modeling targets and contexts still have some shortcomings. For one thing, according to our statistics, more than 25% of the target on the Restaurant and 35% of the target on the Laptop datasets contain at least two words, but almost all researchers ignore the case of target phrase that contains multiple words, and just used the average of target constituting word vectors to represent target. For instance, <ref type="bibr" target="#b12">Tang et al. [2016a]</ref> proposed a target-connection long short-term memory (LSTM) model, which utilizes the connections between target and each context word when composing the representation of a sentence. For another, the rep-resentations of targets and contexts are influenced by each other which is paid not enough attention. Taking Example 1 for example, with respect to the target "the life of battery", "pleased" should be paid with more attention than the other targets not related words (such as "bad") in the context; as for targets, "life" and "battery" should pay more attention in the representation of target "the life of battery". We can see that the representations of contexts are related to targets, meanwhile it is natural that targets are influenced by their contexts.</p><p>In summary, when employing deep neural networks for aspect-based sentiment classification, the following two problems remain to be further studied:</p><p>• Problem 1: how to more efficiently represent the target especially when the target contains multiple words?</p><p>• Problem 2: how to utilize the interaction between targets and contexts to capture the most important words in the representation of targets and contexts?</p><p>With the attempt to better address the two problems, in this paper we propose a left-center-right separated neural network with rotatory attention mechanism (LCR-Rot). Specifically, we design a left-center-right separated LSTMs that contains three LSTMs, i.e., left-, center-and right-LSTM, respectively modeling the three parts of a review (left context, target phrase and right context). On this basis, we further propose a rotatory attention mechanism to take into account the interaction between targets and contexts to better represent targets and contexts. The target2context attention is used to capture the most indicative sentiment words in left/right contexts. Subsequently, the context2target attention is used to capture the most important word in the target. This leads to a two-side representation of the target: left-aware target and right-aware target. Finally, we concatenate the component representations as the final representation of the sentence and feed it into a softmax layer to predict the sentiment polarity.</p><p>The key characteristics of our work can be summarized as follows:</p><p>1. With respect to Problem 1, the target phrase is modeled with two-side representation which is combination of left-aware target and right-aware target. It better support the multi-word targets and leads to a significant improvement of classification performance;</p><p>2. With respect to Problem 2, the rotatory attention mechanism could utilize the interaction between targets and contexts to better represent targets and contexts;</p><p>3. We achieve currently the best aspect-based sentiment classification performance on three benchmark datasets. And we will release our code soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Aspect-based sentiment analysis is a fine-grained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards a target.</p><p>In this work, we focus on the aspect term polarity detection task defined in SemEval 2014: for a given set of labeled aspect terms within a sentence, determine the polarity of each aspect term <ref type="bibr" target="#b9">[Pontiki et al., 2014]</ref>. Traditional approaches to this task normally design effective feature templates by making use of external resources like linguistic parser and sentiment lexicons, and then employ the traditional statistical learning algorithms for prediction <ref type="bibr" target="#b6">[Kiritchenko et al., 2014;</ref><ref type="bibr">Wagner et al., 2014;</ref><ref type="bibr" target="#b12">Vo and Zhang, 2015]</ref>. For example, Vo and <ref type="bibr" target="#b12">Zhang [2015]</ref> manually designed rich features including sentiment-specific word embedding and sentiment lexicons. Although these methods have achieved a comparable performance, their results highly depended on the effect of the handcraft features.</p><p>In recent years, neural network approaches are of growing attention for their capacity of encoding sentence in continuous and low-dimensional vector without feature engineering. Kinds of neural network methods, such as Recursive Neural Network <ref type="bibr" target="#b10">[Socher et al., 2011;</ref><ref type="bibr" target="#b1">Dong et al., 2014;</ref><ref type="bibr" target="#b10">Qian et al., 2015]</ref>, convolutional neural network <ref type="bibr">[Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b5">Kim, 2014]</ref>, LSTM <ref type="bibr" target="#b2">[Hochreiter and Schmidhuber, 1997</ref>] and tree-structured LSTM <ref type="bibr" target="#b11">[Tai et al., 2015]</ref> were applied into the field of sentiment analysis and opinion mining, including aspect-based sentiment classification. However, most of these neural network based approaches just make use of the review context, but ignored the consideration of the target information which is supposed to be very important in indicating the aspect's sentiment polarity.</p><p>The state-of-the-art works in aspect-based sentiment classification pay more attention to incorporating the target information into the model. <ref type="bibr" target="#b12">Tang et al. [2016a]</ref> proposed TD-LSTM and TC-LSTM which develop two target dependent long short-term memory to model the left and right contexts with target, where target information is automatically taken into account. <ref type="bibr" target="#b13">Wang et al. [2016]</ref> proposed an attention-based LSTM to concentrate on different parts of a sentence when different targets are taken as input, but the final improvement is limited. Meanwhile, <ref type="bibr" target="#b12">Tang et al. [2016b]</ref> designed a deep memory networks which consist of multiple computational layers to integrate the aspect information. Each layer is a context-and location-based attention model, which first learns the importance/weight of each context word and then utilizes the information to calculate context representation. <ref type="bibr" target="#b14">Zhang et al. [2016]</ref> used one gated-RNN to learn the representation of sentence in three components and meanwhile use a gated mechanism to leverage the interaction of targets and contexts. <ref type="bibr" target="#b8">Ma et al. [2017]</ref> proposed an interactive attention networks which interactively learn attentions in the contexts and targets. Different from previous models, our model use three LSTMs to model left context, target phrase and right context separately; meanwhile, considering the interaction of targets and contexts, we propose a rotatory attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>The overall architecture of the proposed LCR-Rot model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Left-Center-Right Separated LSTMs</head><p>Firstly, a sentence is separated into three parts, i.e., left context, target phrase and right context. Suppose a sentence s contains N words [w 1 , w 2 , . . . , w N ], and is separated into three parts:</p><formula xml:id="formula_0">left context [w l 1 , w l 2 , . . . , w l L ], target phrase [w t 1 , w t 2 , . . . , w t M ] and right context [w r 1 , w r 2 , . . . , w r R ],</formula><p>where L, M, R is the length of three parts respectively. The sum of L, M, R is equal to N. A unit of the three component parts will be considered as a training/testing example in the network.</p><p>Taking Example 1 for instance, with respect to the target "the life of battery", left context is "i am pleased with", target phrase is "the life of battery", right context is ", but the windows 8 operating system is so bad."; with respect to the target "windows 8 operating system", left context is "i am pleased with the life of battery, but the", target phrase is "windows 8 operating system", right context is "is so bad.".</p><p>Accordingly, the LCR-Rot model is composed of three Bi-LSTMs, i.e., left-, center-, and right-Bi-LSTM, respectively modeling left context, target phrase and right context in the sentence. Specifically, each word is represented as word embedding <ref type="bibr" target="#b0">[Bengio et al., 2003;</ref><ref type="bibr" target="#b8">Mikolov et al., 2013]</ref>. All the word vectors are stacked in a word embedding matrix L w ∈ R d×|V | , where d is the dimension of word vector and |V | is vocabulary size. After we feed word embedding to Bi-LSTM, we can get hidden states</p><formula xml:id="formula_1">[h l 1 , h l 2 , . . . , h l L ] for left context, [h t 1 , h t 2 , . . . , h t M ]</formula><p>for the target phrase and [h r 1 , h r 2 , . . . , h r R ] for right context as the initial representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rotatory Attention Mechanism</head><p>Secondly, a rotatory attention mechanism is designed to capture the most indicative words in target and left/right contexts. The rotatory attention mechanism contains two steps. First step, the target2context attention is used to capture the most indicative sentiment words in left/right contexts; second step, based on the new representations of left/right contexts, the context2target attention is further constructed to capture the most important word in the target and finally get a two-side representation of the target.</p><p>(1) Target2Context Attention</p><p>We first make use of an average representation of the target to obtain better representations of left and right contexts. An average pooling operation is used to obtain the simple representation of target phrase:</p><formula xml:id="formula_2">r t = pooling([h t 1 , h t 2 , . . . , h t M ]).<label>(1)</label></formula><p>In order to obtain the representation of the left and right components respectively, we first define a score function f by using the hidden states of each word in the context h l i (h r i ) and the average pooling of the target phrase r t as inputs (taking left context for example):</p><formula xml:id="formula_3">f (h l i , r t ) = tanh(h l i · W l c · r t + b l c ),<label>(2)</label></formula><p>where W l c and b l c are weight matrix and bias respectively, and tanh is a non-linear function.</p><p>The score f is used as a weight that denotes the importance of a word in the context indicating the sentiment toward a target. On this basis, the normalized importance weight α i in the left contexts are computed as follows:</p><formula xml:id="formula_4">α l i = exp(f (h l i , r t )) L j=1 exp(f (h l j , r t ))</formula><p>.</p><p>( <ref type="formula">3)</ref> At last, a weighted combination of word hidden states is considered as the component representation for left contexts:</p><formula xml:id="formula_5">r l = L i=1 α l i · h l i .<label>(4)</label></formula><p>The same as Equation <ref type="formula" target="#formula_3">(2)</ref>-(4), we can obtain r r for right context.</p><p>(2) Context2Target Attention</p><p>We further make use of the new representations of left/right contexts (r l /r r ), to construct a better representation of the target. Just like target2context attention, we first define a score function f by using the hidden states of each word in the target phrase h t i and the final representations r l /r r of left/right context as inputs (taking left context for example):</p><formula xml:id="formula_6">f (h t i , r l ) = tanh(h t i · W l t · r l + b l t ),<label>(5)</label></formula><p>where W l t and b l t are weight matrix and bias respectively, and tanh is a non-linear function.</p><p>The score f is used as a weight that denotes the importance of a word in the target phrase influenced by left context. On this basis, the normalized importance weight α i are computed as follows:</p><formula xml:id="formula_7">α t l i = exp(f (h t i , r l )) M j=1 exp(f (h t j , r l ))</formula><p>.</p><p>At last, a weighted combination of target phrase hidden states are computed:</p><formula xml:id="formula_9">r t l = M i=1 α t l i · h t i ,<label>(7)</label></formula><p>which we call left-aware target representation. Similar as Equation <ref type="formula" target="#formula_6">(5)-(7)</ref>, we can get the right-aware target representation, r tr . We name the combination of left-aware and right-aware target representations [r t l , r tr ] as the two-side representation of the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Representation Concatenation</head><p>Finally, we concatenate the left-context representation r l , right-context representation r r , and the two-side target representation [r t l , r tr ], and use it as the final representation for the sentence: v = [r l ; r t l ; r tr ; r r ].</p><p>(8) The sentence representation v is feed to a softmax function for aspect-level sentiment prediction:</p><formula xml:id="formula_10">p = softmax(W c · v + b c ),<label>(9)</label></formula><p>where W c and b c are the parameters of the softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>The model is trained in a supervised manner by minimizing the cross entropy error of sentiment classification. The loss function with respect to one training instance is defined as:</p><formula xml:id="formula_11">L = − C i=1 y i log(p i ) + λ || Θ || 2 ,<label>(10)</label></formula><p>where C is the number of class labels; y i is one-hot class labels for the i-th class; p i is the predicted probability for the ith class; λ is weight of L 2 −regularization; Θ is the parameter <ref type="table" target="#tab_0">Train  2164  637  807  Restaurant-Test  728  196  196  Laptop-Train  994  464  870  Laptop-Test  341  169  128  Twitter-Train  1561  3127  1560  Twitter-Test  173</ref> 346 173 and parameters in LSTM. But the initial word embedding vectors are not trained. We use back propagation and stochastic gradient descent optimizer to train the model. The dropout strategy is used to avoid overfitting.</p><formula xml:id="formula_12">set which contains {W l c , b l c , W r c , b r c , W l t , b l t , W r t , b r t , W c , b c } Dataset Pos.(#) Neu.(#) Neg.(#) Restaurant-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>We conduct experiments on three datasets, as shown in <ref type="table" target="#tab_0">Table  1</ref>. The first two are from SemEval 2014 Task 4 1 <ref type="bibr" target="#b9">[Pontiki et al., 2014]</ref>, one from laptop domain and another from restaurant domain. The third one is a collection of tweets, collected by <ref type="bibr" target="#b1">[Dong et al., 2014]</ref>. The evaluation metric is classification accuracy.</p><p>In our work, the dimension of word embedding vectors and hidden state vectors is 300. We use GloVe 2 vectors with 300 dimensions to initialize the word embeddings, the same as <ref type="bibr" target="#b13">[Wang et al., 2016;</ref><ref type="bibr" target="#b12">Tang et al., 2016b]</ref>. All out-ofvocabulary words and weight matrices are randomly initialized by a uniform distribution U(-0.1, 0.1), and all bias are set to zero. TensorFlow is used for implementing our neural network model. In model training, the learning rate is set to 0.1, the weight for L 2 -norm regularization is set to 1e-5, and dropout rate is set to 0.5. We train the model use stochastic gradient descent optimizer with momentum of 0.9. The paired t-test is used for the significance testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Systems</head><p>We compare our LCR-Rot model with the following systems:</p><p>1. Majority assigns the sentiment polarity that has the largest probability in the training set; 2. Simple SVM is a SVM classifier with simple features such as unigrams and bigrams; 3. Feature-enhanced SVM is a SVM classifier with a state-of-  <ref type="bibr" target="#b12">(Tang, 2016a)</ref> 74.30 66.50 66.50 TD-LSTM <ref type="bibr" target="#b12">(Tang, 2016a)</ref> 75.60 68.10 70.80 AE-LSTM <ref type="bibr" target="#b13">(Wang, 2016)</ref> 76.60 68.90 -ATAE-LSTM <ref type="bibr" target="#b13">(Wang, 2016)</ref> 77.20 68.70 -GRNN-G3 <ref type="bibr" target="#b14">(Zhang, 2016)</ref> 79.55* 71.47* 70.09* MemNet <ref type="bibr" target="#b12">(Tang, 2016b)</ref> 79.98* 70.33* 70.52* IAN <ref type="bibr" target="#b8">(Ma, 2017)</ref> 78.60 72.10 -LCR-Rot (our approach) 81.34 75.24 72.69 <ref type="table">Table 2</ref>: The performance (classification accuracy) of different methods on three datasets. The results with * are obtained by running the code posted at original paper.</p><p>6. AE-LSTM is an upgraded version of LSTM. For each word in a sentence, this model appends target embedding to it. Then feed these embeddings to LSTM <ref type="bibr" target="#b13">[Wang et al., 2016]</ref>;</p><p>7. ATAE-LSTM is developed based on AE-LSTM. It further strengthens the effect of target embedding, which appends target embedding to each word hidden vector and leverages attention mechanism to obtain weights of each hidden vector <ref type="bibr" target="#b13">[Wang et al., 2016]</ref>;</p><p>8. GRNN-G3 adopts a Gated-RNN to represent sentence and use a three-way structure to leverage contexts .</p><p>9. MemNet is a deep memory network which considers the content and position of target <ref type="bibr" target="#b12">[Tang et al., 2016b]</ref>.</p><p>10. IAN interactively learns attentions in the contexts and targets, and generate the representations for targets and contexts separately <ref type="bibr" target="#b8">[Ma et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">System Performance Comparison</head><p>The performance of all compared systems are reported in <ref type="table">Table 2</ref>. We can find that the Majority method is the worst, which means the majority sentiment polarity occupies 53.50%, 65.00% and 50% of all samples on the Restaurant, Laptop and Twitter testing datasets respectively. The Simple SVM model performs better than Majority. With the help of feature engineering, the Feature-enhanced SVM achieves much better results. However, feature engineering is labor-intensive and almost reaches its performance bottleneck. Our model achieves significantly better results than feature-enhanced SVM. It shows that neural networks can obtain better representations of sentence without manual feature engineering.</p><p>Among LSTM based neural networks described in this paper, the basic LSTM approach performs the worst. TD-LSTM obtains an improvement of 1-2% over LSTM when target signals are taken into consideration. Because of the introduction of attention mechanism, AE-LSTM and ATAE-LSTM achieve better results than TD-LSTM, and ATAE-LSTM is slightly better among the two. IAN, GRNN-G3 and Mem-Net show different advantages in different datasets. MemNet   achieves better results than other models on the Restaurant dataset, since it considers not only the contexts of targets but also the position of each context word related to the target. IAN considers separate representations of targets and obtains better result on the Laptop dataset. GRNN-G3 achieves competitive results on all datasets because of its three-way structure and special gated-RNN model. In the contrast, our LCR-Rot model achieves the best results on the all datasets among all models. And we will give a detailed analysis in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Effect of Two-side Target Representation</head><p>In order to verify the effectiveness and advantage of our twoside target representation, we design the following two reduced models based on LCR-Rot:</p><p>1. No-Target-Attention is a simplified version of LCR-Rot, where we remove context2target attention and use the average of hidden states of target phrase to represent the target phrase;</p><p>2. No-Target-Learned is based on No-Target-Attention, where the target phrase is not learned by a LSTM independently. Instead, the average of initial word embeddings is used to represent the target phrase.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we report the performance of LCR-Rot and two target-reduced models. It can be seen that No-Target-Attention model performs a little worse than LCR-Rot. The results verify the usage of target attention is rewarding in our model. By comparing LCR-Rot and No-Target-Learned, we find that removing the support of multi-word target phrase will cause a more rapid decline. The decreases are 3.30%, 5.18% and 4.48% on the Restaurant, Laptop and Twitter dataset respectively. It indicates that the two-side way to model the target phrase is very important for aspect-based sentiment classification.</p><p>To analyze the problem more deeply, we summarize the number/percentage of single-word targets and multi-word targets on the datasets in <ref type="table" target="#tab_4">Table 4</ref>. It can be seen that more than 1/4 of targets on the Restaurant contain multiple words. The percentage is even more than 1/3 and 2/3 on the Laptop and Twitter respectively. It is reasonable that the decreases of No-     Target-Learned on the Laptop and Twitter are more than that on the Restaurant. In summary, our two-side target representation way is effective to support multi-word target representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Effect of Rotatory Attention</head><p>To verify the effectiveness of rotatory attention, we further design the following models based on LCR-Rot:</p><p>1. No-Attention is based on No-Target-Attention. We continue to remove the target2context attention mechanism in No-Target-Attention and use the average of hidden states to represent left and right contexts;</p><p>2. Attention-Reverse is based on LCR-Rot, where we reverse the order of attention. We first adopt con-text2target attention and then adopt target2context attention.</p><p>We have known that context2target attention is rewarding in our model according to previous subsection. When the attention is further reduced, from <ref type="table" target="#tab_7">Table 5</ref>, we can see that the performance of No-Attention drops significantly. It illustrates target2context attention is more important than context2target attention. By comparing LCR-Rot and Attention-Reverse, we find that reversing the order of attention will cause 0.5%-1.5% performance decrease on the datasets which proves the advantage of our rotatory attention.</p><p>In order to acquire a better understanding of the left-centerright separated rotatory attention model, we propose a visualization toolkit to show the attention weights (Equation 3 and 6) of contexts and target phrases. In <ref type="figure" target="#fig_2">Figure 2</ref>, we give the visualization of Example 1. The red color denotes words in the target phrases and the blue color denotes words in the contexts to which the model pays attention. The darker of the color, the more important of the word for the representation. We observe <ref type="figure" target="#fig_2">Figure 2</ref> from several angles.</p><p>Firstly, seen from both sub-figures, the most indicative sentiment word in the context can be accurately captured. For example, in sub-figure (a), the word "pleased" has the biggest attention weight for the target "the life of battery". Meanwhile, in sub-figure (b), given the target "windows 8 operation system", although both left and right contexts contain sentiment word ("pleased" in the left context and "bad" in the right context), the correct sentiment word "bad" in the right context is selected as the most important one.</p><p>Secondly, in sub-figure (a) and (b), we can see that attention weights of left-aware target phrase and right-aware target phrase are very different. When the target phrase is more related to left contexts, the attention weights of left-aware target phrase is more suitable. This may be a special explanation of the effectiveness of rotatory attention.</p><p>Thirdly, by comparing ideal values of attention weights that we expect in <ref type="figure" target="#fig_0">Figure 1</ref> and real values that we shown in <ref type="figure" target="#fig_2">Figure 2</ref> with respect to the target "windows 8 operating system", we can see that real values and ideal values are generally the same. This shows that our model behaves as we expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a left-center-right separated neural network with rotatory attention model for aspect-based sentiment analysis. The key idea of our model is to represent a sentence with a specific target as the concatenation of left-center-right component representations. Under such a network framework, we further propose a rotatory attention mechanism to take into account the interaction between targets and contexts to better represent targets and contexts. The experimental results on three benchmark datasets demonstrate that our model achieves currently the best aspect-based sentiment classification performance, in comparison with the state-of-the-art methods proposed in recent years.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Architecture of LCR-Rot. After encoding three parts of the sentence by three Bi-LSTMs, 1 use target2context attention to obtain the representations of left and right context; 2 then use context2target attention to obtain left-aware target and right-aware target. The attention weights in left-aware and right-aware targets are different. A combination of the two are called a two-side representation of target. Target2context and context2target constitute our rotatory attention. The sentence representation consists of four component representations. The attention weights shown in the figure are the ideal values we expect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) the target phrase: windows 8 operating system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Attention visualizations of Example 1, with the targets of "the life of battery" and "windows 8 operating system" respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics (number of examples in each class) of the three datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TD-LSTM adopts two LSTMs to model the left context with target and the right context with target respectively<ref type="bibr" target="#b12">[Tang et al., 2016a]</ref>;</figDesc><table><row><cell>Majority Simple SVM Feature-enhanced SVM LSTM</cell><cell>the-art feature template which contains n-gram features, parse features and lexicon features [Kir-itchenko et al., 2014]; 4. LSTM represents a standard LSTM for aspect-based sentiment classification task [Tang et al., 2016a]; 5. Restaurant Laptop Twitter (%) (%) (%) 53.50 65.00 50.00 73.22 66.97 62.70 80.90 72.10 71.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The performance of LCR-Rot and two target-reduced versions of LCR-Rot.</figDesc><table><row><cell>Single-word Multi-word Multi-word (len=1) (len=2) (len&gt;2) Restaurant 3521/74.5% 819/17.3% 388/8.2% Laptop 1825/61.5% 857/28.9% 284/9.6% Twitter 2081/30.0% 4852/69.9% 7/0.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The number/percentage of single-word and multi-word targets on the datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The performance of LCR-Rot and attention changed versions of LCR-Rot.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The introduction of SemEval 2014 Task 4 can be obtained at http://alt.qcri.org/semeval2014/task4/ 2 Pre-trained word vectors of GloVe can be downloaded at https://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
	<note>Kalchbrenner et al., 2014] Nal Kalchbrenner</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim ; Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the conference on empirical methods in natural language processing</title>
		<meeting>eeding of the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nrc-canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="167" />
		</imprint>
	</monogr>
	<note>Synthesis lectures on human language technologies</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee ; Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Pontiki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. Semeval-2014 task 4: Aspect based sentiment analysis. Proceedings of SemEval</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning tag embeddings and tag-specific composition functions in recursive neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
	<note>Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Targetdependent twitter sentiment classification with rich automatic features</title>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the conference on empirical methods in natural language processing</title>
		<editor>Wagner et al., 2014] Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster, and Lamia Tounsi</editor>
		<meeting>eeding of the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
	<note>Proceedings of the 8th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the conference on empirical methods in natural language processing</title>
		<meeting>eeding of the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gated neural networks for targeted sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3087" to="3093" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
