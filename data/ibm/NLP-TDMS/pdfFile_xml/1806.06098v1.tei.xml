<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Training for 3D Morphable Model Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Training for 3D Morphable Model Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed onthe-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results. nition network [25] into identity parameters for the Basel 2017 Morphable Face Model [8].</p><p>to-image autoencoder with a fixed, morphable-model-based decoder and an image-based loss <ref type="bibr" target="#b27">[28]</ref>. This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels. Instead, the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG-Face <ref type="bibr" target="#b16">[17]</ref> or Google's FaceNet <ref type="bibr" target="#b24">[25]</ref>. These features are robust to pose, expression, lighting, and even non-photorealistic inputs. We exploit this 1 arXiv:1806.06098v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A 3D morphable face model (3DMM) <ref type="bibr" target="#b2">[3]</ref> provides a smooth, low-dimensional "face space" spanning the range of human appearance. Finding the coordinates of a person in this space from a single image of that person is a common task for applications such as 3D avatar creation, facial animation transfer, and video editing (e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29]</ref>). The conventional approach is to search the space through inverse rendering, which generates a face that matches the photograph by optimizing shape, texture, pose, and lighting parameters <ref type="bibr" target="#b13">[14]</ref>. This approach requires a complex, nonlinear optimization that can be difficult to solve in practice.</p><p>Recent work has demonstrated fast, robust fitting by regressing from image pixels to morphable model coordinates using a neural network <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28]</ref>. The major issue with the regression approach is the lack of ground-truth 3D face data for training. Scans of face geometry and texture are difficult to acquire, both because of expense and privacy considerations. Previous approaches have explored synthesizing training pairs of image and morphable model coordinates in a preprocess <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>, or training an image- <ref type="bibr">Figure 1</ref>. Neutral 3D faces computed from input photographs using our regression network. We map features from a facial recog-invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face. The synthetic rendering need not have the same pose, expression, or lighting of the photograph, allowing our network to predict only shape and texture.</p><p>Simply optimizing for similarity between identity features, however, can teach the regression network to fool the recognition network by producing faces that match closely in feature space but look unnatural. We alleviate the fooling problem by applying three novel losses: a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model, a loopback loss to ensure the regression network can correctly reinterpret its own output, and a multi-view identity loss that combines features from multiple, independent views of the predicted shape.</p><p>Using this scheme, we train a 3D shape and texture regression network using only a face recognition network, a morphable face model, and a dataset of unlabeled face images. We show that despite learning from unlabeled photographs, the 3D face results improve on the accuracy of previous work and are often recognizable as the original subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Morphable 3D Face Models</head><p>Blanz and Vetter <ref type="bibr" target="#b2">[3]</ref> introduced the 3D morphable face model as an extension of the 2D active appearance model <ref type="bibr" target="#b5">[6]</ref>. They demonstrated face reconstruction from a single image by iteratively fitting a linear combination of registered scans and pose, camera, and lighting parameters. They decomposed the geometry and texture of the face scans using PCA to produce separate, reduced-dimension geometry and texture spaces. Later work <ref type="bibr" target="#b7">[8]</ref> added more face scans and extended the model to include expressions as another separate space. We build directly off of this work by using the PCA weights as the output of our network.</p><p>Convergence of iterative fitting is sensitive to the initial conditions and the complexity of the scene (i.e., lighting, expression, and pose). Subsequent work ( <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> and others) has applied a range of techniques to improve the accuracy and stability of the fitting, producing very accurate results under good conditions. However, iterative approaches are still unreliable under general, in-the-wild, conditions, leading to the interest in regression-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning to Generate 3D Face Models</head><p>Deep neural networks provide the ability to learn a regression from image pixels to 3D model parameters. The chief difficulty becomes how to collect enough training data to feed the network.</p><p>One solution is to generate synthetic training data by drawing random samples from the morphable model and rendering the resulting faces <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. However, a network trained on purely synthetic data may perform poorly when faced with occlusions, unusual lighting, or ethnicities that are not well-represented by the morphable model. We include randomly generated, synthetic faces in each training batch to provide ground truth 3D coordinates, but train the network on real photographs at the same time.</p><p>Tran et al. <ref type="bibr" target="#b29">[30]</ref> address the lack of training data by using an iterative optimization to fit an expressionless model to a large number of photographs, and treat results where the optimization converged as ground truth. To generalize to faces with expression, identity labels and at least one neutral image are required, so the potential size of the training dataset is restricted. We also directly predict a neutral expression, but our unsupervised approach removes the need for an initial iterative fitting step.</p><p>An approach closely related to ours was recently proposed by Tewari, et al. <ref type="bibr" target="#b27">[28]</ref>, who train an autoencoder network on unlabeled photographs to predict shape, expression, texture, pose, and lighting simultaneously. The encoder is a regression network from images to morphablemodel coordinates, and the decoder is a fixed, differentiable rendering layer that attempts to reproduce the input photograph. Like ours, this approach does not require supervised training pairs. However, since the training loss is based on individual image pixels, the network is vulnerable to confounding variation between related variables. For example, it cannot readily distinguish between dark skin tone and a dim lighting environment. Our approach exploits a pretrained face recognition network, which distinguishes such related variables by extracting and comparing features across the entire image.</p><p>Other recent deep learning approaches predict depth maps <ref type="bibr" target="#b25">[26]</ref> or voxel grids <ref type="bibr" target="#b10">[11]</ref>, trading off a compact and interpretable output mesh for more faithful reproductions of the input image. As for <ref type="bibr" target="#b27">[28]</ref>, identity and expression are confounded in the output mesh. The result may be suitable for image processing tasks, such as relighting, at the expense of animation tasks such as rigging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Facial Identity Features</head><p>Current face recognition networks achieve high accuracy over millions of identities <ref type="bibr" target="#b12">[13]</ref>. The networks operate by embedding images in a high-dimensional space, where images of the same person map to nearby points <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>. Recent work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref> has shown that this mapping is somewhat reversible, meaning the features can be used to produce a likeness of the original person. We build on this work and use FaceNet <ref type="bibr" target="#b24">[25]</ref> to both produce input features for our regression network, and to verify that the output of the regression resembles the input photograph. <ref type="figure">Figure 2</ref>. End-to-end computation graph for unsupervised training of the 3DMM regression network. Training batches consist of combinations of real (blue) and synthetic (red) face images. Identity, loopback and batch distribution losses are applied to real images, while the 3DMM parameter loss is applied to synthetic images. The regression network (yellow) is shown in two places, but both correspond to the same instance during training. The identity encoder network is fixed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>We employ an encoder-decoder architecture that permits end-to-end unsupervised learning of 3D geometry and texture morphable model parameters ( <ref type="figure">Fig. 2</ref>). Our training framework utilizes a realistic, parameterized illumination model and differentiable renderer to form neutralexpression face images under varying pose and lighting conditions. We train our model on hybrid batches of real face images from VGG-Face <ref type="bibr" target="#b16">[17]</ref> and synthetic faces constructed from the Basel Face 3DMM <ref type="bibr" target="#b7">[8]</ref>.</p><p>The main strength and novelty of our approach lies in isolating our loss function to identity. By training the model to preserve identity through conditions of varying expression, pose, and illumination, we are able to avoid network fooling and achieve robust state-of-the-art recognizability in our predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder</head><p>We use FaceNet <ref type="bibr" target="#b24">[25]</ref> for the network encoder, since its features have been shown to be effective for generating face images <ref type="bibr" target="#b4">[5]</ref>. Other facial recognition networks such as VGG-Face <ref type="bibr" target="#b16">[17]</ref>, or even networks not focused on recognition, may work equally well.</p><p>The output of the encoder is the penultimate, 1024-D avgpool layer of the "NN2" FaceNet architecture. We found the avgpool layer more effective than the final, 128-D normalizing layer as input to the decoder, but use the normalizing layer for our identity loss (Sec. 3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoder</head><p>Given encoder outputs generated from a face image, our decoder generates parameters for the Basel Face Model 2017 3DMM <ref type="bibr" target="#b7">[8]</ref>. The Basel 2017 model generates shape</p><formula xml:id="formula_0">meshes S ≡ s i ∈ R 3 |1 ≤ i ≤ N and texture meshes T ≡ t i ∈ R 3 |1 ≤ i ≤ N with N = 53, 149 vertices. S = S(s, e) = µ S + P SS W SS s + P SE W SE e T = T(t) = µ T + P T W T t<label>(1)</label></formula><p>Here, s, t ∈ R 199 and e ∈ R 100 are shape, texture, and expression parameterization vectors with standard normal distributions; µ S , µ T ∈ R 3N are the average face shape and texture; P SS , P T ∈ R 3N ×199 and P SE ∈ R 3N ×100 are linear PCA bases; and W SS , W T ∈ R 199×199 and W SE ∈ R 100×100 are diagonal matrices containing the square roots of the corresponding PCA eigenvalues. The decoder is trained to predict the 398 parameters constituting the shape and texture vectors, s and t, for a face. The expression vector e is not currently predicted and is set to zero. The decoder network consists of two 1024-unit fully connected + ReLU layers followed by a 398-unit regression layer. The weights were regularized towards zero. Deeper networks were considered, but they did not significantly improve performance and were prone to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Differentiable Renderer</head><p>In contrast to previous approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> that backpropagate loss through an image, we employ a general-purpose, differentiable rasterizer based on a deferred shading model. The rasterizer produces screen-space buffers containing triangle IDs and barycentric coordinates at each pixel. After rasterization, per-vertex attributes such as colors and normals are interpolated at the pixels using the barycentric coordinates and IDs. This approach allows rendering with full perspective and any lighting model that can be computed in screen-space, which prevents image quality from being a bottleneck to accurate training. The source code for the renderer is publicly available <ref type="bibr" target="#b0">1</ref> .</p><p>The rasterization derivatives are computed for the barycentric coordinates, but not the triangle IDs. We extend the definition of the derivative of barycentric coordinates with respect to vertex positions to include negative barycentric coordinates, which lie outside the border of a triangle. Including negative barycentric coordinates and omitting triangle IDs effectively treats the shape as locally planar, which is an acceptable approximation away from occlusion boundaries. Faces are largely smooth shapes with few occlusion boundaries, so this approximation is effective in our case, but it could pose problems if the primary source of loss is related to translation or occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Illumination Model</head><p>Because our differentiable renderer uses deferred shading, illumination is computed independently per-pixel with a set of interpolated vertex attribute buffers computed for each image. We use the Phong reflection model <ref type="bibr" target="#b19">[20]</ref> for shading. Because human faces exhibit specular highlights, Phong reflection allows for improved realism over purely diffuse approximations, such as those used in MoFA <ref type="bibr" target="#b27">[28]</ref>. It is both efficient to evaluate and differentiable.</p><p>To create appropriately even lighting, we randomly position two point light sources of varying intensity several meters from the face to be illuminated. We select a random color temperature for each training image from approximations of common indoor and outdoor light sources, and perturb the color to avoid overfitting. Finally, since the Basel Face model does not contain specular color information, we use a heuristic to define specular colors K s from the diffuse colors K d of the predicted model:</p><formula xml:id="formula_1">K s := c − cK d for some manually selected constant c ∈ [0, 1].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Losses</head><p>We propose a novel loss function that focuses on facial identity, and ignores variations in facial expression, illumination, pose, occlusion, and resolution. This loss function is conceptually straightforward and enables unsupervised endto-end training of our network. It combines four terms:</p><formula xml:id="formula_2">L =L param + L id + ω batch L batch + ω loop L loop<label>(2)</label></formula><p>Here, L param imposes 3D shape and texture similarity for the synthetic images, L id imposes identity preservation on 1 http://github.com/google/tf_mesh_renderer the real images in a batch, L batchdistr regularizes the predicted parameter distributions within a batch to the distribution of the morphable model, and L loopback ensures the network can correctly interpret its own output. The effects of removing the batch distribution, loopback, and limiting the identity loss to a single view are shown in <ref type="figure" target="#fig_0">Figure 3</ref>. We use ω batch = 10.0 and ω loop = 0.07 for our results.</p><p>Training proceeds in two stages. First, the model is trained solely on batches of synthetic faces generated by randomly sampling for shape, texture, pose, and illumination parameters. This stage performs only a partial training of the model: since shape and texture parameters are sampled independently in this stage, the model is restricted from learning correlations between them. Second, the partiallytrained model is trained to convergence on batches consisting of a combination of real face images from the VGG-Face dataset <ref type="bibr" target="#b16">[17]</ref> and synthetic faces. Synthetic faces are subject to only the L param loss, while real face images are subject to all losses except L param .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Parameter Loss</head><p>For synthetic faces, the true shape and texture parameters are known, so we use independent Euclidean losses between the randomly generated true synthetic parameter vectors,s b andt b , and the predicted ones, s b and t b , in a batch.</p><formula xml:id="formula_3">L param = ω s b |s b −s b | 2 + ω t b |t b −t b | 2<label>(3)</label></formula><p>where ω s and ω t control the relative contribution of the shape and texture losses. Due to different units, we set ω s = 0.4 and ω t = 0.002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Identity Loss</head><p>Robust prediction of recognizable meshes can be facilitated with a loss that derives from a facial recognition network. We used FaceNet <ref type="bibr" target="#b24">[25]</ref>, though the identity-preserving loss generalizes to other networks such as VGG-Face <ref type="bibr" target="#b16">[17]</ref>. The final FaceNet normalizing layer is a 128-D unit vector such that, regardless of expression, pose, or illumination, same-identity inputs map closer to each other on the hypersphere than different-identity ones. For our identity loss L id , we define similarity of two faces as the cosine score of their respective output unit vectors, γ 1 and γ 2 :</p><formula xml:id="formula_4">L id (γ 1 , γ 2 ) = γ 1 · γ 2<label>(4)</label></formula><p>To use this loss in an unsupervised manner on real faces, we calculate the cosine score between a face image and the image resulting from passing the decoder outputs into the differentiable renderer with random pose and illumination. Identity prediction can be further enhanced by using multiple poses for each face. Multiple poses decrease the presence of occluded regions of the mesh. Additionally, since each pose provides a backpropagation path to the mesh vertices, the model trains in a more robust manner than if only a single pose is used. We use three randomly determined poses for each real face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Batch Distribution Loss</head><p>Applying the identity loss alone allows training to introduce biases into the decoder outputs that change their distribution from the zero-mean standard normal distribution assumption made by the Basel Face Model. These changes are likely due to domain transfer effects between the real images and those rendered from the decoder outputs. Initially, we attempted to compensate for these effects by adding a shallow network to transform the model-rendered encoder outputs prior to calculating the identity loss. While this approach did increase overall recognizability in the model, it also introduced unrealistic artifacts into the model outputs. Instead, we opted to regularize each batch <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12]</ref> to directly constrain the lowest two moments of the shape and texture parameter distributions to match those of a zeromean standard normal distribution. This loss, which is applied at a batch level, combines four terms:</p><formula xml:id="formula_5">L batchdistr = L µ S + L σ S + L µ T + L σ T<label>(5)</label></formula><p>Here, L µ S and L µ T regularize the batch shape and texture parameters to have zero mean, and L σ S and L σ T regularize them to have unit variance.</p><formula xml:id="formula_6">L µ S = |M ean b [s b ] − 0 199 | 2 L σ S = | V ar b [s b ] − 1 199 | 2 L µ T = |M ean b [t b ] − 0 199 | 2 L σ T = | V ar b [t b ] − 1 199 | 2<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Loopback Loss</head><p>A limitation of using real face images for unsupervised training is that the true shape and texture parameters for the faces are unknown. If they were known, then the more direct lower-level parameter loss in Sec. 3.3.1 could be directly imposed instead of the identity loss in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.</head><p>A close approximation to this lower-level loss for real images can be achieved using a "loopback" loss ( <ref type="figure">Fig. 2)</ref>. The nature of this loss lies in generalizing the model near the regions for which real face image data exists. Similar techniques have proven to be successful in generalizing model learning for image applications <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>To compute the loopback loss at any training step, the current-state decoder outputs for a batch of real face images are extracted and used to generate synthetic faces rendered in random poses and illuminations. The synthetic faces are then passed back through the encoder and decoder again, and the parameter loss in Sec. 3.3.1 is imposed between the resulting parameters and those first output by the decoder.</p><p>As shown in <ref type="figure">Fig. 2</ref>, two loopback loss backpropagation paths to the decoder exist. The effects of each are complementary: the synthetic face parameter path generalizes the decoder in the region near that of real face parameters, and the real image channel regularizes the decoder away from generating unrealistic faces. Additionally, the two paths encourage the regression network to match its responses for real and synthetic versions of the same face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first show and discuss the qualitative improvements of our method compared with other morphable model regression approaches (Sec. 4.1). We then evaluate our method quantitatively by comparing reconstruction error against scanned 3D face geometry (Sec. 4.2) and features produced by VGG-Face, which was not used for training (Sec. 4.3 and 4.4). We also show qualitative results on corrupted and non-photorealistic inputs (Sec. 4.5). Tran <ref type="bibr" target="#b29">[30]</ref> MoFA <ref type="bibr" target="#b27">[28]</ref> MoFA+Exp Sela <ref type="bibr" target="#b25">[26]</ref>  <ref type="figure" target="#fig_1">Figure 4</ref>. Results on the MoFA-Test dataset. Our method shows improved likeness and color fidelity over competing methods, especially in the shape of the eyes, eyebrows, and nose. Note that MoFA <ref type="bibr" target="#b27">[28]</ref> solves for pose, expression, lighting, and identity, so is shown both with (row 5) and without (row 4) expression. The unstructured method of Sela, et al. <ref type="bibr" target="#b25">[26]</ref> produces only geometry, so is shown without color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative Comparison</head><p>as part of MoFA. An extended evaluation is available in the supplemental material. Our method improves on the likenesses of previous approaches, especially in features relevant to facial recognition such as the eyebrow texture and nose shape.</p><p>Our method also predicts coloration and skin tone more faithfully. This improvement is likely a consequence of our batch distribution loss, which allows individual faces to vary from the mean of the Basel model (light skin tone), so long as the faces match the mean in aggregate. Previous methods, by contrast, regularize each face towards the mean of the model's distribution, tending to produce light skin tone overall.</p><p>The MoFA approach also sometimes confounds identity and expression ( <ref type="figure" target="#fig_1">Fig. 4, second column)</ref>, and skin tone and lighting <ref type="figure" target="#fig_1">(Fig. 4, first and sixth columns)</ref>. Our method and Tran et al. <ref type="bibr" target="#b29">[30]</ref> are more resistant to confounding variables. The unstructured method of Sela et al. <ref type="bibr" target="#b25">[26]</ref> does not sepa-rate identity and expression, predicts only shape, and is less robust than the model-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Neutral Pose Reconstruction on MICC</head><p>We quantitatively evaluate the ground-truth accuracy of our models on the MICC Florence 3D Faces dataset <ref type="bibr" target="#b0">[1]</ref> (MICC) in <ref type="table" target="#tab_0">Table 1</ref>. This dataset contains the ground truth scans of 53 Caucasian subjects in a neutral expression. Accompanying the scans are three observation videos for each subject, in conditions of increasing difficulty: 'cooperative', 'indoor', and 'outdoor.' We run the methods on each frame of the videos, and average the results over each video to get a single reconstruction. The results of Tran et al. <ref type="bibr" target="#b29">[30]</ref> are averaged over the mesh, as in <ref type="bibr" target="#b29">[30]</ref>. We instead average our encoder embeddings before making a single reconstruction.</p><p>To evaluate our predictions, we crop the ground truth scan to 95mm around the tip of the nose as in <ref type="bibr" target="#b29">[30]</ref>, and run ICP with isotropic scale to find an alignment. We solve for isotropic scale because we do not assume the methods predict absolute scale, and a small misalignment in scale can have a large effect on error. <ref type="table" target="#tab_0">Table 1</ref> shows the symmetric point-to-plane distance in millimeters within the ICPdetermined region of intersection, rather than point-to-point distances, as the methods and ground truth have different vertex densities. Our results indicate that we have improved absolute error to the ground truth by 20-25%, and our results are more consistent from person to person, with less than half the standard deviation when compared to <ref type="bibr" target="#b29">[30]</ref>.</p><p>We are also more stable across changing environments, with similar results for all three test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face Recognition Results</head><p>In order to quantitatively evaluate the likeness of our reconstructions, we use the VGG-Face <ref type="bibr" target="#b16">[17]</ref> recognition network's activations as a measure of similarity. VGG-Face was chosen because FaceNet appears in our training loss, making it unsuitable as an evaluation metric. For each face in our evaluation datasets, we compute the cosine similarity of the φ( t ) layers of VGG-Face between the input image and a rendering of our output geometry, as described in <ref type="bibr" target="#b16">[17]</ref>.</p><p>The similarity distributions for Labeled Faces in the Wild <ref type="bibr" target="#b9">[10]</ref> (LFW), MICC, and MoFA-Test are shown in <ref type="figure">Figure 5</ref>. The similarity between all pairs of photographs in the LFW dataset, separated into same-person and differentperson distributions, is shown for comparison in <ref type="figure">Fig. 5</ref>, top. Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test (the dataset for which results for all methods are available). By comparison, 22.7% of pairs of photos of the same person in LFW have a score below 0.403, and only 0.04% of pairs of photos of different people have a score above 0.403.</p><p>For additional validation, <ref type="table">Table 2</ref> shows the Earth Mover's distance <ref type="bibr" target="#b17">[18]</ref> between the all-pairs LFW distributions and the results of each method. Our method's results are closer to the same-person distribution than the differentperson distribution in all cases, while the other methods results' are closer to the different-person distribution. We conclude that ours is the first method that generates neutralpose, 3D faces with recognizability approaching a photo.</p><p>The scores of the ground-truth 3D face scans from MICC  <ref type="table">Table 2</ref>. Earth mover's distance between distributions of VGG-Face φ( t) similarity and distributions of same and different identities on LFW. A low distance for "Same" means the similarity scores between a photo and its associated 3D rendering are close to the scores of same identity photos in LFW, while a low distance for "Diff." means the scores are close to the scores of different identity photos.</p><p>and their input photographs provide a ceiling for similarity scores. Notably, the distance between the GT distribution and the same-person LFW distribution is very low, with almost the same mean (0.51 vs 0.50), indicating the VGG-Face network has little trouble bridging the domain gap between photograph and rendering, and that our method does not yet reach the ground-truth baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoFA-Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LFW Method</head><p>Top <ref type="table" target="#tab_0">-1 Top-5 Top-1 Top-5</ref> random 0.01 0.06 0.0002 0.001 MoFA <ref type="bibr" target="#b27">[28]</ref> 0.19 0.54 − − Tran et al. <ref type="bibr" target="#b29">[30]</ref> 0.25 0.62 0.001 0.002 ours 0.87 0.96 0.16 0.51 <ref type="table">Table 3</ref>. Identity Clustering Recall using VGG-Face distances on MoFA-Test and LFW. Given a rendered mesh, the task is to recover the unknown source identity by looking up the nearest neighbor photographs according to VGG-Face φ( t) cosine similarity. Top-1 and Top-5 show the fractions for which a photograph of the correct identity was recalled as the nearest neighbor, or in the nearest 5, respectively. Performance is higher for MoFA-Test because it contains 84 images and 78 identities, while the LFW set contains 12,993 images and 5,749 identities. <ref type="figure">Figure 6</ref>. FERET dataset <ref type="bibr" target="#b18">[19]</ref> stress test. The regression network is robust to changes in pose, lighting, expression, occlusion, and blur. See supplemental material for additional results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Face Clustering</head><p>To establish that our reconstructions are recognizable, we perform a clustering task to recover the identities of our generated meshes. For each of LFW and MoFA-Test, we run our method on all faces in the dataset, and render the output geometry as shown in the figures in this paper. For each rendering, we find the nearest neighbors according to the VGG-Face φ( t ) distance. <ref type="table">Table 3</ref> shows the fraction of meshes that recall a photo of the source identity as the nearest neighbor, and within the top 5 nearest neighbors.</p><p>On MoFA-Test, which has 84 images and 78 identities, we achieve a Top-1 recall of 87%, compared to 25% for Tran et al. and 19% for MoFA. On the larger LFW dataset, which contains over 5,000 identities in 13,000 photographs, we still achieve a Top-5 recall of 51%. We conclude our approach generates recognizable 3D morphable models, even in test sets with thousands of candidate identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Reconstruction from Challenging Images</head><p>Our regression network uses a facial identity feature vector as input, yielding results robust to changes in pose, expression, lighting, occlusion, and resolution, while remaining sensitive to changes in identity. <ref type="figure">Figure 6</ref> qualitatively demonstrates this robustness by varying conditions for a single subject and displaying consistent output. <ref type="figure">Figure 7</ref>. Art from the BAM dataset <ref type="bibr" target="#b30">[31]</ref>. Because the inputs to our regression network are high-level identity features, the results are robust to stylized details at the pixel level.</p><p>Additionally, <ref type="figure">Figure 7</ref> shows that our network can reconstruct plausible likenesses from non-photorealistic artwork, in cases where a fitting approach based on inverse rendering would have difficulty. This result is possible because of the invariance of the identity features to unrealistic pixel-level information, and because our unsupervised loss focuses on aspects of reconstruction that are important for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Future Work</head><p>We have shown it is possible to train a regression network from images to neutral, expressionless 3D morphable model coordinates using only unlabeled photographs and improve on the accuracy of supervised methods. Our results approach the face recognition similarity scores of real photographs and exceed the scores of other regression approaches by a large margin. Because of the accuracy of the approach, the predicted face can be directly used for facetracking based on landmarks. This paper focuses on learning an expressionless face, which is suitable for creating VR avatars or landmark-based tracking. In future work, we hope to extend the approach to predict pose, expression, and lighting, similar to Tewari, et al. <ref type="bibr" target="#b27">[28]</ref>. Predicting these factors while avoiding their confounding effects should be possible by adding an inverse rendering stage to our decoder while maintaining the neutral-pose losses we currently apply.</p><p>The method produces generally superior results for young adults and Caucasian ethnicities. The differences could be due to limited representation in the scans used to produce the morphable model, bias in the features extracted from the face recognition network, or limited representation in the VGG-Face dataset we use for training. In future work, we hope to improve the performance of the method on a diverse range of ages and ethnicities.</p><p>A. Appendix <ref type="figure">Figure 8</ref>. Stability under variable lighting conditions for a subject from the FERET <ref type="bibr" target="#b18">[19]</ref> dataset. <ref type="figure">Figure 9</ref>. Pose Stress Test on a subject from the FERET <ref type="bibr" target="#b18">[19]</ref> dataset. Our algorithm is consistent under a 45 • rotation. Under a 90 • rotation, global shape changes. <ref type="figure">Figure 10</ref>. Expression stability test on subjects from the FERET <ref type="bibr" target="#b18">[19]</ref> (left) and MICC <ref type="bibr" target="#b0">[1]</ref> (right) datasets. For both subjects, our method is invariant to expression, while remaining sensitive to identity. <ref type="figure">Figure 11</ref>. Occlusion Stress Test on subjects from the MICC <ref type="bibr" target="#b0">[1]</ref> and FERET <ref type="bibr" target="#b18">[19]</ref> dataset. We increase occlusion in the input image until our algorithm no longer predicts accurate features. Facial features smoothly degrade as the necessary information is no longer present in the input image.  <ref type="bibr" target="#b18">[19]</ref> (bottom) datasets. Beginning with a frontal image of the subject, we apply a progressively larger gaussian blur kernel to examine the effect of lost detail in the input. For the female subject, global shape begins to change subtly as the blur becomes extreme. For both subjects, fine detail in the eyebrow shape and thickness is lost as the input is increasingly blurred. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Tran <ref type="bibr" target="#b29">[30]</ref> MoFA <ref type="bibr" target="#b27">[28]</ref> Input Ours Tran <ref type="bibr" target="#b29">[30]</ref> MoFA <ref type="bibr" target="#b27">[28]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Fitting Pose and Expression</head><p>Our system reconstructs shape and texture of faces, and ignores aspects such as pose, expression, and lighting. Those components are needed to exactly match the reconstruction to the source image, and our neutral face output is an excellent starting point to find them. <ref type="figure" target="#fig_5">Figure 18</ref> shows results of gradient descent that starts with our output and fits the pose and expression by minimizing the distances of landmarks on our mesh and the image (we used the 68 landmark configuration from the Multi-PIE database <ref type="bibr" target="#b8">[9]</ref>). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Ablation test showing failures caused by removing individual losses. Batch distribution (top) keeps the results in the space of human faces, while loopback (middle) helps avoid exaggerated features. Multi-view identity (bottom) increases robustness to expression and lighting variation. Ablated result is computed by rendering a single frontal view for identity loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>compares our results with the methods of of Tran, et al. [30], Tewari, et al. [28] (MoFA), and Sela, et al. [26] on 7 images from an 84-image test set developed Input Ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 12 .</head><label>12</label><figDesc>Resolution Stress Test on subjects from the MICC [1] (top) and FERET</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 13 .</head><label>13</label><figDesc>Views of the six teaser LFW [10] subjects at −90 • , −45 • , 0 • , 45 • , and 90 • rotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 14 .</head><label>14</label><figDesc>Full qualitative comparison on the MoFA-Test dataset. Results 1-22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 18 .</head><label>18</label><figDesc>Starting with a neutral face, we used landmarks to fit pose and expression of the morphable model. Left-to-right: a face image with landmarks, reconstructed neutral face, shaded geometry and albedo overlays with correct pose and expression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Mean Error on MICC Dataset using point-to-plane distance after ICP alignment of video-averaged outputs with isotropic scale estimation. Our errors lower on average and in variance, both within individual subjects and as conditions change.</figDesc><table><row><cell></cell><cell>Cooperative</cell><cell>Indoor</cell><cell>Outdoor</cell></row><row><cell>Method</cell><cell>Mean Std.</cell><cell cols="2">Mean Std. Mean Std.</cell></row><row><cell cols="2">Tran et al.[30] 1.93 0.27</cell><cell cols="2">2.02 0.25 1.86 0.23</cell></row><row><cell>ours</cell><cell>1.50 0.13</cell><cell cols="2">1.50 0.11 1.48 0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure 17. Full qualitative comparison on the MoFA-Test dataset. Results 67-84.</figDesc><table><row><cell>Input Input</cell><cell>Ours Ours</cell><cell>Tran[30] Tran[30]</cell><cell>MoFA[28] MoFA[28]</cell><cell>Input Input</cell><cell>Ours Ours</cell><cell>Tran[30] Tran[30]</cell><cell>MoFA[28] MoFA[28]</cell></row><row><cell>Input</cell><cell>Ours</cell><cell>Tran[30]</cell><cell>MoFA[28]</cell><cell>Input</cell><cell>Ours</cell><cell>Tran[30]</cell><cell>MoFA[28]</cell></row></table><note>Figure 15. Full qualitative comparison on the MoFA-Test dataset. Results 23-44.Figure 16. Full qualitative comparison on the MoFA-Test dataset. Results 45-66.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The florence 2d/3d hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding</title>
		<meeting>the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reanimating faces in images and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="641" to="650" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthesizing normalized faces from facial identity features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reconstruction of personalized 3d face rigs from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>28:1-28:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Morphable face models -an open framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lüthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<idno>abs/1709.08398</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1031" to="1039" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chris) Yu. State-of-the-art of 3d facial reconstruction methods for face recognition based on a single 2d training image per person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="908" to="913" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling surface appearance from a single photograph using selfaugmented convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno>45:1-45:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analysis-by-synthesis by learning to invert generative black boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Artificial Neural Networks, Part I, ICANN &apos;08</title>
		<meeting>the 18th International Conference on Artificial Neural Networks, Part I, ICANN &apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="971" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and robust earth mover&apos;s distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The feret database and evaluation procedure for face recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing J</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Phong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1975-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating 3d shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
	<note>CVPR &apos;05</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bam! the behance artistic media dataset for recognition beyond photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
