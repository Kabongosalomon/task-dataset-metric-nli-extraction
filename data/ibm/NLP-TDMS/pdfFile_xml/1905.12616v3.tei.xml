<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Defending Against Neural Fake News</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><forename type="middle">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><forename type="middle">♥ ♠</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Defending Against Neural Fake News</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -and sampling strategies that alleviate its effects -both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.</p><p>To the best of our knowledge, most disinformation online today is manually written <ref type="bibr" target="#b36">(Vargo et al., 2018)</ref>. However, as progress continues in natural language generation, malicious actors will increasingly be 33rd</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online fake news -news designed to intentionally deceive -has recently emerged as a major societal problem. Malicious actors spread fallacious viral stories in order to gain advertising revenue, influence opinions, and even tip elections <ref type="bibr" target="#b8">(Faris et al., 2017;</ref><ref type="bibr" target="#b41">Wardle and Derakhshan, 2017)</ref>. As such, countering the spread of disinformation online presents an urgent technical and political issue. able to controllably generate realistic-looking propaganda at scale. Thus, while we are excited about recent progress in text generation <ref type="bibr" target="#b18">(Józefowicz et al., 2016;</ref><ref type="bibr" target="#b25">Radford et al., 2018;</ref><ref type="bibr" target="#b42">2019)</ref>, we are also concerned with the inevitability of AI-generated 'neural' fake news. <ref type="bibr">1</ref> With this paper, we seek to understand and respond to neural fake news before it manifests at scale. We draw on the field of computer security, which relies on threat modeling: analyzing the space of potential threats and vulnerabilities in a system to develop robust defenses. To scientifically study the risks of neural disinformation, we present a new generative model called Grover. 2 Our model allows for controllable yet efficient generation of an entire news article -not just the body, but also the title, news source, publication date, and author list. This lets us study an adversary with controllable generations (e.g. <ref type="figure">Figure 1</ref>, an example anti-vaccine article written in the style of the New York Times).</p><p>Humans rate the disinformation generated by Grover as trustworthy, even more so than humanwritten disinformation. Thus, developing robust verification techniques against generators such as Grover is an important research area. We consider a setting in which a discriminator has access to 5000 Grover generations, but unlimited access to real news. In this setting, the best existing fake news discriminators are, themselves, deep pretrained language models (73% accuracy) <ref type="bibr" target="#b24">(Peters et al., 2018;</ref><ref type="bibr" target="#b25">Radford et al., 2018;</ref><ref type="bibr" target="#b42">2019;</ref><ref type="bibr" target="#b4">Devlin et al., 2018)</ref>. However, we find that Grover, when used in a discriminative setting, performs even better at 92% accuracy. This finding represents an exciting opportunity for defense against neural fake news: the best models for generating neural disinformation are also the best models at detecting it.</p><p>Next, we investigate how deep pretrained language models distinguish between real and machinegenerated text. We find that key artifacts are introduced during generation as a result of exposure bias: the generator is not perfect, so randomly sampling from its distribution results in generations that fall increasingly out-of-distribution as length increases. However, sampling strategies that alleviate these effects also introduce artifacts that strong discriminators can pick up on.</p><p>We conclude with a sketch of the ethical territory that must be mapped out in order to understand our responsibilities as researchers when studying fake news, and the potential negative implications of releasing models <ref type="bibr" target="#b13">(Hecht et al., 2018;</ref><ref type="bibr" target="#b42">Zellers, 2019;</ref><ref type="bibr" target="#b31">Solaiman et al., 2019)</ref>. Accordingly, we suggest a provisional policy of how such models should be released and why we believe it to be safe -and perhaps even imperative -to do so. We believe our proposed framework and accompanying models provide a concrete initial proposal for an evolving conversation about ML-based disinformation threats and how they can be countered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Fake News in a Neural and Adversarial Setting</head><p>We present a framework -motivated by today's dynamics of manually created fake news -for understanding what adversaries will attempt with deep models, and how verifiers should respond.</p><p>Scope of fake news. There are many types of false news, ranging from satire to propaganda <ref type="bibr" target="#b40">(Wardle, 2017)</ref>. In this paper, we focus on text-only documents formatted as news articles: stories and their corresponding metadata that contain purposefully false information. Existing fake news is predominantly human-written, for two broad goals: monetization (ad revenue through clicks) and propaganda (communicating targeted information) <ref type="bibr" target="#b1">(Bradshaw and Howard, 2017;</ref><ref type="bibr" target="#b20">Melford and Fagan, 2019)</ref>. Achieving either goal requires the adversary to be selective about the news that they make, whether by producing only viral content, or content that advances a given agenda.</p><p>Fact checking and verification: related work. There is considerable interest in fighting online disinformation. Major platforms such as Facebook prioritize trustworthy sources and shut down accounts linked to disinformation <ref type="bibr" target="#b21">(Mosseri, 2018;</ref><ref type="bibr" target="#b6">Dwoskin and Romm, 2018)</ref>. Some users of these platforms avoid fake news with tools such as NewsGuard and Hoaxy <ref type="bibr" target="#b29">(Shao et al., 2016)</ref> and websites like Snopes and PolitiFact. These services rely on manual fact-checking efforts: verifying the accuracy of claims, articles, and entire websites. Efforts to automate fake news detection generally point out stylistic biases that exist in the text <ref type="bibr" target="#b28">(Rashkin et al., 2017;</ref><ref type="bibr" target="#b39">Wang, 2017;</ref><ref type="bibr" target="#b23">Pérez-Rosas et al., 2018)</ref>. These efforts can help moderators on social media platforms shut down suspicious accounts. However, fact checking is not a panacea -cognitive biases such as the backfire effect and confirmation bias make humans liable to believe fake news that fits their worldview <ref type="bibr" target="#b34">(Swire et al., 2017)</ref>.</p><p>Framework. We cast fake news generation and detection as an adversarial game, with two players:</p><p>• Adversary. Their goal is to generate fake stories that match specified attributes: generally, being viral or persuasive. The stories must read realistically to both human users as well as the verifier. • Verifier. Their goal is to classify news stories as real or fake. The verifier has access to unlimited real news stories, but few fake news stories from a specific adversary. This setup matches the existing landscape: when a platform blocks an account or website, their disinformative stories provide training for the verifier; but it is difficult to collect fake news from newly-created accounts.</p><p>The dual objectives of these two players suggest an escalating "arms race" between attackers and defenders. As verification systems get better, so too will adversaries. We must therefore be prepared to deal with ever-stronger adversarial attacks, which is the focus of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Grover: Modeling Conditional Generation of Neural Fake News</head><p>Given existing online disinformation, we have reason to believe adversaries will try to generate targeted content (e.g. clickbait and propaganda). Recently introduced large-scale generative models produce realistic-looking text , but they do not lend themselves to producing controllable generations <ref type="bibr" target="#b16">(Hu et al., 2017)</ref>. <ref type="bibr" target="#b54">3</ref> Therefore, to probe the feasibility of realistic-looking neural fake news, we introduce Grover, which produces both realistic and controlled generations.</p><p>The current state-of-the-art in unconditional text generation views it as a language modeling problem <ref type="bibr" target="#b0">(Bengio et al., 2003)</ref>, in which the probability of a document x is the product of the conditional probability of generating each token x i given previous tokens:</p><formula xml:id="formula_0">ppxq " N ź i"1 ppx i |x 1 . . . x i´1 q.<label>(1)</label></formula><p>The document is typically treated as a single unstructured text field, beginning with a &lt;start&gt; token and ending with an &lt;end&gt; token. The latter, &lt;end&gt;, is particularly important because it indicates the end of the field, and when to should stop generating. However, a news article has necessary structure beyond the running text, or body field. Metadata fields include the domain where the article is published (indirectly marking the style), the date of publication, the names of the authors, and the headline of the article itself. Not only does generating a news article require producing all of these components, these fields also allow significant control over the generations (e.g. specifying a headline helps control the generated body). An article can be modeled by the joint distribution:</p><p>ppdomain, date, authors, headline, bodyq.</p><p>However, it is not immediately obvious how to sample from Equation 2. One option is to define a canonical order among the article's fields F : ( f 1 ă f 2 ă. . .ă f |F | ), and model the article left-to-right in that order using Equation 1:</p><formula xml:id="formula_2">x f 1 1 , x f 1 2 , . . . , x f |F | | f |F | | .</formula><p>However, this ordering would forbid sampling certain fields without prohibitively expensive marginalization. Alternatively, one could generate fields in any order, but this requires the model to learn to handle |F |! potential orderings during inference time.</p><p>Our solution is Grover, a new approach for efficient learning and generation of multi-field documents. We adopt the language modeling framework of Equation 1 in a way that allows for flexible decomposition of Equation 2. During inference time, we start with a set of fields F as context, with each field f containing field-specific start and end tokens. We sort the fields using a standard order 4 and combine the resulting tokens together. To generate a target field τ, we append the field-specific start token &lt;start´τ&gt; to the context tokens; then, we sample from the model until we hit &lt;end´τ&gt;.   <ref type="figure">Figure 2</ref>: A diagram of three Grover examples for article generation. In row a), the body is generated from partial context (the authors field is missing). In b), the model generates the authors. In c), the model uses the new generations to regenerate the provided headline to one that is more realistic. <ref type="figure">Figure 2</ref> shows an example of using Grover to generate an anti-vaccine article. Here, the adversary specifies a domain, date, and headline. After Grover generates the body, it can be used to generate a fake author, before finally generating a new and more appropriate headline.</p><p>During training, we simulate inference by randomly partitioning an article's fields into two disjoint sets F 1 and F 2 . We also randomly drop out individual fields with probability 10%, and drop out all but the body with probability 35%. This allows the model to learn how to perform unconditional generation. We sort the metadata fields in each set using our standard order, and concatenate the underlying tokens. The model is then trained to minimize the cross-entropy of predicting the tokens in F 1 followed by the tokens in F 2 . 5</p><p>Architecture. We draw on recent progress in training large Transformers for language modeling <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, building Grover using the same architecture as for GPT2 . We consider three model sizes. Our smallest model, Grover-Base, has 12 layers and 124 million parameters, on par with GPT and BERT-Base <ref type="bibr" target="#b25">(Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2018)</ref>. Our next model, Grover-Large, has 24 layers and 355 million parameters, on par with BERT-Large. Our largest model, Grover-Mega, has 48 layers and 1.5 billion parameters, on par with GPT2.</p><p>Dataset. We present RealNews, a large corpus of news articles from Common Crawl. Training Grover requires a large corpus of news articles with metadata, but none currently exists. Thus, we construct one by scraping dumps from Common Crawl, limiting ourselves to the 5000 news domains indexed by Google News. We used the Newspaper Python library to extract the body and metadata from each article. News from Common Crawl dumps from December 2016 through March 2019 were used as training data; articles published in April 2019 from the April 2019 dump were used for evaluation. After deduplication, RealNews is 120 gigabytes without compression.</p><p>Learning. We trained each Grover model on randomly-sampled sequences from RealNews with length 1024. Other optimization hyperparameters are in Appendix A. We trained Grover-Mega for 800k iterations, using a batch size of 512 and 256 TPU v3 cores. Training time was two weeks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Modeling results: measuring the importance of data, context, and size</head><p>We validate Grover, versus standard unconditional language models, on the April 2019 test set. We consider two evaluation modes: unconditional, where no context is provided and the model must generate the article body; and conditional, in which the full metadata is provided as context. In both cases, we calculate the perplexity only over the article body.</p><p>Our results, shown in <ref type="figure">Figure 3</ref>, show several conclusions. First, Grover noticeably improves (between .6 to .9 perplexity points) when conditioned on metadata. Second, perplexity decreases with size, with Grover-Mega obtaining 8.7 perplexity in the conditional setting. Third, the data distribution is still important: though the GPT2 models with 124M parameters and 355M parameters respectively match our Grover-Base and Grover-Large architectures, our model is over 5 perplexity points lower in both cases, possibly because the OpenAI WebText corpus also contains non-news articles.  Style Content Overall <ref type="figure">Figure 4</ref>: Human evaluation. For each article, three annotators evaluated style, content, and the overall trustworthiness; 100 articles of each category were used. The results show that propaganda generated by Grover is rated more plausible than the original human-written propaganda.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Carefully restricting the variance of generations with Nucleus Sampling</head><p>Sampling from Grover is straightforward as it behaves like a left-to-right language model during decoding. However, the choice of decoding algorithm is important. While likelihood-maximization strategies such as beam search work well for closed-ended generation tasks where the output contains the same information as the context (like machine translation), these approaches have been shown to produce degenerate text during open-ended generation <ref type="bibr" target="#b12">(Hashimoto et al., 2019;</ref><ref type="bibr" target="#b14">Holtzman et al., 2019)</ref>. However, as we will show in Section 6, restricting the variance of generations is also crucial.</p><p>In this paper, we primarily use Nucleus Sampling (top-p): for a given threshold p, at each timestep we sample from the most probable words whose cumulative probability comprises the top-p% of the entire vocabulary <ref type="bibr" target="#b14">(Holtzman et al., 2019)</ref>. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Neural Fake News Detection</head><p>The high quality of neural fake news written by Grover, as judged by humans, makes automatic neural fake news detection an important research area. Using models (below) for the role of the Verifier can mitigate the harm of neural fake news by classifying articles as Human or Machine written. These decisions can assist content moderators and end users in identifying likely (neural) disinformation.</p><p>a. Grover. We consider a version of our model adapted for discrimination. Similar to GPT <ref type="bibr" target="#b25">(Radford et al., 2018)</ref>, we place a special <ref type="bibr">[CLS]</ref> token at the end of each article, and extract the final hidden state at that point. The hidden state is fed to a linear layer to predict the label Human or Machine.</p><p>To simulate real conditions, and ensure minimal overlap between the generator and discriminator parameters, we initialize Grover for discrimination using the checkpoint at iteration 700k, whereas the generator uses the checkpoint at iteration 800k. b. GPT2, a 124M or 355M parameter pretrained Transformer language model. Similar to Grover, we follow the GPT approach and extract the hidden state from a newly-added <ref type="bibr">[CLS]</ref> token. c. BERT, a 110M parameter (BERT-Base) or 340M parameter (BERT-Large) bidirectional Transformer encoder commonly used for discriminative tasks. We perform domain adaptation to adapt BERT to the news domain, as well as to account for long articles; details in Appendix C. d. FastText, an off-the-shelf library for bag-of-ngram text classification <ref type="bibr" target="#b17">(Joulin et al., 2017)</ref>. Though not pretrained, similar models do well at detecting human-written fake news.</p><p>All models are trained to minimize the cross-entropy loss of predicting the right label. Hyperparameters used during discrimination are in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A semi-supervised setting for neural fake news detection</head><p>While there are many human-written articles online, most are from the distant past, whereas articles to be detected will likely be set in the present. Likewise, there might be relatively few neural fake news articles from a given adversary. <ref type="bibr">10</ref> We thus frame neural fake news detection as a semi-supervised problem. A neural verifier (or discriminator) has access to many human-written news articles from March 2019 and before -the entire RealNews training set. However, it has limited access to generations, and more recent news articles. Using 10k news articles from April 2019, we generate article body text; another 10k articles are used as a set of human-written news articles. We split the articles in a balanced way, with 10k for training (5k per label), 2k for validation, and 8k for testing.</p><p>We consider two evaluation modes. In the unpaired setting, a discriminator is provided single news articles, and must classify each independently as Human or Machine. In the paired setting, a model is given two news articles with the same metadata, one real and one machine-generated. The discriminator must assign the machine-written article a higher Machine probability than the human-written article. We evaluate both modes in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discrimination results: Grover performs best at detecting Grover's fake news</head><p>We present experimental results in <ref type="table">Table 1</ref> for all generator and discriminator combinations. For each pair, we show the test results using the most adversarial generation hyperparameters (top-p) as judged on the validation set. <ref type="bibr">11</ref> The results show several trends. First, the paired setting appears much easier than the unpaired setting, suggesting that it is difficult for the model to calibrate its predictions. Second, model size is highly important in the arms race between generators and discriminators. Using Grover to discriminate Grover's generations results in roughly 90% accuracy across the range of sizes. If a larger generator is used, accuracy slips below 81%; conversely, if the discriminator is larger, accuracy is above 98%. Third, other discriminators perform worse than Grover overall, even when controlling for architecture size and (for both BERT models) the domain.</p><p>That Grover is the best discriminator is possibly surprising: being unidirectional, it is less expressive than deep bidirectional models such as BERT. <ref type="bibr">12</ref> That the more expressive model here is not the best at <ref type="table">Table 1</ref>: Results of discriminators versus generators, in both the paired and unpaired settings and across architecture sizes. We also vary the generation hyperparameters for each generatordiscriminator pair, reporting the discrimination test accuracy for the hyperparameters with the lowest validation accuracy. Compared with other models such as BERT, Grover is the best at detecting its own generations as neural fake news.  <ref type="figure">Figure 5</ref>: Exploring weak supervision for discriminating Grover-Mega generations. With no weak supervision, the discriminator sees x machine-written articles (from Grover Mega). For`Grover-Base and`Grover-Mega, the discriminator sees 5000´x machine-written articles given by the weaker generator in question. Seeing weaker generations improves performance when few in-domain samples are given.</p><p>discriminating between real and generated news articles suggests that neural fake news discrimination requires having a similar inductive bias as the generator. 13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Weak supervision: what happens if we don't have access to Grover-Mega?</head><p>These results suggest that Grover is an effective discriminator when we have a medium number of fake news examples from the exact adversary that we will encounter at test time. What happens if we relax this assumption? Here, we consider the problem of detecting an adversary who is generating news with Grover-Mega and an unknown top-p threshold. 14 In this setup, during training, we have access to a weaker model (Grover-Base or Grover-Large). We consider the effect of having only x examples from Grover-Mega, and sampling the missing 5000´x articles from one of the weaker models, where the top-p threshold is uniformly chosen for each article in the range of r0.9, 1.0s.</p><p>We show the results of this experiment in <ref type="figure">Figure 5</ref>. The results suggest that observing additional generations greatly helps discrimination performance when few examples of Grover-Mega are available: weak supervision with between 16 and 256 examples from Grover-Large yields around 78% accuracy, while accuracy remains around 50% without weak supervision. As the portion of examples that come from Grover-Mega increases, however, accuracy converges to around 92%. 15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">How does a model distinguish between human and machine text?</head><p>In this section, we explore why Grover performs best at detecting fake news generated by other Grover models. We find that there is a double-bind between exposure bias and variance-reduction algorithms that alleviate these biases while at the same time creating other artifacts.</p><p>Exposure Bias. Models maximizing Equation 1 are trained only conditioned on human-written text, never on its own generations, creating a problem known as exposure bias <ref type="bibr" target="#b27">(Ranzato et al., 2016)</ref>.</p><p>We investigate the importance of exposure bias towards creating artifacts. In <ref type="figure">Figure 6</ref> we plot the perplexities given by Grover-Mega over each position for body text at top-p thresholds of 0.96 and 1, as well as over human text. Generating the first token after &lt;startbody&gt; results in high  <ref type="figure">Figure 6</ref>: Perplexities of Grover-Mega, averaged over each position in the body (after conditioning on metadata). We consider human-written with Grover-Mega generated text at p"1 (random sampling) and p".96.</p><p>The perplexity of randomly sampled text is higher than human-written text, and the gap increases with position. This suggests that sampling without variance reduction increasingly falls out-of-distribution. perplexity. However, the rest of the positions show a curious pattern: the perplexity of human-written text is lower than randomly sampled text, and this gap increases with sequence length, suggesting that random sampling causes Grover to fall increasingly out of the distribution of human language. However, limiting the variance (p"0.96) lowers the resulting perplexity and limits its growth.</p><p>Limiting the variance of a model also creates artifacts On the other hand, clipping the model's variance also leaves an artifact, as prior work has observed for top-k sampling <ref type="bibr" target="#b33">(Strobelt and Gehrmann, 2019)</ref>. A similar phenomenon holds for Nucleus (top-p) sampling. The probability of observing a human-written article where all tokens are drawn from the top-p% of the distribution is p n , where n is the document's length. This probability goes to zero as n increases. However, for Nucleus Sampled text -in which the final 1´p is cut off -all tokens come from the top-p.</p><p>The visibility of the artifacts depends on the choice of discriminator. The top-p at each timestep is calculated under the generator's worldview, meaning that if the discriminator models text in a different way, it might have a harder time pinpointing the empty 1´p tail. This could explain BERT's lower performance during discrimination.</p><p>A sweet spot of careful variance reduction Not reducing the variance, as well as significantly reducing the variance, both cause problems. Might there be a sweet spot for how much to truncate the variance, to make discrimination maximally hard? In <ref type="figure">Figure 7</ref>, we show results varying the top-p threshold for the discrimination task applied to Grover-Mega's generations. The results indeed show a sweet spot, roughly between p"0.94 and p"0.98 depending on the discriminator, wherein discrimination is hardest. Interestingly, we note that the most adversarial top-p threshold for BERT-Large is considerably lower than the corresponding top-p for Grover-Large of the same size. This supports our hypothesis that BERT's view of language differs markedly from Grover; using a lower top-p threshold does not seem to give it much more information about the missing tail.</p><p>Overall, our analysis suggests that Grover might be the best at catching Grover because it is the best at knowing where the tail is, and thus whether it was truncated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion: a Release Strategy for Grover</head><p>This paper investigates the threats posed by adversaries seeking to spread disinformation. Our sketch of what these threats might look like -a controllable language model named Grover -suggests that these threats are real and dangerous. Grover can rewrite propaganda articles, with humans rating the rewritten versions as more trustworthy. At the same time, there are defenses to these models -notably, in the form of Grover itself. We conclude with a discussion of next steps and ethical considerations.</p><p>The Era of Neural Disinformation. Though training Grover was challenging, it is easily achievable by real-world adversaries today. Obtaining the data required through Common Crawl cost $10k in AWS credits and can be massively parallelized over many CPUs. Training Grover-Mega is relatively inexpensive: at a cost of $0.30 per TPU v3 core-hour and two weeks of training, the total cost is $25k. Spending more money and engineering time could yield even more powerful generators.</p><p>Release of generators is critical. At first, it would seem like keeping models like Grover private would make us safer. However, Grover serves as an effective detector of neural fake news, even when the generator is much larger (Section 5). If generators are kept private, then there will be little recourse against adversarial attacks. We thus released our models to researchers <ref type="bibr" target="#b42">(Zellers, 2019)</ref>.</p><p>Future of progress in generation. Models like BERT are strong discriminators for many NLP tasks, but they are not as good at detecting Grover's generations as left-to-right models like Grover, even after domain adaptation. One hypothesis is that the artifacts shown in Section 6 are most visible to a left-to-right discriminator. This also suggests that recent progress on generating text in any order <ref type="bibr" target="#b10">(Gu et al., 2019;</ref><ref type="bibr" target="#b32">Stern et al., 2019;</ref><ref type="bibr" target="#b9">Ghazvininejad et al., 2019)</ref> may lead to models that evade a Grover discriminator. Likewise, models that are trained conditioned on their own predictions might avoid exposure bias, however, these objectives often lead to low performance on language tasks <ref type="bibr" target="#b2">(Caccia et al., 2018)</ref>. One additional possibility is the use of Adversarial Filtering <ref type="bibr" target="#b43">(Zellers et al., 2018;</ref><ref type="bibr" target="#b45">2019b)</ref> to oversample and then select a subset of generations. However, we found this didn't work well for very long sequences (up to 1024 BPE tokens), possibly as these are far from the 'Goldilocks Zone' wherein discrimination is hard for machines.</p><p>Additional threat models. In this paper, we studied the threat model whereby an adversary generates an entire news article from scratch, given minimal context. Other threat models are possible: for instance, an adversary might generate comments or have entire dialogue agents, they might start with a human-written news article and modify a few sentences, and they might fabricate images or video. These threat models ought to be studied by researchers also so that we can create better defenses.</p><p>Machine-generated real news? Our study focused on detecting machine-written fake news, though the same Grover approach can be used for spotting human-written fake news as well <ref type="bibr" target="#b46">(Zellers et al., 2019c)</ref>. However, machines can also generate truthful news using templated systems. Domains with templated news articles exist in our dataset, <ref type="bibr">16</ref> and are easy for Grover to spoof convincingly.</p><p>Future of progress in discrimination. Our discriminators are effective, but they primarily leverage distributional features rather than evidence. In contrast, humans assess whether an article is truthful by relying on a model of the world, assessing whether the evidence in the article matches that model. Future work should investigate integrating knowledge into the discriminator (e.g. for claim verification in FEVER; <ref type="bibr" target="#b35">Thorne et al., 2018)</ref>. An open question is to scale progress in this task towards entire news articles, and without paired evidence (similar to open-domain QA; <ref type="bibr" target="#b3">Chen et al., 2017)</ref>.</p><p>What should platforms do? Video-sharing platforms like YouTube use deep neural networks to scan videos while they are uploaded, to filter out content like pornography <ref type="bibr" target="#b15">(Hosseini et al., 2017)</ref>. We suggest platforms do the same for news articles. An ensemble of deep generative models, such as Grover, can analyze the content of text -together with more shallow models that predict humanwritten disinformation. However, humans must still be in the loop due to dangers of flagging real news as machine-generated, and possible unwanted social biases of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Optimization Hyperparameters</head><p>For our input representation, we use the same BPE vocabulary as . We use Adafactor <ref type="bibr" target="#b30">(Shazeer and Stern, 2018)</ref> as our optimizer. Common optimizers such as Adam (Kingma and Ba, 2014) tend to work well, but the memory cost scales linearly with the number of parameters, which renders training Grover-Mega all but impossible. Adafactor alleviates this problem by factoring the second-order momentum parameters into a tensor product of two vectors. We used a maximum learning rate of 1e-4 with linear warm-up over the first 10,000 iterations, and decay over the remaining iterations. We set Adafactor's β 1 " 0.999 and clipped updates for each parameter to a root-mean-squared of at most 1. Last, we applied weight decay with coefficient 0.01. We used a batch size of 512 on 256 TPU v3 cores. which corresponds to roughly 20 epochs through our news dataset. The total training time required roughly two weeks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Real News and Propaganda Websites</head><p>In our generation experiments (Section 4), we consider a set of mainstream as well as propaganda websites. We used the following websites as 'real news': theguardian.com, reuters.com, nytimes.com, theatlantic.com, usatoday.com, huffingtonpost.com, and nbcnews.com. For propaganda sites, we chose sites that have notably spread misinformation <ref type="bibr" target="#b5">(Dicker, 2016)</ref> or propaganda 17 . These were breitbart.com, infowars.com, wnd.com, bigleaguepolitics.com, and naturalnews.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Domain Adaptation of BERT</head><p>BERT <ref type="bibr" target="#b4">(Devlin et al., 2018</ref>) is a strong model for most classification tasks. However, care must be taken to format the input in the right way, particularly because BERT is pretrained in a setting where it is given two spans (separated by a special [SEP] token). We thus use the following input format. The first span consists of the metadata, with each field prefixed by its name in brackets (e.g. '[title]').</p><p>The second span consists of the body. Because the generations are cased (with capital and lowercase letters), we used the 'cased' version of BERT.</p><p>Past work (e.g. <ref type="bibr" target="#b44">Zellers et al. (2019a)</ref>; <ref type="bibr" target="#b11">Han and Eisenstein (2019)</ref>) has found that BERT, like other language models, benefits greatly from domain adaptation. We thus perform domain adaptation on BERT, adapting it to the news domain, by training it on RealNews for 50k iterations at a batch size of 256. Additionally, BERT was trained with a sequence length of at most 512 WordPiece tokens, but generations from Grover are much longer (1024 BPE tokens). Thus, we initialized new position embeddings for positions 513-1024, and performed domain adaptation at a length of 1024 WordPiece tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters for the Discriminators</head><p>For our discrimination experiments, we limited the lengths of generations (and human-written articles) to 1024 BPE tokens. This was needed because our discriminators only handle documents up to 1024 words. However, we also found that the longer length empirically discrimination easier for models (see Section 6).</p><p>For our discrimination experiments, we used different hyperparameters depending on the model, after an initial grid search. For BERT, we used the Adam (Kingma and Ba, 2014) optimizer with a learning rate of 2e´5 and a batch size of 64. We trained BERT models for 5 epochs, with a linear warm-up of the learning rate over the initial 20% iterations. For GPT2 and Grover, we used the Adam actor optimizer <ref type="bibr" target="#b30">(Shazeer and Stern, 2018)</ref> optimizer with a learning rate of 2e´5 for all models, and a batch size of 64. We applied an auxiliary language modeling loss for these models with a coefficient of 0.5. These models were trained for 10 epochs, with a linear warm-up over the initial 20% iterations.</p><p>In <ref type="figure" target="#fig_3">Figures 8 and 9</ref>, we include examples of articles with the average scores given by human raters, who were asked to evaluate the style, content, and overall trustworthiness. In <ref type="figure" target="#fig_3">Figure 8</ref>, we show a real article (Human News) posted by the Guardian along with an article from Grover (Machine News) made using the same metadata. <ref type="figure">Figure 9</ref> shows a real propaganda article from the Natural News (Human Propaganda) and an article made with Grover (Machine Propaganda) with the original headline and the style of Huffington Post (Grover was used to re-write the title to be more stylistically similar to the Huffington Post, as well).</p><p>We also present several other generated examples, generated from Grover-Mega with a top-p threshold of p"0.95. All of the examples are cut off to 1024 generated BPE tokens, since this is our setup for discrimination.</p><p>a. Grover can generate controlled propaganda. In <ref type="figure">Figure 10</ref>, we show the continuation from <ref type="figure">Figure 1</ref>, about a link found between autism and vaccines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Timing of May's 'festival of Britain' risks Irish anger</head><p>April 13, 2019 theguardian.com It was meant to be a glimmer of positivity to unite a divided nation -a festival to celebrate the best of British, bring communities together and strengthen "our precious union". Yet Theresa May is being warned that her plan for a Festival of Great Britain and Northern Ireland risks doing the opposite. The planned 2022 event, announced at last year's Conservative conference, was criticised as a headline-grabbing distraction. But May now faces concerns that the timing clashes with the centenary of Irish partition and the civil war. Arts industry figures in Northern Ireland and some of those involved in the peace process are also understood to have concerns. These worries are revealed in a report by the thinktank British Future, which examined the potential for arts and heritage to bring the nation together. The study calls on the festival to be delayed by at least three years.</p><p>What is now the Irish republic became the Irish Free State in 1922, while Northern Ireland remained part of the UK. A civil war erupted among Irish nationalists over the remaining links with Britain and raged for a year. Sunder Katwala, the report's author, said: "Holding a festival of Great Britain and Northern Ireland in 2022, on the centenary of Ireland's partition and civil war, would be the worst possible timing. It is only likely to heighten tensions between communities -and that's before we know Brexit's implications for the border. Right across the UK, a festival so closely associated with Brexit may only reinforce divides when it could be bridging them." Jonathan Powell, Tory Blair's former chief of staff and one of the architects of the Good Friday agreement, also warned against anything that could inflame tensions, which have worsened given the prominent role of the Irish border in the Brexit debate. "There is going to be a lot of rebuilding to do in Anglo-Irish relations, which had been put on a completely new footing over the last 20 years and were then trashed by the Brexiters over the last couple of years," he said. "An example is their constant attacks on [Irish prime minister Leo] Varadkar for hostility when all he is doing is protecting the interests of the Irish people. I fear it will take years to put this right."</p><p>The idea of a nationwide event was modelled on the Great Exhibition of 1851 and the Festival of Britain a century later, held to boost the nation's spirits after the second world war.</p><p>May was mocked on social media when she announced she was setting aside £120m to prepare for the event, with some suggesting the festival would only feature damp tents on village greens, an exhibition of the remaining avocados left in Britain and morris dancers. However, after holding focus groups, British Future found it could be more popular than critics think, should organisers get the tone and timing right. More than six in 10 thought the festival a good idea, with only one in 10 opposed, according to new ICM research. "The festival could be much more popular with the public than its social media critics think, and could help bridge social divides if done well," Katwala said. "If not, it could be a second Millennium Dome. It risks being neglected, with the government stuck over Brexit and the prime minister who thought of it on her way out. The next prime minister will have to decide how to rescue this idea."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Timing of May's 'festival of Britain' risks Irish anger</head><p>April 13, 2019 theguardian.com</p><p>The prime minister's decision to launch her much-hyped "great Britain" conference on Monday -the same day as a Lisbon treaty event paving the way for Brexit-free member states to leave the European Union -will be seen as provocative by some, according to senior Tories. Jo Johnson said May's statement in Edinburgh was "instrumental" in chipping away at the strength of domestic opposition to Brexit. He added that the prime minister had also "churned" the membership of the ERG back into service. Tom Pursglove, another Tory MP involved in the campaign to prevent Brexit, said: "By lifting the gagging order on ERG members from the Liaison Committee and starting an intensification of the ethnic profiling of Remainers, the prime minister is doing herself and the ERG proud." Announcing that the conference would launch her vision for the country, May will call for more global Britain to fight for global trade. Although still committed to leaving the single market and customs union, the Conservatives want to highlight the importance of these deals -as well as tackling climate change, tackling modern slavery and tackling poverty. The event will be on Monday 29 April, the day before the EU's 2019 budget is agreed. May's Treasury chief secretary, Liz Truss, is to try to convince European finance ministers that there is no alternative plan to Brexit. EU officials and political leaders are scheduled to decide the EU's £1.2tn budget in mid-October. The Northern Ireland-based DUP, which failed to back May in the no confidence vote she suffered earlier this month, will be encouraged by the event. The DUP said it would be "easy to ignore" the motions at the conference, but would vote against any effort to transfer powers to Brussels. Labour MP Sir Keir Starmer, who now chairs the cross-party Brexit negotiations committee, said: "The timing of her conference announcement raises some worrying issues. We cannot allow the UK's terms of exit to be dictated by no confidence votes. "These checks cannot be on the superficial level, where some make noises on the hill but are wholly unwilling to set out detailed proposals. Tighter controls at Heathrow are essential, and if May really wants to celebrate 'all change', then she should close Britain's borders for a week and see how workable it is to stop EU nationals from flying in on the same visa system as Brits. "Brexit would be fantastic for the business world if you measure economic value only on the quality of the deal. But -and when we say 'if' the prime minister doesn't care that she is still far short of securing that 'good deal' -she needs to work harder to deliver that for her negotiators." Other critics, including party member James Ball, drew parallels with Brexit minister Dominic Raab's similar focus on trade deals to stop other EU states leaving the bloc. They said Raab's speech last week was "the latest Labour-held ploy to quietly delay Brexit, run out the clock or blame everyone except the UK for not being willing to walk away".</p><p>• Follow Guardian Opinion on Twitter at @gdnopinion b. Grover can spoof the identity of writers. In <ref type="figure">Figure 11</ref> we show a realistic-looking editorial seemingly from New York Times columnist Paul Krugman. c. Grover can generate fake political news. In <ref type="figure" target="#fig_5">Figure 12</ref> we show an article generated about Trump being impeached, written in the style of the Washington Post. d. Grover can generate fake movie reviews (opinion spam; <ref type="bibr" target="#b22">Ott et al. (2011)</ref>). In <ref type="figure">Figure 13</ref> we show a movie review, generated in the style of LA Times Movie Critic Kenneth Turan, for Sharknado 6, 'The Last Sharknado: It's About Time' e. Grover can generate fake business news. In <ref type="figure">Figure 14</ref>, we show an article generated about an 'Uber for Dogs' startup. To back up its decision, the government embarked on a series of flawed and poorly designed "scientific" studies, which an expert later lambasted as "especially rich in fallacies, improper design, invalid use of statistical methods, omissions of contrary data, and just plain muddleheadedness and hebetude." They then used these sham studies to enforce a national policy of water fluoridation. Studies confirm fluoride lowers IQ and harms children in other ways Interestingly, even government-backed studies have confirmed the dangers of fluoride in drinking water. For example, a study published in 2017, which was largely funded by the government's National Institutes of Health and National Institute of Environmental Health Sciences, uncovered a "strong relationship" between fluoride exposure in the womb and reduced cognitive function. In addition, Natural News previously reported: More than 50 peer-reviewed studies have linked the consumption of fluoridated water to lower IQ in children. A joint metanalysis by Harvard School of Public Health and China Medical University, which examined 27 studies on the subject, found "strong indications that fluoride may adversely affect cognitive development in children." Fluoridation has also been linked to countless other devastating health effects in children, including premature birth, impaired neurological development, autism and preeclampsia. A recent study also confirmed a significant link between fluoridation and ADHD. Death rates up to 26 percent higher in the most fluoridated states And it gets worse. As reported by Waking Times, death rates among people who live in the 10 states with the highest fluoridation levels are between 5 and 26 percent higher than among people living in the 10 least fluoridated states. Furthermore, studies have linked fluoridated water to cancer, damaging effects on the brain, and dental fluorosis, which experts agree is the first visible sign of fluoride toxicity. The National Research Council of the National Academies also issued a warning in 2006 that fluoride exposure might be a risk factor in the development of Alzheimer's disease. And to top it all off, there is literally no evidence that fluoride even prevents cavities! Even if it did, would that small benefit really be worth the massive potential for serious health problems that fluoridation has been scientifically linked to? Fluoride is regarded by researchers around the world as the "gold standard" in tooth care, and a safe, common ingredient that has almost been universally found to be safe in past studies of health effects. It's absorbed without interference from the body's natural minerals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human-written News Article</head><p>Researchers at the U.S. Geological Survey (USGS) published the results of a multi-state environmental health study last month. It showed that during the first three decades of fluoridation of tap water systems, fluoride produced from the process alone increased rates of dental caries (the biggest contributor to tooth decay) by 16 percent in Mississippi and a whopping 45 percent in Arizona, which implemented fluoridation systems back in 1942. This increase was seen after a decade when fluoride levels didn't change.</p><p>USGS also found that fluoridation increased rates of other toxicants and petrochemicals, as well as deaths from brain, lung, kidney and bladder cancer.</p><p>It bears noting that there is no clear proof that these specific contaminants were caused by fluoridation, but the USGS study at least hints that this was the case. The epidemic of brain cancers across the U.S. -especially in teenagers -has confounded researchers for decades. The USGS study points to links to numerous studies that have linked water fluoridation with increased risks of cancer. Even though the majority of studies on water fluoridation have not produced such alarming results, the mainstream medical community is, apparently, still skeptical. Two years ago, doctors from Harvard and Duke universities suggested that fluoride is associated with lower IQ scores and autoantibodies to water. The results of a recent study that followed more than 700 children over a period of four years demonstrated that the kids were more likely to have symptoms of illness, more likely to have higher blood pressure and sleep problems, had higher mean energy expenditure, more struggles with attention and poorer concentration and performance on educational tasks. The data also showed that the children were not more vulnerable to fluoride when it was administered by toothpaste. Even though the majority of studies on water fluoridation have not produced such alarming results, the mainstream medical community is, apparently, still skeptical. The link between bacteria and tooth decay is legendary. Experts agree that fluoride erases a cavity's effect on surface-level bacteria and increases decay via dangerous pathways to deeper tissue. Fluoride actually quarantines surface-level bacteria to caries-causing toxins in the plaque and oral cavity. The longer the fluoride is in contact with those toxins, the more damage and damage can occur. Evaluations of the impacts of water fluoridation are well underway, but we've seen too many conflicts of interest and dirty laundry in the past few years. One of the largest of those is the Water Fluoridation Corporation (WFC), an industry network that carries on and even finances the work of paid "expert witnesses" in both domestic and international water fluoridation litigation. The WFC boasts 38 former and current members of the American Dental Association, including board members, former ADA commissioners, and representatives of the ADA-funded American Academy of Oral Medicine and American Dental Hygienists. It is arguably the second largest source of fluoride in the U.S. right behind the ADA. Despite denials, none of this new water research paints a rosy picture of water fluoridation. We can blame these new findings on ongoing conflicts of interest and blatant biases. Fluoride in your drinking water isn't safe, even when it's recommended.</p><p>There are three risks that we as consumers need to ensure that are understood: 1) the hidden contaminants that may be causing harm; 2) the effects of over-medicating the fluoride vaccine; and 3) low-dose levels of fluoride in very old bones, causing unnecessary bone damage and irreparable nerve damage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human-written Propaganda Machine-written Propaganda</head><p>Original Headline: Don't drink the water: The dark side of water fluoridation  <ref type="figure">Figure 9</ref>: Example of human-written and machine-written articles arguing against fluoride with the average ratings from human rating study.</p><p>Provided headline: 'Link found between autism and vaccines' Generated body: Those who have been vaccinated against measles have a more than 5-fold higher chance of developing autism, researchers at the University of California San Diego School of Medicine and the Centers for Disease Control and Prevention report today in the Journal of Epidemiology and Community Health. Those who have been vaccinated against measles have a more than 5-fold higher chance of developing autism. Earlier studies had linked past vaccines with the condition, but it was not clear whether those links were causal or indirect. Among research groups, the UCSD and CDC teams used new methods that enabled them to determine the associations between the virus and the disease. While researchers in the previous studies had observed associations between those vaccinated against measles and autism in children whose mothers had received the virus, the current UCSD/CDC team focused on childhood vaccines given during the first two years of life because it is commonly believed those vaccines begin to clear the body's immune system before the onset of autism. The UCSD/CDC team compared children who had been vaccinated against measles, mumps and rubella (MMR) and those who had not. "Because this is the first study to focus on MMR-vaccine syndrome, it was also the first time that this syndrome had been scientifically characterized," said study lead author William Chan, MD, MPH, a researcher in the Department of Pediatrics in the UC San Diego School of Medicine. "We were able to identify certain subgroups of those vaccinated against MMR and compare them to those not vaccinated." Mumps, measles and rubella vaccines are administered in either three or five doses, depending on the age. A recent study by the Public Health Agency of Canada found only 10 percent of children vaccinated against measles had a developing autism disorder when compared to 73 percent of children vaccinated with one dose, 50 percent of those vaccinated with two doses and 45 percent of those vaccinated with three doses. Those results do not surprise the UC San Diego researchers, who found those vaccinated with three or more doses were at an additional 5-to 8-fold increased risk. The current UCSD/CDC analysis followed the same years of data as earlier studies, but used new methods to accurately pinpoint the timing of those immunity-shifting epidemics. For example, previous studies showed that autism was on the rise in 1995-96, but never topped 50 cases until 2001-02, when measles overtook mumps as the leading cause of infant death due to disease. In 2003, several measles outbreaks were triggered by parents who had ignored warnings about the vaccine. "Our research was critical in providing details of when outbreaks occurred in regions where there was significant vaccination efforts," said study co-author Louis Cooley, MD, PhD, professor of Pediatrics and professor of Communicable Diseases and Epidemiology at UCSD School of Medicine. "We were able to show that if you get your MMR vaccine in early childhood, you should reduce your risk of autism by 33 percent, with every passing year bringing a slightly lower risk." Researchers found that low vaccination rates -42 percent in 2003-04 and 75 percent in 2007-08 -and more frequent, and severe, cases of measles predicted higher rates of autism in later years. In 2004-05, the annual rate of autism and other disorders -including intellectual disability -increased by 20 percent, far more than the 5-to 7-fold increase in their model predicted, suggesting there may be a more subtle link between vaccine exposure and autism than previously understood. Preliminary findings from a separate analysis of the 2006-07 data suggest that autism risk spikes occurred during times when children had fewer routine vaccinations and received older doses of vaccines. Autism spectrum disorders include autism spectrum disorders (ASD), a group of neurodevelopmental disorders that can occur with or without symptoms of the immune system disorder, including Asperger's syndrome, pervasive developmental disorder not otherwise specified (PDD-NOS) and pervasive developmental disorder-not otherwise specified (PDD-ENG). The school has been recognized as the first medical school in the world to offer its entire physician-practitioner curriculum online, currently serving more than 1,000 medical students worldwide. The school also trains the next generation of scientists, physicians and public health leaders and ranks first in the nation for its graduates' ability to train other healthcare professionals. UC San Diego is associated with UC San Diego Health System, which has a faculty of more than 9,000 and an annual <ref type="figure">Figure 10</ref>: The rest of the example from the teaser figure, about a hypothetical link found between autism and vaccines. to the idea of impeachment, to the point where last week she endorsed the use of the word in a speech. And this is not to claim that "impeachment" is a possible solution to the current impasse on immigration. Nobody seriously thinks that. Rather, it's to say that the dividing line between Trump's impeachment and, on the other hand, the notion that he's being treated adequately in his courtship of the Republican base, is soft. If Trump runs a replay of his 2016 campaign, the focus on immigration will fade as an issue. Then his campaign's fiery rhetoric on the economy and taxes will dominate, and get bigger. But as we pointed out yesterday, the "soul" of the Trump campaign was white nationalist sloganeering and authoritarianism. It seems unlikely that this will change. If it doesn't change, impeachment seems like the only way to constrain him. Yet the power of the impeachment path is out of Trump's hands. The recent passage of the Defense Authorization Act, to cite just one example, which codifies within the law the requirement that presidents get an honest break from investigations, seems to have guaranteed that Bob Mueller will be working hard into the fall. And "executive privilege" -the right of a president to withhold certain materials from the government -is clearly limited by the extent to which federal judges have looked at and rejected it. So impeachment is less a decision than a historical fact: If you think the election of a president who promised to enrich himself through corruption has happened, a court-ordered investigation, or both, is probably necessary to correct the damage. But it can't happen while he's president, or until he's gone. Even then, there are a few ways that a president can claim executive privilege, or whatever other arguments he can throw out to minimize the pressure on him. So Congress must seize the moment. The first and most urgent must be an investigation of the Trump campaign's possible ties to the Russians, and the subsequent question of whether the president knew what was going on. (For full coverage of the Russia story, see here.) The second, more abstract but no less important issue is whether the president's own actions constitute obstruction of justice. And third, Trump needs to be impeached because his rhetoric has helped put members of his own party who could back his impeachment into near paralysis. If Republicans don't take a bold stand, and appoint a special prosecutor to investigate all this -just as Democrats did during Watergate -then their caucus will become dependent on impeachment proceedings to hold its seats. While that might be a good outcome in the short run, it could also signal a likely government shutdown -just as it signaled the risk of a government shutdown during the Clinton impeachment. So it's a bad decision for the country, or at least for the country's self-interest. That said, it could be an even worse one for Trump. True, other presidents have been impeached in part for behavior far more extreme than Trump's -for cases like Bill Clinton's loveless marriage, and, more frequently, for their handling of women in private. But never before has the impeachment authority come at the point where a man at the pinnacle of power and influence has cozied up to a hostile foreign power, and openly attacked not just the judiciary, but the rule of law itself. For those who would recommend removal now for that alone, it's worth noting that on a serious level, that kind of behavior isn't akin to the brinkmanship that some U.S. presidents have brought to bear during a confrontation with Cuba. It's as close as Trump has come to laying out some of the first lines of battle for our uncertain and uneasy world. And to me, that is a form of obstruction of justice. <ref type="figure">Figure 11</ref>: An example of spoofing someone's identity using Grover-Mega. We generate a convincing New York Times Op-Ed that is seemingly from columnist Paul Krugman.</p><p>Provided domain: washingtonpost.com Provided date: May 29, 2019 Provided headline: US House Votes to Impeach Donald Trump After New Evidence Released from Mueller Report Generated body: WASHINGTON -The House voted to impeach President Donald Trump Wednesday after releasing hundreds of pages of text messages that point to clear evidence of obstruction of justice and communication with the head of the Trump Organization about a potential business deal in Russia. The 220-197 vote came after weeks of debate over whether new evidence released by special counsel Robert Mueller's office signaled sufficient grounds for Trump's removal from office. The president personally denounced the move, announcing his intent to veto the resolution and accusing Democrats of plotting to remove him from office through a "con job." "This is all a big con job," he said during an appearance in Texas. "They got caught." A number of Democratic lawmakers went further, calling for Trump's impeachment on the grounds that he obstructed justice. Democrats hold the House majority, and if the Senate fails to convict Trump, the president can be removed from office by a two-thirds vote. But even with Trump's political allies organizing from the White House, Republicans in the House stood united behind their leader, noting the historical difficulty of making the case for impeachment. "This is not a decision for the president," said House Speaker Paul Ryan, R-Wis. "It's not one for this body, and I think it's a dereliction of duty for the minority party to be making these calls." Democrats on the House Judiciary Committee on Tuesday said they were not bound by Ryan's defense. They released an 11-page joint report with Republicans on the House Oversight and Government Reform Committee that took a different tack, emphasizing that Mueller had not decided whether the case warranted impeachment. Rep. Jerrold Nadler, D-N.Y., the committee's chairman, told reporters before the vote that he hoped the two reports would "send a clear message to the president that we in this body are not going to stand for obstruction of justice." But when asked whether the report would build the case for impeachment, he said: "What's far more likely to build a case for impeachment are the indictments and evidence produced through the special counsel's work." On Tuesday night, the White House issued a statement proclaiming that the book by a former FBI deputy director, which has been cited by Democrats in support of impeachment, had been "made up and defamatory" to the president. That statement drew swift and heavy rebukes from a number of Democrats, who said they had found the statement "beyond astonishing." The Democratic report, citing redacted text messages sent by former FBI counterintelligence agent Peter Strzok, revealed what the party called "a pattern of bias" and "an affinity for the Russian regime." The report did not include a full, uncensored text message Strzok sent to a friend after Trump was elected that said: "I'll stop it." That led Democrats to bemoan the language used, questioning what is or is not in the special counsel's investigative file. The report also quoted a review of emails and witness testimony to Mueller that said Trump "has obstructed justice in the Russia investigation" by firing FBI Director James Comey, by urging Attorney General Jeff Sessions to recuse himself from the Russia investigation and by urging his son, Donald Trump Jr., to meet with a Kremlin-connected Russian lawyer at Trump Tower in 2016. "This has never happened in the history of our country," Nadler said of the president. "No president has ever tried to interfere with an investigation into his own conduct." The vote was also led by Democrats who have been outspoken against Trump, including Rep. Al Green, D-Texas, who took out newspaper ads calling for the president's impeachment. Rep. Maxine Waters, D-Calif., another outspoken Trump critic, said Trump's ties to Russia were "becoming more and more ominous," and said "as we go on, we are going to see if we can't get the articles of impeachment out." Rep. Tony Cárdenas, D-Calif., a member of the Judiciary Committee, said many Democrats would be eager to see whether the newly discovered evidence could be used to support impeachment. "I think it will be the nail in the coffin of the 'non-con,'" he said. Generated headline: Congress voted to impeach President Trump Wednesday; the President vowed to veto the resolution. Provided domain: techcrunch.com Provided date: May 29, 2019 Provided headline: Rowan Zellers' new startup raises 17M in Series A round for "Uber for Dogs" Generated body: In today's competitive eco-system of startups trying to convince consumers to eschew popular apps in favor of their own, you would think that the dog-walking market would be the easiest to crack. Well, it's not. But at least one startup believes it has a new and clever solution: apply its core offering of mobile-based recommendation engine algorithms to locate dog owners near them with the end goal of making dog walking more efficient. The company, owned by Australian startup Yfrog, is called DogSpotter. It is co-founded by entrepreneur Rowan Zellers, along with King's College London-trained dogsman Craig Osbaldoon. Launched in Melbourne in November, DogSpotter has now launched in Sydney, Canberra, Adelaide, Perth, and is set to be introduced to Brisbane and Hobart in June, with a platform the company expects to be available in all capital cities in Australia this year. In addition, it is now prepared to accept payment via credit cards in Australia and New Zealand. In exchange for its name recognition and the unique proposition that it is aiming to turn into a software-as-aservice, DogSpotter has raised $17 million in a Series A round of funding led by Plug and Play Tech Center, with participation from Open Ocean Capital and previous investors, including the Cahill Foundation. So what does DogSpotter offer, beyond a fairly basic service that could theoretically be used by anyone, which is at the base of a human-side algorithm that determines the best potential location for a dog owner's pet -a problem that dogs are notoriously impatient for in cities where dog walking is often unprofitable or a lost art for many? Well, DogSpotter leverages its core recommendation engine to process the thousands of now readily available reviews of dog care providers via its app that users can find by searching for their own city or a curated selection of recommendations. That way, it's much quicker than traditional alternatives, where you have to wade through a fair number of reviews to find the best-rated providers in your city, and does it with far less hassle. From there, the DogSpotter app makes use of the data it provides to recommend relevant walking services, among them a variety of preferred types of walks for your dog, based on everything from their preferences and sorts of paths to the weather, geographic differences, and degree of safety. DogSpotter's founders also contend that their software can -and in some cases already has been -used to make sure that the actual human responsible for dog walking is nearby, by using the location-based data to facilitate long-distance walkings or picking out a dog for a veterinarian who can be contacted. Speaking of which, they're not just operating on the basis that it's an effective way to walk dogs without suffering losses. Although I won't deny that I've seen some big cities suffer from too many small businesses advertising purely online and with little regard for actual human location. As it happens, DogSpotter tells me it has its own network of paying businesses in places like U.S. cities, but it claims to be seeing a company like DogBuddy.com.au as a market that it is disrupting. My hope is that the company ends up developing a number of other application-specific verticals such as veterinary procedures -again, I have not seen this yet but that would be a great and easy way to monetize. The business model is to offer DogSpotter services for a set fee per user, per location per month. It includes complimentary services for canine community or adoption groups, dog run assessment and mapping, dog boarding facilities, dog walking packages, and a myriad of other services. "DogSpotter is different from other dog walking platforms because we have the leading mobile app which is the most effective tool for location-based visual targeting for relevant content and experiences," said Rowan Zellers, founder and CEO of DogSpotter, in a statement. "This means that DogSpotter is able to provide more relevant products and services and, for early users, we can allow companies to target only the users they want to engage with." The problem, of course, is that not all dog walkers have the best apps. DogSpotter says it will compete on performance of its offering, not with other dog walking apps but rather with making it easier for dog owners to find the people they are looking for and, importantly, for those individual dog walkers to save a small bit of money on their operating costs, by saving time and effort with a website. And so on. Generated headline: DogSpotter raises $17M to improve your search for walking services <ref type="figure">Figure 14</ref>: An example of using Grover-Mega to generate fake business news. This generates an article about a fake startup for 'Uber for Dogs', ostensibly created by the first author of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Example of human-written news and machine-written news articles about the same headline from The Guardian with the average ratings from human rating study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The study was supported by the National Institutes of Health's National Institute of Child Health and Human Development (CDER), the NIH Program on Smoking and Health and the San Diego Veterans Affairs (SDVA) Healthcare System. About the University of California, San Diego School of Medicine Founded in 1897, the School of Medicine has been recognized nationally for outstanding achievement in educating clinical and research physicians. Nobel laureates and more than 170 physicians who practice in the United States and abroad are the graduates of UC San Diego.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>An example of using Grover-Mega to generate fake political news, written in the style of the Washington Post. Here, it generates a mostly-realistic-looking article about Trump being impeached.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Learn more at Fluoride.news or TapWater.news.</figDesc><table><row><cell>Fluoride in Your Water Isn't Healthy, Even When It's Recommended</cell></row><row><cell>March 13, 2019 huffingtonpost.com</cell></row><row><cell>Sources include:</cell></row><row><cell>WakingTimes.com</cell></row><row><cell>NaturalNews.com</cell></row><row><cell>ChildrensHealthDefense.org</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>President Donald Trump could get impeached. Or he could get fired. Or he could be indicted for special-counsel matters involving Stormy Daniels, or the Russians, or his own campaign. But, at the end of the day, what would matter is what House Speaker Nancy Pelosi decides. And here's why: First, Pelosi seems open</figDesc><table><row><cell>Provided domain: nytimes.com</cell></row><row><cell>Provided author: Paul Krugman</cell></row><row><cell>Provided date: May 23, 2019</cell></row><row><cell>Generated headline: Why Trump could get impeached</cell></row><row><cell>Generated body:</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We thank past work, such as OpenAI's Staged Release Policy for GPT2 for drawing attention to neural disinformation, alongside other dual-use implications.2 Short for Generating aRticles by Only Viewing mEtadata Records.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A common workaround is to have a human seed the text to provide context. However, this a) is a heavy handed technique for biasing which may not capture the desired attributes, and b) leaves in place a human-written beginning (as tokens are only generated left-to-right), which may create distributional artifacts.4  Our ordering is the following field types in order: domain, date, authors, headline, and then the body.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">All tokens use the same vocabulary. By using a standard order, but partitioning the fields into two sets, the model can generate any field conditioned on others while only needing to learn 2 |F | orderings, versus |F |!.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In early experiments, we found Nucleus Sampling produced better and less-detectable generations than alternatives like top-k sampling, wherein the most probable k tokens are used at each timestep<ref type="bibr" target="#b7">(Fan et al., 2018)</ref>.7  We use the technique described inFigure 2to rewrite the propaganda: given the metadata, generate the article first, and then rewrite the headline.8  With these guidelines, we tried to separate style versus content. Overall trustworthiness asks 'Does the article read like it comes from a trustworthy source?' which emphasizes style, while content sensibility asks whether the content is believable on a semantic level.9  This difference is statistically significant at p " 0.01. One possible hypothesis for this effect is that Grover ignores the provided context. To test this hypothesis, we did a human evaluation of the consistency of the article body with the headline, date, and author. We found that human-written propaganda articles are consistent with the headline with an average score of 2.85 of 3 on the same 1-3 scale, while machine-written propaganda is consistent with 2.64 of 3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Moreover, since disinformation can be shared on a heterogeneous mix of platforms, it might be challenging to pin down a single generated model.11  For each discriminator/generator pair, we search over p P t.9, .92, .94, .96, .98, 1.0u. 12 Indeed, bidirectional approaches perform best on leaderboards like GLUE<ref type="bibr" target="#b38">(Wang et al., 2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">This matches findings on the HellaSwag dataset<ref type="bibr" target="#b45">(Zellers et al., 2019b)</ref>. Given human text and machine text written by a finetuned GPT model, a GPT discriminator outperforms BERT-Base at picking out human text.14 The top-p threshold used was p"0.96, but we are not supposed to know this!15  In additional experiments we show that accuracy increases even more -up to 98% -when the number of examples is increased<ref type="bibr" target="#b46">(Zellers et al., 2019c)</ref>. We also find that Grover when trained to discriminate between real and fake Grover-generated news can detect GPT2-Mega generated news as fake with 96% accuracy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">An example is https://americanbankingnews.com.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">For more information, see the Media Bias Chart at adfontesmedia.com/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Humans are Easily Fooled by Grover-written Propaganda</head><p>We evaluate the quality of disinformation generated by our largest model, Grover-Mega, using p".96. We consider four classes of articles: human-written articles from reputable news websites (Human News), Grover-written articles conditioned on the same metadata (Machine News), human-written articles from known propaganda websites (Human Propaganda), and Grover-written articles conditioned on the propaganda metadata (Machine Propaganda). 7 The domains used are in Appendix B; examples are in Appendix F. We asked a pool of qualified workers on Amazon Mechanical Turk to rate each article on three dimensions: stylistic consistency, content sensibility, and overall trustworthiness. 8</p><p>Results <ref type="figure">(Figure 4)</ref> show a striking trend: though the quality of Grover-written news is not as high as human-written news, it is adept at rewriting propaganda. The overall trustworthiness score of propaganda increases from 2.19 to 2.42 (out of 3) when rewritten by Grover. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, as well as Dan Weld, for their helpful feedback. Thanks also to Zak Stone and the Google Cloud TPU team for help with the computing infrastructure. This work was supported by the National Science Foundation through a Graduate Research Fellowship (DGE-1256082) and NSF grants (IIS-1524371, 1637479, 165205, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the Sloan Research Foundation through a Sloan Fellowship, the Allen Institute for Artificial Intelligence, the NVIDIA Artificial Intelligence Lab, Samsung through a Samsung AI research grant, and gifts by Google and Facebook. Computations on beaker.org were supported in part by credits from Google Cloud.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Troops, trolls and troublemakers: A global inventory of organized social media manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samantha</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Oxford Internet Institute</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language gans falling short</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02549</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Avoid These Fake News Sites at All Costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Dicker</surname></persName>
		</author>
		<ptr target="https://www.usnews.com/news/national-news/articles/2016-11-14/avoid-these-fake-news-sites-at-all-costs" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>Online; accessed 22</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facebook says it has uncovered a coordinated disinformation operation ahead of the 2018 midterm elections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Dwoskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Romm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Washington Post</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Partisanship, propaganda, and disinformation: Online media and the 2016 us presidential election</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Etling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikki</forename><surname>Bourassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Zuckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Benkler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
			<publisher>Berkman Klein Center Research Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Constant-time machine translation with conditional masked language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09324</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Insertion-based decoding with automatically inferred generation order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation of contextualized embeddings: A case study in early modern english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02817</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unifying human and statistical evaluation for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02792</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">It&apos;s time to do something: Mitigating the negative impacts of computing through a change to the peer review process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Schöning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ernnst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">De</forename><surname>Russis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lana</forename><surname>Yarosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bushra</forename><surname>Anjum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathy</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Future of Computing Blog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attacking automatic video analysis algorithms: A case study of google cloud video intelligence api</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baicen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 on Multimedia Privacy and Security</title>
		<meeting>the 2017 on Multimedia Privacy and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cutting the funding of disinformation: The ad-tech solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Melford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Fagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Global Disinformation Index</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">News feed fyi: Helping ensure news on facebook is from trusted sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Mosseri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Facebook Newsroom</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding deceptive opinion spam by any stretch of the imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey T</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic detection of fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bennett</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://blog.openai.com/language-unsupervised/" />
	</analytic>
	<monogr>
		<title level="j">OpenAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Truth of varying shades: Analyzing language in fake news and political fact-checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Yea</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svitlana</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2931" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hoaxy: A platform for tracking online misinformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengcheng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Luca</forename><surname>Ciampaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Menczer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference companion on world wide web</title>
		<meeting>the 25th international conference companion on world wide web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="745" to="750" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4603" to="4611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09203</idno>
		<title level="m">Release strategies and the social impacts of language models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Insertion transformer: Flexible sequence generation via insertion operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Catching a unicorn with gltr: A tool to detect automatically generated text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Harvard</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Journal of experimental psychology: learning, memory, and cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Briony</forename><surname>Swire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewandowsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">1948</biblScope>
		</imprint>
	</monogr>
	<note>The role of familiarity in correcting inaccurate information</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fever: a largescale dataset for fact extraction and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The agenda-setting power of fake news: A big data analysis of the online media landscape from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Vargo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">A</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amazeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Media &amp; Society</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2028" to="2049" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amapreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">liar, liar pants on fire&quot;: A new benchmark dataset for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fake news. it&apos;s complicated. First Draft News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Wardle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Information disorder: Toward an interdisciplinary framework for research and policy making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Wardle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Derakhshan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
		<respStmt>
			<orgName>Council of Europe report</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Why we released grover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<ptr target="https://thegradient.pub/why-we-released-grover/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Counteracting neural disinformation with grover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://medium.com/ai2-blog/counteracting-neural-disinformation-with-grover-6cf6690d463b" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Style) Is the style of this article consistent? 3. Yes, this sounds like an article I would find at an online news source. 2. Sort of</title>
		<imprint/>
	</monogr>
	<note>but there are certain sentences that are awkward or strange</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">No, it reads like it&apos;s written by a madman</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Yes, this article reads coherently. 2. Sort of, but I don&apos;t understand what the author means in certain places</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">No, I have no (or almost no) idea what the author is trying to say</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Yes, I feel that this article could come from a news source I would trust. 2. Sort of, but something seems a bit fishy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">No, this seems like it comes from an unreliable source</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Evaluating consistency To measure consistency between the article and the metadata, we asked the following questions: (a) (Headline) How well does the article body match the following headline?</title>
		<imprint/>
	</monogr>
	<note>headline</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Yes, the article makes sense as something that I would see given the headline. 2. Sort of, the article is somewhat related to the headline, but seems slightly off</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Authors) How well does the article body match the following author(s)? [authors] 3. Yes, the article makes sense as something that could be written by the author(s). 2. Sort of, the article might have been written by the author(s) above</title>
		<imprint/>
	</monogr>
	<note>but it sounds unlikely</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">No, the article body contains information that says it was written by someone else</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Date) How well does the article body match the following date?</title>
		<imprint/>
	</monogr>
	<note>date] 3. Yes, the article makes sense as something that could have been written on [date</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Sort of, the article might have been written on</title>
		<imprint/>
	</monogr>
	<note>but it sounds unlikely</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Provided authors: Kenneth Turan Provided headline: Review: &quot;The Last Sharknado: It&apos;s About Time&quot; is a gem of an action movie Generated body: Some heroes take a vacation every now and then, even temporarily, a couple of beachfront condos in a movie world they hope, a la Rick James in &quot;The Beautiful Daze,&quot; won&apos;t have to live through. Unfortunately, that has never happened to one of the most exceptional directors in the business</title>
		<imprint>
			<date type="published" when="2018-08-01" />
			<publisher>Anthony &amp; Joe Russo</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Provided domain: latimes.com Provided date</note>
	<note>No, there&apos;s information in the article that conflicts the proposed date. so the moment is altogether too good to spoil. at least for us Southern Californians</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">which had its Los Angeles premiere at the Tower Theatre on Tuesday, we can expect the San Diego-born Anthony Russo to somehow make it through seven years without being inside a sharknado</title>
	</analytic>
	<monogr>
		<title level="m">The Last Sharknado: It&apos;s About Time</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The Last Sharknado&quot; is exactly what you would expect: very silly and thrilling. Once again the Ryan Newman-and Ian Ziering-costumed stars of the YouTube smash &quot;Sharknado&quot; have a lot of fun poking fun at themselves and the silly film business (&quot;I don&apos;t know if being caged for a few days after we get out is too much to ask&quot;) along with helping a once-beleaguered government figure out what to do. We start on a lake in North Carolina as Newman, now an exotic dancer, does a good-natured impersonation of Seltzer&apos;s Agent Slutz. He&apos;s not a bad actor, though, and by the end of the first reel &quot;it&apos;s clear he&apos;s never a lunatic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">he has a couple of other movies during that time, mostly that stupid&apos;d-up New York &quot;Avengers&quot; picture, but they have nothing to do with sharks or serious action-film plotting</title>
		<editor>Jason Friedberg and Aaron Seltzer</editor>
		<imprint/>
	</monogr>
	<note>Don&apos;t worry, he lives to fight another day. So Slutz has just been rehired by the Trump Administration, even though the agency has been effectively taken over by the tag team of the Governator and Alex Jones (the viewer is never told why they are being hired</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Newman is working with his deputy, Ziering, the impetuous marine officer no longer needled by his loyalty to his best friend, Tara Reid, who showed up in the third &quot;Sharknado&quot; and is absent this time. The three are doing their best to fix things, with Newman looking in particular for more information on how to deal with the five remaining sharks in New York who seem to think they should go with the Trump administration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">As</forename><surname>Slutz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Svelt&apos;s plotting goes way beyond the call of duty, of course, for despite the presence of Anthony Mackie and Juliette Lewis, for example, there are many disappointments here</title>
		<imprint/>
	</monogr>
	<note>especially in the casting of a grouchy Jeff Goldblum as the villain of the piece</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">The Avengers&quot;) and some deeply, bizarrely funny dialogue (try spending a few days in a sharknado and after a while questioning your sanity)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Still</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharknado</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>all the elements are exactly right, including some fancy effects (stunt coordinator Zak Penn</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Come again? Might we welcome it? If by a miracle we&apos;re not sated by our popcorn, that would be the best way to look at it. kenneth.turan@latimes.com Follow me on Twitter @KennethTuran Generated headline: &apos;The Last Sharknado&apos; returns with the dramatic thriller you expected -and some unexpected laughs Figure 13: An example of using Grover-Mega to generate a fake movie review. Here, we spoof LA Times Film Critic Kenneth Turan and generate a positive movie review for &apos;The Last Sharknado: It&apos;s About Time</title>
		<imprint/>
	</monogr>
	<note>Whether we go sharknado-free is on you. the sixth installment in the Sharknado series of movies</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
