<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incomplete Utterance Rewriting as Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
							<email>†qian.liu@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
							<email>§beichen@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
							<email>jlou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
							<email>zhoubin@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
							<email>dongmeiz@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incomplete Utterance Rewriting as Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix. Benefiting from being able to capture both local and global information, our approach achieves state-ofthe-art performance on several public datasets. Furthermore, our approach is four times faster than the standard approach in inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A dramatic progress has been achieved in singleturn dialogue modeling such as open-domain response generation <ref type="bibr" target="#b34">(Shang et al., 2015)</ref>, question answering <ref type="bibr" target="#b29">(Rajpurkar et al., 2016)</ref>, etc. By contrast, multi-turn dialogue modeling is still in its infancy, as users tend to use incomplete utterances which usually omit or refer back to entities or concepts appeared in the dialogue context, namely ellipsis and coreference. According to previous studies, ellipsis and coreference exist in more than 70% of the utterances <ref type="bibr" target="#b35">(Su et al., 2019)</ref>, for which a dialogue system must be equipped with the ability of understanding them. To tackle the problem, early works include learning a hierarchical representation <ref type="bibr" target="#b33">(Serban et al., 2017;</ref><ref type="bibr" target="#b44">Zhang et al., 2018)</ref> and concatenating the dialogue utterances selectively <ref type="bibr" target="#b43">(Yan et al., 2016)</ref>. Recently, researchers focus on a more explicit and explainable solution: the task of Incomplete Utterance Rewriting (IUR, also known as context rewriting) <ref type="bibr" target="#b15">(Kumar and Joshi, 2016;</ref><ref type="bibr" target="#b35">Su et al., 2019;</ref> Work done during an internship at Microsoft Research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Turn</head><p>Utterance (Translation)</p><p>x 1 (A) 北京今天天气如何 How is the weather in Beijing today</p><formula xml:id="formula_0">x 2 (B)</formula><p>北京今天是阴天 Beijing is cloudy today</p><formula xml:id="formula_1">x 3 (A)</formula><p>为什么总是这样 Why is always this</p><p>x * 3 北京为什么总是阴天 Why is Beijing always cloudy <ref type="table">Table 1</ref>: An example dialogue between user A and B, including the context utterances (x 1 , x 2 ), the incomplete utterance (x 3 ) and the rewritten utterance (x * 3 ).</p><p>2019a; <ref type="bibr" target="#b23">Pan et al., 2019;</ref><ref type="bibr" target="#b5">Elgohary et al., 2019;</ref>. IUR aims to rewrite an incomplete utterance into an utterance which is semantically equivalent but self-contained to be understood without context. As shown in <ref type="table">Table 1</ref>, the incomplete utterance x 3 not only omits the subject "北京"(Beijing), but also refers to the semantic of "阴 天"(cloudy) via "这 样"(this). By explicitly recovering the hidden semantics behind x 3 into x * 3 , IUR makes the downstream dialogue modeling more precise.</p><p>To deal with IUR, a natural idea is to transfer models from coreference resolution <ref type="bibr" target="#b3">(Clark and Manning, 2016)</ref>. However, this idea is not easy to realize, as ellipsis also accounts for a large proportion. Despite being different, coreference and ellipsis both can be resolved without introducing out-of-dialogue words in most cases. That is to say, words of the rewritten utterance are nearly from either the context utterances or the incomplete utterance. Observing it, most previous works employ the pointer network <ref type="bibr" target="#b37">(Vinyals et al., 2015)</ref> or the sequence to sequence model with copy mechanism <ref type="bibr" target="#b7">(Gu et al., 2016;</ref><ref type="bibr" target="#b32">See et al., 2017)</ref>. However, they generate the rewritten utterance from scratch, neglecting a key trait that the main structure of a rewritten utterance is always the same as the incomplete utterance. To highlight it, we imagine the rewritten utterance as the outcome after a series of edit operations (i.e. substitute and insert) on the incomplete utterance. Taking the example from Table 1, x * 3 can be obtained by substituting "这样"(this) in x 3 with "阴天"(cloudy) in x 2 and inserting "北京"(Beijing) before "为什 么"(Why), much easier than producing x * 3 via decoding word by word. These edit operations are carried out between word pairs of the context utterances and the incomplete utterance, analogous to semantic segmentation (a well-known task in computer vision): Given relevance features between word pairs as an image, the model is to predict the edit type for each word pair as a pixel-level mask (elaborated in Section 3). Inspired by the above, in this paper, we propose a novel and extensive approach which formulates IUR as semantic segmentation 1 . Our contributions are as follows:</p><p>• As far as we know, we are the first to present such a highly extensive approach which formulates the incomplete utterance rewriting as a semantic segmentation task.</p><p>• Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several datasets across different domains and languages.</p><p>• Furthermore, our model predicts the edit operations in parallel, and thus obtains a much faster inference speed than traditional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most related work to ours is the line of incomplete utterance rewriting. Recently, it has raised a large attention in several domains. In question answering, previous works include non-sentential utterance resolution using the sequence to sequence based architecture <ref type="bibr" target="#b15">(Kumar and Joshi, 2016)</ref>, incomplete follow-up question resolution via a retrieval sequence to sequence model <ref type="bibr" target="#b16">(Kumar and Joshi, 2017)</ref> and sequence to sequence model with a copy mechanism <ref type="bibr" target="#b5">(Elgohary et al., 2019;</ref><ref type="bibr" target="#b28">Quan et al., 2019)</ref>. In conversational semantic parsing, <ref type="bibr" target="#b21">Liu et al. (2019b)</ref> proposed a novel approach which considers the structures of questions, while Different from all of them, we formulate the task as a semantic segmentation task. Our work is also closely related to coreference resolution. It is an active task that has been studied years, and deep learning based methods have achieved state-of-the-art performance via the paradigm of scoring span or mention pairs <ref type="bibr">Manning, 2015, 2016;</ref><ref type="bibr" target="#b17">Lee et al., 2017</ref><ref type="bibr" target="#b18">Lee et al., , 2018</ref>. Researchers also explored to use unsupervised contextualized representations to enhance the coreference resolution. <ref type="bibr" target="#b12">Joshi et al. (2019)</ref> applied SpanBERT <ref type="bibr" target="#b11">(Joshi et al., 2020)</ref> to enhance the span representation in coreference resolution, and <ref type="bibr" target="#b40">Wu et al. (2020)</ref> formulated coreference resolution as query-based span prediction and employed SpanBERT to solve it as a machine reading task. The above works only focus on coreference resolution, while our work deals with coreference and ellipsis under a unified approach.</p><p>From the perspective of the methodology, our work is correlated with directions of edit-based text generation and semantic similarity measurement.  proposed a prototype-thenedit paradigm for open-domain response generation, while <ref type="bibr" target="#b22">Malmi et al. (2019)</ref> cast text generation as a text editing task and tackled it with a sequence tagging approach. Our work is different from theirs since we model the editing process between two sentences as a semantic segmentation task. As for semantic similarity measurement, similar to us, both <ref type="bibr" target="#b8">He and Lin (2016)</ref> and <ref type="bibr" target="#b24">Pang et al. (2016)</ref> used convolutional neural networks to capture similarities between sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Incomplete Utterance Rewriting as Semantic Segmentation</head><p>In this section, we will have a glance at the fundamental idea behind our approach: incomplete utterance rewriting as semantic segmentation. In a multi-turn dialogue, given the context utterances (x 1 , · · · , x t−1 ) and the incomplete utterance x t , IUR is to rewrite x t to x * t using contextual information, where x * t has the same meaning with x t . The rewritten utterance x * t has self-contained semantics and can be understood solely. To produce x * t , our approach formulates the problem as a semantic segmentation task. Concretely, we concatenate all the context utterances to produce an M -length word sequence c = (c 1 , c 2 , · · · , c M ). To separate context utterances in different turns, we insert a special word [S] between each context utterance. Meanwhile, the incomplete utterance is denoted by x = (x 1 , x 2 , · · · , x N ). As mentioned, the rewritten utterance x * can be obtained by editing the incomplete utterance x with in-dialogue words (i.e. words in c). To model edit operations between x and c, we define a M × N matrix Y , where the entry Y m,n indicates the edit type between c m and x n . There are three kinds of edit types: Substitute means replacing the span in x with the corresponding context span in c; Insert aims to insert the context span before a certain token in x, and None represents no operation. For example, as shown schematically in <ref type="figure">Figure 1</ref>, we can edit x by replacing (x 2 , x 3 ) with (c 2 , c 3 , c 4 ) and insert (c 6 , c 7 , c 9 , c 10 ) before x 7 . It is notable that we append a special word [E] to x, to enable Insert take place after x. More concrete examples can be found in Section 5.3.</p><p>Then, we propose to emit such a matrix Y in a way analogous to the task of semantic segmentation. Specially, we build a M × N feature map via capturing the word-to-word relevance between c and x. Taking the feature map as an image, the output word-level edit matrix Y is parallel to the pixel-level mask in semantic segmentation, which bridges IUR and semantic segmentation. Such a formulation comes with several key advantages: (i) Easy: compared with traditional methods generating the rewritten utterance from scratch, such a formulation introduces edit operations to lower the difficulty of generation; (ii) Fast: these edits are predicted concurrently, so our model naturally enjoys the fast inference speed than conventional models which decode word by word; (iii) Transferable: taking the formulation as a bridge between IUR and semantic segmentation, one can transfer empirical models from the community of semantic segmentation with ease.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, our approach firstly obtains the word-level edit matrix through three neural layers. Then based on the word-level edit matrix, it applies a generation algorithm to produce the rewritten utterance. Since the model yields a Ushaped architecture (illustrated later), we name our approach as Rewritten U-shaped Network (RUN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word-level Edit Matrix Construction</head><p>To construct a word-level edit matrix, our model passes through three neural layers: a context layer, an encoding layer and a subsequent segmentation layer. The context layer produces a context-aware representation for each word in both c and x, based on which the encoding layer forms a feature map matrix F to capture word-to-word relevance. Finally a segmentation layer is applied to emit the word-level edit matrix.</p><p>Context Layer As shown in the left of <ref type="figure" target="#fig_0">Figure 2</ref>, at first the concatenation of c and x passes by the word embedding φ to get the representation for each word in both utterances. The embedding is initialized using GloVe <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref>, and then updated along with other parameters. On top of the joint word embedding sequence φ(c 1 ),· · ·, φ(c M ), φ(x 1 ),· · ·, φ(x N ) , Bidirectional Long Short-Term Memory Network (BiLSTM) <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b31">Schuster and Paliwal, 1997)</ref> is applied to capture contextual information inter and intra utterances. Although c and x are jointly encoded by BiLSTM (see the left of <ref type="figure" target="#fig_0">Figure 2</ref>), below we distinguish their hidden states for clear illustration. For a word c m (m = 1, . . . , M ) in c, its hidden state is denoted by u m obtained through BiLSTM, while the hidden state h n is for word x n (n = 1, · · · , N ) in the incomplete utterance.</p><p>Encoding Layer On top of the context-aware hidden states, we consider several similarity functions to encode the word-to-word relevance. Concretely, for each word x n in the incomplete utterance and c m in the context utterances, their relevance is captured by a D-dimensional feature vector F(x n , c m ). It is produced by concatenating element-wise similarity (Ele Sim.), cosine similarity (Cos Sim.) and learned bi-linear similarity (Bi-Linear Sim.) between them as:</p><formula xml:id="formula_2">F(x n , c m ) = h n u m ; cos(h n ,u m ); h n Wu m , (1)</formula><p>where W is a learned parameter. These similarity functions are expected to model the word-to-word relevance from different perspectives, important for the follow-up edit type classification. However, they concentrate on local rather than global information (see discussion in Section 5.3). To capture global information, a segmentation layer is proposed.</p><p>Segmentation Layer Taking the feature map matrix F ∈ R M ×N ×D as a D-channel image, the segmentation layer is to predict the word-level edit matrix Y ∈ R M ×N , analogous to a pixel-level mask. Inspired by UNet <ref type="bibr" target="#b30">(Ronneberger et al., 2015)</ref>, the layer is formed as a U-shaped structure: two down-sampling blocks and two up-sampling blocks with skip connection. A down-sampling block contains two separate "Conv" modules and a subsequent max pooling. Each down-sampling block doubles the number of channels. Intuitively, the down-sampling block expands the receptive fields of each cell, hence providing rich global information for the final decision. An up-sampling block contains two separate "Conv" modules, and a subsequent deconvloution neural network. Each up-sampling block halves the number of channels and concatenates the correspondingly cropped feature map in down-sampling as the output (skip connect in <ref type="figure" target="#fig_0">Figure 2</ref>). Finally a feedforward neural network is employed to map each feature vector to one of three edit types, obtaining the word-level edit matrix Y . By incorporating an encoding layer and a segmentation layer, our model is able to capture both local and global information.</p><p>BERT Enhanced Embedding Since pretrained language models have been proven to be effective on several tasks, we also experiment with employing BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> to augment our model via BERT enhanced embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rewritten Utterance Generation</head><p>Once a word-level edit matrix is emitted, a subsequent generation algorithm is applied for producing the rewritten utterance. As indicated in <ref type="figure">Figure 1</ref>, to apply edit operations without ambiguity, we assume each edit region in Y is a rectangle. However, the predicted Y is not guaranteed to meet this requirement, indicating the need for a standardization step. Therefore, the overall procedure of generation is divided into two stages: first the algorithm delimits standard edit regions via searching minimal covering rectangles for each connected region; then it manipulates the incomplete utterance based on these standard edit regions to produce the rewritten utterance. Since the second step has been illustrated in Section 3, in the following we concentrate on the first standardization step.</p><p>In the standardization step, we employ the twopass algorithm (also known as Hoshen-Kopelman algorithm) to find connected regions <ref type="bibr" target="#b10">(Hoshen and Kopelman, 1976)</ref>. In a nutshell, the algorithm makes two passes over the word-level edit matrix. The first pass is to assign temporary cluster labels and record equivalences between clusters in an order of left to right and top to down. Concretely, for each cell, if its neighbors (i.e. left or top cells with the same edit type) have been assigned temporary cluster labels, it is labeled as the smallest neighboring label. Meanwhile, its neighboring clusters are recorded as equivalent. Otherwise, a new temporary cluster label is created for the cell. The second pass is to merge temporary cluster labels which are recorded as equivalent. Finally, cells with the same label form a connected region. For each connected region, we use its minimal covering rectangle to serve as the output of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Distant Supervision</head><p>As mentioned in Section 3, the expected supervision for our model is the word-level edit matrix, but existing datasets only contain rewritten utterances. Therefore, we use a procedure to automatically derive (noisy) word-level edit matrices (i.e. distant supervision), and use these examples to train our model. We use the following process to build our training set. First, we find a Longest Common Subsequence (LCS) between x and x * . Then, for each word in x * , if it is not in LCS, it is marked as ADD. Conversely, for each word in x but not in LCS, it is marked as DEL. Contiguous words with the same mark are merged into one span. By a span-level comparison, any ADD span in x * with a DEL counterpart (i.e. under the same context) relates it to Substitute. Otherwise, the span is inserted into x, corresponding to Insert.</p><p>Taking the example from <ref type="table">Table 1</ref>, given x as "为什么总是这样"(Why is always this) and x * as "北京为什么总是阴天"(Why is Beijing always cloudy), their longest common subsequence is "为什么总是"(Why is always). Therefore, with "这样"(this) in x being marked as DEL and "阴 天"(cloudy) in x * being marked as ADD, they cor-respond to the edit type Substitute. In comparison, since "北京"(Beijing) cannot find a counterpart, it is related to the edit type Insert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct thorough experiments to demonstrate the superiority of our approach.  <ref type="bibr">CANARD Elgohary et al., 2019</ref> . We use the same data split for these datasets as their original paper, and some statistics are shown in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Baselines We consider a bunch of baselines, including LSTM-based models, Transformer-based models and state-of-the-art models on each dataset. (i) LSTM-based models consist of the vanilla sequence to sequence model with attention (L-Gen) <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>, the pointer network architecture (L-Ptr) <ref type="bibr" target="#b37">(Vinyals et al., 2015)</ref> and the hybrid pointer generator network (L-Ptr-Gen) <ref type="bibr" target="#b32">(See et al., 2017)</ref>. (ii) Transformer-based models consist of the basic transformer model (T-Gen) <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, the transformer-based pointer network (T-Ptr), and the transformer-based pointer generator (T-Ptr-Gen). (iii) State-of-theart models consist of Syntactic <ref type="bibr" target="#b15">(Kumar and Joshi, 2016)</ref>, PAC <ref type="bibr" target="#b23">(Pan et al., 2019)</ref>, GECOR <ref type="bibr" target="#b28">(Quan et al., 2019)</ref>, L-Ptr-λ and T-Ptr-λ <ref type="bibr" target="#b35">(Su et al., 2019)</ref>. We refer readers to their papers for more details. It is remarkable that above methods all generate rewritten utterances from scratch.</p><p>Evaluation We employ both automatic metrics and human evaluations to evaluate our approach. As in literature <ref type="bibr" target="#b23">(Pan et al., 2019)</ref>, we examine RUN using the widely used automatic metrics BLEU, ROUGE, EM and Rewriting F-score. (i) BLEU n (B n ) evaluates how similar the rewritten utterances are to the golden ones via the cumulative n-gram BLEU score <ref type="bibr" target="#b25">(Papineni et al., 2002)</ref>. (ii) ROUGE n (R n ) measures the n-gram overlapping between the rewritten utterances and the golden ones, while ROUGE L (R L ) measures the longest matching sequence between them <ref type="bibr" target="#b19">(Lin, 2004)</ref>. (iii) EM stands for the exact match ac-   <ref type="table">Table 3</ref>: Statistics of different datasets. NA means the development set is also the test set. "Ques" is short for questions, "Avg" for average, "len" for length, "Con" for context utterance, "Cur" for current utterance, and "Rew" for rewritten utterance.</p><formula xml:id="formula_3">Model P 1 R 1 F 1 P 2 R 2 F 2 P 3 R 3 F 3 B 1 B 2 R 1 R 2</formula><p>curacy, which is the strictest evaluation metric.</p><p>(iv) Rewriting Precision n , Recall n and F-score n (P n , R n , F n ) emphasize more on words from c which are argued to be harder to copy <ref type="bibr" target="#b23">(Pan et al., 2019)</ref>. Therefore, they are calculated on the collection of n-grams that contain at least one word from c. As validated by <ref type="bibr" target="#b23">Pan et al. (2019)</ref>, above automatic metrics are credible indicators to reflect the rewrite quality. However, none of automatic metrics reflects the utterance fluency or the improvement on downstream tasks. Therefore, human evaluations are included to evaluate the fluency of rewritten utterances and their boost on the downstream task.</p><p>Implementation Details Our implementation was based on PyTorch <ref type="bibr" target="#b26">(Paszke et al., 2019)</ref>, Al-lenNLP <ref type="bibr" target="#b6">(Gardner et al., 2018)</ref> and HuggingFace's transformers library <ref type="bibr" target="#b39">(Wolf et al., 2019)</ref>. Since the distribution of edit types is severely unbalanced (e.g. None accounts for nearly 90%), we employed weighted cross-entropy loss and tuned the weight on development sets. We used Adam <ref type="bibr" target="#b13">(Kingma and Ba, 2015)</ref>   mentioned above refer to BERT base . All results of baselines without specific marks were reproduced by ours using OpenNMT with beam size as 4 <ref type="bibr" target="#b14">(Klein et al., 2017)</ref>.</p><p>Connection Words Similar to pointer network <ref type="bibr" target="#b37">(Vinyals et al., 2015)</ref>, RUN is restricted to predict words which have appeared in the dialogue.</p><p>Although most examples work well under the restriction, there still exist a few cases which rely on certain words to generate fluent utterances. For example, when rewriting possessive pronouns such as "their", we usually need an extra word "of" to enhance the fluency. Such common words, named after connection words, improve fluency of the rewritten utterances. In practice, we append a small list of connection words to the tail of c, enabling our model to pick connection words as well. For each dataset, their connection word list is automatically derived from the training data.    <ref type="table">Table 7</ref>: Human rating evaluations about the response quality on sampled 300 dialogues from the development set of MULTI. The score ranges from 0 to 2. "NR" represents the proportion of rewritten utterances which are equal to current utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Comparison</head><p>example, our approach exceeds the best baseline L-Ptr-Gen by a large margin, reaching a new stateof-the-art performance on almost all automatic metrics. To illustrate, our approach improves the previous best model by 6.4 points and 10.0 points on B 1 and F 1 respectively. Furthermore, our approach leaves a striking impression when augmented with BERT. It not only fully surpasses the best sequence generation baseline with BERT (i.e. T-Ptr-λ+BERT on REWRITE), but also obtains a considerable boost over a cascade model designed for stimulating potential of BERT (i.e. PAC on MULTI). Even for the most challenging metric EM on REWRITE, RUN with BERT improves 8.9 points, demonstrating the superiority of our model. Besides, our approach also achieves comparable or better results against all baselines on TASK and CANARD, as shown in <ref type="table" target="#tab_7">Table 5</ref>.</p><p>Besides automatic results, we perform two groups of human evaluation to answer (i) how fluent the rewritten utterances are and (ii) how much IUR can contribute to downstream tasks. For the evaluation of fluency, we randomly sampled 500  dialogues in the development set of REWRITE. Then we fed them to representative IUR models and presented generated rewritten utterances to 10 judges, who are asked to decide which of the rewritten utterances is of higher fluency in pairwise comparisons. Ties are acceptable. <ref type="table" target="#tab_8">Table 6</ref> shows the evaluation results. In comparison to the best baseline T-Ptr-λ, our model only loses in 20.4% cases, which is extremely competitive.</p><p>To access the influence of IUR on downstream tasks, we choose multi-turn response selection as a representative, which aims to retrieve suitable responses from a candidate pool considering the context. Concretely, an SMN model trained on the Douban Conversation Corpus is selected as the backbone in multi-turn response selection <ref type="bibr" target="#b42">(Wu et al., 2017)</ref>. At first we sampled 300 dialogues from the development set of MULTI as the input to IUR models. Then their predicted rewritten utterances and the context utterances were fed into the SMN model, to help it select suitable responses.  <ref type="table">Table 9</ref>: The ablation results on the development set of MULTI. "w/o Edit" means directly using the current utterance as the rewritten utterance. "w/o U-shape seg." means that our segmentation layer is replaced by a feed-forward neural network with comparable parameters. The remaining variants ablate different similarity functions in the encoding layer. The response candidate pool was formed by all utterances in MULTI. Finally, 5 workers were asked to evaluate responses following a multi-scale rating from 0 to 2: 0 means the response is not related to the dialogue; 1 means the response is related but not interesting enough; and 2 means the response is satisfying. To illustrate more clearly, we also conduct human rating evaluation on responses under the settings of original dialogue (i.e. without rewriting, relying on the SMN model itself to understand the context) and gold dialogue (i.e. human rewriting). As shown in <ref type="table">Table 7</ref>, our model achieves the highest response quality score among IUR models, improving the original setting by 19% relatively. Considering that the SMN model is capable of aggregating implicit context information, it is non-trivial for our model to further improve the response quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Closer Analysis</head><p>We conduct a series of experiments to analyze our model deeply. First we conduct an inference speed comparison between our model and representative  baselines under the same run-time environment. Then we verify the effectiveness of components in our model by a thorough ablation study. Meanwhile, we touch how the amount of connection words affect the performance. Finally, we present two real cases to illustrate our model concretely. <ref type="table" target="#tab_10">Table 8</ref> compares inference speed between our model and baselines. Since L-Ptr-λ and T-Ptr-λ are not implemented under Py-Torch, we do not show their inference time for fair consideration. Noticing the beam size would affect the inference time of baselines, we also show the results with beam size as 1. Using the simplest L-Gen as a standard, one can find that our model is nearly four times faster, with the highest improvement ∆B 4 . Meanwhile, our model is the only one which can improve both performance and inference speed, significantly surpassing all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference Speed</head><p>Ablation Study To verify the effectiveness of different components in our model, we present a thorough ablation study in <ref type="table">Table 9</ref>. As expected, "w/o Edit" causes a huge drop on all evaluation metrics. Notably, the extreme drop on F n indicates that it is more suited for IUR than common metrics. "w/o U-shape Seg.", which ablates the segmentation layer, also brings a great performance drop. Without our segmentation layer capturing global information, an encoding layer only achieves comparable performance with L-Gen, suggesting there are considerable benefits with bridging IUR and semantic segmentation. We also ablates different feature similarity functions (i.e. "w/o Ele Sim.", "w/o Cos Sim." and "w/o Bi-Linear Sim.") for an in-depth analysis. As shown in <ref type="table">Table 9</ref>, ablating each similarity function will hurt most metrics. Meanwhile, our model does not depend on any similarity function severely, showing its robustness. Furthermore, we explore how the amount of connection words affect the performance in <ref type="figure" target="#fig_1">Figure 3</ref>. As indicated, except TASK, the number of connection words affect slightly. Nevertheless, it shows a positive effect overall, providing a way to generate out-of-dialogue words. We present two real cases in <ref type="figure" target="#fig_2">Figure 4</ref> from REWRITE to illustrate the rewritten process of our model concretely. For both (a) coreference and (b) ellipsis, our model deals with them flexibly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>While our approach has made some progress, it still has several limitations. First, our model severely relies on the word order implied by the dialogue. It makes our model vulnerable to some complex cases (i.e. multiple Insert corresponds to one position). The second limitation is that we predict edit types of each cell independently, ignoring the relationship between neighboring edit types. It is hopefully resolved by the conditional random field algorithm <ref type="bibr" target="#b0">(Arnab et al., 2018)</ref>. The above limitations may raise concerns about the performance upper bound of our approach. In fact, it is not an issue. On three out of four datasets used in the experiments, more than 85% examples could be tackled perfectly by our approach (87.6% in TASK, 91.0% in REWRITE, 95.3% in MULTI). The number in CANARD is relatively low (42.5%) since human annotators introduce many new words in rewriting. Nevertheless, the BLEU upper bound in CANARD could be as high as 72.5% with our approach, which is acceptable.</p><p>The last point we focus on is why similarities can be good features for determining edits. We think it can be elaborated from two aspects. For coreference, the similarity function is suitable for identifying whether two spans refer to the same entity. For ellipsis, the similarity function is an effective indicator to find matching anchors, which indicate the possible insertion positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Future Work</head><p>In this paper, we present a novel and extensive approach which formulates the incomplete utterance rewriting as a semantic segmentation task. On top of the formulation, we carefully design a U-shaped rewritten network, which outperforms existing baselines significantly on several datasets. In the future, we will investigate on extending our approach to more areas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The illustration of the word-level edit matrix construction. The dashed boxes represent the intermediate results of our proposed model (bottom) and their counterparts in semantic segmentation (above). Inside the segmentation layer, a "Conv" module consists of a convolutional neural network, batch normalization and an activation function ReLU. "Pool" and "DeConv" are short for max pooling and deconvolution neural network respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(Left) BLEU4 (B 4 ) performance with different number of connection words on the development sets of different datasets. (Right) Connection words in decreasing order of frequency on each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The illustration of (Left) the word-level edit matrix and (Right) the rewritten utterance generation process of two real cases (a) and (b) from REWRITE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>None, Substitute and Insert.</figDesc><table><row><cell>None</cell><cell cols="2">Substitute</cell><cell>Insert</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>…</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 2 3</cell><cell>4 5</cell><cell>6 7</cell><cell>8 …</cell></row><row><cell cols="4">*  = ( 1 , 2 , 3 , 4 , 4 , 5 , 6 , 6 , 7 , 9 , 10 , 7 , 8 , … , )</cell></row><row><cell cols="4">Figure 1: The illustration of the word-level edit matrix</cell></row><row><cell cols="4">applied in our formulation. Each cell belongs to one of</cell></row><row><cell>three edit types:</cell><cell></cell><cell></cell><cell></cell></row></table><note>Liu et al. (2019a) imposed an intermediate struc- ture span and decomposed the incomplete utter- ance rewriting into two sub-tasks. In dialogue generation, Pan et al. (2019) presented a cascaded model which first picks words from the context via BERT, and then combines these words to generate the rewritten utterance, and Su et al. (2019) distin- guished the weights of context utterances and the incomplete utterance using a hyper-parameter λ.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Syntactic † 67.4 37.2 47.9 53.9 30.3 38.8 45.3 25.3 32.5 84.1 81.2 89.3 80.6 L-Gen † 65.5 40.8 50.3 52.2 32.6 40.1 43.6 27.0 33.4 84.9 81.7 88.8 80.3 L-Ptr-Gen † 66.6 40.4 50.3 54.0 33.1 41.1 45.9 28.1 34.9 84.7 81.7 89.0 80.9 RUN (Ours) 66.9 54.9 60.3 53.0 43.4 47.7 43.8 35.7 39.3 91.1 88.0 91.0 83.3</figDesc><table><row><cell>PAC  †</cell><cell>70.5 58.1 63.7 55.4 45.1 49.7 45.2 36.6 40.4 89.9 86.3 91.6 82.8</cell></row><row><cell cols="2">RUN + BERT (Ours) 73.2 64.6 68.6 59.5 53.0 56.0 50.7 45.1 47.7 92.3 89.6 92.4 85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The experimental results of (Top) general and (Bottom) BERT-based results on MULTI. †: Results from<ref type="bibr" target="#b23">Pan et al. (2019)</ref>. A bolded number in a column indicates a statistically significant improvement against all the baselines (p &lt; 0.05), whereas underline numbers show comparable performances. Both are same forTable 4&amp;5.</figDesc><table><row><cell></cell><cell cols="2">MULTI REWRITE</cell><cell>TASK</cell><cell>CANARD</cell></row><row><cell>Language</cell><cell>Chinese</cell><cell>Chinese</cell><cell cols="2">English English</cell></row><row><cell># Ques. (Train)</cell><cell>194 K</cell><cell>18 K</cell><cell>2.2 K</cell><cell>32 K</cell></row><row><cell># Ques. (Dev)</cell><cell>5 K</cell><cell>2 K</cell><cell>0.5 K</cell><cell>4 K</cell></row><row><cell># Ques. (Test)</cell><cell>5 K</cell><cell>NA</cell><cell>NA</cell><cell>6 K</cell></row><row><cell>Avg. Con len</cell><cell>25.8</cell><cell>17.7</cell><cell>52.6</cell><cell>85.4</cell></row><row><cell>Avg. Cur len</cell><cell>8.6</cell><cell>6.5</cell><cell>9.4</cell><cell>7.5</cell></row><row><cell>Avg. Rew len</cell><cell>12.4</cell><cell>10.5</cell><cell>11.3</cell><cell>11.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The experimental results on REWRITE. †: Reproduced from the code released by<ref type="bibr" target="#b35">Su et al. (2019)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 and</head><label>2</label><figDesc>Table 4 show experimental results of our approach and baselines on MULTI and REWRITE. As shown, our approach outperforms all baselines significantly. Taking MULTI as an Pronoun Sub 60.4 55.3 47.4 73.1 63.7 73.9 L-Ptr-Gen 67.2 60.3 50.2 78.9 62.9 74.9 RUN (Ours) 70.5 61.2 49.1 79.1 61.2 74.7</figDesc><table><row><cell></cell><cell>TASK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CANARD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>EM</cell><cell>B 4</cell><cell>F 1</cell><cell>Model</cell><cell>B 1</cell><cell>B 2</cell><cell>B 4</cell><cell>R 1</cell><cell>R 2</cell><cell>R L</cell></row><row><cell cols="4">Ellipsis Recovery  † 50.4 74.1 44.1</cell><cell>Copy</cell><cell cols="6">52.4 46.7 37.8 72.7 54.9 68.5</cell></row><row><cell>GECOR 1  †</cell><cell cols="3">68.5 83.9 66.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GECOR 2  †</cell><cell cols="3">66.2 83.0 66.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RUN (Ours)</cell><cell cols="3">69.2 85.6 70.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The experimental results on (Left) TASK and (Right) CANARD. †: Results from Quan et al. (2019).</figDesc><table><row><cell></cell><cell>Win</cell><cell>Tie</cell><cell>Loss</cell></row><row><cell>RUN v.s. L-Ptr-λ</cell><cell cols="3">41.6 % 42.4 % 16.0 %</cell></row><row><cell cols="4">RUN v.s. T-Ptr-Gen 23.6 % 56.4 % 20.0 %</cell></row><row><cell>RUN v.s. T-Ptr-λ</cell><cell cols="3">22.6 % 57.0 % 20.4 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="5">: Pairwise human evaluation results about the</cell></row><row><cell cols="5">rewritten utterance fluency on randomly sampled 500</cell></row><row><cell cols="5">dialogues from REWRITE. Our approach achieves sim-</cell></row><row><cell cols="5">ilar or better fluency compared with top baselines.</cell></row><row><cell></cell><cell cols="4">Origin L-Gen L-Ptr-Gen RUN Gold</cell></row><row><cell>Avg. Score</cell><cell>0.92</cell><cell>0.93</cell><cell>0.91</cell><cell>1.09 1.10</cell></row><row><cell>NR</cell><cell>100%</cell><cell>74%</cell><cell>68%</cell><cell>51% 46%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: The inference speed comparison between</cell></row><row><cell>RUN and baselines. Beam stands for the beam size in</cell></row><row><cell>beam search, not applicable for RUN. Latency is com-</cell></row><row><cell>puted as the time to produce a single sentence with-</cell></row><row><cell>out data batching, averaged over the development set</cell></row><row><cell>of REWRITE. All models are implemented in PyTorch</cell></row><row><cell>on a single NVIDIA V100.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Do you like Fang Datong [S] Quite like你喜欢方大同哪首歌Which song of Fang Datong do you like</figDesc><table><row><cell>None</cell><cell>Substitute</cell><cell cols="2">Insert</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[S]</cell></row><row><cell></cell><cell cols="3">你喜欢他哪首歌[E]</cell></row><row><cell></cell><cell cols="3">Which song of him do you like [E]</cell></row><row><cell></cell><cell>你最喜欢哪本书</cell><cell>[S]</cell><cell>玫瑰的故事</cell></row><row><cell></cell><cell cols="3">Which book do you like best [S] The Story of Rose</cell></row><row><cell></cell><cell cols="3">讲的什么内容</cell><cell>[E]</cell></row><row><cell></cell><cell cols="3">What is talking about [E]</cell></row><row><cell></cell><cell cols="3">玫瑰的故事讲的什么内容</cell></row><row><cell></cell><cell cols="3">What is The Story of Rose talking about</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ microsoft/ContextualSP.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviewers for their valuable comments. This work was supported in part by National Natural Science Foundation of China (U1736217 and 61932003), and National Key R&amp;D Program of China (2019YFF0302902).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional random fields meet deep neural networks for semantic segmentation: Combining probabilistic graphical models with deep learning for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Måns</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2017.2762355</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1405" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Can you unpack that? learning to rewrite questions-in-context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Peskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1605</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5918" to="5924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AllenNLP: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2501</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</title>
		<meeting>Workshop for NLP Open Source Software (NLP-OSS)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Percolation and cluster distribution. i. cluster multiple labeling technique and critical concentration algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>Kopelman</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevB.14.3438</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review B</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5803" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">OpenNMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonsentential question resolution using sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachindra</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2022" to="2031" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incomplete follow-up question resolution using retrieval based sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachindra</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080801</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A split-and-recombine approach for follow-up query analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1535</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5316" to="5326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FANDA: A novel approach to perform follow-up query analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016770</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu</title>
		<meeting><address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27" />
			<biblScope unit="page" from="6770" to="6777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Encode, tag, realize: High-precision text editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Mirylenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1510</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5054" to="5065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving open-domain dialogue systems via multi-turn incomplete utterance restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhufeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1191</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1824" to="1833" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2793" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Py-Torch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019-12-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GECOR: An end-to-end generative ellipsis and co-reference resolution model for taskoriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjian</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1462</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4547" to="4557" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
		<idno type="DOI">10.1109/78.650093</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving multi-turn dialogue modelling with utterance ReWriter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<editor>C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Morgan Funtowicz, and Jamie Brew</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CorefQA: Coreference resolution as query-based span prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Response generation by context-aware prototype editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7281" to="7288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911542</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;16</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context-sensitive generation of open-domain conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2437" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised context rewriting for open domain conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1192</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1834" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
