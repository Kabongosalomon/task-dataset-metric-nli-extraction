<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Similarity Reasoning and Filtration for Image-Text Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Diao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<email>yinggzhang@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Meituan, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Similarity Reasoning and Filtration for Image-Text Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image-text matching plays a critical role in bridging the vision and language, and great progress has been made by exploiting the global alignment between image and sentence, or local alignments between regions and words. However, how to make the most of these alignments to infer more accurate matching scores is still underexplored. In this paper, we propose a novel Similarity Graph Reasoning and Attention Filtration (SGRAF) network for image-text matching. Specifically, the vector-based similarity representations are firstly learned to characterize the local and global alignments in a more comprehensive manner, and then the Similarity Graph Reasoning (SGR) module relying on one graph convolutional neural network is introduced to infer relationaware similarities with both the local and global alignments. The Similarity Attention Filtration (SAF) module is further developed to integrate these alignments effectively by selectively attending on the significant and representative alignments and meanwhile casting aside the interferences of nonmeaningful alignments. We demonstrate the superiority of the proposed method with achieving state-of-the-art performances on the Flickr30K and MSCOCO datasets, and the good interpretability of SGR and SAF modules with extensive qualitative experiments and analyses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Image-text matching refers to measuring the visual-semantic similarity between image and text, which is becoming increasingly significant for various vision-and-language tasks, such as cross-modal retrieval , image captioning ), text-to-image synthesis , and multimodal neural machine translation . Although great progress has been made in recent years, image-text matching remains a challenging problem due to complex matching patterns and large semantic discrepancies between image and text.</p><p>To accurately establish the association between the visual and textual observations, a large proportion of methods ) utilize deep neural networks to firstly encode image and text into compact representations, and then Figure 1: Illustration of the SGRAF. Nodes of red and other colors encode image-text and region-word alignments respectively. SGR module captures their relationships to achieve comprehensive similarity reasoning and SAF module reduces the interferences of less-meaningful alignments learn to measure their similarity under the guidance of a matching criterion. For example, Wang et al.  and ) map the whole image and the full sentence into a common vector space, and compute the cosine similarity between the global representations. To improve the discriminative ability of the unified embeddings, many strategies such as semantic concept learning ) and region relationship reasoning ) are developed to enhance visual features by incorporating local region semantics. However, these approaches fail to capture the local interactions between image regions and sentence fragments, leading to limited interpretability and performance gains. To address this problem, Karpathy et al.  and  propose to discover all the possible alignments between image regions and sentence fragments, which produce impressive retrieval results and inspire a surge of works  to explore more accurate finegrained correspondence. Although noticeable improvements have been made by designing various mechanisms to encode more powerful features or capture more accurate alignments, these approaches neglect the importance of similarity computation, which is the key to explore the complex matching patterns between image and text.</p><p>To be more specific, there are three defects in previous approaches. Firstly, these methods compute scalar-based co-sine similarities between local features, which may not be powerful enough to characterize the association patterns between regions and words. Secondly, most of them aggregate all the latent alignments between regions and words simply with max pooling  or average pooling ), which hinders the information communication between local and global alignments, and thirdly, fails to consider the distractions of lessmeaningful alignments, such as the alignments built with "a" and "in", as shown in <ref type="figure">Figure 1</ref>.</p><p>To address these problems, in this paper we propose a novel Similarity Graph Reasoning and Attention Filtration (SGRAF) network for image-text matching. Specifically, we start with capturing the global alignments between the whole image and the full sentence, as well as the local alignments between image regions and sentence fragments. Instead of characterizing these alignments with scalar-based cosine similarity, we propose to learn the vector-based similarity representations to model the cross-modal associations more effectively. Then we introduce the Similarity Graph Reasoning (SGR) module, which relies on a Graph Convolution Neural Network (GCNN) to reason more accurate image-text similarity via capturing the relationship between local and global alignments. Furthermore, we develop the Similarity Attention Filtration (SAF) module to aggregate all the alignments attended by different significance scores, which reduces the interferences of non-meaningful alignments and achieves more accurate cross-modal matching results. Our main contributions are summarized as follows:</p><p>• We propose to learn the vector-based similarity representations for image-text matching, which enables greater capacity in characterizing the global alignments between images and sentences, as well as the local alignments between regions and words.</p><p>• We propose the Similarity Graph Reasoning (SGR) module to infer the image-text similarity with graph reasoning, which can identify more complex matching patterns and achieve more accurate predictions via capturing the relationship between local and global alignments.</p><p>• We attempt to consider the interferences of nonmeaningful words in similarity aggregation, and propose an effective Similarity Attention Filtration (SAF) module to suppress the irrelevant interactions for further improving the matching accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Image-Text Matching Feature Encoding Many prior Approaches ) focused on feature extraction and optimization for cross-modal retrieval. For textual features, Frome et al. ) employed Skip-Gram    ) exploited blockbased visual attention to gather semantics on feature maps, while <ref type="bibr">Wang et al. 2019c,b;</ref> followed ) to obtain region-based features of visual objects with the pre-trained model on Visual Genomes . Especially, (Chen and Luo 2020) explored Bi-GRU to gain high-level object features, while  proposed GCN-based networks to generate relationship-enhanced object features. We employ selfattention ) on region or word features to get image or text representation. We concentrate on the similarity encoding mechanism that models global imagetext and local region-word alignments comprehensively and fully encodes fine-grained relations between image and text. Similarity Prediction Most existing works ) for image-text matching learned the joint embedding and the similarity measures for cross-modal matching. For global alignments, some works ) explored a joint space and calculated the inner product (e.g. cosine distance) for similarity computation. Others  introduced an ordered representations to measure antisymmetric visual-semantic hierarchy. For local alignments, most networks ) computed scalarbased alignments and adopted simple operation (e.g. sum and average) to fuse local alignments. For example, Lee et al. ) studied the latent semantic alignments among region-words pairs and integrated local cosine alignments by average or LogSumExp. Differently, our network aggregates similarities by exploring global-local relationships among vector-based alignments and reducing the distraction from less-meaningful ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolution Network</head><p>The researches based on Graph modeled the dependencies between concepts and facilitated graph reasoning such as GCNN , and Gated Graph Neural Network (GGNN) ). These graph neural networks have been widely employed in various visual semantic tasks, such as image captioning , VQA (Teney, Liu, and van den Hengel 2017), and grounding referring expressions . In recent years, there are several approaches to utilize graph structures to enhance single visual or textual features referring to image-text matching.  adopted Scene Concept Graph (SCG) by using image scene graphs and frequently co-occurred concept pairs as scene common-sense knowledge.  proposed Visual Semantic Reasoning to build up connec-tions between image regions and generate visual representations with semantic relationships. ) employed visual scene graph and textual scene graph, each of which separately refines visual and textual features including objects and relationships. They all focus on "feature encoding" by learning single-modality contextualized representations, while our SGR targets at "similarity reasoning" and explores more complex matching patterns with global and local cross-modal alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanism</head><p>The attention mechanism has been applied to adaptively filter and aggregate information in natural language processing. When it comes to image-text matching, it has been intended to attend to certain parts of visual and textual data. ) developed Stacked Cross Attention to match latent alignments using both image regions and textual words as context. ) designed more complicated Cross Attentions to improve image-text matching.  proposed an Iterative Matching with Recurrent Attention Memory to explore fine-grained regionword correspondence progressively. We adopt textual-tovisual attention ) with region-word pairs and calculate textual-attended alignments. In this paper, our SAF aims to discard less-semantic alignments instead of exploiting precise cross-modal attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this section, we focus on improving the visual-semantic similarity learning via capturing the relationship between local and global alignments, and suppressing the disturbance of less-meaningful alignments. As illustrated in <ref type="figure">Figure 2</ref>, we begin with introducing how to encode the visual and textual observations, and then compute the similarity representations of all local and global representation pairs. Afterwards, we elaborate on the proposed Similarity Graph Reasoning (SGR) module for relation-aware similarity reasoning and Similarity Attention Filtration (SAF) module for representative similarity aggregation. Finally, we present the detailed implementations of training objectives and inference strategies with both the SGR and SAF modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generic Representation Extraction</head><p>Visual Representations. For each input image, we follow ) to extract K region-level visual features, with the Faster R-CNN ) model pretrained on Visual Genomes ). We add a fully-connect layer to transform them into d-dimensional vectors as local region representations V = {v 1 , ..., v K }, with v i ∈ R d . Afterwards, we perform self-attention mechanism ) over the local regions, which adopts average featureq v = 1 K K i=1 v i as the query and aggregates all the regions to obtain global representationv.</p><p>Textual Representations. Given a sentence, we split it into L words with tokenization technique, and sequentially feed the word embeddings into a bi-directional GRU (Schuster and Paliwal 1997). The representation of each word is then obtained by averaging the forward and backward hidden state at each time step, with T = {t 1 , ..., t L }, and t j ∈ R d denoting the representation of j-th word. Similarly, the global text representationt is computed by the self-attention method over all the word features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Representation Learning</head><p>Vector Similarity Function. Most previous methods utilize the cosine or Euclidean distance to represent the similarity between two feature vectors, which can capture the relevance to a certain degree while lacks the detailed correspondence. In this paper, we compute a similarity representation, which is a similarity vector instead of a similarity scalar, to capture more detailed associations between feature representations from different modalities. The similarity function between vector x ∈ R d and y ∈ R d is defined as</p><formula xml:id="formula_0">s(x, y; W ) = W |x − y| 2 W |x − y| 2 2 (1)</formula><p>where |·| 2 and · 2 indicate element-wise square and 2norm respectively, and W ∈ R m×d is a learnable parameter matrix to obtain the m-dimensional similarity vector.</p><p>Global Similarity Representation. We compute the similarity representation between the global image featurev and sentence featurest with Eq. (1),</p><formula xml:id="formula_1">s g = s(v,t; W g )<label>(2)</label></formula><p>where W g ∈ R m×d aims to learn the global similarity representation.</p><p>Local Similarity Representation. To exploit local similarity representations between local features of visual and textual observations, we apply textual-to-visual attention  to attend on each region with respect to each word. Attention weight for each region is computed by</p><formula xml:id="formula_2">α ij = exp(λĉ ij ) K i=1 exp(λĉ ij )<label>(3)</label></formula><p>Here the weight α ij is calculated by the softmax function with a temperature parameter λ. c ij indicates the cosine similarity between region feature v i and word feature t j ,</p><formula xml:id="formula_3">c ij = [c ij ] + / L j=1 [c ij ]</formula><p>2 + aims to normalize the cosine similarity matrix, and [x] + = max(x, 0).</p><p>Then we generate the attended visual features a v j with respect to j-th word by</p><formula xml:id="formula_4">a v j = K i=1 α ij v i ,<label>(4)</label></formula><p>and finally we compute the local similarity representation between a v j and t j as</p><formula xml:id="formula_5">s l j = s(a v j , t j ; W l )<label>(5)</label></formula><p>where W l ∈ R m×d is also a learnable parameter matrix. The local similarity representations capture the associations between a specific word and its corresponding image regions, which exploit more fine-grained visual-semantic alignments to boost the similarity prediction.  <ref type="figure">Figure 2</ref>: The proposed SGRAF network for image-text matching. The image and sentence are firstly encoded into local and global feature representations, followed by a similarity representation computation module to capture the correspondence between all local and global cross-modal pairs. The Similarity Graph Reasoning (SGR) module reasons the similarity with giving consideration to the relationship between all the alignments, and the Similarity Attention Filtration (SAF) module attends to more informative alignments for more accurate similarity prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Graph Reasoning</head><p>Graph Building. To achieve more comprehensive similarity reasoning, we build a similarity graph to propagate similarity messages among the possible alignments at both local and global levels. More specifically, we take all the wordattended similarity representations and the global similarity representation as graph nodes, i.e. N = {s l 1 , ...., s l L , s g }, and follow  to compute the edge from node s q ∈ N to s p ∈ N as</p><formula xml:id="formula_6">e(s p , s q ; W in , W out ) = exp((W in s p )(W out s q )) q exp((W in s p )(W out s q )) ,<label>(6)</label></formula><p>where W in ∈ R m×m and W out ∈ R m×m are the linear transformations for incoming and outgoing nodes, respectively. Note that the edges between node s p and s q are directed, which allow efficient and complex information propagation for similarity reasoning.</p><p>Graph Reasoning. With the constructed graph nodes and edges, we perform similarity graph reasoning by updating the nodes and edges witĥ</p><formula xml:id="formula_7">s n p = q e(s n p , s n q ; W n in , W n out ) · s n q (7) s n+1 p = ReLU (W n rŝ n p )<label>(8)</label></formula><p>with s 0 p and s 0 q taken from N at step n = 0, and W n r , W n in , W n out are learnable parameters in each step. After current step of graph reasoning, the node s n p is replaced with s n+1 p . We iteratively reason the similarity for N steps, and take the output of the global node at the last step as the reasoned similarity representation, and then feed it into a fullyconnect layer to infer the final similarity score. The SGR module enables the information propagation between local and global alignments, which can capture more comprehensive interactions to facilitate the similarity prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Attention Filtration</head><p>Although the exploitation of local alignments can boost the matching performance via discovering more fine-grained correspondence between image regions and sentence fragments, we notice that the less-meaningful alignments hinder the distinguishing ability when aggregating all the possible alignments in an undifferentiated way. Therefore we propose a Similarity Attention Filtration (SAF) module to enhance important alignments, as well as suppress ineffectual alignments, such as the alignments with "the", "be" and etc.</p><p>Given the local and global similarity representations, we calculate an aggregation weight β p for each similarity representation s p ∈ N by</p><formula xml:id="formula_8">β p = δ(BN (W f s p )) sq∈N δ(BN (W f s q ))<label>(9)</label></formula><p>where δ(·) is the Sigmoid function, BN indicates the batch normalization, and W f ∈ R m×1 is a linear transformation. Then we aggregate the similarity representations with s f = sp∈N β p s p , and feed s f into a fully-connect layer to predict the final similarity between the input image and sentence. The SAF module learns the significance scores to increase the contribution of more-informative similarity representations and meanwhile reduce the disturbance of lessmeaningful alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Objectives and Inference Strategies</head><p>We utilize the bidirectional ranking loss  to train both the SGR and SAF modules. Given a matched image-text pair (v, t), and the corresponding hardest negative image v − and the hardest negative text t − within a minibatch, we compute the bidirectional ranking loss with where γ is the margin parameter and S r (·, ·) indicates similarity prediction function implemented with SGR. Similarly, we define the training objectives on SAF module as L f . In this paper, we explore different training and inference strategies with the proposed SGR and SAF modules: joint training and independent training. For joint training, we combine L r and L f to train SGR and SAF modules simultaneously, where the similarity representations are shared for the proposed two modules. For independent training, we train the SGR and SAF modules separately. At the inference stage, we average the similarities predicted by SGR and SAF modules for the retrieval evaluation.</p><formula xml:id="formula_9">L r (v, t) = [γ − S r (v, t) + S r (v, t − )] + +[γ − S r (v, t) + S r (v − , t)] +<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>To verify the effectiveness of the our model, in this section we demonstrate extensive experiments on two benchmark datasets. We also introduce detailed implementations and training strategy of the proposed SGRAF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Settings</head><p>Datasets. We evaluate our model on the MSCOCO ) and Flickr30K  datasets. The MSCOCO dataset contains 123,287 images, and each image is annotated with 5 annotated captions. The dataset is split into 113,287 images for training, 5000 images for validation and 5000 images for testing. We report results by averaging over 5 folds of 1K test images and testing on the full 5K images. The Flickr30K dataset contains 31,783 images with 5 corresponding captions each. Following the split in , we use 1,000 images for validation, 1,000 images for testing and the rest for training.</p><p>Protocols. For image-text retrieval, we measure the performance by Recall at K (R@K) defined as the proportion   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quatitative Results and Analysis</head><p>In this section, we present the retrieval results on the MSCOCO and Flickr30K datasets, aiming to demonstrate the effectiveness and superiority of the proposed approach.</p><p>Comparisons on MSCOCO. <ref type="table">Table 1</ref> and 2 report the experimental results on MSCOCO dataset with 1K and 5K test images, separately. We can see that our proposed SGRAF model outperforms the existing methods, with the best R@1=79.6% for sentence retrieval and R@1=63.2% for image retrieval with 1K test images. For 5K test images, the proposed approach maintains the superiority with an improvement of more than 3% on the R@1 results. It should be noted that competitive retrieval performance can be also achieved with the SGR/SAF module alone, demonstrating the effectiveness and complementarity of our modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons on Flickr30K</head><p>. <ref type="table">Table 1</ref> compares the bidirectional retrieval results on Flickr30K dataset with the latest algorithms. We can observe that the SAF module alone produces comparable retrieval results and the SGR module achieves state-of-the-art performance with R@1 of 75.2% and 56.2% for sentence and image retrieval, separately. This verifies the effectiveness of exploiting the relationship between alignments to boost similarity reasoning. When we combine the SAF and SGR module, the performance is further improved to achieve the best R@1 of 77.8% and 58.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In this section, we carry a series of ablation studies to explore the impact of different configurations for the SGR module, the similarity representation learning module and the process of training. We also compare different strategies of similarity prediction to demonstrate the superiority of SGR and SAF modules. All the comparative experiments are conducted on the Flickr30K dataset.</p><p>Configurations of SGR module. In <ref type="table" target="#tab_3">Table 3</ref> we investigate the effectiveness of each component in the SGR module. 1) Graph reasoning. We employ a framework without    <ref type="formula">(#1)</ref>, which adopts a fullyconnected layer and sigmoid function on the global alignment to obtain the final similarity. Comparing #1 and #6 based on R@1, the SGR module achieves 12.8% improvement for sentence retrieval and 10.2% for image retrieval. 2) Reasoning steps setting. Comparing #4, #5, #6 and #7, we set the step of the SGR module to 3 for maximum performance. 3) Global and local alignments. #2 and #3 only utilize local alignments for graph reasoning and adopt a meanpooling operation on them after reasoning. Comparing #2, #4 and #3, #6, we discover that global similarity is beneficial for aggregating local similarities and exploring their relations which improves at least 1.6% for sentence retrieval and 1.9% for image retrieval on R@1.</p><p>Configurations for Similarity Computation. <ref type="table" target="#tab_5">Table 4</ref> illustrates the impact of different strategies in similarity representation computation and the similarity score prediction. We test the results on local alignments and set the reasoning step of the SGR module to 3. we following  to explore two types of the cross-attention modes, i.e. I2T and T2I. Comparing #1, #2, #5 and #6, we find that averaging the local alignments calculated by a fully-connected layer and sigmoid function leads to better performance than  <ref type="figure">(I2T)</ref>. Besides, the SGR module fails to achieve significant improvement on I2T which indicates that the region features are redundant, relatively independent and irregular in order. Therefore, it is difficult for the SGR module to exploit semantic connections compared with word features. In terms of #4 and #8, the SAF module achieves impressive progress both in I2T and T2I modes that demonstrates that the SAF module filters and aggregates plenty of discriminative local alignments steadily to improve the precision of image-text matching.</p><p>Configurations for Training Process. In table 5, we report the results of different training strategies: joint learning and independent learning. Compared with the SGR/SAF module alone, joint learning can help the SAF module improve the performance of sentence retrieval, and also help the SGR module enhance the ability of image retrieval. In terms of independent learning, the SGRAF network gains an exact and impressive promotion. We assume that the SGR module frequently captures several crucial cues by propagating information between local and global alignments and throws out some relatively unimportant interactions. Moreover, the SAF module attempts to gather all the meaningful alignments and eliminates completely irrelevant interactions. Therefore, the global and local alignments for the SAF and SGR modules are seemingly not incompatible resulting in the unobvious improvement. It is worth noting that the SAF module tends to be more susceptible to the hard negative samples than the SGR module because of the high correlation. On the other hand, it is more challenging for the SGR module to resolve the transmission and integration of numerous semantic alignments. As a result, they can cooperate with each other and further achieve more accurate similarity prediction through independent training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results and Analysis</head><p>As it is shown in <ref type="figure">Figure 3</ref>, we illustrate the distribution of attention weights learned by the SAF module.</p><p>Given an image query, the SAF module captures the key cues ("dog runs", "green grass", "wooden fence") for positive image-text pairs, and also highlights the meaningful instances ("brown dog", "white paws", "trotting", "green grass") for negative pairs. Note that there exists a crucial discrepancy ("brown") which is submerged by AVE operation between negative text and image that depicts a black and white dog. Compared with the wrong matching of AVE, SAF module can stress on all the useful alignments including unmatched instance ("brown") and suppress irrelevant interactions ("of", "with", "is", and etc). On the other hand, the process of SGR module reinforces the role of the alignment ("brown"), which leads to lower similarity between hard negative and query image. Our implementation of this paper is publicly available on GitHub at: https://github.com/Paranioar/SGRAF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we present a SGRAF network consisting of similarity graph reasoning (SGR) and similarity attention filtration (SAF) module. The SGR module performs multistep reasoning based on global and local similarity nodes and captures their relations through information propagation, while the SAF module attends more to discriminative and meaningful alignments for similarity aggregation. We demonstrate that it is important to exploit the relationship between local and global alignments, and suppress the disturbances of less-meaningful alignments. Extensive experiments on benchmark datasets show that both SGR and SAF modules can effectively discover the associations between image and text and achieve further improvements when cooperating with each other. <ref type="table" target="#tab_9">Table 6</ref> shows the detailed implementations of the proposed SGRAF network including generic representation extraction, similarity representation learning, similarity graph reasoning and attention filtration. Generic Representation Extraction. Given an image, we first apply Faster R-CNN  to extract the top K=36 region proposals and obtain 2048-d feature for each region, then we add a FC layer to transform region features into 1024-d vectors V , and perform the selfattention mechanism  to output a 1024d global visual vector v. Given a sentence with L words, we transform each word into a 300-d vector with wordembedding, and use Bi-GRU to encode words into 1024-d vectors T . Similarly, we exploit the self-attention mechanism ) illustrated in <ref type="figure">Figure 5</ref> to output a 1024-d global textual vector t.</p><p>Similarity Representation Learning. We compute L textual-attended 256-d similarity vectors s l with Eq.(5), and one global similarity vector s g with Eq.</p><p>(2), which obtain L+1 (local+global) 256-d similarity vectors N .</p><p>Similarity Graph Reasoning. As shown in <ref type="figure">Figure 4</ref>, we take the above-introduced L+1 (256-d) similarity vectors N as graph nodes, and then compute the weight of each edge via Eq.(6) with learnable parameter matrices. Graph reasoning is conducted with Eq.(7-8), which means that, for each node s p at step n, we learn the weight of its connected nodes (including itself) to aggregate their features from step n-1, and then perform a non-linear transformation to update the feature of s p at step n. In this way, the information from both local and global alignments is aggregated to produce more accurate similarity predictions. Then we feed the reasoned 256-d global vector s r into a FC+sigmoid layer to output a scalar similarity.</p><p>Similarity Attention Filtration. The SAF module takes the L+1 (256-d) similarity vectors N as inputs to learn L+1 attention weights β with Eq.(9) and performs aggregation to output one 256-d similarity vector s f , which is then fed into another FC+sigmoid layer to output a scalar similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Studies</head><p>We evaluate the SGR module with different graph dimension m as illustrated in <ref type="table" target="#tab_10">Table 7</ref>. We test the results on global and local alignments and set the reasoning step to 3. The parameters during each step are not shared. We observe that the SGR module is insensitive to the dimension of similarity representation that implies the stabilization and robustness of the SGR module. Note that we set graph dimension m to 256, which can yield the best results for image-text retrieval. Weighted sum s f 1×256 <ref type="bibr">[16]</ref> [15] FC+Sigmoid 1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Studies</head><p>In this section, we exhibit the retrieval examples of sentence retrieval in <ref type="figure">Figure 6</ref>. Retrieval examples of image retrieval are shown in <ref type="figure" target="#fig_4">Figure 7</ref>. Furthermore, we demonstrate additional visualization of the SGRAF model in <ref type="figure">Figure 8</ref> where the local alignments are attended by textual words. Retrieval Examples of Bidirectional Retrieval. For sentence retrieval, our proposed SGRAF model can efficiently retrieve the correct sentences. Note that the mismatch of F30K-Query3 is also reasonable, which includes highly relevant descriptions of concepts ("young boy", "handheld shovel") and scene ("dirt") with the image. For image retrieval, our network can distinguish hard samples well and retrieve the ground-truth image accurately, even if negative samples consist of the same semantic concepts, attributes, and relations with the text descriptions.</p><p>Visualization of the SGRAF Model. In <ref type="figure">Figure 8</ref>, the SAF module can selectively aggregate the discriminative alignments and meanwhile reduce the interferences of less-meaningful alignments, e.g. for the first image query, the SAF module can highlight the key alignments ("two  <ref type="figure">Figure 4</ref>: The proposed SGR module for image-text matching. All local alignments {s l } and the global alignment {s g } are firstly taken as initial graph nodes {s 0 }. We compute the edge from node s q ∈ {s 0 } to s p ∈ {s 0 } by the inner product between incoming and outgoing representations &lt; s 0 in,p , s 0 out,q &gt;, followed by a row-wise softmax. Then the node s p is updated by aggregating its connected nodes (including itself). We iteratively reason the similarity for 3 steps, and take the global node s g ∈ {s 3 } as the reasoned similarity representation  <ref type="figure">Figure 5</ref>: The self-attention module for global representation extraction. n denotes the number of local features, that is, n = 36 for image regions and n = L for sentence words man", "dancing", "street", "synchronized martial arts performance", etc.) and suppress irrelevant ones ("the", "of", "in", "a", "be", etc). Besides, the SGR module can capture fine-grained alignments to achieve comprehensive similarity reasoning, e.g. for the second image query, the SGR module stresses on the alignments ("young boy", "Texas") and produces larger gaps between matched and unmatched pairs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F30K_Query1 F30K_Query2 F30K_Query3</head><p>Rank1： A cow on the sidewalk on a corner in front of a store Rank2： A cow standing near a curb in front of a store . Rank3： A cow on a city sidewalk in front of a business .</p><p>Rank4： There is a cow on the sidewalk standing in front of a door . Rank5： Cow standing on sidewalk in city area near shops .</p><p>Rank1： a brown sofa with pineapple pillows and ottoman with two remotes on it. Rank2： A small oval orange couch and ottoman with pineapple pillows . Rank3： A brown sofa and ottoman with pillows and remotes . Rank4： a small couch covered with blankets and pineapple designed pillows Rank5： A couch and ottoman are shown with remotes .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank1：</head><p>The wooden bow of a ship with an out of focus boat in the back ground . Rank2： A close up of a front of a boat with another in the background . Rank3： This is an image of a trunk in a damaged home .</p><p>Rank4： The bottom of a rustic boat overlooks a brightly painted one .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank5：</head><p>The bow of a ship on land with another on the edge of the water .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO_Query1</head><p>MSCOCO_Query2 MSCOCO_Query3 <ref type="figure">Figure 6</ref>: Additional qualitative examples of sentence retrieval on Flickr30K (top) and MSCOCO (bottom). The top-5 retrieved results are displayed. Green denotes the ground-truth sentence and red denotes the unmatched retrieval F30K_Query1： A mountain biker rides up a hill on a red bicycle .</p><p>F30K_Query2： A man in an orange jersey with the letter " 12 " on it plays football .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F30K_Query3：</head><p>There is a little boy ready to hit the tennis ball , holding a racquet .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO _Query2：</head><p>Smiling man wearing black shirt and pale green tie .</p><p>MSCOCO_Query1： A motor bike and some wine in a room .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO_Query3：</head><p>Window view from the inside of airplanes , baggage carrier and tarmac .    <ref type="figure">Figure 8</ref>: The visualization of the SGRAF model. Positive and Negative denotes ground-truth and hard negative example respectively. SAF β denotes attention weight distribution of SAF module. SGR α denotes the cosine distance between final alignment and raw alignments. Final sim denotes similarity calculated by AVE (average), SAF or SGR module. The key cues of hard negative examples for each query are {"two man"} and {"young boy","Texas"}. We observe that SAF module can suppress the irrelevant interactions effectively while SGR module can capture fine-grained and crucial alignments by propagating information among all the similarities</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Additional qualitative examples of image retrieval on Flickr30K (top) and MSCOCO (bottom). The top-5 retrieved results are displayed. Green denotes the ground-truth image and red denotes the unmatched retrieval</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc> adapted Recurrent Residual networks to refine global embeddings.) employed multi-head self-attention to combine global context with locally-guided features. Besides, Some works</figDesc><table /><note>to extract word representations. Klein et al. (Klein et al. 2015) explored Fisher Vectors (FV) (Perronnin and Dance 2007) for text representation. Kiros et al. (Kiros, Salakhutdinov, and Zemel 2014) adopted a GRU as the text encoder. For visual features,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Wang et al. 2019c) 72.3 94.8 98.3 58.5 87.9 95.0 68.1 89.7 95.2 51.5 77.1 85.3 SCAN (Lee et al. 2018) 72.7 94.8 98.4 58.8 88.4 94.8 67.4 90.3 95.8 48.6 77.7 85.2 SGM (Wang et al. 2020) 73.4 93.8 97.8 57.5 87.3 94.3 71.8 91.7 95.5 53.5 79.6 86.5 VSRN* (Li et al. 2019) 74.0 94.3 97.8 60.8 88.4 94.1 70.4 89.2 93.7 53.0 77.9 85.7 Table</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MSCOCO dataset</cell><cell></cell><cell></cell><cell cols="2">Flickr30K dataset</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Sentence Retrieval</cell><cell cols="2">Image Retrieval</cell><cell cols="2">Sentence Retrieval</cell><cell cols="2">Image Retrieval</cell></row><row><cell></cell><cell cols="8">R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>CAMP (RDAN (Hu et al. 2019)</cell><cell cols="8">74.6 96.2 98.7 61.6 89.2 94.7 68.1 91.0 95.9 54.1 80.9 87.2</cell></row><row><cell>MMCA (Wei et al. 2020)</cell><cell cols="8">74.8 95.6 97.7 61.6 89.8 95.2 74.2 92.8 96.4 54.8 81.4 87.8</cell></row><row><cell>BFAN (Liu et al. 2019)</cell><cell>74.9 95.2</cell><cell>-</cell><cell>59.4 88.4</cell><cell>-</cell><cell>68.1 91.4</cell><cell>-</cell><cell>50.8 78.4</cell><cell>-</cell></row><row><cell>CAAN (Zhang et al. 2020)</cell><cell cols="8">75.5 95.4 98.5 61.3 89.7 95.2 70.1 91.6 97.2 52.8 79.0 87.9</cell></row><row><cell cols="9">DPRNN (Chen and Luo 2020) 75.3 95.8 98.6 62.5 89.7 95.1 70.2 91.6 95.8 55.5 81.3 88.2</cell></row><row><cell>PFAN (Wang et al. 2019b)</cell><cell cols="8">76.5 96.3 99.0 61.6 89.6 95.2 70.0 91.8 95.0 50.4 78.7 86.1</cell></row><row><cell>VSRN (Li et al. 2019)</cell><cell cols="8">76.2 94.8 98.2 62.8 89.7 95.1 71.3 90.6 96.0 54.7 81.8 88.2</cell></row><row><cell>IMRAM (Chen et al. 2020)</cell><cell cols="8">76.7 95.6 98.5 61.7 89.1 95.0 74.1 93.0 96.6 53.9 79.4 87.2</cell></row><row><cell>Ours(SAF)</cell><cell cols="8">76.1 95.4 98.3 61.8 89.4 95.3 73.7 93.3 96.3 56.1 81.5 88.0</cell></row><row><cell>Ours(SGR)</cell><cell cols="8">78.0 95.8 98.2 61.4 89.3 95.4 75.2 93.3 96.6 56.2 81.0 86.5</cell></row><row><cell>Ours(SGRAF)</cell><cell cols="8">79.6 96.2 98.5 63.2 90.7 96.1 77.8 94.1 97.4 58.5 83.0 88.8</cell></row></table><note>1: Comparison of bi-directional retrieval results (R@K(%)) on MSCOCO 1K test set and Flickr30K test set. VSRN* denotes a single model for a fair comparison with SGR. SGRAF denotes the whole framework with independent training</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of bi-directional retrieval results (R@K(%)) on MSCOCO 5K test set of queries whose ground-truth is ranked within the top K. We adopt R@1, R@5 and R@10 as our evaluation metrics.</figDesc><table><row><cell>Implementation Details. For each image, we take the</cell></row><row><cell>Faster-RCNN (Ren et al. 2015) detector with ResNet-101</cell></row><row><cell>provided by (Anderson et al. 2018) to extract the top K = 36</cell></row><row><cell>region proposals and obtain a 2048-dimensional feature for</cell></row><row><cell>each region. For each sentence, we set the word embedding</cell></row><row><cell>size as 300, and the number of hidden states as 1024. The di-</cell></row><row><cell>mension of similarity representation m is 256, with smooth</cell></row><row><cell>temperature λ = 9, reasoning steps N = 3, and margin</cell></row></table><note>γ = 0.2. Our model employs the Adam optimizer (Kingma and Ba 2015) to train the SGRAF network with the mini- batch size of 128. The learning rate is set to be 0.0002 for the first 10 epochs and 0.00002 for the next 10 epochs on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The impact of SGR configurations. GLO and LOC respectively indicates the employment of global and local alignments, and Step denotes the graph reasoning steps</figDesc><table><row><cell>MSCOCO. For Flickr30K, we start training the SGR (SAF)</cell></row><row><cell>module with learning rate 0.0002 for 30 (20) epochs and de-</cell></row><row><cell>cay it by 0.1 for the next 10 epochs. We select the snapshot</cell></row><row><cell>with the best performance on the validation set for testing.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: The impact of Similarity configurations. I2T and</cell></row><row><cell cols="2">T2I denotes the visual-to-textual and textual-to-visual atten-</cell></row><row><cell cols="2">tion to generate local similarity representations separately.</cell></row><row><cell cols="2">SS denotes the scalar-based cosine similarity and SV indi-</cell></row><row><cell cols="2">cates the vector-based similarity, and AA represents the av-</cell></row><row><cell cols="2">erage aggregation of all alignments</cell></row><row><cell>Dataset SAF SGR Joint Split</cell><cell>Sen. Ret. Ima. Ret. R@1 R@10 R@1 R@10</cell></row><row><cell></cell><cell>76.1 98.3 61.8 95.3</cell></row><row><cell>MSCOCO</cell><cell>78.0 98.2 61.4 95.4 77.8 98.2 61.6 95.2</cell></row><row><cell></cell><cell>79.6 98.5 63.2 96.1</cell></row><row><cell></cell><cell>73.7 96.3 56.1 88.0</cell></row><row><cell>Flickr30K</cell><cell>75.2 96.6 56.2 86.5 75.1 96.1 56.2 85.8</cell></row><row><cell></cell><cell>77.8 97.4 58.5 88.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The impact of Training configurations on MSCOCO 1K test set and Flickr30K test set. Split and Joint denotes independent and joint training of two modules graph reasoning as the baseline</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The visualization of SAF and SGR module. Positive and Negative denotes ground-truth and hard negative examples respectively. SAF β denotes attention weight distribution of SAF module. SGR α denotes the cosine distance between final alignment and raw alignments. Final sim denotes similarity calculated by AVE (average), SAF or SGR module averaging local cosine distance. Comparing #3 and #7, it is more reasonable for the SGR module to count on the local alignments attended by word features (T2I) than the ones by region features</figDesc><table><row><cell>Query:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Positive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Local alignments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Global</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Caption</cell><cell>A</cell><cell>dog</cell><cell>runs</cell><cell>on</cell><cell>the</cell><cell cols="2">green</cell><cell cols="2">grass</cell><cell>near</cell><cell>a</cell><cell>wooden</cell><cell>fence</cell><cell>.</cell><cell>---</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SAF β</cell><cell>0.0</cell><cell>0.2</cell><cell>0.04</cell><cell>0.01</cell><cell>0.01</cell><cell>0.07</cell><cell></cell><cell>0.06</cell><cell></cell><cell>0.01</cell><cell>0.0</cell><cell>0.2</cell><cell>0.14</cell><cell>0.0</cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SGR α</cell><cell cols="2">0.37 0.37</cell><cell>0.39</cell><cell>0.37</cell><cell>0.46</cell><cell>0.48</cell><cell></cell><cell>0.53</cell><cell></cell><cell>0.46</cell><cell>0.3</cell><cell>0.29</cell><cell>0.33</cell><cell>0.3</cell><cell>0.18</cell></row><row><cell></cell><cell></cell><cell></cell><cell>cosine</cell><cell>0.0</cell><cell>0.9</cell><cell>0.8</cell><cell>0.3</cell><cell>0.4</cell><cell>0.7</cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.3</cell><cell>0.0</cell><cell>0.8</cell><cell>0.98</cell><cell>0.0</cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Final sim</cell><cell></cell><cell cols="3">AVE score:0.54</cell><cell></cell><cell></cell><cell cols="4">SAF score:0.89</cell><cell></cell><cell></cell><cell cols="2">SGR score: 0.92</cell></row><row><cell>Negative</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Local alignments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Global</cell></row><row><cell>Caption</cell><cell>A</cell><cell>brown</cell><cell>dog</cell><cell>with</cell><cell>white</cell><cell>paws</cell><cell>is</cell><cell cols="2">trotting</cell><cell cols="2">through</cell><cell>a</cell><cell>field</cell><cell>of</cell><cell>green</cell><cell>grass</cell><cell>.</cell><cell>---</cell></row><row><cell>SAF β</cell><cell>0.0</cell><cell>0.12</cell><cell>0.3</cell><cell>0.01</cell><cell>0.05</cell><cell>0.04</cell><cell>0</cell><cell>0.1</cell><cell></cell><cell>.01</cell><cell></cell><cell>0</cell><cell>.05</cell><cell>0</cell><cell>0.09</cell><cell>0.07</cell><cell>0</cell><cell>0.2</cell></row><row><cell>SGR α</cell><cell>0.0</cell><cell>0.35</cell><cell>0.31</cell><cell>0.03</cell><cell>0.30</cell><cell>0.22</cell><cell>0.13</cell><cell>0.45</cell><cell></cell><cell cols="2">0.29</cell><cell>0.29</cell><cell>0.46</cell><cell>0.46</cell><cell>0.46</cell><cell>0.46</cell><cell>0.3</cell><cell>0.0</cell></row><row><cell>cosine</cell><cell>0.1</cell><cell>0.1</cell><cell>0.6</cell><cell>0.4</cell><cell>0.8</cell><cell>0.7</cell><cell>0.2</cell><cell>0.8</cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.8</cell><cell>0.8</cell><cell>0.6</cell><cell>0.8</cell><cell>0.8</cell><cell>0.3</cell><cell>0.2</cell></row><row><cell>Final sim</cell><cell></cell><cell></cell><cell cols="2">AVE score:0.56</cell><cell></cell><cell></cell><cell></cell><cell cols="4">SAF score:0.54</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SGR score:0.38</cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The details of the SGRAF network.</figDesc><table><row><cell>L represents the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>The impact of Graph Dimension on Flickr30K</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The paper is supported in part by the National <ref type="figure">Key</ref>  </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Overview</head><p>This supplementary document for similarity reasoning and filtration is organized as follows: 1) more diagrams and descriptions of the SGRAF network: self-attention and SGR module; 2) more quantitative studies: the impact of graph dimension; 3) more qualitative studies: retrieval examples of bidirectional retrieval and visualization of our model.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12655" to="12663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Expressing Objects Just Like Words: Recurrent Visual Embedding for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10583" to="10590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<title level="m">VSE++: Improved Visual-Semantic Embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeViSE: A Deep Visual-Semantic Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval With Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7181" to="7189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-Level Visual-Semantic Alignments with Relation-Wise Dual Attention Network for Image and Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="789" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Semantic Concepts and Order for Image and Sentence Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saliency-Guided Attention Network for Image-Sentence Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<title level="m">Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using Fisher Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked Cross Attention for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="212" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual Semantic Reasoning for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4653" to="4661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focus Your Attention: A Bidirectional Focal Attention Network for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a Recurrent Residual Fusion Network for Multimodal Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4127" to="4136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dual Attention Networks for Multimodal Reasoning and Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2156" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fisher Kernels on Visual Vocabularies for Image Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSP</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge Aware Semantic Concept Expansion for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5182" to="5189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1979" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph-Structured Representations for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3233" to="3241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural Machine Translation with Latent Semantic of Image and Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Misono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Order-Embeddings of Images and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning Deep Structure-Preserving Image-Text Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neighbourhood Watch: Referring Expression Comprehension via Language-Guided Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1960" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1497" to="1506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Position Focused Attention Network for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3792" to="3798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5763" to="5772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adaptive Cross-Modal Embeddings for Image-Text Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kolling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12313" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-Modality Cross Attention Network for Image and Sentence Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10941" to="10950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AttnGAN: Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Auto-Encoding Scene Graphs for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context-Aware Attention Network for Image-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3536" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05535.References</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Dual-Path Convolutional Image-Text Embedding</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12655" to="12663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Expressing Objects Just Like Words: Recurrent Visual Embedding for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10583" to="10590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<title level="m">VSE++: Improved Visual-Semantic Embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DeViSE: A Deep Visual-Semantic Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval With Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7181" to="7189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-Level Visual-Semantic Alignments with Relation-Wise Dual Attention Network for Image and Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="789" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning Semantic Concepts and Order for Image and Sentence Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Saliency-Guided Attention Network for Image-Sentence Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<title level="m">Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using Fisher Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Stacked Cross Attention for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="212" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Visual Semantic Reasoning for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4653" to="4661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Focus Your Attention: A Bidirectional Focal Attention Network for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning a Recurrent Residual Fusion Network for Multimodal Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4127" to="4136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Dual Attention Networks for Multimodal Reasoning and Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2156" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Fisher Kernels on Visual Vocabularies for Image Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSP</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Knowledge Aware Semantic Concept Expansion for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5182" to="5189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1979" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Graph-Structured Representations for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3233" to="3241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Neural Machine Translation with Latent Semantic of Image and Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Misono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Order-Embeddings of Images and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning Deep Structure-Preserving Image-Text Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Neighbourhood Watch: Referring Expression Comprehension via Language-Guided Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1960" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1497" to="1506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Position Focused Attention Network for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3792" to="3798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5763" to="5772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Adaptive Cross-Modal Embeddings for Image-Text Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kolling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12313" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-Modality Cross Attention Network for Image and Sentence Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10941" to="10950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">AttnGAN: Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Auto-Encoding Scene Graphs for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Context-Aware Attention Network for Image-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3536" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05535</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Dual-Path Convolutional Image-Text Embedding</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
