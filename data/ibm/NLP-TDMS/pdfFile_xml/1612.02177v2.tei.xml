<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-07">7 May 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Hyun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Kyoung</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-07">7 May 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multiscale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion blur is one of the most commonly arising types of artifacts when taking photos. Shakes of camera and fast object motions degrade image quality to undesired blurry images. Furthermore, various causes such as depth variation, occlusion in motion boundaries make blurs even more complex. Single image deblurring problem is to estimate the unknown sharp image given a blurry image. Earlier studies focused on removing blurs caused by simple translational or rotational camera motions. More recent works try to handle general non-uniform blurs caused by depth variation, camera shakes and object motions in dynamic environments. Most of these approaches are based on following blur model <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>.</p><formula xml:id="formula_0">B = KS + n,<label>(1)</label></formula><p>where B, S and n are vectorized blurry image, latent sharp image, and noise, respectively. K is a large sparse matrix whose rows each contain a local blur kernel acting on S to generate a blurry pixel. In practice, blur kernel is unknown. Thus, blind deblurring methods try to estimate latent sharp image S and blur kernel K simultaneously.</p><p>Finding blur kernel for every pixel is a severely ill-posed problem. Thus, some approaches tried to parametrize blur models with simple assumptions on the sources of blurs. In <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10]</ref>, they assumed that blur is caused by 3D camera motion only. However, in dynamic scenes, the kernel estimation is more challenging as there are multiple moving objects as well as camera motion. Thus, Kim et al. <ref type="bibr" target="#b13">[14]</ref> proposed a dynamic scene deblurring method that jointly segments and deblurs a non-uniformly blurred image, allowing the estimation of complex (non-linear) kernel within a segment. In addition, Kim and Lee <ref type="bibr" target="#b14">[15]</ref> approximated the blur kernel to be locally linear and proposed an approach that estimates both the latent image and the locally linear motions jointly. However, these blur kernel approximations are still inaccurate, especially in the cases of abrupt motion discontinuities and occlusions. Note that such erroneous kernel estimation directly affects the quality of the latent image, resulting in undesired ringing artifacts.</p><p>Recently, CNNs (Convolutional Neural Networks) have been applied in numerous computer vision problems including deblurring problem and showed promising results <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b0">1]</ref>. Since no pairs of real blurry image and ground truth sharp image are available for supervised learning, they commonly used blurry images generated by convolving synthetic blur kernels. In <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1]</ref>, synthesized blur images with uniform blur kernel are used for training. And, in <ref type="bibr" target="#b25">[26]</ref>, classification CNN is trained to estimate locally linear blur kernels. Thus, CNN-based models are still suited only to some specific types of blurs, and there are restrictions on more common spatially varying blurs. Therefore, all the existing methods still have many problems before they could be generalized and used in practice. These are mainly due to the use of simple and unrealistic blur kernel models. Thus, to solve those problems, in this work, we propose a novel end-to-end deep learning approach for dynamic scene deblurring.</p><p>First, we propose a multi-scale CNN that directly restores latent images without assuming any restricted blur kernel model. Especially, the multi-scale architecture is designed to mimic conventional coarse-to-fine optimization methods. Unlike other approaches, our method does not estimate explicit blur kernels. Accordingly, our method is free from artifacts that arise from kernel estimation errors. Second, we train the proposed model with a multi-scale loss that is appropriate for coarse-to-fine architecture that enhances convergence greatly. In addition, we further improve the results by employing adversarial loss <ref type="bibr" target="#b8">[9]</ref>. Third, we propose a new realistic blurry image dataset with ground truth sharp images. To obtain kernel model-free dataset for training, we employ the dataset acquisition method introduced in <ref type="bibr" target="#b16">[17]</ref>. As the blurring process can be modeled by the integration of sharp images during shutter time <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16]</ref>, we captured a sequence of sharp frames of a dynamic scene with a high-speed camera and averaged them to generate a blurry image by considering gamma correction.</p><p>By training with the proposed dataset and adding proper augmentation, our model can handle general local blur kernel implicitly. As the loss term optimizes the result to resemble the ground truth, it even restores occluded regions where blur kernel is extremely complex as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We trained our model with millions of pairs of image patches and achieved significant improvements in dynamic scene deblurring. Extensive experimental results demonstrate that the performance of the proposed method is far superior to those of the state-of-the-art dynamic scene deblurring methods in both qualitative and quantitative evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Works</head><p>There are several approaches that employed CNNs for deblurring <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Xu et al. <ref type="bibr" target="#b28">[29]</ref> proposed an image deconvolution CNN to deblur a blurry image in a non-blind setting. They built a network based on the separable kernel property that the (inverse) blur kernel can be decomposed into a small number of significant filters. Additionally, they incorporated the denoising network <ref type="bibr" target="#b6">[7]</ref> to reduce visual artifacts such as noise and color saturation by concatenating the module at the end of their proposed network.</p><p>On the other hand, Schuler et al. <ref type="bibr" target="#b24">[25]</ref> proposed a blind deblurring method with CNN. Their proposed network mimics conventional optimization-based deblurring methods and iterates the feature extraction, kernel estimation, and the latent image estimation steps in a coarse-to-fine manner. To obtain pairs of sharp and blurry images for network training, they generated uniform blur kernels using a Gaussian process and synthesized lots of blurry images by convolving them to the sharp images collected from the Im-ageNet dataset <ref type="bibr" target="#b2">[3]</ref>. However, they reported performance limits for large blurs due to their suboptimal architecture.</p><p>Similarly to the work of Couzinie-Devy et al. <ref type="bibr" target="#b1">[2]</ref>, Sun et al. <ref type="bibr" target="#b25">[26]</ref> proposed a sequential deblurring approach. First, they generated pairs of blurry and sharp patches with 73 candidate blur kernels. Next, they trained classification CNN to measure the likelihood of a specific blur kernel of a local patch. And then smoothly varying blur kernel is obtained by optimizing an energy model that is composed of the CNN likelihoods and smoothness priors. Final latent image estimation is performed with conventional optimization method <ref type="bibr" target="#b29">[30]</ref>.</p><p>Note that all these methods require an accurate kernel estimation step for restoring the latent sharp image. In contrast, our proposed model is learned to produce the latent image directly without estimating blur kernels.</p><p>In other computer vision tasks, several forms of coarseto-fine architecture or multi-scale architecture were applied <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5]</ref>. However, not all multi-scale CNNs are designed to produce optimal results, similarly to <ref type="bibr" target="#b24">[25]</ref>. In depth estimation, optical flow estimation, etc., networks usually produce outputs having smaller resolution compared to input image resolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>. These methods have difficulties in handling long-range dependency even if multi-scale architecture is used.</p><p>Therefore, we make a multi-scale architecture that preserves fine-grained detail information as well as long-range dependency from coarser scales. Furthermore, we make sure intermediate level networks help the final stage in an explicit way by training network with multi-scale losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Kernel-Free Learning for Dynamic Scene Deblurring</head><p>Conventionally, it was essential to find blur kernel before estimating latent image. CNN based methods were no exception <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. However, estimating kernel involves several problems. First, assuming simple kernel convolution cannot model several challenging cases such as occluded regions or depth variations. Second, kernel estimation process is subtle and sensitive to noise and saturation, unless blur model is carefully designed. Furthermore, incorrectly estimated kernels give rise to artifacts in latent images. Third, finding spatially varying kernel for every pixel in dynamic scene requires a huge amount of memory and computation.</p><p>Therefore, we adopt kernel-free methods in both blur dataset generation and latent image estimation. In blurry image generation, we follow to approximate camera imaging process, rather than assuming specific motions, instead of finding or designing complex blur kernel. We capture successive sharp frames and integrate to simulate blurring process. The detailed procedure is described in section 2. Note that our dataset is composed of blurry and sharp image pairs only, and that the local kernel information is implicitly embedded in it. In <ref type="figure" target="#fig_1">Fig. 2</ref>, our kernel-free blurry image is compared with a conventional synthesized image with uniform blur kernel. Notably, the blur image generated by our method exhibits realistic and spatially varying blurs caused by the moving person and the static background, while the blur image synthesized by conventional method does not. For latent image estimation, we do not assume blur sources and train the model solely on our blurry and sharp image pairs. Thus, our proposed method does not suffer from kernel-related problems in deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Blur Dataset</head><p>Instead of modeling a kernel to convolve on a sharp image, we choose to record the sharp information to be integrated over time for blur image generation. As camera sensor receives light during the exposure, sharp image stimulation at every time is accumulated, generating blurry image <ref type="bibr" target="#b12">[13]</ref>. The integrated signal is then transformed into pixel value by nonlinear CRF (Camera Response Function). Thus, the process could be approximated by accumulating signals from high-speed video frames.</p><p>Blur accumulation process can be modeled as follows.</p><formula xml:id="formula_1">B = g 1 T T t=0 S(t)dt ≃ g 1 M M−1 i=0 S[i] ,<label>(2)</label></formula><p>where T and S(t) denote the exposure time and the sensor signal of a sharp image at time t, respectively. Similarly, M , S[i] are the number of sampled frames and the i-th sharp frame signal captured during the exposure time, respectively. g is the CRF that maps a sharp latent signal S(t) into an observed imageŜ(t) such thatŜ(t) = g(S(t)), orŜ[i] = g(S[i]). In practice, we only have observed video frames while the original signal and the CRF is unknown.</p><p>It is known that non-uniform deblurring becomes significantly difficult when nonlinear CRF is involved, and nonlinearity should be taken into account. However, currently, there are no CRF estimation techniques available for an image with spatially varying blur <ref type="bibr" target="#b26">[27]</ref>. When the ground truth CRF is not given, a common practical method is to approximate CRF as a gamma curve with γ = 2.2 as follows, since it is known as an aproximated average of known CRFs <ref type="bibr" target="#b26">[27]</ref>.</p><formula xml:id="formula_2">g(x) = x 1/γ .<label>(3)</label></formula><p>Thus, by correcting the gamma function, we obtain the latent frame signal S[i] from the observed imageŜ[i] by S[i] = g −1 (Ŝ[i]), and then synthesize the corresponding blur image B by using <ref type="bibr" target="#b1">(2)</ref>.</p><p>We used GOPRO4 Hero Black camera to generate our dataset. We took 240 fps videos with GOPRO camera and then averaged varying number (7 -13) of successive latent frames to produce blurs of different strengths. For example, averaging 15 frames simulates a photo taken at 1/16 shutter speed, while corresponding sharp image shutter speed is 1/240. Notably, the sharp latent image corresponding to each blurry one is defined as the mid-frame among the sharp frames that are used to make the blurry image. Finally, our dataset is composed of 3214 pairs of blurry and sharp images at 1280x720 resolution. The proposed GOPRO dataset is publicly available on our website 1 . In this case, blur is mostly caused by person motion, leaving the background as it is. The blur kernel is non-uniform, complex shaped. However, when the blurry image is synthesized by convolution with a uniform kernel, the background also gets blurred as if blur was caused by camera shake. To model dynamic scene blur, our kernel-free method is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In our model, finer scale image deblurring is aided by coarser scale features. To exploit coarse and middle level information while preserving fine level information at the same time, input and output to our network take the form of Gaussian pyramids. Note that most of other coarse-to-fine networks take a single image as input and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>In addition to the multi-scale architecture, we employ a slightly modified version of residual network structure <ref type="bibr" target="#b11">[12]</ref> as a building block of our model. Using residual network structure enables deeper architecture compared to a plain CNN. Also, as blurry and sharp image pairs are similar in values, it is efficient to let parameters learn the difference only. We found that removing the rectified linear unit after the shortcut connection of the original residual building block boosts the convergence speed at training time. We denote the modified building block as ResBlock. The original and our modified building block are compared in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>By stacking enough number of convolution layers with ResBlocks, the receptive field at each scale is expanded. Details are described in the following paragraphs. For sake of consistency, we define scale levels in the order of decreasing resolution (i.e. level 1 for finest scale). Unless denoted otherwise, we use total K = 3 scales. At training time, we set the resolution of the input and output Gaussian pyramid patches to be {256 × 256, 128 × 128, 64 × 64}. The scale ratio between consecutive scales is 0.5. For all convolution layers, we set the filter size to be 5 × 5. As our model is fully convolutional, at test time, the patch size may vary as the GPU memory allows. The overall architecture is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>.  Modified building block of our network. We did not use batch normalization layers since we trained model with mini-batch of size 2, which is smaller than usual for batch normalization. We found removing rectified linear unit just before the block output is beneficial in terms of performance empirically. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarsest level network</head><p>At the front of the network locates the coarsest level network. The first convolution layer transforms 1/4 resolution, 64 × 64 size image into 64 feature maps. Then, 19 ResBlocks are stacked followed by last convolution layer that transforms the feature map into input dimension. Every convolution layer preserves resolution with zero padding. In total, there are 40 convolution layers. The number of convolution layers at each scale level is determined so that total model should have 120 convolution layers. Thus, the coarsest level network has receptive field large enough to cover the whole patch. At the end of the stage, the coarsest level latent sharp image is generated. Moreover, information from the coarsest level output is delivered to the next stage where finer scale network is. To convert a coarsest output to fit the input size of the next finer scale, the output patch passes an upconvolution <ref type="bibr" target="#b21">[22]</ref> layer, while other multi-scale methods use reshaping <ref type="bibr" target="#b7">[8]</ref> or upsampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>. Since the sharp and blurry patches share low-frequency information, learning suitable feature with upconvolution helps to remove redundancy. In our experiment, using upconvolution showed better performance than upsampling. Then, the upconvolution feature is concatenated with the finer scale blurry patch as an input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finer level network</head><p>Finer level networks basically have the same structure as in the coarsest level network. However, the first convolution layer takes the sharp feature from the previous stage as well as its own blurry input image, in a concatenated form. Every convolution filter size is 5 × 5 with the same number of feature maps as in the coarsest level. Except for the last finest scale, there is an upconvolution layer before the next stage. At the finest scale, the original resolution sharp image is restored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>Our model is trained on the proposed GOPRO dataset. Among 3214 pairs, 2103 pairs were used for training and remainings were used for the test. To prevent our network from overfitting, several data augmentation techniques are involved. In terms of geometric transformations, patches are randomly flipped horizontally and vertically, rotated by 90 degrees. For color, RGB channels are randomly permuted. To take image degradations into account, saturation in HSV colorspace is multiplied by a random number within [0.5, 1.5]. Also, Gaussian random noise is added to blurry images. To make our network be robust against different strengths of noise, standard deviation of noise is also randomly sampled from Gaussian distribution, N (0, (2/255) 2 ). Then, value outside [0, 1] is clipped. Finally, 0.5 is subtracted to set input and output value range zero-centered, having range [-0.5, 0.5].</p><p>In optimizing the network parameters, we trained the model in a combination of two losses, multi-scale content loss and adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale content loss</head><p>Basically, the coarse-to-fine approach desires that every intermediate output becomes the sharp image of the corresponding scale. Thus, we train our network so that intermediate outputs should form a Gaussian pyramid of sharp images. MSE criterion is applied to every level of pyramids. Hence, the loss function is defined as follows:</p><formula xml:id="formula_3">L cont = 1 2K K k=1 1 c k w k h k L k − S k 2 ,<label>(4)</label></formula><p>where L k , S k denote the model output and ground truth image at scale level k, respectively. The loss at each scale is normalized by the number of channels c k , width w k , and the height h k (i.e. the total number of elements).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial loss</head><p>Recently, adversarial networks are reported to generate sharp realistic images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref>. Following the architecture introduced in <ref type="bibr" target="#b23">[24]</ref>, we build discriminator as in <ref type="table" target="#tab_3">Table 1</ref>. Discriminator takes the output of the finest scale or the ground truth sharp image as input and classifies if it is deblurred image or sharp image. The adversarial loss is defined as follows.</p><formula xml:id="formula_4">L adv = E S∼p sharp (S) [log D(S)]+ E B∼p blurry (B) [log(1 − D(G(B)))],<label>(5)</label></formula><p>where G and D denote the generator, that is our multiscale deblurring network in <ref type="figure" target="#fig_4">Fig. 4</ref> and the discriminator (classifier), respectively. When training, G tries to minimize the adversarial loss while D tries to maximize it.</p><p>Finally, by combining the multi-scale content loss and adversarial loss, the generator network and discriminator network is jointly trained. Thus, our final loss term is</p><formula xml:id="formula_5">L total = L cont + λ × L adv ,<label>(6)</label></formula><p>where the weight constant λ = 1 × 10 −4 . We used ADAM <ref type="bibr" target="#b17">[18]</ref> optimizer with a mini-batch size 2 for training. The learning rate is adaptively tuned beginning from 5 × 10 −5 . After 3 × 10 5 iterations, the learning rate is decreased to 1/10 of the previous learning rate. Total training takes 9 × 10 5 iterations to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We implemented our model with torch7 library. All the following experiments were performed in a desktop with i7-6700K CPU and NVIDIA GTX Titan X (Maxwell) GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">GOPRO Dataset</head><p>We evaluate the performance of our model in the proposed GOPRO dataset. Our test dataset consists of 1111 pairs, which is approximately 1/3 of the total dataset. We compare the results with those of the state-of-the-art methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref> in both qualitative and quantitative ways. Our results show significant improvement in terms of image quality. Some deblurring results are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. We notice from the results of Sun et al. <ref type="bibr" target="#b25">[26]</ref>, deblurring is not successful on the regions where blurs are nonlinearly shaped or located at the boundary of motion. Kim and Lee <ref type="bibr" target="#b14">[15]</ref>'s results also fail in cases where strong edges are not found. In contrast, our results are free from those kernel-estimation related problems. <ref type="table" target="#tab_1">Table 2</ref>, shows the quantitative evaluation results of the competing methods and ours with different scale level k in terms of PSNR, SSIM over the test data. Also, the runtime is compared. We observe that our system with K = 2 produces the best results in terms of both PSNR and SSIM, while K = 3 is the fastest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Köhler Dataset</head><p>Köhler dataset <ref type="bibr" target="#b18">[19]</ref> consists of 4 latent images and 12 differently blurred images for each of them. The blurs are caused by replaying recorded 6D camera motion, assuming linear CRF. We report the quantitative results on this  <ref type="bibr" target="#b25">[26]</ref>, results of Kim and Lee <ref type="bibr" target="#b14">[15]</ref>, and results of the proposed method.  dataset in <ref type="table" target="#tab_3">Table 3</ref>. Our model is trained by setting g as identity function in <ref type="bibr" target="#b1">(2)</ref>. We note that our system with K = 3 produces the best results in PSNR, and the system K = 2 exhibits the best MSSIM result.  <ref type="table" target="#tab_3">Table 3</ref>. Quantitative comparison on the Köhler dataset. The dataset has its own evaluation code, thus we report multi-scale SSIM instead of SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dataset of Lai et al.</head><p>Lai et al. <ref type="bibr" target="#b19">[20]</ref> generated synthetic dataset by convolving nonuniform blur kernels and imposing several common degradations. They also recorded 6D camera trajectories to generate blur kernels. However, their blurry images and sharp images are not aligned in the way of our dataset, making simple image quality measures such as PSNR and SSIM less correlated with perceptual quality. Thus, we show qualitative comparisons in <ref type="figure" target="#fig_6">Fig. 6</ref>. Clearly, our results avoid ringing artifacts while preserving details such as wave ripple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a blind deblurring neural network for sharp image estimation. Unlike previous studies, our model avoids problems related to kernel estimation. The proposed model follows a coarse-to-fine approach and is trained in multi-scale space. We also constructed a realistic ground-truth blur dataset, enabling efficient supervised learning and rigorous evaluation. Experimental results show that our approach outperforms the state-of-theart methods in both qualitative and quantitative ways while being much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Comparison of loss function</head><p>In section 3.2, we employed a loss function that combines both the multi-scale content loss (MSE) and the adversarial loss for training our network. We examine the effect of the adversarial loss term quantitatively and qualitatively. The PSNR and SSIM results are shown in table A.1. From this results, we observe that adding adversarial loss does not increases PSNR, but increase SSIM, which means that it encourages to generate more natural and structure preserving images. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Input blurry image. (b) Result of Sun et al. [26]. (c) Our deblurring result. Our results show clear object boundaries without artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Ground truth sharp image. (b) Blurry image generated by convolving a uniform blur kernel. (c) Blurry image by averaging sharp frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>(a) Original residual network building block. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Multi-scale network architecture. B k , L k , S k denote blurry and latent, and ground truth sharp images, respectively. Subscript k denotes k-th scale level in the Gaussian pyramid, which is downsampled to 1/2 k scale. Our model takes a blurry image pyramid as the input and outputs an estimated latent image pyramid. Every intermediate scale output is trained to be sharp. At test time, original scale image is chosen as the final result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Test results on the GOPRO dataset. From top to bottom: Blurry images, results of Sun et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Deblurring results on the dataset<ref type="bibr" target="#b19">[20]</ref>. The top row shows results of results of Sun et al.<ref type="bibr" target="#b25">[26]</ref> and the bottom row shows our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative deblurring performance comparison on the GOPRO dataset. K denotes the scale level.</figDesc><table><row><cell>Measure</cell><cell>[26]</cell><cell>[15]</cell><cell cols="3">Ours K = 1 K = 2 K = 3</cell></row><row><cell>PSNR</cell><cell>24.64</cell><cell>23.64</cell><cell>28.93</cell><cell>29.23</cell><cell>29.08</cell></row><row><cell>SSIM</cell><cell cols="5">0.8429 0.8239 0.9100 0.9162 0.9135</cell></row><row><cell cols="2">Runtime 20 min</cell><cell>1 hr</cell><cell>7.21 s</cell><cell>4.33 s</cell><cell>3.09 s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table A .</head><label>A</label><figDesc>1. Quantitative deblurring performance comparison of loss used to optimize our model (K = 3, λ = 1 × 10 −4 ). Evaluated on the GOPRO test dataset assuming linear CRF. Loss L cont (M SE) L cont + λL adv Fig. A.1 and A.2 show some qualitative comparisons between the results of our network trained with L cont and L cont + λL adv .</figDesc><table><row><cell>PSNR</cell><cell>28.62</cell><cell>28.45</cell></row><row><cell>SSIM</cell><cell>0.9094</cell><cell>0.9170</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/SeungjunNah/DeepDeblur_release</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This project is partially funded by Microsoft Research Asia.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this appendix, we present more comparative experimental results to demonstrate the effectiveness of our proposed deblurring method.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to estimate and remove non-uniform image blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Couzinie-Devy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Ppocessing Ssytems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image deblurring using motion density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spacevariant single-image blind deconvolution for removing camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast removal of non-uniform camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation-free dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dynamic scene deblurring using a locally adaptive linear blur model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04265</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparative study for single image blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating sharp panoramas from motion-blurred videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Nonlinear camera response functions and image deblurring: Theoretical analysis and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2498" to="2512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
