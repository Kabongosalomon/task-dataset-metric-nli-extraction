<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Precision and Recall Metric for Assessing Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynkäänniemi</surname></persName>
							<email>nvidiatuomas.kynkaanniemi@aalto.fi</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
							<email>tkarras@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
							<email>jlehtinen@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
							<email>taila@nvidia.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Aalto University NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Precision and Recall Metric for Assessing Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of generative methods is to learn the manifold of the training data so that we can subsequently generate novel samples that are indistinguishable from the training set. While the quality of results from generative adversarial networks (GAN) <ref type="bibr" target="#b7">[8]</ref>, variational autoencoders (VAE) <ref type="bibr" target="#b14">[15]</ref>, autoregressive models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, and likelihood-based models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> have seen rapid improvement recently <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>, the automatic evaluation of these results continues to be challenging.</p><p>When modeling a complex manifold for sampling purposes, two separate goals emerge: individual samples drawn from the model should be faithful to the examples (they should be of "high quality"), and their variation should match that observed in the training set. The most widely used metrics, such as Fréchet Inception Distance (FID) <ref type="bibr" target="#b9">[10]</ref>, Inception Score (IS) <ref type="bibr" target="#b25">[26]</ref>, and Kernel Inception Distance (KID) <ref type="bibr" target="#b1">[2]</ref>, group these two aspects to a single value without a clear tradeoff. We illustrate by examples that this makes diagnosis of model performance difficult. For instance, it is interesting that while recent state-of-the-art generative methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> claim to optimize FID, in the end the (uncurated) results are almost always produced using another model that explicitly sacrifices variation, and often FID, in favor of higher quality samples from a truncated subset of the domain <ref type="bibr" target="#b17">[18]</ref>.  <ref type="figure">Figure 1</ref>: Definition of precision and recall for distributions <ref type="bibr" target="#b24">[25]</ref>. (a) Denote the distribution of real images with P r (blue) and the distribution of generated images with P g (red). (b) Precision is the probability that a random image from P g falls within the support of P r . (c) Recall is the probability that a random image from P r falls within the support of P g .</p><p>Meanwhile, insufficient coverage of the underlying manifold continues to be a challenge for GANs. Various improvements to network architectures and training procedures tackle this issue directly <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. While metrics have been proposed to estimate the degree of variation, these have not seen widespread use as they are subjective <ref type="bibr" target="#b0">[1]</ref>, domain specific <ref type="bibr" target="#b19">[20]</ref>, or not reliable enough <ref type="bibr" target="#b23">[24]</ref>.</p><p>Recently, Sajjadi et al. <ref type="bibr" target="#b24">[25]</ref> proposed a novel metric that expresses the quality of the generated samples using two separate components: precision and recall. Informally, these correspond to the average sample quality and the coverage of the sample distribution, respectively. We discuss their metric (Section 1.1) and characterize its weaknesses that we later demonstrate experimentally. Our primary contribution is an improved precision and recall metric (Section 2) which provides explicit visibility of the tradeoff between sample quality and variety. Source code of our metric is available at https://github.com/kynkaat/improved-precision-and-recall-metric.</p><p>We demonstrate the effectiveness of our metric using two recent generative models (Section 3), StyleGAN <ref type="bibr" target="#b12">[13]</ref> and BigGAN <ref type="bibr" target="#b3">[4]</ref>. We then use our metric to analyze several variants of StyleGAN (Section 4) to better understand the design decisions that determine result quality, and identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods (Appendix C) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>. Finally, we extend our metric to estimate the quality of individual generated samples (Section 5), offering a way to measure the quality of latent space interpolations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>Sajjadi et al. <ref type="bibr" target="#b24">[25]</ref> introduce the classic concepts of precision and recall to the study of generative models, motivated by the observation that FID and related density metrics cannot be used for making conclusions about precision and recall: a low FID may indicate high precision (realistic images), high recall (large amount of variation), or anything in between. We share this motivation.</p><p>From the classic viewpoint, precision denotes the fraction of generated images that are realistic, and recall measures the fraction of the training data manifold covered by the generator <ref type="figure">(Figure 1</ref>). Both are computed as expectations of binary set membership over a distribution, i.e., by measuring how likely is it that an image drawn from one distribution is classified as falling under the support of the other distribution. In contrast, Sajjadi et al. <ref type="bibr" target="#b24">[25]</ref> formulate precision and recall through the relative probability densities of the two distributions. The choice of modeling the relative densities comes from an ambiguity, i.e., should the differences between the two distributions be attributed to the generator covering the real distribution inadequately or is the generator producing samples that are unrealistic. The authors resolve this ambiguity by modeling a continuum of precision/recall values where the extrema correspond to the classic definitions. In addition to raising the question of which value to use, their practical algorithm cannot reliably estimate the extrema due to its reliance on relative densities: it cannot, for instance, correctly interpret situations where large numbers of samples are packed together, e.g., as a result of mode collapse or truncation. The k-nearest neighbors based two-sample test by Lopez-Paz et. al. <ref type="bibr" target="#b16">[17]</ref> suffers from the same problem. Parallel with our work, Simon et al. <ref type="bibr" target="#b26">[27]</ref> extend Sajjadi's formulation to arbitrary probability distributions and provide a practical algorithm that estimates precision and recall by training a post hoc classifier. We argue that the classic definition of precision and recall is sufficient for disentangling the effects of sample quality and manifold coverage. This can be partially justified by observing that precision and recall correspond to the vertical and horizontal extremal cases in Lin et al.'s <ref type="bibr" target="#b15">[16]</ref> theoretically founded analysis of mode collapse regions. In order to approximate these quantities directly, we construct adaptive-resolution finite approximations to the real and generated manifolds that are able to answer binary membership queries: "does sample x lie in the support of distribution P ?". Together with existing density-based metrics, such as FID, our precision and recall scores paint a highly informative picture of the distributions produced by generative image models. In particular, they make effects in the "null space" of FID clearly visible.</p><p>2 Improved precision and recall metric using k-nearest neighbors</p><p>We will now describe our improved precision and recall metric that does not suffer from the weaknesses listed in Section 1.1. The key idea is to form explicit non-parametric representations of the manifolds of real and generated data, from which precision and recall can be estimated.</p><p>Similar to Sajjadi et al. <ref type="bibr" target="#b24">[25]</ref>, we draw real and generated samples from X r ∼ P r and X g ∼ P g , respectively, and embed them into a high-dimensional feature space using a pre-trained classifier network. We denote feature vectors of the real and generated images by φ r and φ g , respectively, and the corresponding sets of feature vectors by Φ r and Φ g . We take an equal number of samples from each distribution, i.e., |Φ r | = |Φ g |.</p><p>For each set of feature vectors Φ ∈ {Φ r , Φ g }, we estimate the corresponding manifold in the feature space as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We obtain the estimate by calculating pairwise Euclidean distances between all feature vectors in the set and, for each feature vector, forming a hypersphere with radius equal to the distance to its kth nearest neighbor. Together, these hyperspheres define a volume in the feature space that serves as an estimate of the true manifold. To determine whether a given sample φ is located within this volume, we define a binary function</p><formula xml:id="formula_0">f (φ, Φ) = 1, if φ − φ 2 ≤ φ − NN k φ , Φ 2 for at least one φ ∈ Φ 0, otherwise,<label>(1)</label></formula><p>where NN k φ , Φ returns kth nearest feature vector of φ from set Φ. In essence, f (φ, Φ r ) provides a way to determine whether a given image looks realistic, whereas f (φ, Φ g ) provides a way to determine whether it could be reproduced by the generator. We can now define our metric as</p><formula xml:id="formula_1">precision(Φ r , Φ g ) = 1 |Φ g | φ g ∈Φg f (φ g , Φ r ) recall(Φ r , Φ g ) = 1 |Φ r | φ r ∈Φr f (φ r , Φ g ) (2)</formula><p>In Equation (2), precision is quantified by querying for each generated image whether the image is within the estimated manifold of real images. Symmetrically, recall is calculated by querying for each real image whether the image is within estimated manifold of generated images. See Appendix A for pseudocode.</p><p>In practice, we compute the feature vector φ for a given image by feeding it to a pre-trained VGG-16 classifier <ref type="bibr" target="#b27">[28]</ref> and extracting the corresponding activation vector after the second fully connected layer. Brock et al. <ref type="bibr" target="#b3">[4]</ref> show that the nearest neighbors in this feature space are meaningful in the sense that they correspond to semantically similar images. Meanwhile, Zhang et al. <ref type="bibr" target="#b32">[33]</ref>   show to correlates well with human judgment for image corruptions. We have tested both approaches and found that feature space, used by Brock at al., works considerably better for the purposes of our metric, presumably because it places less emphasis on the exact spatial arrangement -sparsely sampled manifolds rarely include near-exact matches in terms of spatial structure.</p><p>Like FID, our metric is weakly affected by the number of samples taken ( <ref type="figure" target="#fig_2">Figure 3a</ref>). Since it is standard practice to quote FIDs with 50k samples, we adopt the same design point for our metric as well. The size of the neighborhood, k, is a compromise between covering the entire manifold (large values) and overestimating its volume as little as possible (small values). In practice, we have found that higher values of k increase the precision and recall estimates in a fairly consistent fashion, and lower values of k decrease them, until they start saturating at 1.0 or 0.0 ( <ref type="figure" target="#fig_2">Figure 3b</ref>). Tests with various datasets and GANs showed that k = 3 is a robust choice that avoids saturating the values most of the time. Thus we use k = 3 and |Φ| = 50000 in all our experiments unless stated otherwise. <ref type="figure" target="#fig_2">Figure 3c</ref> further shows that the qualitative behavior of our metric is not limited to VGG-16 -which we use in all tests -as Inception-v3 features lead to very similar results. See Appendix B for results using synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Precision and recall of state-of-the-art generative models</head><p>In this section, we demonstrate that precision and recall computed using our method correlate well with the perceived quality and variation of generated distributions, and compare our metric with Sajjadi et al.'s method <ref type="bibr" target="#b24">[25]</ref> as well as the widely used FID metric <ref type="bibr" target="#b9">[10]</ref>. For Sajjadi et al.'s method, we use 20 clusters and report F 1/8 and F 8 as proxies for precision and recall, respectively, as recommended by the authors. We examine two state-of-the-art generative models, StyleGAN <ref type="bibr" target="#b12">[13]</ref> trained with the FFHQ dataset, and BigGAN [4] trained on ImageNet <ref type="bibr" target="#b4">[5]</ref>.</p><p>StyleGAN <ref type="figure">Figure 4</ref> shows the results of various metrics in four StyleGAN setups. These setups exhibit different amounts of truncation and training time, and have been selected to illustrate how the metrics behave with varying output image distributions. Setup A is heavily truncated, and the generated images are of high quality but very similar to each other in terms of color, pose, background, etc. This leads to high precision and low recall, as one would expect. Moving to setup B increases variation, which improves recall, while the image quality and thus precision is somewhat compromised. Setup C is the FID-optimized configuration in <ref type="bibr" target="#b12">[13]</ref>. It has even more variation in terms of color schemes and accessories such as hats and sunglasses, further improving recall. However, some of the faces start to become distorted which reduces precision. Finally, setup D preserves variation and recall, but nearly all of the generated images have low quality, indicated by much lower precision as expected.</p><p>In contrast, the method of Sajjadi et al. <ref type="bibr" target="#b24">[25]</ref> indicates that setups B, C and D are all essentially perfect, and incorrectly assigns setup A the lowest precision. Looking at FID, setups B and D appear almost equally good, illustrating how much weight FID places on variation compared to image quality, also evidenced by the high FID of setup A. Setup C is ranked as clearly the best by FID despite the obvious image artifacts. The ideal tradeoff between quality and variation depends on the intended application, but it is unclear which application might favor setup D where practically all images are broken over setup B that produces high-quality samples at a lower variation. Our metric provides explicit visibility on this tradeoff and allows quantifying the suitability of a given model for a particular application. <ref type="figure">Figure 5</ref> applies gradually stronger truncation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> on precision and recall using a single StyleGAN generator. Our method again works as expected, while the method of Sajjadi et al. does not. We hypothesize that their difficulties are a result of truncation packing a large number of generated images into a small region in the embedding space. This may result in clusters that contain no real images in that region, and ultimately causes the metric to incorrectly report low precision. The tendency to underestimate precision can be alleviated by using fewer clusters, but doing so leads to overestimation of recall. Our metric does not suffer from this problem because the manifolds of real and generated images are estimated separately, and the distributions are never mixed together.</p><p>BigGAN Brock et al. recently presented BigGAN <ref type="bibr" target="#b3">[4]</ref>, a high-quality generative network able to synthesize images for ImageNet <ref type="bibr" target="#b4">[5]</ref>. ImageNet is a diverse dataset containing 1000 classes with ∼1300 training images for each class. Due to the large amount of variation within and between classes, generative modeling of ImageNet has proven to be a challenging problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b3">4]</ref>. Brock et al. <ref type="bibr" target="#b3">[4]</ref> list several ImageNet classes that are particularly easy or difficult for their method. The difficult classes often contain precise global structure or unaligned human faces, or they are underrepresented in the dataset. The easy classes are largely textural, lack exact global structure, and are common in the dataset. Dogs are a noteworthy special case in ImageNet: with almost a hundred different dog breeds listed as separate classes, there is much more training data for dogs than for any other class, making them artificially easy. To a lesser extent, the same applies to cats that occupy ∼10 classes. <ref type="figure" target="#fig_4">Figure 6</ref> illustrates the precision and recall for some of these classes over a range of truncation values. We notice that precision is invariably high for the suspected easy classes, including cats and dogs, and clearly lower for the difficult ones. Brock et al. state that the quality of generated samples increases as more truncation is applied, and the precision as reported by our method is in line with this observation. Recall paints a more detailed picture. It is very low for classes such as "Lemon" or "Broccoli", implying much of the variation has been missed, but FID is nevertheless quite good for both. Since FID corresponds to a Wasserstein-2 distance in the feature space, low intrinsic variation implies low FID even when much of that variation is missed. Correspondingly, recall is clearly higher for the difficult classes. Based on visual inspection, these classes have a lot of intra-class variation that BigGAN training has successfully modeled. Dogs and cats show recall similar to the difficult classes, and their image quality and thus precision is likely boosted by the additional training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Using precision and recall to analyze and improve StyleGAN</head><p>Generative models have seen rapid improvements recently, and FID has risen as the de facto standard for determining whether a proposed technique is considered beneficial or not. However, as we have shown in Section 3, relying on FID alone may hide important qualitative differences in the results and it may inadvertently favor a particular tradeoff between precision and recall that is not necessarily aligned with the actual goals. In this section, we use our metric to shed light onto some of the design decisions associated with the model itself. Appendix C performs a similar, principled analysis for truncation methods. We use StyleGAN <ref type="bibr" target="#b12">[13]</ref> in all experiments, trained with FFHQ at 1024 × 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network architectures and training configurations</head><p>To avoid drawing false conclusions when comparing different training runs, we must properly account for the stochastic nature of the training process. For example, we have observed that FID can often vary by up to ±14% between consecutive training iterations with StyleGAN. The common approach is to amortize this variation by taking multiple snapshots of the model at regular intervals and selecting the best one for further analysis <ref type="bibr" target="#b12">[13]</ref>. With our metric, however, we are faced with the problem of multiobjective optimization <ref type="bibr" target="#b2">[3]</ref>: the snapshots represent a wide range of different tradeoffs between precision and recall, as illustrated in <ref type="figure" target="#fig_5">Figure 7a</ref>. To avoid making assumptions about the desired tradeoff, we identify the Pareto frontier, i.e., the minimal subset of snapshots that is guaranteed to contain the optimal choice for any given tradeoff.   <ref type="figure" target="#fig_5">Figure 7b</ref> shows the Pareto frontiers for several variants of StyleGAN. The baseline configuration (A) has a dedicated minibatch standard deviation layer that aims to increase variation in the generated images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>. Using our metric, we can confirm that this is indeed the case: removing the layer shifts the tradeoff considerably in favor of precision over recall (B). We observe that R 1 regularization <ref type="bibr" target="#b18">[19]</ref> has a similar effect: reducing the γ parameter by 100× shifts the balance even further (C). Karras et al. <ref type="bibr" target="#b11">[12]</ref> argue that their progressive growing technique improves both quality and variation, and indeed, disabling it reduces both aspects (D). Moreover, we see that randomly translating the inputs of the discriminator by −16 . . . 16 pixels improves precision (E), whereas disabling instance normalization in the AdaIN operation <ref type="bibr" target="#b10">[11]</ref>, unexpectedly, improves recall (F). <ref type="figure" target="#fig_5">Figure 7c</ref> shows the best FID obtained for each configuration; the corresponding snapshots are highlighted in <ref type="figure" target="#fig_5">Figure 7a</ref>,b. We see that FID favors configurations with high recall (A, F) over the ones with high precision (B, C), and the same is also true for the individual snapshots. The best configuration in terms of recall (F) yields a new state-of-the-art FID for this dataset. Random translation (E) is an exceptional case: it improves precision at the cost of recall, similar to (B), but also manages to slightly improve FID at the same time. We leave an in-depth study of these effects for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Estimating the quality of individual samples</head><p>While our precision metric provides a way to assess the overall quality of a population of generated images, it yields only a binary result for an individual sample and therefore is not suitable for ranking images by their quality. Here, we present an extension of the classification function f <ref type="figure">(Equation 1</ref>) that provides a continuous estimate of how close a given sample is to the manifold of real images.</p><p>We define a realism score R that increases the closer an image is to the manifold and decreases the further an image is from the manifold. Let φ g be a feature vector of a generated image and φ r a feature vector of a real image from set Φ r . Realism score of φ g is calculated as</p><formula xml:id="formula_2">R(φ g , Φ r ) = max φ r φ r − NN k (φ r , Φ r ) 2 φ g − φ r 2 .<label>(3)</label></formula><p>This is a continuous extension of f (φ g , Φ r ) with the simple relation that f (φ g , Φ r ) = 1 iff R(φ g , Φ r ) ≥ 1. In other words, when R ≥ 1, the feature vector φ g is inside the (k-NN induced) hypersphere of at least one φ r .</p><p>With any finite training set, the k-NN hyperspheres become larger in regions where the training samples are sparse, i.e., regions with low representation. When measuring the quality of a large population of generated images, these underrepresented regions have little impact as it is unlikely that too many generated samples land there -even though the hyperspheres may be large, they are sparsely located and cover a small volume of space in total. However, when computing the realism score for a single image, a sample that happens to land in such a fringe hypersphere may obtain a wildly inaccurate score. Large errors, even if they are rare, would undermine the usefulness of the metric. We tackle this problem by discarding half of the hyperspheres with the largest radii. In other words, the maximum in Equation 3 is not taken over all φ r ∈ Φ r but only over those φ r whose associated hypersphere is smaller than the median. This pruning yields an overconservative estimate of the real manifold, but it leads to more consistent realism scores. Note that we use this approach only with R, not with f . <ref type="figure" target="#fig_6">Figure 8</ref> shows example images from BigGAN with high and low realism. In general, the samples with high realism display a clear object from the given class, whereas the object is often distorted to unrecognizable for the low realism images. Appendix D provides more examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quality of interpolations</head><p>An interesting application for the realism score is to evaluate the quality of interpolations. We do this with StyleGAN using linear interpolation in the intermediate latent space W as suggested by Karras et al. <ref type="bibr" target="#b12">[13]</ref>. <ref type="figure" target="#fig_7">Figure 9</ref> shows four example interpolation paths with randomly sampled latent vectors as endpoints. Paths A appears to be located completely inside the real manifold, path D completely outside it, and paths B and C have one endpoint inside the real manifold and one outside it. The realism scores assigned to paths A-D correlate well with the perceived image quality: Images with low scores contain multiple artifacts and can be judged to be outside the real manifold, and vice versa for high-scoring images. See Appendix D for additional examples.</p><p>We can use interpolations to investigate the shape of the subset of W that produces realistic-looking images. In this experiment, we sampled without truncation 1M latent vectors in W for which R ≥ 1, giving rise to 500k interpolation paths with both endpoints on the real manifold. It would be unrealistic to expect all intermediate images on these paths to also have R ≥ 1, so we chose to consider an interpolation path where more than 25% of the intermediate images have R &lt; 0.9 as straying too far from the real manifold. Somewhat surprisingly, we found that only 2.4% of the paths crossed unrealistic parts of W under this definition, suggesting that the subset of W on the real manifold is highly convex. We see potential in using the realism score for measuring the shape of this region in W with greater accuracy, possibly allowing the exclusion of unrealistic images in a more refined manner than with truncation-like methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have demonstrated through several experiments that the separate assessment of precision and recall can reveal interesting insights about generative models and can help to improve them further. We believe that the separate quantification of precision can also be useful in the context of image-to-image translation <ref type="bibr" target="#b33">[34]</ref>, where the quality of individual images is of great interest.</p><p>Using our metric, we have identified previously unknown training configuration-related effects in Section 4.1, raising the question whether truncation is really necessary if similar tradeoffs can be achieved by modifying the training configuration appropriately. We leave the in-depth study of these effects for future work.</p><p>Finally, it has recently emerged that density models can be incapable of assessing whether a given example belongs to the training distribution <ref type="bibr" target="#b22">[23]</ref>. By explicitly modeling the real manifold, our metrics may provide an alternative way for estimating this.  <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref> and the generated data is expanded, one mode at a time, to cover the real modes (1-5) and five extraneous modes <ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref>. Both metrics were evaluated using 20k real and generated samples. (b) Results from our metric with k = 3. (c) Results from the method of Sajjadi et al. <ref type="bibr" target="#b24">[25]</ref>.</p><p>Algorithm 1 k-NN precision and recall pseudocode.</p><p>Input: Set of real and generated images (X r , X g ), feature network F, neighborhood size k.</p><formula xml:id="formula_3">1: function PRECISION-RECALL(X r , X g , F, k) 2: Φ r ← F (X r ) 3: Φ g ← F (X g ) 4: precision ← MANIFOLD-ESTIMATE(Φ r , Φ g , k) 5: recall ← MANIFOLD-ESTIMATE(Φ g , Φ r , k) 6:</formula><p>return precision, recall</p><formula xml:id="formula_4">7: function MANIFOLD-ESTIMATE(Φ a , Φ b , k) 8:</formula><p>Approximate manifold of Φ a . <ref type="bibr">9:</ref> for φ ∈ Φ a do 10:</p><formula xml:id="formula_5">d ← φ − φ 2 for all φ ∈ Φ a</formula><p>Pairwise distances to all points in Φ a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>r φ ← min k+1 (d) (k + 1)-th smallest value to exclude φ itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Compute how many points from Φ b are within the approximated manifold of Φ a . <ref type="bibr">13:</ref> n ← 0 <ref type="bibr">14:</ref> for φ ∈ Φ b do <ref type="bibr">15:</ref> if φ − φ 2 ≤ r φ for any φ ∈ Φ a then <ref type="bibr">16:</ref> n ← n + 1 <ref type="bibr">17:</ref> return n/|Φ b |</p><p>We use NVIDIA Tesla V100 GPU to run our implementation. A high-quality estimate using 50k images in both X r and X g takes ∼ 8 minutes to run on a single GPU. For comparison, evaluating FID using the same data takes ∼ 4 minutes and generating 50k images (1024 × 1024) with StyleGAN using one GPU takes ∼14 minutes. Our implementation can be found at https://github.com/ kynkaat/improved-precision-and-recall-metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Precision and recall with synthetic dataset</head><p>In <ref type="figure" target="#fig_8">Figure 10</ref> we replicate the mode dropping and invention experiment from <ref type="bibr" target="#b24">[25]</ref>, albeit with a 10class 2D Gaussian mixture model instead of CIFAR-10 images. As in <ref type="bibr" target="#b24">[25]</ref>, the real data covers five modes, and we measure precision and recall when 1-10 of the modes are covered by a hypothetical generator that draws samples from the corresponding Gaussian distributions. In <ref type="figure" target="#fig_8">Figure 10b</ref> we see that our method yields the correct values for precision and recall in all cases: when not all modes are being generated, precision is perfect and recall measures the fraction of modes covered, and when extraneous modes are generated, recall remains perfect while precision measures the fraction of real vs. generated modes. <ref type="figure" target="#fig_8">Figure 10c</ref> illustrates that the method of Sajjadi et al. <ref type="bibr" target="#b24">[25]</ref> performs similarly except for artifacts from k-means clustering.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis of truncation methods</head><p>Many generative methods employ some sort of truncation trick <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> to allow trading variation for quality after the training, which is highly desirable when, e.g., showcasing uncurated results. However, quantitative evaluation of these tricks has proven difficult, and they are largely seen as an ad-hoc way to fine-tune the perceived quality for illustrative purposes. Using our metric, we can study these effects in a principled way.</p><p>StyleGAN is well suited for comparing different truncation strategies because it has an intermediate latent space W in addition to the input latent space Z. We evaluate four primary strategies illustrated in <ref type="figure" target="#fig_9">Figure 11a</ref>: A) generating random latent vectors in W via the mapping network <ref type="bibr" target="#b12">[13]</ref> and rejecting ones that are too far from their mean with respect to a fixed threshold, B) approximating the distribution of latent vectors with a multivariate Gaussian and rejecting the ones that correspond to a low probability density, C) clamping low-density latent vectors to the boundary of a higher-density region by finding their closest points on the corresponding hyperellipsoid <ref type="bibr" target="#b6">[7]</ref>, and D) interpolating all latent vectors linearly toward the mean <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref>. We also consider three secondary strategies: E) interpolating the latent vectors in Z instead of W, F) truncating the latent vector distribution in Z along the coordinate axes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref>, and G) replacing a random subset of latent vectors with the mean of the distribution. As suggested by Karras et al. <ref type="bibr" target="#b12">[13]</ref>, we also tried applying truncation to only some of the layers, but this did not have a meaningful impact on the results. <ref type="figure" target="#fig_9">Figure 11b</ref> shows the precision and recall of each strategy for different amounts of truncation. Strategies that operate in Z yield a clearly inferior tradeoff (E, F), confirming that the sampling density in Z is not a good predictor of image quality. Rejecting latent vectors by density (B) is superior to rejecting them by distance (A), corroborating the Gaussian approximation as a viable proxy for image quality. Clamping outliers (C) is considerably better than rejecting them, because it provides better coverage around the extremes of the distribution. Interpolation (D) appears very competitive with clamping, even though it ought to perform no better than rejection in terms of covering the extremes. The important difference, however, is that it affects all latent vectors equallyunlike the other strategies (A-C) that are only concerned with the outliers. As a result, it effectively increases the average density of the latent vectors, countering the reduced recall by artificially inflating precision. Random replacement (G) takes this to the extreme: removing a random subset of the latent vectors does not reduce the support of the distribution but inserting them back at the highest-density point increases the average quality. <ref type="bibr" target="#b1">2</ref> Our findings highlight that recall alone is not enough to judge the quality of the distribution -it only measures the extent. To illustrate the difference, we replace recall with FID in <ref type="figure" target="#fig_9">Figure 11c</ref>. Our other observations remain largely unchanged, but interpolation and random replacement (D, G) become considerably less desirable as we account for the differences in probability density. Clamping (C) becomes a clear winner in this comparison, because it effectively minimizes the Wasserstein-2 distance between the truncated distribution and the original one in W. We have inspected the generated images visually and confirmed that clamping appears to generally yield the best tradeoff. <ref type="figure" target="#fig_1">Figure 12</ref> shows BigGAN-generated images for which the estimated realism score is very high or very low. Images with high realism score contain a clear object from the given class, whereas low-scoring images generally lack such object or the object is distorted in various ways. High and low quality images for each class were obtained from 1k generated samples. <ref type="figure" target="#fig_2">Figure 13</ref> demonstrates StyleGAN-generated images that have very high or very low realism score. Some variation in backgrounds, accessories, etc. is lost in high quality samples. We hypothesize that the generator could not realistically recreate these features, and thus they are not observed in high quality samples, whereas low quality samples often contain hats, microphones, occlusions, and varying backgrounds that are challenging for the generator to model. High and low quality images were obtained from 1k generated samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Quality of samples and interpolations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) An example manifold in a feature space. (b) Estimate of the manifold obtained by sampling a set of points and surrounding each with a hypersphere that reaches its kth nearest neighbor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIDFigure 3 :</head><label>3</label><figDesc>Varying |Φ|, VGG-16 (b) Varying k, VGG-16 (c)Varying k, Inception-v3 (a) Our metric behaves similarly to FID in terms of varying sample count. (b) Precision (blue) and recall (orange) for several neighborhood sizes k. Larger k increases both numbers. Here a trained model (ψ = 1) was expanded to a family of models by artificially limiting the variation in the results. We would expect the precision and recall to reach 1.0 and 0.0, respectively, when ψ → 0. (c) Using Inception-v3 features instead of VGG-16 yields a substantially similar result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>AFigure 4 :Figure 5 :</head><label>45</label><figDesc>(FID = 91.<ref type="bibr" target="#b6">7)</ref> B (FID = 16.<ref type="bibr" target="#b8">9)</ref> C (FID = 4.5) D (FID = 16.7) Comparison of our method (black dots), Sajjadi et al.'s method<ref type="bibr" target="#b24">[25]</ref> (red triangles), and FID for 4 StyleGAN setups. We recommend zooming in to better assess the quality of images. Our method (c) Sajjadi et al.<ref type="bibr" target="#b24">[25]</ref> (a) Example images produced by StyleGAN<ref type="bibr" target="#b12">[13]</ref> trained using the FFHQ dataset. It is generally agreed<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> that truncation provides a tradeoff between perceptual quality and variation. (b) With our method, the maximally truncated setup (ψ = 0) has zero recall but high precision. As truncation is gradually removed, precision drops and recall increases as expected. The final recall value approximates the fraction of training set the generator can reproduce (generally well below 100%). (c) The method of Sajjadi et al. reports both precision and recall increasing as truncation is removed, contrary to the expected behavior, and the final numerical values of both precision and recall seem excessively high.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Our precision and recall for four easy (a) and four difficult (b) ImageNet classes using BigGAN. For each class we sweep the truncation parameter ψ linearly from 0.3 to 1.0, left-to-right. The FIDs refer to a non-truncated model, i.e., ψ = 1.0. The per-class metrics were computed using all available training images of the class and an equal number of generated images, while the curve for the entire dataset was computed using 50k real and generated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a) Precision and recall for different snapshots of StyleGAN taken during the training, along with their corresponding Pareto frontier. We use the standard training configuration by Karras et al. [13] with FFHQ and ψ = 1. (b) Different training configurations lead to vastly different tradeoffs between precision and recall. (c) Best FID obtained for each configuration (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Quality of individual samples of BigGAN from eight classes. Top: Images with high realism.Bottom: Images with low realism. We show two images with the highest and lowest realism score selected from 1000 non-truncated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Realism score for four interpolation paths as function of linear interpolation parameter t and corresponding images from paths A-D. We did not use truncation when generating the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>(a) Real data covers five modes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>(a) Our primary truncation strategies avoid sampling the extremal regions of StyleGAN's intermediate latent space. (b) Precision and recall for different amounts of truncation with FFHQ. (c) Using FID instead of recall to measure distribution quality. Note that the x-axis is flipped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 Figure 12 :</head><label>1412</label><figDesc>presents further examples of high and low quality interpolations. High-quality interpolations consist of images with high perceptual quality and coherent background despite the endpoints being potentially quite different from each other. On the contrary, low-quality interpolations are usually significantly distorted and contain incoherent patterns in the image background. Examples of (a) high and (b) low quality BigGAN samples according to our realism scores.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Interestingly, random replacement (G) actually leads to a slight increase in recall. This is an artifact of our k-NN manifold approximation, which becomes increasingly conservative as the density of samples decreases.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We thank David Luebke for helpful comments; Janne Hellsten, and Tero Kuosmanen for compute infrastructure.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Pseudocode and implementation details</head><p>Algorithm 1 shows the pseudocode for our method. The main function PRECISION-RECALL evaluates precision and recall for given sets of real and generated images, X r and X g , by embedding them in a feature space defined by F (lines 2-3) and estimating the corresponding manifolds using MANIFOLD-ESTIMATE (lines 4-6). The helper function MANIFOLD-ESTIMATE takes two sets of feature vectors Φ a , Φ b as inputs. It forms an estimate for the manifold of Φ a and counts how many points from Φ b are located within the manifold. Estimating the manifold requires computing the pairwise distances between all feature vectors φ ∈ Φ a and, for each φ, tabulating the distance to its k-th nearest neighbor (lines 9-11). These distances are then used to determine the fraction of feature vectors φ ∈ Φ b that are located within the manifold (lines <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Note that in the pseudocode feature vectors φ are processed one by one on lines 9 and 14 but in a practical implementation they can be processed in mini-batches to improve efficiency.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do GANs actually learn the distribution? An empirical study. CoRR, abs/1706.08224</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1801.01401</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiobjective optimization: Interactive and evolutionary approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miettinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Slowiński</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">5252</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Density estimation using Real NVP. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distance from a point to an ellipse, an ellipsoid, or a hyperellipsoid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eberly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geometric Tools, LLC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Networks. In NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flow-GAN: Combining maximum likelihood and adversarial learning in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1703.06868</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>abs/1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<idno>abs/1807.03039</idno>
		<title level="m">Generative flow with invertible 1x1 convolutions. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PacGAN: The power of two samples in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<idno>abs/1712.04086</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting classifier two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Megapixel size image creation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchesi</surname></persName>
		</author>
		<idno>abs/1706.00082</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Which training methods for GANs do actually converge? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>abs/1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>abs/1611.02163</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno>abs/1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno>abs/1802.05637</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>abs/1806.00035</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Revisiting precision and recall definition for generative model evaluation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1606.05328</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno>abs/1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRR, abs/1703.10593, 2017. (a) High-quality samples (b) Low-quality samples</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High (a) and low quality (b) StyleGAN samples according to our realism scores. (a) High-quality interpolations (b) Low-quality interpolations</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Examples of (a) high and (b) low quality interpolations according to our realism scores</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
