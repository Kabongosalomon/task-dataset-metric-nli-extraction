<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of Deep Convolutional Generative Adversarial Networks for data augmentation of chest X-ray images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-03">September 3, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Preprint</forename><surname>Sagar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Analytics and Data Science</orgName>
								<orgName type="institution">Harrisburg University of Science and Technology Harrisburg</orgName>
								<address>
									<postCode>17101</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kora</forename><surname>Venu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Analytics and Data Science</orgName>
								<orgName type="institution">Harrisburg University of Science and Technology Harrisburg</orgName>
								<address>
									<postCode>17101</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of Deep Convolutional Generative Adversarial Networks for data augmentation of chest X-ray images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-03">September 3, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>K eywords DCGAN · Chest X-ray · Medical Imaging · Fréchet Distance of Inception</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image datasets are usually imbalanced, due to high costs of obtaining the data and time-consuming annotations. Training deep neural network model on such datasets to accurately classify the medical condition does not yield desired results and often over-fits the data on majority class samples. In order to address this issue, data augmentation is often performed on training data by position augmentation techniques such as scaling, cropping, flipping, padding, rotation, translation, affine transformation, and color augmentation techniques such as brightness , contrast, saturation, and hue to increase the dataset sizes. These augmentation techniques are not guaranteed to be advantageous in domains with limited data, especially medical image data, and could lead to further overfitting. In this work, we performed data augmentation on Chest X-rays dataset through generative modeling (deep convolutional generative adversarial network) which creates artificial instances retaining similar characteristics to the original data and evaluation of the model resulted in Fréchet Distance of Inception (FID) of 1.289.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Data sets for medical imaging are limited in size due to privacy issues and getting annotation of medical images is expensive and time-consuming, which often leads to having only small amounts of labeled medical imaging data to use for image classification tasks. Deep learning techniques need a huge volume of data to train effective models for tasks such as image recognition/ classification. Data augmentation is a technique commonly used in deep learning to expand data and prevent over-fitting in such data-limited situations. In this work, we investigate the use of Deep Convolutional Generative Adversarial Networks for generating chest X-ray images to augment the original dataset 1 . Generative Adversarial Networks (GAN's) were introduced by Ian Goodfellow and his colleagues in 2014 <ref type="bibr" target="#b0">[1]</ref>. GAN's utilize two neural networks, a generator which takes random noise as input to create samples (data) as realistic as possible to the original dataset and a discriminator to distinguish between data that is real (original data) vs fake (generated data) as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The DCGAN proposed by <ref type="bibr" target="#b1">[2]</ref> is direct extension of the original GAN <ref type="bibr" target="#b0">[1]</ref>, except that the discriminator and generator explicitly uses convolutional and convolutional-transpose layers, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head><p>We used X-ray images data obtained by <ref type="bibr" target="#b2">[3]</ref> in the experiment. The dataset was already organized into three folders (train, test, val) and each folder contained sub-folders for each image category (Normal/ Pneumonia) with 5216 X-ray images in the train folder (1341 images are labeled with Normal and 3875 images labeled with Pneumonia), 16 X-ray images in val folder and 624 X-ray images in test folder. Its obvious that the data in the train folder is imbalanced and training a neural network to classify the data among two categories will over-fits the data . So, in this experiment, we augment the Normal X-ray images by Deep Convolutional Generative Adversarial Networks with the architecture as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The images were resized to 128x128 pixels due to GPU memory constraints and then the images are scaled to a [-1,1] pixel value range to match the output of the generator as it uses Tanh activation function. In this architecture, a 100x1 noise vector is fed as an input to the generator. There are then four Convolutional layers with 2D-upsampling layers applied with Leaky ReLu activation function interlaced in between to scale to the appropriate 128x128 image size. The discriminator network is a similar network with four convolutional layers and a stride of 2 with leaky ReLU as an activation function except for the final node which is a sigmoid activation function to output if the image is real (original data) or fake (generated data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN Objective Function</head><p>Let z be the latent space vector sampled from a standard normal distribution, G(z) represents the generator function which maps the latent vector z to data-space, x be the data representing an image, and D(x) is the discriminator network which outputs the probability that x came from the training data (real) rather than the generated data (fake) from the generator distribution p z (z). The goal of G is to estimate the distribution that the training data came from p data so it can generate fake samples from that estimated distribution p g <ref type="bibr" target="#b0">[1]</ref>.</p><p>The learning process of the GANs is to train a discriminator and a generator simultaneously, which is a mini-max game between discriminator and generator where, the discriminator tries to maximize the loss function, i.e., D tries to maximize the probability that it correctly classifies real images and fake images (log D(x)) and the generator tries to minimize the loss function, i.e., G tries to minimize the probability that D will predict its outputs are fake (log(1 − D(G(z)))) as shown in the equation <ref type="formula" target="#formula_0">(1)</ref>.</p><formula xml:id="formula_0">min G max D V GAN (D, G) = E x∼p data (x) [log D(x)] + E z∼p z (z) [log(1 − D(G(z)))].<label>(1)</label></formula><p>Theoretically, the solution to this mini-max game is where p g = p data , such that the discriminator guesses randomly if the inputs are real images (training data) or fake images (generated data). However, GANs' theory of convergence is still being actively studied, and in fact models have not always been trained to this extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The DCGAN was trained for 500 epochs and in just around 50 epochs, the DCGAN was able to generate images that resembled the chest X-ray images and then the quality of generated images further improved over 500 epochs. For comparison, we show a grid of Real images (original data) and the fake images (generated The loss and accuracy of the generator and discriminator during training is shown in <ref type="figure" target="#fig_4">Figure 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>Deep learning models are usually trained with a loss function until neural network convergence. Since GAN's are trained with two neural networks simultaneously to reach a Nash Equilibrium, there is no objective loss function to train GAN generator models to objectively access the training progress and quality of the model from the loss of the discriminator network and/ or the loss of the generator network <ref type="bibr" target="#b3">[4]</ref>. In general, to access the quality of the generated images based on the performance of the GAN models, two techniques have been developed: 1. Quantitative Measures such as Average Log-likelihood, Inception Score (IS) <ref type="bibr" target="#b3">[4]</ref>, Fréchet Inception Distance (FID) <ref type="bibr" target="#b4">[5]</ref>, Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b5">[6]</ref> etc. and 2. Qualitative Measures such as Nearest Neighbours, Rating and Preference Judgment, Evaluating Mode Drop and Mode Collapse <ref type="bibr" target="#b6">[7]</ref> etc. Initiation Score (IS) and Fréchet Distance of Inception (FID) are two of the GAN evaluation measures that are widely accepted <ref type="bibr" target="#b7">[8]</ref>. In this work, we evaluate the DCGAN model using Fréchet Distance of Inception (FID) measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fréchet Distance of Inception (FID)</head><p>Fréchet Distance of Inception (FID) score is a measure used to evaluate the performance of the Generative Adversarial Network based on the quality of generated images which captures the similarity of the generated images to the real images proposed by <ref type="bibr" target="#b4">[5]</ref> as an improvement to the Inception Score (IS) <ref type="bibr" target="#b3">[4]</ref>. FID score is calculated using the statistics of generated images to real images using the Fréchet distance also known as Wasserstein-2 distance between the two multivariate Gaussian's as shown in the equation (2)</p><formula xml:id="formula_1">d F ID (x, g) = µ x − µ g 2 + T r Σ x + Σ g − 2(Σ x Σ g ) 1 2<label>(2)</label></formula><p>where µ x and µ g are the feature-wise mean of real and generated images respectively, Σ x and Σ g are the covariance matrix of real and generated images repectively, T r is the trace whichis the sum of the elements along the main diagonal of the square matrix, X x ∼ N (µ x , Σ x ) and X g ∼ N (µ g , Σ g ) are the 2048-dimensional activation's of the Inception-V3 pool3 layer for real images and generated images respectively.</p><p>To calculate the Gaussian statistics (mean and covariance), the number of samples (real images and generated images respectively) should be greater than the dimension of the coding layer i.e., the samples should be greater than 2048 for the Inception-V3 pool 3 layer, otherwise the covariance is not full rank resulting in complex numbers and NAN's. Since, we had very limited samples (less than 2048) in our training dataset, we could not take advantage of the Inception-V3 pool3 layer, so we used the previous layer which is a Pre-aux classifier that is a 768-dimensional feature. We then calculated the Fréchet Distance of Inception (FID) score using [9] and the model achieved a FID score of 1.289 (lower scores correspond to better GAN performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>The contributions of Generative Adversarial Networks to the field of Medical imaging are highly appreciated, especially where there is limited access to the medical imaging data and and the high costs of obtaining the labeled data. In this study, we applied deep convolutional generative adversarial networks (DCGAN) to generate artificial instances of chest X-ray images of the under-represented class in the dataset that resemble the chest X-ray images from the original dataset and evaluated the model using Fréchet Distance of Inception (FID) achieving a score of 1.289.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forthcoming Research</head><p>• To test the visual quality of the generated X-ray images, we intend to supply the generated images to a clinician to label the images as either real or fake (generated). • Develop a deep convolutional neural network to improve the accuracy in classifying the medical condition by utilizing the generated images alongside real training images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Generative Adversarial Network Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Deep Convolutional Generative Adversarial Network Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Images from Original dataset (Real) and Images generated by the generator of DCGAN (fake) images) in the Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>DCGAN Loss and Accuracy during training</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank <ref type="bibr" target="#b2">[3]</ref> for making the datasets publicly accessible and we also thank Harrisburg University of Science and Technology for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying medical diagnoses and treatable diseases by image-based deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Goldbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Carolina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Valentim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1122" to="1131" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convergence of the empirical distribution towards the theoretical distribution. Scientific annals of the Ecole Normale Sup &apos;erieure, 3e s &apos;e rie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fortet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edith</forename><surname>Mourier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazar</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
