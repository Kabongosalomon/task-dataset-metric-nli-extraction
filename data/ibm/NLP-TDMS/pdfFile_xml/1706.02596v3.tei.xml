<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Integration of Background Knowledge in Neural NLU Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
							<email>dirk.weissenborn@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
							<email>tkocisky@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Dyer</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Integration of Background Knowledge in Neural NLU Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of freetext statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding natural language depends crucially on common-sense and background knowledge, for example, knowledge about what concepts are expressed by the words being read (lexical knowledge), and what relations hold between these concepts (relational knowledge). As a simple illustration, if an agent needs to understand that the statement "King Farouk signed his abdication" is entailed by "King Farouk was exiled to France in 1952, after signing his resignation", it must know (among other things) that abdication means resignation of a king.</p><p>In most neural natural language understanding (NLU) systems, the requisite background knowl-edge is implicitly encoded in the models' parameters. That is, what background knowledge is present has been learned from task supervision and also by pre-training word embeddings (where distributional properties correlate with certain kinds of useful background knowledge, such as semantic relatedness). However, acquisition of background knowledge from static training corpora is limiting for two reasons. First, it is unreasonable to expect that all background knowledge that could be important for solving an NLU task can be extracted from a limited amount of training data. Second, as the world changes, the facts that may influence how a text is understood will likewise change. In short: building suitably large corpora to capture all relevant information, and keeping the corpus and derived models up to date with changes to the world would be impractical.</p><p>In this paper, we develop a new architecture for dynamically incorporating external background knowledge in NLU models. Rather than relying only on static knowledge implicitly present in the training data, supplementary knowledge is retrieved from external knowledge sources (in this paper, ConceptNet and Wikipedia) to assist with understanding text inputs. Since NLU systems must already read and understand text inputs, we assume that background knowledge will likewise be provided in text form ( §2). The retrieved supplementary texts are read together with the task inputs by an initial reading module whose outputs are contextually refined word embeddings ( §3). These refined embeddings are then used as input to a task-specific NLU architecture (any architecture that reads text as a sequence of word embeddings can be used here). The initial reading module and the task module are learnt jointly, end-to-end.</p><p>We experiment with several different datasets on the tasks of document question answering (DQA) and recognizing textual entailment <ref type="bibr">(RTE)</ref> evaluating the impact of our proposed solution with both basic task architectures and a sophisticated task architecture for RTE ( §4). We find that our embedding refinement strategy is effective ( §5). On four competitive benchmarks, we show that refinement helps. First, simply refining the embeddings just using the context (and no additional background information) can improve performance significantly, but adding background knowledge helps further. Our results are competitive with the best systems, achieving a new state of the art on the recent TriviaQA benchmarks. Our success on this task is especially noteworthy because the task-specific architecture is a simple reading architecture, in particular a single layer BiLSTM with a feed-forward neural network for span prediction. Finally, we provide an analysis demonstrating that our systems are able to exploit background knowledge in a semantically appropriate manner ( §5.3). It includes, for instance, an experiment showing that our system is capable of making appropriate counterfactual inferences when provided with "alternative facts".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">External Knowledge as Supplementary Text Inputs</head><p>Knowledge resources make information that could potentially be useful for improving NLU available in a variety different formats, such as natural language text, (subject, predicate, object)triples, relational databases, and other structured formats. Rather than tailoring our solution to a particular structured representation, we assume that all supplementary information either already exists in natural language statements (e.g., encyclopedias) or can easily be recoded as natural language. Furthermore, while mapping from unstructured to structured representations is hard, the inverse problem is easy. For example, given a triple (abdication, ISA, resignation) we can construct the free-text assertion "Abdication is a resignation." using simple rules. Finally, the freetext format means that knowledge that exists only in unstructured text form such as encyclopedic knowledge (e.g., Wikipedia) is usable by our system.</p><p>An important question that remains to be answered is: given some text that is to be understood, what supplementary knowledge should be incorporated? The retrieval and preparation of contextually relevant information from knowledge sources is a complex research topic by itself, and there are several statistical <ref type="bibr" target="#b15">(Manning et al., 2008)</ref> and more recently neural approaches <ref type="bibr" target="#b19">(Mitra and Craswell, 2017)</ref> as well as approaches based on reinforcement learning <ref type="bibr" target="#b20">(Nogueira and Cho, 2017)</ref>. Rather than learning both how to incorporate relevant information and which information is relevant, we use a heuristic retrieval mechanism ( §4) and focus on the integration model.</p><p>In the next section, we turn to the question of how to leverage the retrieved supplementary knowledge (encoded as text) in a NLU system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Refining Word Embeddings by Reading</head><p>Virtually every NLU task-from document classification to translation to question answeringshould in theory be able to benefit from supplementary knowledge. While one could develop custom architectures for each task so as to read supplementary inputs, we would like ours to augment any existing NLU task architectures with the ability to read relevant information with minimal effort. To realize this goal, we adopt the strategy of refining word embeddings; that is, we replace static word embeddings with embeddings that are functions of the task inputs and any supplementary inputs. Word embeddings can be considered a simple form of key-value memory stores that, in our case, not only contain general-purpose knowledge (as in typical neural NLU systems) but also contextual information (including background knowledge). The use of word-embeddings as memory has the advantage that it is transparent to the task-architecture which kinds of embeddings (refined or unrefined) are used. Our incremental refinement process encodes input texts followed by updates on the word embedding matrix in multiple reading steps. Words are first represented non-contextually (i.e., standard word embeddings), which can be conceived of as the columns in an embedding matrix E 0 . At each progressive reading step ≥ 1, a new embedding matrix E is constructed by refining the embeddings from the previous step E −1 using (userspecified) contextual information X for reading step , which is a set of natural language sequences (i.e., texts). An illustration of our incremental refinement strategy can be found in <ref type="figure">Figure 1</ref>.</p><p>In the following, we define this procedure formally. We denote the hidden dimensionality of ... <ref type="figure">Figure 1</ref>: Illustration of our context-dependent, refinement strategy for word representations on an example from the SNLI dataset comprising the premise (X 1 = {p}), hypothesis (X 2 = {q}) and additional external information in form of free-text assertions from ConceptNet (X 1 = A). Note that for the QA task there would be another stage that additionally integrates Wikipedia abstracts of answer candidates (X 4 = W, see §4). The reading architecture constructs refinements of word representations incrementally (conceptually represented as columns in a series of embedding matrices) E are incrementally refined by reading the input text and textual renderings of relevant background knowledge before computing the representations used by the task model (in this figure, RTE). our model by n and a fully-connected layer by</p><formula xml:id="formula_0">FC(z) = Wz+b, W ∈ R n×m , b ∈ R n , z ∈ R m .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unrefined Word Embeddings (E 0 )</head><p>The first representation level consists of noncontextual word representations, that is, word representations that do not depend on any input; these can be conceived of as an embedding matrix E 0 whose columns are indexed by words in Σ * . The non-contextual word representation e 0 w for a single word w is computed by using a gated combination of fixed, pre-trained word vectors e p w ∈ R n with learned character-based embeddings e char w ∈ R n . We compute e char w using a single-layer convolutional neural network with n convolutional filters of width 5 followed by a max-pooling operation over time <ref type="bibr" target="#b27">(Seo et al., 2017;</ref><ref type="bibr" target="#b30">Weissenborn et al., 2017)</ref>. The formal definition of this combination is given in Eq. 1.</p><formula xml:id="formula_1">e p w = ReLU(FC(e p w )) g w = sigmoid FC e p w e char w e 0 w = g w e p w + (1 − g w ) e char w<label>(1)</label></formula><p>3.2 Refined Word Embeddings (E , ≥ 1)</p><p>In order to compute contextually refined word embeddings E given prior representations E −1 we assume a given set of texts X = {x 1 , x 2 , . . .} that are to be read at refinement iteration . Each text x i is a sequence of word tokens. We embed all tokens of every x i using the embedding matrix from the previous layer, E −1 . To each word, we concatenate a one-hot vector of length L with position set to 1, indicating which layer is currently being processed. 1 Stacking the vectors into a matrix, we obtain a X i ∈ R d×|x i | . This matrix is processed by a bidirectional recurrent neural network, a BiLSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref> in this work. The resulting output is further projected toX i by a fully-connected layer with ReLU activation (Eq. 2).</p><formula xml:id="formula_2">X i = ReLU(FC(BiLSTM(X i )))<label>(2)</label></formula><p>To finally update the previous embedding e −1 w of word w, we initially maxpool all representations of occurrences matching the lemma of w in every x ∈ X resulting inê w (Eq. 3). Finally, we combine the previous representation e −1 w withê w to form an updated representation e w via a gated addition. This lets the model determine how much to revise the previous embedding with the newly read information (Eq. 5).</p><formula xml:id="formula_3">e w = max x k | lemma(x k ) = lemma(w) (3) u w = sigmoid FC e −1 ŵ e w (4) e w = u w e −1 w + (1 − u w ) ê w<label>(5)</label></formula><p>Note that we soften the matching condition for w using lemmatization, 2 lemma(w), during the pooling operation of Eq. 3 because contextual information about certain words is usually independent of the current word form w they appear in. As a consequence, this minor linguistic preprocessing step allows for additional interaction between tokens of the same lemma.</p><p>Pooling over lemma-occurrences effectively connects different text passages (even across texts) that are otherwise disconnected, mitigating the problems arising from long-distance dependencies. This is reminiscent of the (soft) attention mechanism used in reading comprehension models (e.g., <ref type="bibr" target="#b4">Cheng et al. (2016)</ref>; <ref type="bibr" target="#b29">Wang et al. (2017)</ref>). However, our setup is more general as it allows for the connection of multiple passages (via pooling) at once and is able to deal with multiple inputs which is necessary to make use of additional input texts such as relevant background knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We run experiments on four benchmarks for two standard NLU tasks: recognizing textual entailment (RTE) and document question answering (DQA). In the following we describe our experimental setup.</p><p>Task-specific Models Since we wish to assess the value of the proposed embedding refinement strategy, we focus on relatively simple task architectures. We use single-layer bidirectional LSTMs (BiLSTMs) as encoders of the inputs represented by the refined or unrefined embeddings with a task-specific, feed-forward network for the final prediction. Such models are general reading architectures <ref type="bibr" target="#b2">(Bowman et al., 2015;</ref><ref type="bibr" target="#b25">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b30">Weissenborn et al., 2017)</ref>. To demonstrate that our reading module can be integrated into arbitrary task architectures, we also add our refinement module to a reimplementation of a state of the art architecture for RTE called ESIM <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>. We refer the interested reader to the ESIM paper for details of the model.</p><p>All models are trained end-to-end jointly with the refinement module using a dimensionality of n = 300 for all but the TriviaQA experiments for which we had to reduce n to 150 due to memory constraints. All baselines operate on the unrefined word embeddings E 0 described in §3.1. For the DQA baseline system we add the lemmain-question feature (liq) suggested in <ref type="bibr" target="#b30">Weissenborn et al. (2017)</ref>. Implementation details for the BiL-STM task architectures, as well as training details, are available in Appendix A.</p><p>Question Answering We use 2 recent DQA benchmark training and evaluation datasets, SQuAD <ref type="bibr" target="#b24">(Rajpurkar et al., 2016)</ref> and TriviaQA <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref>. The task is to predict an answer span within a provided document p given a question q. Both datasets are large-scale, containing on the order of 100k examples, however, Trivi-aQA is more complex in that the supporting documents are much larger than those for SQuAD. Because TriviaQA is collected via distant supervision the test set is divided into a large but noisy distant supervision part and a much smaller (on the order of hundreds) human verified part. We report results on both. See Appendix A.1 for implementation details of the DQA system.</p><p>Recognizing Textual Entailment We test on both the SNLI dataset <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>, a collection of 570k sentence pairs, and the more recent MultiNLI dataset (433k sentence pairs) <ref type="bibr" target="#b31">(Williams et al., 2017)</ref>. Given two sentences, a premise p and a hypothesis q, the task is to determine whether p either entails, contradicts or is neutral to q. See Appendix A.2 for implementation details of the RTE system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Knowledge Sources We use</head><p>ConceptNet 3 <ref type="bibr" target="#b28">(Speer and Havasi, 2012)</ref>, a freelyavailable, multi-lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources, such as Wiktionary, Open Multilingual WordNet, OpenCyc and DBpedia. It presents information in the form of relational triples. 4 Additionally, we exploit Wikipedia abstracts in our DQA experiments as described below.</p><p>ConceptNet Integration Here we describe the heuristic we use to obtain plausibly relevant supplementary knowledge for understanding a text pair (p, q) from ConceptNet. Our hypothesis is that relations that link words and phrases across p and q are likely to be most valuable. Because assertions a in ConceptNet come in form of (subject, predicate, object)-triples (s, r, o), we retrieve all assertions for which s appears in q and o appears in p, or vice versa. Because still too many such assertions might be retrieved for an instance, we rank all retrievals based on their respective subject and object. The ranking score we use is the inverse product of appearances of the subject and the object in the KB, that is score(a) = ( a I(s a = s a ) · a I(o a = o a )) −1 , where I denotes the indicator function. During training and evaluation we retain the top-k assertions, using k = 50 for DQA and k = 20 for RTE. Note that fewer or even no assertions might be retrieved for a particular instance during training and testing.</p><p>Wikipedia Integration Here we describe the heuristic we use to obtain plausibly relevant supplementary knowledge from Wikipedia. We wish to use Wikipedia abstracts 5 as an addi-tional knowledge source to gather more information about the top answer predictions of our DQA model. To this end, we let the system first predict the top-16 answer spans without any information from Wikipedia. For each answer candidate string, we collect abstracts for their 3 most frequently linked Wikipedia entries. 6 Using more than only the most frequently linked Wikipedia entry for a given answer string, lets us mitigate problems arising from polysemous entity names, although it does mean the refinement model needs to be selective in extracting relevant information. The refinement module additionally reads the initial 50 tokens of each retrieved Wikipedia abstract and computes the final predictions.</p><p>Refinement Order When employing our embedding-refinement strategy, we first read the document (p) followed by the question (q) in case of DQA, and the premise (p) followed by the hypothesis (q) for RTE, that is, X 1 = {p} and X 2 = {q}. Additional knowledge in the form of a set of assertions A is integrated after reading the task-specific input for both DQA and RTE, that is, X 3 = A. Finally, for DQA we additionally add Wikipedia abstracts as background knowledge as described previously, that is, X 4 = W. In preliminary experiments we found that the final performance is not significantly sensitive to the order of presentation so we decided to fix our order as defined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>This section presents results. We provide ablations for a total of 7 task-dataset-model combinations and compare our final results to other works on the most recent benchmark datasets for each task (TriviaQA and MultiNLI), demonstrating that our results are competitive, and in some cases, state of the art, even without sophisticated task architectures.    <ref type="bibr" target="#b5">Clark and Gardner (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Question Answering</head><p>Net (A). Wikipedia (W) yields further, significant improvements on TriviaQA, slightly outperforming the current state of the art model <ref type="table" target="#tab_3">(Table 2)</ref>. This is especially noteworthy given the simplicity of our QA architecture (i.e., a single layer BiL-STM) compared to the previous SotA attained by <ref type="bibr" target="#b5">Clark and Gardner (2017)</ref>. The development results on SQuAD 7 show the same pattern of improvement, but here the results are slightly worse than the model of <ref type="bibr" target="#b5">Clark and Gardner (2017)</ref>, and they are way off from the current best-known results (currently at 87% F1); 8 however, our intention with these experiments is to show of the value that external knowledge and our refinement process can bring, not to compete with highly tuned task architectures on a single dataset.</p><p>Controlling for computation. One potential explanation for the improvement obtained using the <ref type="bibr">7</ref> We do not report test set results for SQuAD due to restrictions on code sharing. 8 https://rajpurkar.github.io/SQuAD-explorer/  refinement module is that we are enabling more computation over the information present in the inputs, that is, we are effectively using a deeper architecture. To test whether this might be the case, we also ran an experiment with a 2-layer BiLSTM (+liq). This setup exhibits similar computational complexity and number of parameters to BiLSTM + p + q. We found that the second layer did not improve performance, suggesting that pooling over word/lemma occurrences in a given context between layers, is a powerful, yet simple technique. <ref type="table" target="#tab_5">Table 3</ref> shows the results of our RTE experiments. In general, the introduction of our refinement strategy almost always helps, both with and without external knowledge. When providing additional background knowledge from ConceptNet, our BiLSTM based models improve substantially, while the ESIM-based models improve only on the more difficult MultiNLI dataset. Compared to previously published state of the art systems, our models acquit themselves quite well on the MultiNLI benchmark, and competitively on the SNLI benchmark. In parallel to this work, <ref type="bibr" target="#b7">Gong et al. (2017)</ref> developed a novel task-specific architecture for RTE that achieves slightly better performance on MultiNLI than our ESIM + p + q + A based models. 9 It draws attention to the fact that when using our knowledge-enhanced embed-ding module, on the MultiNLI, the basic BiLSTM task model outperforms the task-specific ESIM model, which is architecturally much more complex and designed specifically for the RTE task. We do find that there is little impact of using external knowledge on the RTE task with ESIM, although the refinement strategy helps using just p + q. A more detailed set of experiments reported in Appendix B shows that by impoverishing the amount of training data and information present in the GloVe embeddings, the positive impact of supplemental information becomes much more pronounced. These results suggest that ESIM is able to learn important background information from the large-scale datasets and from pretrained embeddings, but this can be supplemented when necessary. Nevertheless, both ESIM and our BiL-STM models when trained with knowledge from ConceptNet are sensitive to the semantics of the provided assertions as demonstrated in our analysis in §5.3. We argue that this is a desirable side effect because it makes the predictions of our model more interpretable than those not trained with knowledge. Furthermore, increasing the coverage of assertions in ConceptNet would most likely yield improved performance even without retraining our models. Finally, we remark that despite careful tuning, our re-implementation of ESIM fails to match the 88% reported in <ref type="bibr" target="#b3">Chen et al. (2017)</ref> by 0.8%; however, with MultiNLI, we find that our implementation of ESIM performs considerably better (by approximately 5%). The instability of the results suggests, as well as the failure of a custom RTEarchitecture to consistently perform well suggests that current SotA RTE models may be overfit to the SNLI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Recognizing Textual Entailment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis</head><p>Although our empirical results show our knowledge-incorporation approach improves performance, in this section we attempt to assess whether we are learning to use the provided knowledge in a semantically appropriate way.</p><p>RTE To test our models sensitivity towards the semantics of the assertions for recognizing textual entailment, we run an experiment in which we swap the synonym with the antonym predicate in the provided assertions during test time. We hypothesize that in many cases these two predicates are very important for predicting either contradic-tion or entailment. Indeed, there is a strong performance drop of about 10% on MultiNLI examples for both the BiLSTM and the ESIM model for which either a synonym or an antonym-assertion is present. This very large drop clearly shows that our models are sensitive to the semantics of the provided knowledge. Examples of prediction changes are presented in <ref type="table" target="#tab_8">Table 4</ref>. They demonstrate that the system has learned to trust the presented assertions to the point that it will make appropriate counterfactual inferences-that is, the change in knowledge has caused the change in prediction. For the interested reader we provide additional RTE analysis results in Appendix C DQA The following is an example question from the TriviaQA dataset: Their corresponding abstracts were retrieved from Wikipedia and then given to our model in a second pass (i.e., BiLSTM + p + q + A + W). In this example, the final best prediction of the model changes from Denmark to Corfu after integrating the abstracts (here, the abstract clearly states that Corfu is an island). We studied a total of 25 similar answer changes, 14 of which went from incorrect to correct, and 11 of which went from correct to incorrect. In 11 of the 14 corrections, obvious information is present in the Wikipedia abstracts that reinforced the correct answer. Where the system was confused by the answers (i.e., when the abstracts switched the production from correct to incorrect), no obvious information was present in 8 of the 11 cases, suggesting that the model had difficulty coping with unrelated background information. In 3 of the 11, plausibly relevant information was present in the abstract of the correct answer, yet the model still made the incorrect answer change.</p><p>The existence of counterfactual inferences in RTE and the tendency to use reinforcing informa-p:</p><p>His off-the-cuff style seems amateurish <ref type="bibr">[...]</ref> the net cost of operations.</p><p>but uh these guys <ref type="bibr">[...]</ref>   tion about candidate answers in DQA suggest that our knowledge incorporating strategy is exploiting heterogeneous knowledge sources in semantically sensible ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The role of background knowledge in natural language understanding has long been remarked on, especially in the context of classical models of AI <ref type="bibr" target="#b26">(Schank and Abelson, 1977;</ref><ref type="bibr" target="#b18">Minsky, 2000)</ref>; however, it has only recently begun to play a role in neural network models of NLU <ref type="bibr" target="#b0">(Ahn et al., 2016;</ref><ref type="bibr" target="#b33">Xu et al., 2016;</ref><ref type="bibr" target="#b14">Long et al., 2017;</ref><ref type="bibr" target="#b6">Dhingra et al., 2017)</ref>. Previous efforts have focused on specific tasks or certain kinds of knowledge, whereas we take a step towards a more generalpurpose solution for the integration of heterogeneous knowledge for NLU systems by providing a simple, general-purpose reading architecture that can read background knowledge encoded in simple natural language statements, e.g., "abdication is a type of resignation". In the area of visual question answering <ref type="bibr" target="#b32">Wu et al. (2016)</ref>  (2017) incorporate information about word senses into their representations before solving the downstream NLU task, which is similar. We go one step further by seamlessly integrating all kinds of fine-grained assertions about concepts that might be relevant for the task at hand.</p><p>Another important aspect of our approach is the notion of dynamically updating wordrepresentations with contextual information. Tracking and updating concepts, entities or sentences with dynamic memories is a very active research direction <ref type="bibr" target="#b13">(Kumar et al., 2016;</ref><ref type="bibr" target="#b8">Henaff et al., 2017;</ref><ref type="bibr" target="#b10">Ji et al., 2017;</ref><ref type="bibr" target="#b12">Kobayashi et al., 2017)</ref>. However, those works typically focus on particular tasks whereas our approach is taskagnostic and most importantly allows for the easy integration of external background knowledge. Important progress has also been made in creating pre-trained, contextualized token representations <ref type="bibr" target="#b22">(Peters et al., 2017;</ref><ref type="bibr" target="#b17">McCann et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a novel reading architecture that allows for the dynamic integration of background knowledge into neural NLU models. Our solution, which is based on the incremental refinement of word representations by reading supplementary inputs, is flexible and can be used with virtually any existing NLU architecture that rely on word embeddings as input. Our results show that embedding refinement using both the system's text inputs, as well as supplementary text from external background knowledge can yield large improvements. In particular, we have shown that relatively simple task architectures (e.g., based on simple BiLSTM readers) can become competitive with state of the art, task-specific architectures when augmented with our reading architecture. Our analysis demonstrates that our model learns to exploit provided background knowledge in a semantically appropriate way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>All our models were trained with 3 different random seeds and the top performance is reported 10 . An overview of hyper-parameters used in our experiments can be found in <ref type="table" target="#tab_10">Table 5</ref>. In the following we explain the detailed implementation of our two task-specific, baseline models.</p><p>We assume to have computed the contextually (un-)refined word representations depending on the setup and embedded our input sequences q = (q 1 , ..., q L Q ) and p = (p 1 , ..., p L P ) to Q ∈ R n×L Q and P ∈ R n×L P , respectively. The word representation update gate in Eq. 4 is initialized with a bias of 1 to refine representations only slightly in the beginning of training. In the following as before, we denote the hidden dimensionality of our model by n and a fully-connected layer by</p><formula xml:id="formula_4">FC(z) = Wz + b, W ∈ R n×m , b ∈ R n , u ∈ R m .</formula><p>A.1 Question Answering Encoding In the DQA task q refers to the question and p to the supporting text. For our baseline (i.e., BiLSTM + liq) we additionally concatenate a binary feature to p and q indicating whether the corresponding token lemma appeared in the question. However, it is omitted in the following for the sake of brevity. At first we process both sequences by identical BiLSTMs in parallel (Eq. 6) followed by a linear projection and a tanh nonlinearity (Eq. 7) .</p><formula xml:id="formula_5">Q = BiLSTM (Q)Q ∈ R 2n×L Q P = BiLSTM (P)P ∈ R 2n×L P (6) Q = tanh UQ Q ∈ R n×L Q P = tanh UP P ∈ R n×L P<label>(7)</label></formula><p>U ∈ R n×2n is initialized by [I; I] where I ∈ R n×n is the identity matrix.</p><p>Prediction Our prediction-or answer layer is similar to <ref type="bibr" target="#b30">Weissenborn et al. (2017)</ref>. We first compute a weighted, n-dimensional representationq of the processed questionQ (Eq. 8). <ref type="bibr">10</ref> Result variations were small, that is within less than a percentage point in all experiments.</p><formula xml:id="formula_6">α = softmax(v qQ ) , v q ∈ R ñ q = i α iqi (8)</formula><p>The probability distributions p s /p e for the start/end location of the answer is computed by a 2-layer MLP with a ReLU activated, hidden layer s j as follows:</p><formula xml:id="formula_7">s j = ReLU   FC s    p j q p j q       e j = ReLU   FC e    p j q p j q       p s (j) = softmax j (v s s j ) v s ∈ R n p e (j) = softmax j (v e s j ) v e ∈ R n (9)</formula><p>The model is trained to maximize the loglikelihood of the correct answer spans by computing the sum of the correct span probabilities p s (i) · p e (k) for span (i, k) under our model (Eq. 9). During evaluation we extract the span (i, k) with the best score and maximum token length k − i ≤ 16 for SQuAD and k − i ≤ 8 for TriviaQA.</p><p>TriviaQA Properly training a QA system on TriviaQA is much more challenging than SQuAD because of the large document sizes and the use of multiple paragraphs. Therefore, we adopt the approach of <ref type="bibr" target="#b5">Clark and Gardner (2017)</ref> who were the first to properly train neural QA models on Trivi-aQA. It relies on splitting documents and merging paragraphs up to a certain maximum token length (600 per paragraph in our experiments), and only retaining the top-k paragraphs (6 in our case) for prediction. Paragraphs are ranked using the tfidf cosine similarity between question and paragraph. To speed up training only 2 paragraphs out of the top 4/6 for the W eb/W ikipedia datasets were sampled. The only architectural difference for this multi-paragraph setup is that we encode multiple p for each question q and the softmax of Eq. 9 is taken over all tokens of all paragraphs instead of only a single paragraph. For further details, we refer the interested reader to <ref type="bibr" target="#b5">Clark and Gardner (2017)</ref> who explain this process in more detail.  For optimization we employed ADAM with a learning rate of 10 −3 which was halved when performance dropped between checkpoint (ckpt) intervals. We use 300-dimensional wordembeddings from GloVe <ref type="bibr" target="#b21">(Pennington et al., 2014)</ref> as pre-trained word embeddings in all experiments. For regularization we make use of dropout on the computed non-contextual word representations e w defined in §3.1 with the same dropout mask for all words in a batch. For QA we additionally applied dropout on the projections computed in Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Recognizing Textual Entailment</head><p>Encoding Analogous to DQA we encode our input sequences by BiLSTMs, however, for RTE we use conditional encoding <ref type="bibr" target="#b25">(Rocktäschel et al., 2015)</ref> instead. Therefore, we initially process the embedded hypothesis Q by a BiLSTM and use the respective end states of the forward and backward LSTM as initial states for the forward and backward LSTM that processes the embedded premise P.</p><p>Prediction We concatenate the outputs of the forward and backward LSTMs processing the premise p, i.e., p f w t ;p bw t ∈ R 2n and run each of the resulting L P outputs through a fully-connected layer with ReLU activation (h t ) followed by a max-pooling operation over time resulting in a hidden state h ∈ R n . Finally, h is used to predict the RTE label as follows:</p><formula xml:id="formula_8">h t = ReLU FC p f w t p bw t h = maxpool t h t p(c) = softmax c (v c h) , v c ∈ R n<label>(10)</label></formula><p>The probability of choosing category c ∈ {entailment, contradiction, neutral} is defined in Eq. 10. Finally, the model is trained to maximize the log-likelihood of the correct category label given probability distribution p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Reducing Training Data &amp; Dimensionality of Pre-trained Word Embeddings</head><p>We find that there is only little impact when using external knowledge on the RTE task when using a more sophisticated task model such as ESIM. We hypothesize that the attention mechanisms within ESIM together with powerful, pre-trained word representations allow for the recovery of some important lexical relations when trained on a large dataset. It follows that by reducing the number of training data and impoverishing pre-trained word representations the impact of using external knowledge should become larger.</p><p>To test this hypothesis, we gradually impoverish pre-trained word embeddings by reducing their dimensionality with PCA while reducing the number of training instances at the same time. 11 Our joint data and dimensionality reduction results are presented in <ref type="table" target="#tab_12">Table 6</ref>. They show that there is indeed a slightly larger benefit when employing background knowledge from ConcepNet (A) in the more impoverished settings with largest improvements when using around 10k examples and reduced dimensionality to 10. However, we observe that the biggest overall impact over the baseline ESIM model stems from our contextual refinement strategy (i.e., reading only the premise p and hypothesis q) which is especially pronounced for the 1k and 3k experiments. This highlights once more the usefulness of our refinement strategy even without the use of additional knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further Analysis of Knowledge Utilization in RTE</head><p>Is additional knowledge used? To verify whether and how our models make use of additional knowledge, we conducted several experiments. First, we evaluated models trained with knowledge on our tasks while not providing any knowledge at test time. This ablation drops performance by 3.7-3.9% accuracy on MultiNLI, and by 4% F1 on SQuAD. This indicates the model is refining the representations using the provided assertions in a useful way.</p><p>What knowledge is used? After establishing that our models are somehow sensitive to seman-    tics we wanted to find out which type of knowledge is important for which task. For this analysis we exclude assertions including the most prominent predicates in our knowledge base individually when evaluating our models. The results are presented in <ref type="figure" target="#fig_2">Figure 2</ref>. They demonstrate that the biggest performance drop in total (blue bars) stems from related to assertions. This very prominent predicate appears much more frequently than other assertions and helps connecting related parts of the 2 input sequences with each other. We believe that related to assertions offer benefits mainly from a modeling perspective by strongly connecting the input sequences with each other and thus bridging long-range dependencies similar to attention. Looking at the relative drops obtained by normalizing the performance differences on the actually affected examples (green) we find that our models depend highly on the presence of antonym and synonym assertions for all tasks as well as partially on is a and derived from assertions. This is an interesting finding which shows that the sensitivity of our models is selective wrt. the type of knowledge and task. The fact that the largest relative impact stems from antonyms is very interesting because it is known that such information is hard to capture with distributional semantics contained in pre-trained word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>utilize external knowledge in form of DBpedia comments (short abstracts/definitions) to improve the answering ability of a model. Marino et al. (2017) explicitly incorporate knowledge graphs into an image classification model. Xu et al. (2016) created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a conversation model. Concurrently, Dhingra et al. (2017) exploit linguistic knowledge using MAGE-GRUs, an adapation of GRUs to handle graphs, however, external knowledge has to be present in form of triples. Ahn et al. (2016) exploit knowledge base facts about mentioned entities for neural language models.<ref type="bibr" target="#b1">Bahdanau et al. (2017)</ref> and<ref type="bibr" target="#b14">Long et al. (2017)</ref> create word embeddings on-the-fly by reading word definitions prior to processing the task at hand. Pilehvar et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) BiLSTM on MultiNLI. (b) ESIM on MultiNLI. (c) BiLSTM on SQuAD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Performance differences when ignoring certain types of knowledge, i.e., relation predicates during evaluation. Normalized performance differences are measured on the subset of examples for which an assertion of the respective relation predicate occurs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>presents our results on two question answering benchmarks. The results demonstrate that the introduction of the refinement module helps consistently, and further improvements come from using common sense knowledge from Concept-6 Statistics were extracted from the DBpedia Anchor Text dataset (http://downloads.dbpedia.org/ 2016-10/core-i18n/en/anchor_text_en.ttl. bz2).</figDesc><table><row><cell>Model</cell><cell cols="3">SQuAD T-Wiki T-Web</cell></row><row><cell>BiLSTM ( = 0) + liq</cell><cell>75.9</cell><cell>62.1</cell><cell>65.0</cell></row><row><cell>+ p + q (E2)</cell><cell>78.6</cell><cell>65.5</cell><cell>68.7</cell></row><row><cell>+ p + q + A (E3)</cell><cell>79.7</cell><cell>67.1</cell><cell>70.3</cell></row><row><cell>+ p + q + A + W (E4)</cell><cell>79.7</cell><cell>69.5</cell><cell>72.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation on the SQuAD and TriviaQA (T-Wiki and T-Web) development sets for the F1 metric. Information used for embedding refinement: p/qrefinement on task input (i.e., document and question); Atop-50 retrieved Con-ceptNet assertions; W-Wikipedia abstracts for the top-16 answer candidates. The liq-feature (lemma-in-question) is only used in the baseline.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Exact</cell><cell>F1</cell></row><row><cell cols="2">TriviaQA Wiki (1)</cell><cell cols="2">64.0 / 68.0 68.9 / 72.9</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">64.6 / 72.8 69.9 / 77.4</cell></row><row><cell cols="2">TriviaQA Web (1)</cell><cell cols="2">66.4 / 80.0 71.3 / 83.7</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">67.5 / 77.6 72.8 / 82.0</cell></row><row><cell>SQuAD Dev</cell><cell>(1)</cell><cell>71.6</cell><cell>80.8</cell></row><row><cell></cell><cell>Ours</cell><cell>69.5</cell><cell>79.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Test set results of our full model (BiLSTM+p+q+A+W, i.e., using E 4 as embed- dings). Results for TriviaQA are divided by dis- tant supervision results (left) and human verified results (right, comprise only several hundreds of examples). We compare against the concurrent work on TriviaQA of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Ablation on the SNLI and MultiNLI-</cell></row><row><cell>Matched and -Mismatched development set and</cell></row><row><cell>final results on the corresponding test sets. p/q-</cell></row><row><cell>refinement on task input (i.e., premise and hypoth-</cell></row><row><cell>esis); A-top-20 retrieved ConceptNet assertions.</cell></row></table><note>1 Chen et al. (2017), 2 Gong et al. (2017).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Corfu is a Greek island in the Ionian Sea [. . . ] • Greece, officially the Hellenic Republic, [. . . ] is a transcontinental country [. . . ] • Vanuatu is a Pacific island nation located in the South Pacific Ocean [. . .</figDesc><table><row><cell>Prince Philip [. . . ] was born on which island?</cell></row><row><cell>Answer candidates with corresponding abstracts:</cell></row><row><cell>• Denmark is a Scandinavian country with terri-</cell></row><row><cell>tory in Europe and North America [. . . ]</cell></row><row><cell>•</cell></row></table><note>] Answer candidates (i.e., Denmark, Corfu, Greece, Vanuata) were obtained from the top predicted answer spans computed by our model exclud- ing Wikipedia (i.e., BiLSTM + p + q + A).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>file their uh their final exams [...] h:He didn't look like an amateur The gross cost. These men filed their midterm exams[...]    </figDesc><table><row><cell>a:</cell><cell>look like synonym seem</cell><cell>gross antonym net</cell><cell>midterm antonym final</cell></row><row><cell cols="2">→ contradiction</cell><cell>contradiction</cell><cell>contradiction</cell></row><row><cell>a:</cell><cell>look like antonym seem</cell><cell>gross synonym net</cell><cell>midterm synonym final</cell></row><row><cell cols="2">→ entailment</cell><cell>entailment</cell><cell>entailment</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Three examples for the antonym ↔ synonym swapping experiment on MultiNLI. p-premise, h-hypothesis, a-assertion,ā-swapped assertion.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Training hyper-parameters for our models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>+ q 51.8(+7.5) 55.8(+5.8) 60.1(+4.6) 65.0(+3.1) 70.7(+2.6) 78.1(+1.2) + p + q + A 52.4(+0.6) 57.9(+2.1) 62.4(+2.3) 66.6(+1.6) 71.3(+0.6) 78.8(+0.7)</figDesc><table><row><cell>Dim/Data</cell><cell>1/1k</cell><cell>3/3k</cell><cell>10/10k</cell><cell>30/30k</cell><cell>100/100k</cell><cell>300/Full</cell></row><row><cell>ESIM</cell><cell>44.3</cell><cell>50.0</cell><cell>55.5</cell><cell>61.9</cell><cell>68.1</cell><cell>76.9</cell></row><row><cell>+ p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Development set results for MultiNLI (Matched + Mismatched) when reducing training data and embedding dimensionality with PCA. In parenthesis we report the relative differences to the respective result directly above.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Adding this one-hot feature lets the refinement model learn to update embeddings differently in different levels.2 https://spacy.io is used for lemmatization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://conceptnet.io/ 4 We exclude ConceptNet 4 assertions created by only one contributor and from Verbosity to reduce noise. 5 Downloaded from http://wiki.dbpedia.org/ downloads-2016-10</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Our refinement architecture can be used of course with this new model, but we report ESIM results since that was best when this work was carried out.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Although reducing either embedding dimensionality or data individually exhibit similar (but less pronounced) results we only report the joint reduction results here.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Sungjin Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Pärnamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<title level="m">A neural knowledge language model. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Learning to compute word embeddings on the fly. arXiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Potts</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Linguistic Knowledge as Memory for Recurrent Neural Networks. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Natural language inference over interaction space. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic entity representations in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A neural language model for dynamically representing the meanings of unknown words and entities in a discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01679</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Commonsense-based interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Minsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01509</idno>
		<title level="m">Neural models for information retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Taskoriented query reformulation with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards a seamless integration of word senses into downstream nlp applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Mohammad Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<title level="m">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<title level="m">Reasoning about entailment with neural attention. ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Scripts, Plans, Goals, and Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bi-Directional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hananneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representing General Relational Knowledge in ConceptNet 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Making Neural QA as Simple as Possible but not Simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>Anton van den Hengel, and Anthony Dick</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
