<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Models of Interactions Across Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hartford</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><forename type="middle">R</forename><surname>Graham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
						</author>
						<title level="a" type="main">Deep Models of Interactions Across Sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use deep learning to model interactions across two or more sets of objects, such as user-movie ratings, protein-drug bindings, or ternary useritem-tag interactions. The canonical representation of such interactions is a matrix (or a higherdimensional tensor) with an exchangeability property: the encoding's meaning is not changed by permuting rows or columns. We argue that models should hence be Permutation Equivariant (PE): constrained to make the same predictions across such permutations. We present a parameter-sharing scheme and prove that it could not be made any more expressive without violating PE. This scheme yields three benefits. First, we demonstrate state-of-the-art performance on multiple matrix completion benchmarks. Second, our models require a number of parameters independent of the numbers of objects, and thus scale well to large datasets. Third, models can be queried about new objects that were not available at training time, but for which interactions have since been observed. In experiments, our models achieved surprisingly good generalization performance on this matrix extrapolation task, both within domains (e.g., new users and new movies drawn from the same distribution used for training) and even across domains (e.g., predicting music ratings after training on movies).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Suppose we are given a set of users N = {1, . . . , N }, a set of movies M = {1, . . . , M }, and their interaction in the form of tuples X = ⟨n, m, x⟩ with n ∈ N, m ∈ M and x ∈ R. Our goal is to learn a function that models the interaction between users and movies, i.e., mapping from N × M to R. The canonical representation of such a function is a matrix X ∈ R N ×M ; of course, we want X n,m = x for each ⟨n, m, x⟩ ∈ X. Learning our function corresponds to matrix completion: using patterns in X to predict values for the remaining elements of X. X is what we will call an exchangeable matrix: any row-and column-wise permutation of X represents the same set of ratings and hence the same matrix completion problem.</p><p>Exchangeability has a long history in machine learning and statistics. de Finetti's theorem states that exchangeable sequences are the product of a latent variable model. Extensions of this theorem characterize distributions over other exchangeable structures, from matrices to graphs; see <ref type="bibr" target="#b19">Orbanz &amp; Roy (2015)</ref> for a detailed treatment. In machine learning, a variety of frameworks formalize exchangeability in data, from plate notation to statistical relational models <ref type="bibr" target="#b20">(Raedt et al., 2016;</ref><ref type="bibr" target="#b8">Getoor &amp; Taskar, 2007)</ref>. When dealing with exchangeable arrays (or tensors), a common approach is tensor factorization. In particular, one thread of work leverages tensor decomposition for inference in latent variable models <ref type="bibr" target="#b0">(Anandkumar et al., 2014)</ref>. However, in addition to having limited expressive power, tensor factorization requires retraining models for each new input.</p><p>We call a learning algorithm Permutation Equivariant (PE) if it is constrained to give the same answer across all exchangeable inputs; we argue that PE is an important form of inductive bias in exchangeable domains. However, it is not trivial to achieve; indeed, all existing parametric factorization and matrix/tensor completion methods associate parameters with each row and column, and thus are not PE. How can we enforce this property? One approach is to augment the input with all M ! × N ! permutations of rows and columns. However, this is computationally wasteful and becomes infeasible for all but the smallest N and M . Instead, we show that a simple parameter-sharing scheme suffices to ensure that a deep model can represent only PE functions. The result is analogous to the idea of a convolution layer: a lower-dimensional effective parameter space that enforces a desired equivariance property. Indeed, parameter-sharing is a generic and efficient approach for achieving equivariance in deep models .</p><p>When the matrix models the interaction between the members of the same group, one could further con-arXiv:1803.02879v2 [stat.ML] 8 Jun 2018 <ref type="figure">Figure 1</ref>. Structure of our parameter matrix for the 1D (left), 2D (centre), and 3D (right) input arrays. The parameter-sharing patterns for the weight matrix of the higher dimensional arrays can be produced via the Kronecker product of the weight matrix for the 1D array (i.e., vector). strain permutations to be identical across both rows and columns. An example of such a jointly exchangeable matrix <ref type="bibr" target="#b19">(Orbanz &amp; Roy, 2015)</ref>, modelling the interaction of the nodes in a graph, is the adjacency matrix deployed by graph convolution. Our approach reduces to graph convolution in the special case of 2D arrays with this additional parametersharing constraint, but also applies to arbitrary matrices and higher dimensional arrays.</p><p>Finally, we explain connections to some of our own past work. First, we introduced a similar parameter-sharing scheme in the context of behavioral game theory <ref type="bibr" target="#b11">(Hartford et al., 2016)</ref>: rows and columns represented players' actions and the exchangeable matrix encoded payoffs. The current work provides a theoretical foundation for these ideas and shows how a similar architecture can be applied much more generally. Second, our model for exchangeable matrices can be seen as a generalization of the deep sets architecture <ref type="bibr" target="#b30">(Zaheer et al., 2017)</ref>, where a set can be seen as a one-dimensional exchangeable array.</p><p>In what follows, we begin by introducing our parametersharing approach in Section 2, considering the cases of both dense and sparse matrices. In Section 3.2 we study two architectures for matrix completion that use an exchangeable matrix layer. In particular the factorized autoencoding model provides a powerful alternative to commonly used matrix factorization methods; notably, it does not require retraining to be evaluated on previously unseen data. In Section 4 we present extensive results on benchmark matrix completion datasets. We generalize our approach to higher-dimensional tensors in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Exchangeable Matrix Layer</head><p>Let X ∈ R N ×M be our "exchangeable" input matrix. We use vec(X) ∈ R N M to denote its vectorized form and vec −1 (x) to denote the inverse of the vectorization that reshapes a vector of length N M into an N × M matrixi.e., vec −1 (vec(X)) = X. Consider a fully connected layer Y ∶= vec −1 (σ(W vec(X))) where σ is an element-wise <ref type="figure">Figure 2</ref>. Parameter sharing in an exchangeable matrix layer. The application of different tied parameters to input elements is illustrated using dark blue for w1, green for w2, red for w3, and light blue for w4. The same structure (with the same four parameters) repeats across all units of the output layer.</p><p>nonlinear function such as sigmoid, W ∈ R N M ×N M , and Y ∈ R N ×M is the output matrix. Exchangeablity of X motivates the following property: suppose we permute the rows and columns X using two arbitrary permutation ma-</p><formula xml:id="formula_0">trices G (N ) ∈ {0, 1} N ×N and G (M ) ∈ {0, 1} M ×M to get X ′ ∶= G (N ) XG (M ) . Permutation Equivariance (PE) re- quires the new output matrix Y ′ ∶= vec −1 (σ(W vec(X ′ )))</formula><p>to experience the same permutation of rows and columnsthat is, we require</p><formula xml:id="formula_1">Y ′ = G (N ) Y G (M ) .</formula><p>Definition 2.1 (exchangeable matrix layer, simplified 1 ) Given X ∈ R N ×M , a fully connected layer σ(W vec(X)) with W ∈ R N M ×N M is called an exchangeable matrix layer if, for all permutation matrices G (N ) ∈ {0, 1} N ×N and G (M ) ∈ {0, 1} M ×M , permutation of the rows and columns results in the same permutations of the output:</p><formula xml:id="formula_2">vec −1 (σ(W vec(G (N ) XG (M ) ))) = (1) G (N ) vec −1 (σ(W vec(X)))G (M ) .</formula><p>This requirement heavily constrains the weight matrix W : indeed, its number of effective degrees of freedom cannot even grow with N and M . Instead, the resulting layer is forced to have the following, simple form:</p><formula xml:id="formula_3">W (n,m),(n ′ ,m ′ ) ∶= ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ w 1 n = n ′ , m = m ′ w 2 n = n ′ , m ≠ m ′ w 3 n ≠ n ′ , m = m ′ w 4 n ≠ n ′ , m ≠ m ′ .<label>(2)</label></formula><p>For each output element Y n,m , we have the following parameters: one connecting it to its counterpart X n,m ; one each connecting it to the inputs of the same row and the inputs of the same column; and one shared by all the other connections. We also include a bias parameter; see <ref type="figure">Fig. 2</ref> for an illustration of the action of this parameter matrix, and <ref type="figure">Fig. 1</ref> for a visual illustration of it. Theorem 2.1 formalizes the requirement on our parameter matrix. All proofs are deferred to the appendix.</p><p>Theorem 2.1 Given a strictly monotonic function σ, a neural layer σ(W vec(X)) is an exchangeable matrix layer iff the elements of the parameter matrix W are tied together such that the resulting fully connected layer simplifies to</p><formula xml:id="formula_4">Y = σ w ′ 1 X + w ′ 2 N (1 N 1 T N X) + w ′ 3 M (X1 M 1 T M ) (3) + w ′ 4 N M (1 N 1 T N X1 M 1 T M ) + w ′ 5 1 N 1 T M where 1 N = [1, . . . , 1] T lengthN and w ′ 1 , . . . , w ′ 5 ∈ R.</formula><p>This parameter sharing is translated to summing or averaging elements across rows and columns; more generally, PE is preserved by any commutative pooling operation. Moreover, stacking multiple layers with the same equivariance properties preserves equivariance . This allows us to build deep PE models.</p><p>Multiple Input-Output Channels Equation (3) describes the layer as though it has single input and output matrices. However, as with convolution, we may have K input and O output channels. We use superscripts X ⟨k⟩ and Y ⟨o⟩ to denote such channels. Cross-channel interactions are fully connected-that is, we have five unique parameters w ⟨k,o⟩ 1 , . . . , w ⟨k,o⟩ 4</p><p>for each combination of input-output channels; note that the bias parameter w 5 does not depend on the input. Similar to convolution, the number of channels provides a tuning nob for the expressive power of the model. In this setting, the scalar output Y ⟨o⟩ n,m is given as</p><formula xml:id="formula_5">Y ⟨o⟩ n,m = σ ⎛ ⎝ K k=1 w ⟨k,o⟩ 1 X ⟨k⟩ n,m + w ⟨k,o⟩ 2 N ( n ′ X ⟨k⟩ n ′ ,m )+ (4) w ⟨k,o⟩ 3 M ( m ′ X ⟨k⟩ n,m ′ ) + w ⟨k,o⟩ 4 N M ( n ′ ,m ′ X ⟨k⟩ n ′ ,m ′ ) + w ⟨o⟩ 5 ⎞ ⎠</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Features for Rows and Columns</head><p>In some applications, in addition to the matrix X ∈ R N ×M ×K , where K is the number of input channels/features, we may have additional features for rows R ∈ R N ×K ′ and/or columns C ∈ R M ×K ′′ . We can preserve PE by broadcasting these row/column features over the whole matrix and treating them as additional input channels.</p><p>Jointly Exchangeable Matrices For jointly exchangeable matrices, such as adjacency matrix, Equation (1) is constrained to have N = M and G (N ) = G (M ) . This will in turn constrain the corresponding parameter-sharing so that w 2 = w 3 in Equation (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sparse Inputs</head><p>Real-world arrays are often extremely sparse. Indeed, matrix and tensor completion is only meaningful with missing entries. Fortunately, the equivariance properties of Theorem 2.1 continue to hold when we only consider the nonzero (observed) entries. For sparse matrices, we continue to use the same parameter-sharing scheme across rows and columns, with the only difference being that we limit the model to observed entries. We now make this precise. Let X ∈ R N ×M ×K be a sparse exchangeable array with K channels, where all the channels for each row-column pair ⟨n, m⟩ are either fully observed or completely missing. Let I identify the set of such non-zero indices. Let R n = {m (n, m) ∈ I} be the non-zero entries in the n th row of X, and let C m be the non-zero entries of its m th column. For this sparse matrix, the terms in the layer of (4) are adapted as one would expect:</p><formula xml:id="formula_6">w ⟨k,o⟩ 2 N n ′ X ⟨k⟩ n ′ ,m → w ⟨k,o⟩ 2 C m n ′ ∈Cm X ⟨k⟩ n ′ ,m w ⟨k,o⟩ 3 M m ′ X ⟨k⟩ n,m ′ → w ⟨k,o⟩ 3 R n m ′ ∈Rn X ⟨k⟩ n,m ′ w ⟨k,o⟩ 4 N M n ′ ,m ′ X ⟨k⟩ n ′ ,m ′ → w ⟨k,o⟩ 4 I (n ′ ,m ′ )∈I X ⟨k⟩ n ′ ,m ′</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Matrix Factorization and Completion</head><p>Recommender systems are very widely applied, with many modern applications suggesting new items (e.g., movies, friends, restaurants, etc.) to users based on previous ratings of other items. The core underlying problem is naturally posed as a matrix completion task: each row corresponds to a user and each column corresponds to an item; the matrix has a value for each rating given to an item by a given user; the goal is to fill in the missing values of this matrix.</p><p>In Section 3.1 we review two types of analysis in dealing with exchangeable matrices. Section 3.2 introduces two architectures: a self-supervised model-a simple composition of exchangeable matrix layers-that is trained to produce randomly removed entries of the observed matrix during the training; and a factorized model that uses an auto-encoding nonlinear factorization scheme. While there are innumerable methods for (nonlinear) matrix factorization and completion, both of these models are the first to generalize to inductive settings while achieving competitive performance <ref type="figure">Figure 3</ref>. Factorized exchangeable autoencoder. The encoder maps from the input tensor to an embedding layer of row / column factors via one or more hidden layers. The decoder attempts to reconstruct the input using the factors via one or more hidden layers.</p><p>in transductive settings. Section 3.3 introduces two subsampling techniques for large sparse matrices followed by a literature review in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Inductive and Transductive Analysis</head><p>In matrix completion, during training we are given a sparse input matrix X tr with observed entries I tr = {(n, m)}. At test time, we are given X ts with observed entries I ts = {(n ′ , m ′ )}, and we are interested in predicting (some of) the missing entries of X ts , expressed through I ′ ts . In the transductive or matrix interpolation setting, I tr and I ts have overlapping rows and/or columns-that is, at least one of the following is true:</p><formula xml:id="formula_7">{m (n, m) ∈ I} ∩ {m ′ (n ′ , m ′ ) ∈ I ′ } ≠ ∅ or {n (n, m) ∈ I} ∩ {n ′ (n ′ , m ′ ) ∈ I ′ } ≠ ∅.</formula><p>In fact, often X tr and X ts are identical. Conversely, in the inductive or matrix extrapolation setting, we are interested in making predictions about completely unseen entries: the training and test row/column indices are completely disjoint. We will even consider cases where X tr and X ts are completely different datasets-e.g., movie-rating vs music-rating. The same distinction applies in matrix factorization. Training a model to factorize a particular given matrix is transductive, while factorizing unseen matrices after training is inductive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architectures</head><p>Self-supervised Exchangeable Model When the task is matrix completion, a simple deep yet PE model is a composition of exchangeable matrix layers. That is the function f ss ∶ R N ×M ×K → R N ×M ×K is simply a composition of exchangeable matrix layers. Given the matrix X with observed entries I, we divide I = I in ∪ I pr into disjoint input and prediction entries. We then train f ss (X in ) to predict the prediction entries X pr .</p><p>Factorized Exchangeable Autoencoder (FEA) Our factorized autoencoder is composed of an encoder and a de-</p><formula xml:id="formula_8">coder. The encoder f enc ∶ R N ×M ×K → R K N ×N × R K M ×M maps the (sparse) input matrix X ∈ R N ×M ×K to a row- factor Z N ∈ R K N ×N and a column-factor Z M ∈ R K M ×M .</formula><p>To do so, the encoder uses a composition of exchangeable matrix layers. The output of the final layer Y l ∈ R N ×M ×K l is pooled across rows and columns (and optionally passed through a feed-forward layer) to produce latent factors Z N and Z M . The decoder g dec ∶ R K N ×N × R K M ×M → R N ×M ×K also uses a composition of exchangeable matrix layers, and reconstructs the input matrix X from the factors. The optimization objective is to minimize reconstruction error (X, g dec (f enc (X))); similar to classical auto-encoders.</p><p>This procedure is also analogous to classical matrix factorization, with an an important distinction that once trained, we can readily factorize the unseen matrices without performing any optimization. Note that the same architecture trivially extends to tensor-factorization, where we use an exchangeable tensor layer (see Section 5).</p><p>Channel Dropout Both the factorized autoencoder and self-supervised exchangeable model are flexible enough to make regularization important for good generalization performance. Dropout <ref type="bibr" target="#b26">(Srivastava et al., 2014)</ref> can be extended to apply to exchangeable matrix layers by noticing that each channel in an exchangeable matrix layer is analogous to a single unit in a standard feed-forward network. We therefore randomly drop out whole channels during training (as opposed to dropping out individual elements). This procedure is equivalent to the SpatialDropout technique used in convolutional networks <ref type="bibr" target="#b27">(Tompson et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Subsampling in Large Matrices</head><p>A key practical challenge arising from our approach is that our models are designed to take the whole data matrix X as input and will make different (and typically worse) predictions if given only a subset of the data matrix. As datasets grow, the model and input data become too large to fit within fixed-size GPU memory. This is problematic both during training and at evaluation time because our models rely on aggregating shared representations across data points to make their predictions. To address this, we explore two subsampling procedures.</p><p>Uniform sampling The simplest approach is to sample sub-matrices of X by uniformly sampling from its (typically sparse) elements. This has the advantage that each submatrix is an unbiased sample of the full matrix and that the procedure is computationally cheap, but has the potential to limit the performance of the model because the relationships between the elements of X are sparsified.</p><p>Conditional sampling Rather than sparsifying interactions between all set members, we can pick a subset of rows and columns and maintain all their interactions; see <ref type="figure" target="#fig_0">Figure 4</ref>. This procedure is unbiased as long as each element (n, m) ∈ I has the same probability of being sampled. To achieve this, we first sample a subset of rows N ′ ⊆ N = {1, . . . , N } from the marginal P (n) ∶= Rn I , followed by subsampling of colums using the marginal distribution over the columns, within the selected rows:</p><formula xml:id="formula_9">P (m N ′ ) = {(m,n)∈I n∈N ′ } {(m ′ ,n)∈I n∈N ′ } .</formula><p>This gives us a set of columns M ′ ⊆ M. We consider any observation within N ′ × M ′ as our subsample:</p><formula xml:id="formula_10">I sample ∶= {(n, m) ∈ I n ∈ N ′ , m ∈ M ′ }.</formula><p>This sampling procedure is more expensive than uniform sampling, as we have to calculate conditional marginal distributions for each set of samples. Sampling at test time At training time all we must ensure is that we sample in an unbiased way; however, at test time we need to ensure that the test indices I ts of large test matrix X ts are all sampled. Fortunately, according to the coupon collectors' lemma, in expectation we only need to repeat random sampling of indices log( I ts ) times more (≈ 10× in practice) than an ideal partitioning of I ts , in order to cover all the relevant indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Related Literature</head><p>The literature in matrix factorization and completion is vast. The classical approach to solving the matrix completion problem is to find some low rank (or sparse) approximation that minimizes a reconstruction loss for the observed ratings (see e.g., <ref type="bibr" target="#b3">Candès &amp; Recht, 2009;</ref><ref type="bibr" target="#b14">Koren et al., 2009;</ref><ref type="bibr" target="#b17">Mnih &amp; Salakhutdinov, 2008)</ref>. Because these procedures learn embeddings for each user and item to make predictions, they are transductive, meaning they can only make predictions about users and items observed during training. To our knowledge this is also true for all recent deep factorization and other collaborative filtering techniques (e.g., <ref type="bibr" target="#b23">Salakhutdinov et al., 2007;</ref><ref type="bibr" target="#b5">Deng et al.;</ref><ref type="bibr" target="#b25">Sedhain et al., 2015;</ref><ref type="bibr" target="#b28">Wang et al., 2015;</ref><ref type="bibr" target="#b16">Li et al., 2015;</ref><ref type="bibr" target="#b31">Zheng et al., 2016;</ref><ref type="bibr" target="#b7">Dziugaite &amp; Roy, 2015)</ref>. An exception is a recent work by <ref type="bibr" target="#b29">Yang et al. (2016)</ref> that extends factorization-style approaches to the inductive setting (where predictions can be made on unseen users and items). However their method relies on additional side information to represent users and items. By contrast, our approach is able to make inductive completions on rating matrices that may differ from that which was observed during training without using any side information (though our approach can easily incorporate side information).</p><p>Matrix completion may also be posed as predicting edge weights in bipartite graphs <ref type="bibr" target="#b1">(Berg et al., 2017;</ref><ref type="bibr" target="#b18">Monti et al., 2017)</ref>. This approach builds on recent work applying convolutional neural networks to graph-structured data <ref type="bibr" target="#b24">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b2">Bruna et al., 2014;</ref><ref type="bibr" target="#b6">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b4">Defferrard et al., 2016;</ref><ref type="bibr" target="#b13">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b9">Hamilton et al., 2017)</ref>. As we saw, parameter sharing in graph convolution is closely related to parameter sharing in exchangeable matrices, and indeed it is a special case where w 2 = w 3 in Equation <ref type="formula" target="#formula_3">(2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Results</head><p>For reproducibility we have released Tensorflow and Pytorch implementations of our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Transductive Setting (Matrix Interpolation)</head><p>Here, we demonstrate that exploiting the PE structure of the exchangeable matrices allows us to achieve results competitive with state-of-the-art, while maintaining a constant number of parameters. Note that the number of parameters in all the competing methods grow with N and/or M .</p><p>In <ref type="table" target="#tab_1">Table 2</ref> we report that the self-supervised exchangeable model is able to achieve state of the art performance on 2 Tensorflow: https://github.com/mravanba/ deep_exchangeable_tensors.</p><p>Pytorch: https: //github.com/jhartford/AutoEncSets. MovieLens-100K dataset. For MovieLens-1M dataset, we cannot fit the whole dataset into the GPU memory for training and therefore use conditional subsampling; also see Section 4.3. Our results on this dataset are summarized in <ref type="table" target="#tab_2">table Table 3</ref>. On this larger dataset both models gave comparatively weaker performance than what we observed on the smaller ML-100k dataset and in the extrapolation results. We suspect this is largely due to memory constraints: there is a trade-off between the size of the model (in terms of number of layers and units per layer) and the batch size one can train. We found that both larger batches and deeper models tended to offer better performance, but on these larger datasets it is not currently possible to have both. The results for the factorized exchangeable autoencoder architecture are similar and reported in the same table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-100K</head><p>MC <ref type="bibr" target="#b3">(Candès &amp; Recht, 2009)</ref> 0.973 GMC <ref type="bibr" target="#b12">(Kalofolias et al., 2014)</ref> 0.996 GRALS <ref type="bibr" target="#b21">(Rao et al., 2015)</ref> 0.945 sRGCNN <ref type="bibr" target="#b18">(Monti et al., 2017)</ref> 0.929 Factorized EAE (ours) 0.920 GC-MC <ref type="bibr" target="#b1">(Berg et al., 2017)</ref> 0.910 Self-Supervised Model (ours) 0.910 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-1M</head><p>PMF <ref type="bibr" target="#b17">(Mnih &amp; Salakhutdinov, 2008)</ref> 0.883 Self-Supervised Model (ours) 0.863 Factorized EAE (ours) 0.860 I-RBM <ref type="bibr" target="#b23">(Salakhutdinov et al., 2007)</ref> 0.854 BiasMF <ref type="bibr" target="#b14">(Koren et al., 2009)</ref> 0.845 NNMF <ref type="bibr" target="#b7">(Dziugaite &amp; Roy, 2015)</ref> 0.843 LLORMA-Local <ref type="bibr" target="#b15">(Lee et al., 2013)</ref> 0.833 GC-MC <ref type="bibr" target="#b1">(Berg et al., 2017)</ref> 0.832 I-AUTOREC <ref type="bibr" target="#b25">(Sedhain et al., 2015)</ref> 0.831 CF-NADE <ref type="bibr" target="#b31">(Zheng et al., 2016)</ref> 0.829 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inductive Setting (Matrix Extrapolation)</head><p>Because our model does not rely on any per-user or permovie parameters, it should be able to generalize to new users and movies that were not present during training. We tested this by training an exchangeable factorized autoencoder on the MovieLens-100k dataset and then evaluating it on a subsample of data from the MovieLens-1M dataset. At test time, the model was shown a portion of the new ratings and then made to make predictions on the remaining ratings. <ref type="figure">Figure 5</ref> summarizes the results where we vary the amount of data shown to the model from 5% of the new ratings up to 95% and compare against K-nearest neighbours approaches. Our model significantly outperforms the baselines in this task and performance degrades gracefully as we reduce the amount of data observed. <ref type="figure">Figure 5</ref>. Evaluation of our model's ability to generalize. We trained on ML-100k and evaluated on a subset of the ML-1M data. At evaluation time, p% of the ML-1M data was treated as observed and the model was required to complete the remaining (1 − p)% (p varied from 5% to 95%). The model outperforms nearest-neighbour approaches for all values of p and degrades gracefully to the baseline of predicting the mean in the small data case.</p><p>Extrapolation to new datasets Perhaps most surprisingly, we were able to achieve competitive results when training and testing on completely disjoint datasets. For this experiment we stress-tested our model's inductive ability by testing how it generalizes to new datasets without retraining. We used a Factorized Exchangeable Autoencoder that was trained to make predictions on the MovieLens-100k dataset and tasked it with making predictions on the Flixster, Douban and YahooMusic datasets. We then evaluated its performance against models trained for each of these individual datasets. All the datasets involve rating prediction tasks, so they share some similar semantics with Movie-Lens, but they have different user bases and (in the case of YahooMusic) deal with music not movie ratings, so we may expect some change in the rating distributions and user-item interactions. Furthermore, the Flixster ratings are in 0.5 point increments from 1 − 5 and YahooMusic allows ratings from 1 − 100, while Douban and MovieLens ratings are on 1 − 5 scale. To account for the different rating scales, we simply binned the inputs to our model to a 1 − 5 range and, where applicable, linearly rescaled the output before comparing it to the true rating 3 . Despite all of this, <ref type="table">Table 4</ref> shows that our model achieves very competitive results with models that were trained for the task.</p><p>For comparison, we also include the performance of a Factorized EAE trained on the respective datasets. This im-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Flixster Douban YahooMusic</head><p>Netflix GRALS <ref type="bibr" target="#b21">(Rao et al., 2015)</ref> 1.313 0.833 38.0 -sRGCNN <ref type="bibr" target="#b18">(Monti et al., 2017)</ref> 1.179 0.801 22.4 -GC-MC <ref type="bibr" target="#b1">(Berg et al., 2017)</ref> 0.941 0.734 20.5 -Factorize EAE (ours) 0.908 0.738 20.0 -Factorize EAE <ref type="bibr">(trained on ML100k)</ref> 0.987 0.766 23.3 0.918 Netflix Baseline ---0.951 PMF <ref type="bibr" target="#b17">(Mnih &amp; Salakhutdinov, 2008)</ref> ---0.897 LLORMA-Local <ref type="bibr" target="#b15">(Lee et al., 2013)</ref> ---0.834 I-AUTOREC <ref type="bibr" target="#b25">(Sedhain et al., 2015)</ref> ---0.823 CF-NADE <ref type="bibr" target="#b31">(Zheng et al., 2016)</ref> ---0.803 <ref type="table">Table 4</ref>. Evaluation of our model's ability to generalize across datasets. We trained a factorized model on ML100k and then evaluated it on four new datasets. Results for the smaller datasets are taken from <ref type="bibr" target="#b1">(Berg et al., 2017)</ref>. Netflix Baseline shows state of the art prior to the Netflix Challenge.</p><p>proves performance of our model over previous state of the art results on the Flixster and YahooMusic datasets and gives very similar performance to <ref type="bibr" target="#b1">Berg et al. (2017)</ref>'s GC-MC model on the Douban dataset. Interestingly, we see the largest performance gains over existing approaches on the datasets in which ratings are sparse (see <ref type="table">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison of sampling procedures</head><p>We evaluated the effect of subsampling the input matrix X on performance, for the MovieLens-100k dataset. The results are summarized in <ref type="figure">Figure 6</ref>. The two key findings are: I) even with large batch sizes of 20 000 examples, performance for both sampling methods is diminished compared to the full batch case. We suspect that our models' weaker results on the larger ML-1M dataset may be partially attributed to the need to subsample. II) the conditional sampling method was able to recover some of the diminished performance. We believe it is likely that more sophisticated sampling schemes that explicitly take into account the dependence between hidden nodes will lead to better performance but we leave that to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Extention to Tensors</head><p>In this section we generalize the exchangeable matrix layer to higher-dimensional arrays (tensors) and formalize its optimal qualities. Suppose X ∈ R N1×...×N D is our Ddimensional data tensor, and vec(X) its vectorized form. We index vec(X) by tuples (n 1 , n 2 , ..., n D ), corresponding to the original dimensions of X. The precise method of vectorization is irrelevant, provided it is used consistently. Let N = ∏ i N i and let n = (n i , n −i ) be an element of N 1 × ... × N D such that n i is the value of the i-th entry of n, and n −i the values of the remaining entries (where it is understood that the ordering of the elements of n is un- <ref type="figure">Figure 6</ref>. Performance difference between sampling methods on the ML-100k. The two sampling methods use minibatches of size 20 000, while the full batch method used all 75 000 training examples. Note that the y-axis does not begin at 0. <ref type="figure">Figure 7</ref>. Pooling structure implied by the tied weights for matrices (left) and 3D tensors (right). The pink cube highlights one element of the output. It is calculated as a function of the corresponding element from the input (dark blue), pooled aggregations over the rows and columns of the input (green and yellow), and pooled aggregation over the whole input matrix (red). In the tensor case (right), we pool over all sub-tensors (orange and purple submatrices, green sub-vectors and red scalar). For clarity, the output connections are not shown in the tensor case. changed). We seek a layer that is equivariant only to certain, meaningful, permutations of vec(X). This motivates our definition of an exchangeable tensor layer in a manner that is completely analogous to Definition 2.1 for matrices.  <ref type="bibr">N)</ref> . We want a layer that is equivariant to only those permutations in S (N1) × ... × S (N D ) , but not to any other member of S (N) .</p><formula xml:id="formula_11">∏ i N i objects. So S (N1) × ... × S (N D ) is a proper subgroup of S (N) having ∏ i (N i !) members, compared to (∏ i N i )! members in S (</formula><p>Definition 5.1 (exchangeable tensor layer) Let g (N) ∈ S (N1) × S (N2) × ... × S (N D ) and G (N) be the corresponding permutation matrix. Then the neural layer vec −1 (σ(W vec(X))) with X ∈ R N1×...×N D and W ∈ R N×N is an exchangeable tensor layer if permuting the elements of the input along any set of axes results in the same permutation of the output tensor: <ref type="formula">(5)</ref> and moreover for any other permutation of the elements X (i.e., permutations that are not along axes), there exists an input X for which this equality does not hold.</p><formula xml:id="formula_12">G (N) σ(W vec(X)) = σ(W G (N) vec(X)) ∀X,</formula><p>The following theorem asserts that a simple parameter tying scheme achieves the desired permutation equivariance for tensor-structured data.</p><p>Theorem 5.1 The layer Y = vec −1 (σ(W vec(X))), where σ is any strictly monotonic, element-wise nonlinearity (e.g., sigmoid), is an exchangeable tensor layer iff the parameter matrix W ∈ R N×N is defined as follows.</p><p>For each S ⊆ [D] = {1, . . . , D}, define a distinct parameter w S ∈ R, and tie the entries of W as follows</p><formula xml:id="formula_13">W n,n ′ ∶= w S s.t. n i = n ′ i ⇐⇒ i ∈ S.<label>(6)</label></formula><p>That is, the (n, n ′ )-th element of W is uniquely determined by the set of indices at which n and n ′ are equal.</p><p>In the special case that X ∈ R N1×N2 is a matrix, this gives the formulation of W described in Section 2. Theorem 5.1 says that a layer constructed in this manner is equivariant with respect to only those permutations of N objects that correspond to permutations of sub-tensors along the D dimensions of the input. The proof is in the Appendix. Equivariance with respect to permutations in S (N1) × S (N2) × ... × S (N D ) ) follows as a simple corollary of Theorem 2.1 in . Nonequivariance with respect to other permutations follows from the following Propositions.</p><p>Proposition 5.2 Let g (N) ∈ S (N) be an "illegal" permutation of elements of the tensor X -that is g</p><formula xml:id="formula_14">(N) ∈ S (N1) × S (N2) × ... × S (N D )</formula><p>. Then there exists a dimension i ∈ [D] such that, for some n i , n ′ i , n −i , n ′ −i :</p><formula xml:id="formula_15">g (N) ((n i , n −i )) = (n ′ i , n −i ), but g (N) ((n i , n ′ −i )) ≠ (n ′ i , n ′ −i ).</formula><p>If we consider the sub-tensor of X obtained by fixing the value of the i-th dimension to n i , we expect a "legal" permutation to move this whole subtensor to some n ′ i (it could additionally shuffle the elements within this subtensor.) This Proposition asserts that an "illegal" permutation g (N) ∈ S (N1) × S (N2) × ... × S (N D ) is guaranteed to violate this constraint for some dimension/subtensor combination. The next proposition asserts that if we can identify such inconsistency in permutation of sub-tensors then we can identify and entry in G (N) W that will differ from W G <ref type="bibr">(N)</ref> , and therefore for some input tensor X, the equivariance property is violated -i.e., G (N) σ(W vec(X)) ≠ σ(W G (N) vec(X)).</p><p>Proposition 5.3 Let g (N) ∈ S (N) with G (N) ∈ {0, 1} N×N the corresponding permutation matrix. Suppose g (N) ∈ S (N1) ×S (N2) ×...×S (N D ) , and let W ∈ R N×N be as defined above. If there exists an i ∈ [D], and some n i , n ′ i , n −i , n ′ −i such that</p><formula xml:id="formula_16">g (N) ((n i , n −i )) = (n ′ i , n −i ), and g (N) ((n i , n ′ −i )) ≠ (n ′ i , n ′ −i ), then G (N) W (n ′ i ,n ′ −i ),(ni,n−i) ≠ W G (N) (n ′ i ,n ′ −i ),(ni,n−i)</formula><p>Proposition 5.3 makes explicit a particular element at which the products G (N ) W and W G (N ) will differ, provided g (N)</p><p>is not of the desired form.</p><p>Theorem 5.1 shows that this parameter sharing scheme produces a layer that is equvariant to exactly those permutations we desire, and moreover, it is optimal in the sense that any layer having fewer ties in its parameters (i.e., more parameters) would fail to be equivariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper has considered the problem of predicting relationships between the elements of two or more distinct sets of objects, where the data can also be expressed as an exchangeable matrix or tensor. We introduced a weight tying scheme enabling the application of deep models to this type of data. We proved that our scheme always produces permutation equivariant models and that no increase in model expressiveness is possible without violating this guarantee. Experimentally, we showed that our models achieve state-ofthe-art or competitive performance on widely studied matrix completion benchmarks. Notably, our models achieved this performance despite having a number of parameters independent of the size of the matrix to be completed, unlike all other approaches that offer strong performance. Also unlike these other approaches, our models can achieve competitive results for the problem of matrix extrapolation: asking an already-trained model to complete a new matrix involving new objects that were unobserved at training time. Finally, we observed that our methods were surprisingly strong on various transfer learning tasks: extrapolating from Movie-Lens ratings to Fixter, Douban, and YahooMusic ratings. All of these contained different user populations and different distributions over the objects being rated; indeed, in the YahooMusic dataset, the underlying objects were not even of the same kind as those rated in training data.</p><p>• No equivariance wrt any other permutation: Let g (N) ∈ S (N) , with g (N) ∈ S (N1) × ⋅ ⋅ ⋅ × S (N D ) , and let G (N) be the corresponding permutation matrix. Using Propositions 5.2 and 5.3 we have:</p><formula xml:id="formula_17">G (N) W ≠ W G (N)</formula><p>So there exists an index at which these two matrices differ, call it (n, n ′ ). Then if we take vec(X) to be the vector of all 0's with a single 1 in position n ′ , we will have:</p><formula xml:id="formula_18">G (N) W vec(X) ≠ W G (N) vec(X).</formula><p>And since σ is assumed to be strictly monotonic, we have:</p><formula xml:id="formula_19">σ(G (N) W vec(X)) ≠ σ(W G (N) vec(X)).</formula><p>And finally, since G (N) is a permutation matrix and σ is applied element-wise, we have:</p><formula xml:id="formula_20">G (N) σ(W vec(X)) ≠ σ(W G (N) vec(X)).</formula><p>Therefore, the layer σ(W vec(X)) is not a exchangeable tensor layer, and the proof is completed.</p><p>This proves the first direction.</p><formula xml:id="formula_21">(⇒) We prove the contrapositive. Suppose W n,n ′ ≠ W m,m ′ for some n, n ′ , m, m ′ ∈ N 1 × ... × N D with {i ∶ n i = n ′ i } = {i ∶ m i = m ′ i }.</formula><p>We want to show that the layer Y = vec −1 (σ(W vec(X))) is not equivariant to some permutation g (N) ∈ S (N1) ×...×S (N D ) . We define this permutation as follows:</p><formula xml:id="formula_22">g (N) (ν) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ m if ν = n m ′ if ν = n ′ n if ν = m n ′ if ν = m ′ ν otherwise</formula><p>That is, g (N) "swaps" n with m and n ′ with m ′ . This is a valid permutation first since it acts element-wise, but also since {i ∶ n i = n ′ i } = {i ∶ m i = m ′ i } implies that n = n ′ iff m = m ′ (and so g (N) is injective, and thus bijective). So if G (N) is the permutation matrix of g (N) then we have (G (N) ) (n ′ ,n) = (G (N) ) (m ′ ,m) = 1, and: W (m,k) (G (N) ) (k,n ′ ) = (W G (N) ) (m,n ′ ) And so G (N) W ≠ W G (N) and by an argument identical to the above, with appropriate choice of X, this layer does not satisfy the requirements of an exchangeable tensor layer. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Proof of Theorem 2.1</head><p>A simple reparameterization allows us to write the matrix W of (6) in the form of (3). Thus Theorem 2.1 is just the special case of Theorem 5.1 where D = 2 and the proof follows from that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of Architecture and Training</head><p>SELF-SUPERVISED MODEL. Details of architecture and training: we train a simple feed-forward network with 9 exchangeable matrix layers using a leaky ReLU activation function. Each hidden layer has 256 channels and we apply a channel-wise dropout with probability 0.5 after the first to seventh layers. We found this channel-wise dropout to be crucial to achieving good performance. Before the input layer we mask out a proportion of the ratings be setting their values to 0 uniformly at random with probability 0.15. We convert the input ratings to one-hot vectors and interpret the model output as a probability distribution over potential rating levels. We tuned hyper-parameters by training on 75% of the data, evaluating on a 5% validation set. We test this model using the canonical u1.base/u1.test training/test split, which reserves 20% of the ratings for testing. For the MovieLens-1M dataset, we use the same architecture as for ML-100k and trained on 85% of the data, validating on 5%, and reserving 10% for testing. The limited size of GPU memory becomes an issue for this larger dataset, so we had to employ conditional sampling for training. At validation time we used full batch predictions using the CPU in order to avoid memory issues.</p><p>FACTORIZED EXCHANGEABLE AUTOENCODER MODEL. Details of architecture and training: we use three exchangeable matrix layers for the encoder. The first two have 220 channels, and the third layer maps the input to 100 features for each entry, with no activation function applied. This is followed by mean pooling along both dimensions of the input. Thus, each user and movie is encoded into a length 100 vector of real-valued latent features. The decoder uses five similar exchangeable matrix layers, with the final layer having five channels. We apply a channel-wise dropout with probability 0.5 after the third and fourth layers, which we again found to be crucial for good performance. We convert the input ratings to one-hot vectors and optimize using cross-entropy loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Uniform sampling (left) selects samples (red) uniformly from the non-zero indices of the the matrix X while conditional sampling (right) first samples a set of rows (shown in orange) from the row marginal distribution (green) and then selects sample from the resulting column conditional distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>any positive integer N , let S (N ) denote the symmetric group of all permutations of N objects. Then S (N1) × ... × S (N D ) refers to the product group of all permutations of N 1 through N D objects, while S (N) refers to the group of all permutations of N =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>G (N) W ) (m,n ′ ) = k∈[N1×...×N D ] (G (N) ) (m,k) W (k,n ′ ) = W (n,n ′ ) ≠ W (m,m ′ ) = k∈[N1×...×N D ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Comparison of RMSE scores for the MovieLens-100k dataset, based on the canonical 80/20 training/test split. Baseline numbers are taken from (Berg et al., 2017).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of RMSE scores for the MovieLens-1M dataset on random 90/10 training/test split. Baseline numbers are taken from<ref type="bibr" target="#b1">(Berg et al., 2017)</ref>.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This definition is simplified to ease exposition; the full definition (see Section 5) adds the additional constraint that the layer not be equivariant wrt any other permutation of the elements of X. Otherwise, a trivial layer with a constant weight matrix Wn,m = c would also satisfy the stated equivariance property.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Because of this binning procedure, our model received input data that is considerably coarser-grained than that which was used for the comparison models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We want to thank the anonymous reviewers for their constructive feedback. This research was enabled in part by support provided by NSERC Discovery Grant, WestGrid and Compute Canada.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Models of Interactions Across Sets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>• X ∈ R N1×...×N D , the data tensor • x ∈ R ∏ i Ni , vectorized X, also denoted by vec(X).</p><p>• [N ]: the sequence {n} n=1,...,N = (1, 2, ...N )</p><p>, both can refer to the matrix form of the permutation g Proof Let X ∈ R N1×...×N D be the data matrix. We prove the contrapositive by induction on D, the dimension of X.</p><p>. This means that, for any n = (n i , n −i ) = (n 1 , ..., n i , ..., n D ) the action g (N) takes on n i is independent of the action g (N) takes on n −i , the remaining dimensions. Thus we can write</p><p>Where it is understood that the order of the group product is maintained (this is a slight abuse of notation).</p><p>, and we are done. Otherwise, an inductive argument on g (N Ni) allows us to write g (N)</p><p>, completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proof of Proposition 5.3</head><p>Proof First, observe that</p><p>Now, let i ∈ [D] be such that, for some n i , n ′ i , n −i , n ′ −i we have g <ref type="bibr">(N)</ref> ((n i , n −i )) = (n ′ i , n −i ), and g <ref type="bibr">(N)</ref> ((n i , n ′ −i )) ≠ (n ′ i , n ′ −i ).</p><p>Then by the observation above we have:</p><p>The last line follows from the observation above and the fact that G (N) is a permutation matrix and so has only one 1 per row. Similarly,</p><p>Where again the last line follows from the above observation. Now, consider W (ni,n ′ −i ),(ni,n−i) and W (n ′ i ,n ′ −i ),(n ′ i ,n−i) . Observe that (n i , n ′ −i ) differs from (n i , n −i ) at exactly the same indices that (n ′ i , n ′ −i ) differs from (n ′ i , n −i ). Let S ⊆ [D] be the set of indices at which n −i differs from n ′ −i . We therefore have W (ni,n ′ −i ),(ni,n−i) = W (n ′ i ,n ′ −i ),(n ′ i ,n−i) = θ S , Which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Proof of Theorem 5.1</head><p>Proof We will prove both the forward and backward direction: (⇐) Suppose W has the form given by (6). We must show the layer is only equivariant with respect to permutations in S (N1) × ... × S (N D ) :</p><p>• Equivariance: Let g (N) ∈ S (N1) × ... × S (N D ) , and let G (N) be the corresponding permutation matrix. Then a simple extension of Theorem 2.1 in  implies G (N) W X = W G (N) X for all X, and thus the layer is equivariant. Intuitively, if g (N) ∈ S (N1) ×...×S (N D ) we can "decompose" g (N) (n) into D permutations S (N1) (n 1 ), ..., S (N D ) (n D ) which act independently on the D dimensions of X.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor decompositions for learning latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2773" to="2832" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exact matrix completion via convex optimization. Foundations of Computational mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Factorized variational autoencoders for modeling audience reactions to movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Navarathna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06443</idno>
		<title level="m">Neural network matrix factorization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for predicting human strategic behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2424" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.1717</idno>
		<title level="m">Matrix completion on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local low-rank matrix approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML)</title>
		<editor>Sanjoy Dasgupta and David McAllester</editor>
		<meeting>the 30th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep collaborative filtering via marginalized denoising auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bayesian models of graphs, arrays and other exchangeable random structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Orbanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical relational artificial intelligence: Logic, probability, and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="189" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Informa-tion Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">21072115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Equivariance through parameter-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poczos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
