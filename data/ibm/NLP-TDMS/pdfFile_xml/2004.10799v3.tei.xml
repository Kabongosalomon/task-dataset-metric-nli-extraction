<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Andrusenko</surname></persName>
							<email>andrusenko@speechpro.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ITMO University</orgName>
								<address>
									<settlement>St. Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Laptev</surname></persName>
							<email>laptev@speechpro.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ITMO University</orgName>
								<address>
									<settlement>St. Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Medennikov</surname></persName>
							<email>medennikov@speechpro.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ITMO University</orgName>
								<address>
									<settlement>St. Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">STC-innovations Ltd</orgName>
								<address>
									<settlement>St. Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: End-to-end speech recognition</term>
					<term>CHiME-6 Chal- lenge</term>
					<term>RNN-Transducer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While end-to-end ASR systems have proven competitive with the conventional hybrid approach, they are prone to accuracy degradation when it comes to noisy and low-resource conditions. In this paper, we argue that, even in such difficult cases, some end-to-end approaches show performance close to the hybrid baseline. To demonstrate this, we use the CHiME-6 Challenge data as an example of challenging environments and noisy conditions of everyday speech. We experimentally compare and analyze CTC-Attention versus RNN-Transducer approaches along with RNN versus Transformer architectures. We also provide a comparison of acoustic features and speech enhancements. Besides, we evaluate the effectiveness of neural network language models for hypothesis re-scoring in lowresource conditions. Our best end-to-end model based on RNN-Transducer, together with improved beam search, reaches quality by only 3.8% WER abs. worse than the LF-MMI TDNN-F CHiME-6 Challenge baseline. With the Guided Source Separation based training data augmentation, this approach outperforms the hybrid baseline system by 2.7% WER abs. and the end-to-end system best known before by 25.7% WER abs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the active development of deep learning techniques has allowed researchers to make a great performance improvement of Automatic Speech Recognition (ASR) systems. Although conventional hybrid ASR systems <ref type="bibr">[1]</ref> have been remaining preferred for a long time, their ever-increasing difficulty of construction and training <ref type="bibr" target="#b1">[2]</ref> has led to an interest in end-to-end approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Unlike HMM-DNN-based, these methods are trained to directly map an input acoustic feature sequence to a sequence of characters with a single objective function that is highly relevant to the final evaluation criteria, in particular, Word Error Rate (WER). The freedom from the necessity of intermediate modeling, such as acoustic and language models with pronunciation lexicons, makes the process of building the system clearer and more straightforward.</p><p>Also, there is an informal competition in accuracy between these systems <ref type="bibr" target="#b4">[5]</ref>. In most speech recognition tasks with good sound conditions and speech quality (e.g. LibriSpeech <ref type="bibr" target="#b5">[6]</ref>), end-to-end models' performance is great <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. However, in case of robust far-field speech recognition in noisy environments and with low resources, such models face problems <ref type="bibr" target="#b9">[10]</ref>. * Equal contribution.</p><p>There is a series of CHiME Challenges, which task is to encourage researchers to solve the problem of speech recognition in such conditions. According to the previous CHiME-5 Challenge <ref type="bibr" target="#b10">[11]</ref> reports, the leading positions in quality can be reached only by conventional hybrid systems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Attempts to get a somehow comparable result for end-to-end systems have not shown any notable success <ref type="bibr" target="#b14">[15]</ref>.</p><p>There is a new line of research aimed at solving a similar problem of the multichannel robust speech recognition task using the end-to-end approaches. Works such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> demonstrate the effectiveness of joint training of neuralnetwork-based front-end (speech enhancement) and back-end (speech recognition) models compared to the use of separate models. However, if the utterance boundaries are given, currently, the most effective approach is the speech enhancement technique based on spatial GMM blind source separation, named Guided Source Separation (GSS) <ref type="bibr" target="#b18">[19]</ref>. Thus, while maintaining the flexibility of end-to-end modeling, combining the same GSS front-end and end-to-end models might be capable of achieving the quality of conventional hybrid systems for a difficult CHiME-6 dinner party recognition task.</p><p>This paper describes an investigation of the aforementioned conjunction of techniques. We used the sixth CHiME Challenge 1 data, as it has an additional accurate array synchronization, and the baselines for speech enhancement and recognition. Our experimental setup was focused on the replacement of the hybrid recognition system from the baseline with an end-to-end system. We explored the effectiveness of RNN and Transformer architectures along with CTC-Attention and RNN-Transducer (RNN-T) training-decoding approaches.</p><p>This study also considers some extensions for the basic RNN-T model:</p><p>• Transformer-transducer <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. The idea behind this approach is to replace conventional LSTM/BLSTM encoder and predictor modules of the basic RNN-T implementation with the Transformer networks, which have proven effective in sequence modeling tasks.</p><p>• Improved beam search for RNN-T decoding <ref type="bibr" target="#b21">[22]</ref>. This is a modification of the standard beam search algorithm aimed at improving the computational decoding efficiency by using several hypothesis pruning heuristics.</p><p>• ASGD Weight-Dropped LSTM language model (AWD-LSTM-LM) <ref type="bibr" target="#b22">[23]</ref> for hypotheses re-scoring. This approach is to apply various regularization strategies to address overfitting caused by a low text data amount.</p><p>• The use of GSS-based speech enhancement for training and testing data <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Our main findings are available as an ESPnet CHiME-6 recipe 2 .</p><p>The rest of the paper is organized as follows. Section 2 provides an overview of common end-to-end ASR approaches. The CHiME-6 data and baseline are described in Section 3. Section 4 provides an experimental setup for the data and the strategies along with a discussion of our findings. Finally, Section 5 presents our conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">End-to-end ASR overview 2.1. CTC-Attention</head><p>Connectionist temporal classification (CTC), originally introduced in <ref type="bibr" target="#b2">[3]</ref>, in the ASR field is implied as a type of neural network output and an associated scoring function for training sequence-to-sequence (Seq2Seq) models. This approach is intended to relax the requirement of one-to-one mapping (alignment) between the input and output sequences. It uses an intermediate label representation with a special "blank" symbol indicating that no label is seen. It is also used when the label is repeated. One of the key features worth mentioning is the conditional label independence at the inference process, which hinders the effective usage of CTC models without an external language model.</p><p>The attention-based encoder-decoder approach, in contrast to CTC, incorporates contextual information by using both the input frames and the history of the target label for the inference process <ref type="bibr" target="#b3">[4]</ref>. It learns to predict the alignment between frames and labels using the attention mechanism. While generally performing better than CTC, attention-based approaches are more susceptible to noise and require more effort to train. It is proven effective to combine these approaches to alleviate their shortcomings and improve recognition quality <ref type="bibr" target="#b25">[26]</ref>.</p><p>Typical CTC-Attention architecture contains a deep RNN (mostly, bi-or unidirectional LSTM) encoder and a unidirectional RNN decoder with the optional use of an external language model (LM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformer</head><p>Examining ASR Seq2Seq models, it is worth mentioning the Transformer architecture. Originally proposed for neural machine translation (NMT) <ref type="bibr" target="#b26">[27]</ref>, it delivers generally better accuracy compared to RNN <ref type="bibr" target="#b27">[28]</ref>. The Transformer incorporates self-attention to utilize sequential information in contrast to the recurrent connection employed in RNN. Similar to traditional CTC-Attention, it uses joint CTC-and attention-based decoding.</p><p>The transformer architecture consists of both deep Multihead attention (MHA) encoder and decoder. To represent the time location, linear or convolutional positional encoding is applied before the encoder and decoder modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neural Transducer</head><p>The main alternative to the CTC-Attention approach is the neural transducer <ref type="bibr" target="#b28">[29]</ref>. In many details, this approach is similar to the CTC. They both use the "blank" label and compute the probability of all possible alignment paths during training to obtain the probability of the entire target transcription. Architecturally, having essentially the same encoder, the decoder of the neural transducer is different from the attention-based. It consists of:</p><p>• A prediction network, which plays the role of a language model.</p><p>• A joint network, intended for aligning audio and label sequences.</p><p>The standard neural transducer structure is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. An input acoustic feature sequence x = (x0, x1, x2, ..., xt) is passed to the Encoder and converted to a sequence of embeddings h = (h0, h1, h2, ..., h T ). In most cases, the Encoder performs subsampling by using convolutional layers, therefore T ≤ T . Next, the Joiner, which is a shallow fully connected (FC) neural network, receives the current Encoder embedding ht and predictor embedding gu to yield logits zt,u. The most probable label yu is determined by the Softmax. It is important that, since the Predictor practically works like a neural LM, it accepts only a non-blank yu−1. For the blank yu−1, it yields gu as if for the last non-blank yu−1.</p><p>Traditionally, most neural transducer studies investigate RNN-transducer. It consists of deep bi-or unidirectional LSTM encoder and a unidirectional LSTM predictor. According to <ref type="bibr" target="#b19">[20]</ref>, the neural transducer is able to reach better accuracy with the use of the Transformer as an architecture of encoder or predictor. Also, the Transformer-Transducer from <ref type="bibr" target="#b20">[21]</ref> outperformed the basic RNN-T model for the Librispeech task.</p><p>Apart from architecture improvement, the neural transducer beam search is also can be improved. Work <ref type="bibr" target="#b21">[22]</ref> introduces expand beam and state beam parameters to explicitly limit the beam size in the decoding process, and thus, to increase decoding efficiency. These parameters allow the search to reject "bad" hypotheses and to choose only the most probable, in terms of confidence, in the predictions of the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CHiME-6 dinner party transcription</head><p>The previous CHiME-5 Challenge featured fully transcribed speech materials collected with multiple 4-channel microphone arrays from real dinner parties that have taken place in real homes. Conversational speech with a large amount of overlapped segments recorded in reverberant and noisy conditions significantly complicates recognition. Details on the Challenge can be found in <ref type="bibr" target="#b10">[11]</ref>. CHiME-6 Challenge <ref type="bibr" target="#b29">[30]</ref> used the same recordings as the previous one, but improved initial data preparation and established the following strong baselines:</p><p>• Two stages array synchronization by frame-dropping and clock-drift. This allowed to obtain utterances with the same start and end time on every device.</p><p>• GSS-based speech enhancement <ref type="bibr" target="#b18">[19]</ref> applied to multiple arrays.</p><p>• Factorized time-delayed neural network (TDNN-F) acoustic model, trained with lattice-free maximum mutual information (LF-MMI), from Kaldi 3 toolkit.</p><p>This baseline demonstrated 51.76% WER on the development set by using train worn simu u400k cleaned rvb training set, which was obtained by various augmentations of 40 unique training data. Specific rooms simulation, speed, and volume perturbation increased the total amount of data to about 1400 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>We used the ESPnet 4 speech recognition toolkit <ref type="bibr" target="#b30">[31]</ref> as the main framework for our experiments. It supports most of the basic end-to-end models and training pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">End-to-end modeling</head><p>The first step of our investigation was to discover the most suitable end-to-end architecture for solving the task under consideration. Approaches like joint CTC-Attention, RNN-Transducer, and Transformer are the most popular for ASR tasks. Quasioptimal configurations of end-to-end architectures for our task are presented in <ref type="table" target="#tab_0">Table 1</ref>. "T-T" and "dp" stand for Transformer-Transducer and dropout, respectively. The Transformer network is used only in the Encoder in this architecture.</p><p>For all the presented approaches, we used a convolutional network (CNN) in front of the encoder. It consists of four Visual Geometry Group (VGG) CNN layers designed to compress the input frames in time scale by the factor of 4. Thus, after applying this network, the output features represent the transformed information from 4 initial frames. Although not originally intended, this approach allowed the model to converge more stably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental evaluation</head><p>We used the official data from the Kaldi baseline recipe: train worn simu u400k cleaned rvb and dev gss multiarray (or dev gss12 in our simplified notation) as the training and development sets, respectively.</p><p>Characters (26 letters of the English alphabet and 7 auxiliary symbols) were used as acoustic units. Other versions of word units, namely position-depending letters, Byte Pair Encoding (BPE) <ref type="bibr" target="#b31">[32]</ref> with different numbers of units 500, 1000, 3 https://github.com/kaldi-asr/kaldi 4 https://github.com/espnet/espnet 2000 led only to performance degradation. This might be due to the lack of training data for such large unit numbers. The next step was to choose the input acoustic features. The hybrid baseline model used 40-dimensional high-resolution MFCC vectors (hires) concatenated with 100-dimensional ivectors. However, such features may not be the best option for an end-to-end model. We considered the following features: 80-dimensional log-Mel filterbank coefficients (fbank) with or without 3-dimensional pitch features and 512-dimensional wav2vec context network output vectors <ref type="bibr" target="#b32">[33]</ref>. A comparison of these acoustic features with the RNN-T model configured as in <ref type="table" target="#tab_0">Table 1</ref> is shown in <ref type="table" target="#tab_1">Table 2</ref>. All decoding results were obtained using the beam size of 10. Limited computing power did not allow us to train the wav2vec features extractor for a sufficient number of epochs (the model was trained only in 6 epochs), which apparently caused such a bad result. The underperforming of hires+ivectors against the single hires might be due to the VGG network usage. All further experiments were carried out using fbank+pitch features.</p><p>Having settled on 33 character acoustics units and the fbank+pitch acoustics features, we compared the end-to-end architectures from the <ref type="table" target="#tab_0">Table 1</ref>. We also used SpecAugment <ref type="bibr" target="#b33">[34]</ref> to further augment the training data. The results of this comparison are presented in <ref type="table" target="#tab_2">Table 3</ref>. The results show that the RNN-T demonstrates the best WER in this task. It is worth noting the great positive impact of SpecAugment. The applying of this augmentation reduced WER by 4.2% for the RNN-T model. This suggests the importance of augmentation methods for low-resource tasks. Models that utilize the Transformer architecture (T-T, Transformer) perform worse than LSTM due to severe overfitting.</p><p>We also faced a problem in the re-scoring process using external RNN-based language models. Character and word-based LSTM LMs (1-layer 1024-units) were trained on the training data transcriptions only. However, the use of these LMs for hypothesis re-scoring in the beam search algorithm for RNN-T only resulted in reduced recognition accuracy. We also used the pre-trained AWD-LSTM-LM, which showed a significant WER improvement for lattice re-scoring in the STC CHiME-6 system <ref type="bibr" target="#b24">[25]</ref>. Unfortunately, none of these models improved accuracy. Apart from the beam search rescoring, we tried to use the n-best rescoring approach, but the n-best hypotheses produced from the development set utterances did not have significant variability in words. Thus, this rescoring method did not improve the result either. We have two assumptions for this to happen. The first one is that in case of a low-resource task, with properly tuned Predictor network parameters can perform sufficiently good without using any re-scoring techniques. The second one is that the beam search algorithm we used is not optimal for the hypothesis re-scoring. For example, in the case of a hybrid system decoding, there are separate acoustic and language model scores of a word unit. And in the corresponding re-scoring algorithm, the language score is re-weighted according to the external LSTM LM. But in the case of RNN-T, there is only one single score for word unit. The default ESPnet rescoring is simply a weighted sum of the external LSTM LM and RNN-T scores. The results of hypothesis re-scoring with an external NNLM are demonstrated in <ref type="table" target="#tab_3">Table 4</ref>. We used the beam size of 10 and the NNLM weight of 0.3 for all experiments. The improved beam search algorithm, implemented according to <ref type="bibr" target="#b21">[22]</ref>, demonstrated results beyond expectations. Values expand beam and state beam handpicked as 2 and 1, respectively, allowed to accelerate the decoding process by an average of 15-20% with a simultaneous improvement in the recognition quality by 0.5 absolute WER, thus delivering 55.00 WER for the dev gss12. We assume that the WER improvement was due to a decrease in the impact of the Predictor overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model WER(%)</head><p>Joint CTC/Attention E2E <ref type="bibr" target="#b14">[15]</ref> 82.1 CNN-based Multichannel E2E <ref type="bibr" target="#b15">[16]</ref> 80.7 CHiME-6 TDNN-F baseline <ref type="bibr" target="#b29">[30]</ref> 51.7</p><p>RNN-T + dev gss12 55.0 RNN-T + train gss + dev gss12 52.6 RNN-T + train gss + dev gss24 49.0</p><p>Hybrid system (n-gram LM) <ref type="bibr" target="#b24">[25]</ref> 36.8 Hybrid system (AWD-LSTM-LM) <ref type="bibr" target="#b24">[25]</ref> 33.8</p><p>We also applied additional GSS-based speech enhancement for the training (train gss) data according to <ref type="bibr" target="#b24">[25]</ref> as well as improved 24-microphone GSS enhancement for the development data (dev gss24).</p><p>The final comparison of some of the currently published end-to-end models, the official baseline, and our RNN-T models is presented in <ref type="table" target="#tab_4">Table 5</ref> (only the CHiME-6 development data WER results were considered). Note that results from <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref> are obtained for the CHiME-5 data, i.e. without improvements mentioned in Section 3. To demonstrate a gap between our system and one of the best hybrid systems known to us at the moment, we also placed in the comparison the best single model from the STC CHiME-6 system <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we presented an end-to-end systems exploration of the robust far-field speech recognition in noisy environments and low resources such as the CHiME-6 dinner party transcription task. We showed that the combination of powerful GSSbased speech enhancement and the use of the RNN-Transducer model is able to achieve competitive results with hybrid systems. With the improved beam search algorithm and addition speech enhancement, our system outperformed the hybrid baseline system by 2.7% WER abs.</p><p>Further research might be the study on how, for the task under consideration, to achieve a recognition accuracy improvement when re-scoring hypotheses using external NNLM. Also, it would be interesting to incorporate GSS-based enhancement in the end-to-end pipeline to train the system jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Neural Transducer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Models configuration</figDesc><table><row><cell>CTC-Attention</cell><cell></cell></row><row><cell>Encoder</cell><cell>VGG-BLSTM, 6-layer 512-units, dp 0.4</cell></row><row><cell>Attention</cell><cell>1-head 256-units, dp 0.4</cell></row><row><cell>Decoder</cell><cell>LSTM, 2-layer 256 units, dp 0.4</cell></row><row><cell>RNN-T</cell><cell></cell></row><row><cell>Encoder</cell><cell>VGG-BLSTM, 6-layer 512-units, dp 0.4</cell></row><row><cell>Predictor</cell><cell>LSTM, 2-layer 256-units, dp 0.4</cell></row><row><cell>Joiner</cell><cell>FC 256 units</cell></row><row><cell>T-T</cell><cell></cell></row><row><cell>Encoder</cell><cell>MHA, 12-layer 1024-units, dp 0.4</cell></row><row><cell>Attention</cell><cell>8-head 512-units, dp 0.4</cell></row><row><cell>Predictor</cell><cell>LSTM, 2-layer 256-units, dp 0.4</cell></row><row><cell>Joiner</cell><cell>FC 256 units</cell></row><row><cell>Transformer</cell><cell></cell></row><row><cell>Encoder</cell><cell>MHA, 12-layer 1024-units, dp 0.5</cell></row><row><cell>Attention</cell><cell>4-head 256-units, dp 0.5</cell></row><row><cell>Decoder</cell><cell>MHA, 2-layer 1024 units, dp 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>RNN-T features comparison</figDesc><table><row><cell>Feats</cell><cell cols="2">Dimension WER(%)</cell></row><row><cell>wav2vec</cell><cell>512</cell><cell>68.3</cell></row><row><cell>hires+i-vectors (baseline)</cell><cell>40+100</cell><cell>64.1</cell></row><row><cell>hires</cell><cell>40</cell><cell>63.6</cell></row><row><cell>fbank</cell><cell>80</cell><cell>60.4</cell></row><row><cell>fbank+pitch</cell><cell>80+3</cell><cell>59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of end-to-end models</figDesc><table><row><cell></cell><cell cols="4">CTC-Att RNN-T T-T Transf.</cell></row><row><cell>WER(%)</cell><cell>73.8</cell><cell>55.5</cell><cell>60.3</cell><cell>86.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>External NNLM for RNN-T beam search</figDesc><table><row><cell>NNLM</cell><cell>WER(%)</cell></row><row><cell>-</cell><cell>55.5</cell></row><row><cell>Char NNLM</cell><cell>57.5</cell></row><row><cell>Word NNLM</cell><cell>57.4</cell></row><row><cell>AWD-LSTM</cell><cell>56.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The final models' performance comparison on the development set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/espnet/espnet/tree/ master/egs/chime6/asr1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We would like to show our sincere gratitude to A. Mitrofanov, I. Podluzhny, and A. Romanenko for sharing their developments in external NNLM modeling for CHiME-6 data. We also want to thank the ESPnet development team for the excellent opensource ASR toolkit that helped us doing this research. This work was partially financially supported by the Government of the Russian Federation (Grant 08-08). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine, IEEE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The stc asr system for the voices from a distance challenge 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Khokhlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sorokin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning -ICML. Pittsburgh</title>
		<meeting>the 23rd international conference on Machine learning -ICML. Pittsburgh<address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rwth asr systems for librispeech: Hybrid vs attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end asr: From supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08460</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Jasper: An end-to-end convolutional neural acoustic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<editor>Interspeech. ISCA</editor>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring end-to-end techniques for low-resource speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bataev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korenevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zatvornitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Speech and Computer</title>
		<imprint>
			<biblScope unit="page">3241</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The fifth &apos;CHiME&apos; speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<editor>Interspeech. ISCA</editor>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="1561" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The USTC-iFlytek systems for CHiME-5 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME 2018 Workshop on Speech Processing in Everyday Environments. ISCA</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Hitachi/JHU CHiME-5 system: Advances in speech recognition for everyday home environments using multiple microphone arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ikeshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME 2018 Workshop on Speech Processing in Everyday Environments. ISCA</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The STC system for the CHiME 2018 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME 2018 Workshop on Speech Processing in Everyday Environments. ISCA</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Situation informed end-toend ASR for noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME 2018 Workshop on Speech Processing in Everyday Environments. ISCA</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CNNbased multichannel end-to-end speech recognition for everyday home environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An investigation of end-to-end multichannel speech recognition for reverberant and mismatch conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Taniguchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09049</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mimo-speech: End-to-end multi-channel multi-speaker speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="237" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Front-end processing for the CHiME-5 dinner party scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boeddecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmalenstroeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME 2018 Workshop on Speech Processing in Everyday Environments. ISCA</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Self-attention transducers for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<editor>Interspeech. ISCA</editor>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Transformer-transducer: End-to-end speech recognition with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kalgaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12977</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rnnt for latency controlled asr with improved beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01629</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An investigation into the effectiveness of enhancement in ASR training and test for CHiME-5 dinner party transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zorila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). SG</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="47" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The stc system for the chime-6 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korenevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Prisyach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Khokhlov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME 2020 Workshop on Speech Processing in Everyday Environments</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparative study on Transformer vs RNN in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). SG</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09249</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ESPnet: Endto-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2018. ISCA</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<editor>Interspeech. ISCA</editor>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="3465" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
