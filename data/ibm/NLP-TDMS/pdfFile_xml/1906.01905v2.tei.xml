<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Baby Steps Towards Few-Shot Learning with Multiple Semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Technion</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Baby Steps Towards Few-Shot Learning with Multiple Semantics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning from one or few visual examples is one of the key capabilities of humans since early infancy, but is still a significant challenge for modern AI systems. While considerable progress has been achieved in few-shot learning from a few image examples, much less attention has been given to the verbal descriptions that are usually provided to infants when they are presented with a new object. In this paper, we focus on the role of additional semantics that can significantly facilitate few-shot visual learning. Building upon recent advances in few-shot learning with additional semantic information, we demonstrate that further improvements are possible by combining multiple and richer semantics (category labels, attributes, and natural language descriptions). Using these ideas, we offer the community new results on the popular miniImageNet and CUB few-shot benchmarks, comparing favorably to the previous state-of-theart results for both visual only and visual plus semantics-based approaches. We also performed an ablation study investigating the components and design choices of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern day computer vision has experienced a tremendous leap due to the advent of deep learning (DL) techniques. The DL-based approaches reach higher levels of performance even compared to humans in tasks requiring expertise, such as recognizing dog breeds, or faces of thousands of celebrities. Yet, despite all the advances, some innate human abilities available to us at a very young age, still elude modern AI systems. One of these abilities is to be able to learn and later successfully recognize new, previously unseen, visual categories when presented to us with one or very few examples. This 'few-shot learning' task has been thoroughly explored in the computer vision literature and numerous approaches have *The authors have contributed equally to this work Corresponding authors: Eli Schwartz elisch@ibm.com; Leonid Karlinsky leonidka@il.ibm.com been proposed (please see <ref type="bibr" target="#b3">[4]</ref> for a recent review). Yet so far, the performance of even the best few-shot learning methods fall short by a significant margin from the performance of the fully supervised learning methods trained with a large number of examples (for example ImageNet <ref type="bibr" target="#b25">[26]</ref>).</p><p>One important ingredient of human infant learning, which has only very recently found its way into the visual few-shot learning approaches, is the associated semantics that comes with the provided example. For example, it has been shown in the child development literature that infants' object recognition ability is linked to their language skills and it is hypothesized that it might be related to the ability to describe objects <ref type="bibr" target="#b20">[21]</ref>. Indeed, when a parent points a finger at a new category to be learned ('look, here is a puppy', figure 1), it is commonly accompanied by additional semantic references or descriptions for that category (e.g., 'look at his nice fluffy ears', 'look at his nice silky fur', 'the puppy goes woof-woof'). This additional, and seldom rich, semantic information can be very useful to the learner, and has been exploited in the context of zero-shot learning and visual-semantic embeddings. Indeed, language as well as vision domains, both describe the same physical world in different ways, and in many cases contain useful complementary information that can be carried over to the learner in the other domain (visual to language and vice versa).  <ref type="bibr">(1 âˆ’ )</ref> task CE loss branch #1 task CE loss branch #k task CE loss visual <ref type="figure">Figure 2</ref>: The proposed model, best viewed in color. Some connecting lines are excluded for brevity. Filled boxes represent neural nets and losses, bluish nets are jointly learned as part of our approach, yellowish ones are for word / sentence embedding and are pre-trained and fixed. Please see section 3 for details.</p><p>In the recent few-shot learning literature, the additional power of using semantics to facilitate few-shot learning was realized in only a handful of works. In <ref type="bibr" target="#b4">[5]</ref> an embedding vector of either the category label or of the given set of category attributes is used to regularize the latent representation of an auto-encoder TriNet network by adding a loss for making the sample latent vector as close as possible to the corresponding semantic vector. In <ref type="bibr" target="#b36">[37]</ref> the semantic representation of visual categories is learned on top of the GloVe <ref type="bibr" target="#b22">[23]</ref> word embedding, jointly with a Proto-Net <ref type="bibr" target="#b29">[30]</ref> based few-shot classifier, and jointly with the convex combination of both. The result of this joint training is a powerful few-shot and zero-shot (that is a semantic-based) ensemble that surpassed the performance of all other few-shot learning methods to-date on the challenging miniImageNet few-shot learning benchmark <ref type="bibr" target="#b31">[32]</ref>. In both of these cases, combining few-shot learning with some category semantics (labels or attributes) proved highly beneficial to the performance of the few-shot learner. Yet in both cases, only the simple one word embedding or a set of several prescribed numerical attributes were used to encode the semantics.</p><p>In this work, we show that more can be gained by exploring a more realistic human-like learning setting. This is done by providing the learner access to multiple and richer semantics.</p><p>These semantics can include, depending on what available for the dataset: category labels; richer 'description level' semantic information (a sentence, or a few sentences, in a natural language with a description of the category); or attributes. We demonstrate how this learning with semantic setting can facilitate few-shot learning (leveraging the intuition of how human infants learn). The results compare favorably to the previous visual and visual + semantics state-of-the-art results on the challenging miniImageNet <ref type="bibr" target="#b31">[32]</ref> and CUB <ref type="bibr" target="#b35">[36]</ref> fewshot benchmarks.</p><p>To summarize, the contributions of this work are threefold. First, we propose the community to consider a new, perhaps closer to 'infant learning' setting of Few-Shot Learning with Multiple and Complex Semantics (FSL-MCS). Second, in this context we propose a new benchmark for FSL-MCS, and an associated training and evaluation protocol. Third, we propose a new multi-branch network architecture that provides the first batch of encouraging results for the proposed FSL-MCS setting benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-Shot Learning</head><p>The major approaches to few-shot learning include: metric learning, meta learning (or learning-to-learn), and generative (or augmentation) based methods.</p><p>Few-shot learning by metric learning: this type of methods <ref type="bibr">[30; 25]</ref> learn a non-linear embedding into a metric space where L 2 nearest neighbor (or similar) approach is used to classify instances of new categories according to their proximity to the few labeled training examples. Additional proposed variants include <ref type="bibr" target="#b11">[12]</ref> that uses a metric learning method based on graph neural networks, that goes beyond the L 2 metric. Similarly, <ref type="bibr">[28; 31]</ref> introduce metric learning methods where the similarity is computed by an implicit learned function rather than via the L 2 metric over an embedding space.</p><p>The embedding space based metric-learning approaches are either posed as a general discriminative distance metric learning <ref type="bibr" target="#b24">[25]</ref>, or optimized on the few-shot tasks <ref type="bibr">[30; 12]</ref>, via the meta-learning paradigm that will be described next. These approaches show a great promise, and in some cases are able to learn embedding spaces with quite meaningful semantics embedded in the metric <ref type="bibr" target="#b24">[25]</ref>. The higher end of the performance spectrum for the metric learning based approaches has been achieved when combining these approaches with some additional semantic information. In <ref type="bibr" target="#b15">[16]</ref> class conditioned embedding is used, and in <ref type="bibr" target="#b36">[37]</ref> the visual prototypes are refined using a corresponding label embedding.</p><p>Few-shot meta-learning (learning-to-learn): These methods are trained on a set of few-shot tasks (also known as 'episodes') instead of a set of object instances, with the motivation to learn a learning strategy that will allow effective adaptation to new such (few-shot) tasks using one or few examples. An important sub-category of meta learning methods is metric-meta-learning, combining metric learning as explained above with task-based (episodic) training of metalearning. In Matching Networks <ref type="bibr" target="#b31">[32]</ref>, a non-parametric k-NN classifier is meta-learned such that for each few-shot task the learned model generates an adaptive embedding space for which the task can be better solved. In <ref type="bibr" target="#b29">[30]</ref> the metric (embedding) space is optimized such that in the resulting space different categories form compact and well separated unimodal distributions around the category 'prototypes' (centers of the category modes). Another family of meta learning approaches is the so-called 'gradient based approaches', that try to maximize the 'adaptability', or speed of convergence, of the networks they train to new (few-shot) tasks (usually assuming an SGD optimizer). In other words, the meta-learned classifiers are optimized to be easily fine-tuned on new few-shot tasks using small training data. The first of these approaches is MAML <ref type="bibr" target="#b8">[9]</ref> that due to its universality was later extended through many works such as, Meta-SGD <ref type="bibr" target="#b18">[19]</ref>, DEML+Meta-SGD <ref type="bibr" target="#b41">[42]</ref>, and Meta-Learn LSTM <ref type="bibr" target="#b23">[24]</ref>. In MetAdapt <ref type="bibr" target="#b6">[7]</ref> a model is trained to predict the optimal classifier architecture and adapt it at test time to the given task. In LEO <ref type="bibr" target="#b26">[27]</ref> a MAML like loss is applied not directly on the model parameters, but rather on a latent representation encoding them.</p><p>Generative and augmentation-based few-shot: This family of approaches refers to methods that (learn to) generate more samples from the one or a few examples available for training in a given few-shot learning task. These methods include synthesizing new data from few exam-ples using a generative model, or using external data for obtaining additional examples that facilitate learning on a given few shot task. These approaches include: (i) semisupervised approaches using additional unlabeled data <ref type="bibr" target="#b10">[11]</ref>; (ii) fine tuning from pre-trained models <ref type="bibr" target="#b33">[34]</ref>; (iii) applying domain transfer by borrowing examples from relevant categories <ref type="bibr" target="#b19">[20]</ref> or using semantic vocabularies <ref type="bibr" target="#b1">[2]</ref>; (iv) rendering synthetic examples <ref type="bibr" target="#b21">[22]</ref>; (v) augmenting the training examples using geometric and photometric transformations <ref type="bibr" target="#b16">[17]</ref> or learning adaptive augmentation strategies <ref type="bibr" target="#b12">[13]</ref>; (vi) example synthesis using Generative Adversarial Networks. In <ref type="bibr">[14; 29]</ref> additional examples are synthesized via extracting, encoding, and transferring to the novel category instances, of the intra-class relations between pairs of instances of reference categories. Notably, in <ref type="bibr">[5; 39]</ref> label and attribute semantics are used as additional information for training an example synthesis network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Zero-Shot Learning</head><p>A closely related task is zero-shot learning (ZSL). In the ZSL setting only the semantics information is given for the unseen classes. Usually, in ZSL the relation between semantic representations and visual representations are model. Either by mapping semantics to visual <ref type="bibr" target="#b39">[40]</ref>, visual to semantics <ref type="bibr" target="#b9">[10]</ref>, or mapping both to a common embedding space <ref type="bibr" target="#b40">[41]</ref>. Other methods use generative models for generating visual samples given the given semantics, e.g. <ref type="bibr" target="#b2">[3]</ref>, these generated samples can be used to train a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our approach builds upon the work of <ref type="bibr" target="#b36">[37]</ref>. Our general model architecture is summarized in <ref type="figure">Figure 2</ref>. It has been shown recently that pre-training the backbone in a standard supervised fashion is good for performance, i.e. simple linear classifier with a softmax non-linearity predicting probabilities for all training categories (no episodic training), for example <ref type="bibr" target="#b32">[33]</ref>. For this reason we split the training to two phases. In the first phase we perform the fully supervised training of the CNN backbone on the training categories (training procedure similar to <ref type="bibr" target="#b32">[33]</ref>). At the second phase, the last layer (linear classifier) is discarded and replaced by 2-layer MLP and all previous backbone layers are freezed, and we add the semantics branches. The full model is then trained using the episode-based meta-learning approach proposed by <ref type="bibr" target="#b31">[32]</ref>. The training is performed on few-shot tasks (episodes) comprised of one or few image examples for each of the task categories (the so-called support set), as well as one or several query images belonging to these categories (the so-called query set). Each task is simulating a few-shot learning problem. In addition, for our multiple semantics approach, each task is accompanied by semantic information (label and/or description sentence and/or attributes) for each of the task categories. For the labels we use the GloVe embedding <ref type="bibr" target="#b22">[23]</ref> and for descriptions the BERT embedding <ref type="bibr" target="#b5">[6]</ref>, as we observed GloVe performs better for words and BERT for sentences.</p><p>The model is comprised of a visual information branch supported by a CNN backbone computing features both for the training images of the few-shot task and for the query images. As in Proto-Nets <ref type="bibr" target="#b29">[30]</ref>, the feature vectors for each set of the task category support examples are averaged to form a visual prototype feature vector V for that category. In addition, the model contains one or more "semantic branches" for learning to incorporate the additional semantic information. Each semantic branch starts with a pre-trained word or sentence embedding feature extractor (or just a vector in case of attributes), followed by an Multi Layer Perceptron (MLP) generating a "semantic prototypes" S i to be combined with the corresponding (same category) visual prototype. For the sake of this combination, each semantic branch is equipped with an MLP calculating "semantic attention" -a coefficient Î± i of the semantic prototype of the branch in the overall convex combination of the category prototypes. For computing Î± i , the attention MLP for each branch can be set to receive as an input one of the task category prototypes generated by either the visual or the semantic branches. We examine the effect of different inputs to the attention MLP of different semantic branches in the ablation study section 4.3 below.</p><p>Optionally, our model also allows for adding into the convex combination additional branches with visual prototypes V attended by either of the S i or V itself. Finally, our model features a task specific cross-entropy loss on the prototype resulting from each (semantic or visual) branch which allows for providing intermediate level supervision for each branch output using the ground truth labels associated to the few-shot tasks (episodes) used for meta-training. These losses admit the softmax normalized logits computed as negative distances between the task query samples and the prototypes produced by each respective semantic (or visual) branch.</p><p>To summarize, for each task category, each semantic branch is uniquely determined by its two inputs -the semantic information being processed into the semantic prototype S i (category label, category description, or attributes vector), and the prototype (visual or semantic) being processed into the semantic attention coefficient Î± i . The final prototype P for a category in a given few shot task with an associated visual prototype V and semantic prototypes {S 1 , ..., S k } is computed as:</p><formula xml:id="formula_0">P = V Â· k i=1 Î± i + k i=1 ï£® ï£° S i Â· (1 âˆ’ Î± i ) Â· k j=i+1 Î± j ï£¹ ï£» (1)</formula><p>(please see <ref type="figure">Fig 2 for</ref> the intuitive visualization of Eq. 1). The final category prototype P is then compared to the query visual feature vector Q (produced by the CNN backbone) for computing the category probability as prob(Q, P ) = SM (âˆ’||P âˆ’ Q|| 2 ), where SM stands for the softmax normalization operator.</p><p>Assuming the correct category for the query Q has visual prototype V and semantic prototypes {S 1 , ..., S k }, than the final training loss incorporating the CE losses for all the visual and semantic branches can be written as:</p><formula xml:id="formula_1">Loss = âˆ’ log(prob(Q, V )) + k r=1 âˆ’log(prob(Q, P r )) (2)</formula><p>where P r is the output of the partial computation of equation 1 up until the semantic branch #r:</p><formula xml:id="formula_2">P r = V Â· r i=1 Î± i + r i=1 ï£® ï£° S i Â· (1 âˆ’ Î± i ) Â· r j=i+1 Î± j ï£¹ ï£» (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation details</head><p>The CNN backbone is based on the implementation of <ref type="bibr" target="#b32">[33]</ref> and trained using their provided code and hyper-parameters. The full model combining visual features with multiple semantics is built on top and extends the code provided by the authors of <ref type="bibr" target="#b36">[37]</ref>, also keeping their hyper-parameter setting. The main differences compared to <ref type="bibr" target="#b36">[37]</ref> are: the addition of multiple semantic branches, allowing their crossconditioning and their cascaded merge scheme; adding intermediate losses after each branch; and pre-training the base layers of the CNN backbone without meta-training as a regular multi-class classifier to all the base classes (and freezing the backbone after pre-training). Our code will be made available upon acceptance.</p><p>In our experiments we use the DenseNet-121 backbone CNN <ref type="bibr" target="#b14">[15]</ref>. After pre-training the linear classifier is replaced by MLP with 512-features hidden and output layers. For each semantic branch, the semantic backbone is a two-layer MLP with a 300-sized hidden layer, and 512-sized output layer. The semantic attention for each branch is a two-layer MLP with a 300-sized hidden layer and a scalar output layer followed by a sigmoid (to normalize the coefficients into a [0, 1] range). All semantic branches MLPs include a dropout layer with 0.7 rate between the hidden layer and the output layer.</p><p>After the pre-training of the CNN backbone, all other layers (visual and semantics branches) are trained jointly using the per branch Cross Entropy losses (applied to the predicted logits after a softmax for each branch). The training is performed using only the training subset of the categories of the few-shot dataset. All parameters are randomly initialized (random normal initialization for the weights and a constant zero for the biases). The category descriptions miniImageNet are obtained automatically from WordNet (see examples in <ref type="table">Table 1</ref>). For CUB we sampled 10 random descriptions per category from those used in <ref type="bibr" target="#b37">[38]</ref> and used their mean as the category embedding.  For miniImageNet the semantics used are category labels and textual descriptions; for CUB they are category labels, textual descriptions and attributes. For 1-shot, we observe significant improvement by using multiple semantics. For 5-shot, since the visual information is more reliable semantic information is not as helpful (as observed in previous works). For context we also report human performance of one of the authors and his daughter. The adult performance is very high mainly due to prior familiarity with the categories in question.</p><p>(* Our run of the code released by the authors)</p><p>We have evaluated our approach on the challenging fewshot benchmark of miniImageNet <ref type="bibr" target="#b31">[32]</ref> used for evaluation by most (if not all) the few-shot learning works. We also evaluated on the CUB dataset <ref type="bibr" target="#b34">[35]</ref> which includes another form of semantics, the attributes vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>miniImageNet <ref type="bibr" target="#b31">[32]</ref> is a subset of the ImageNet dataset <ref type="bibr" target="#b25">[26]</ref>. It contains 100 randomly sampled categories, each with 600 images of size 84 Ã— 84. We have used the standard evaluation protocol of <ref type="bibr" target="#b23">[24]</ref> and evaluated the 1-shot and the 5-shot performance of our method in a 5-way scenario (that is having 1 or 5 training examples for each of the 5 categories in the support set), using 64 categories for training, 16 for validation, and 20 for test. For testing we used 1000 random test episodes (sampled from the test categories unseen during training). The semantics used for each category were the labels and descriptions of the category label in WordNet.</p><p>Caltech Birds (CUB) <ref type="bibr" target="#b34">[35]</ref> is a dataset consists of 11, 788 images of birds of 200 species. We use the standard train, validation, and test splits, which were created by randomly splitting the 200 species into 100 for training, 50 for validation, and 50 for testing. All images are downsampled to 84Ã—84. For testing we used 1000 random test episodes (sampled from the test categories unseen during training). The semantics used for each category were the labels, attributes (from the original CUB dataset), and descriptions of the categories from <ref type="bibr" target="#b37">[38]</ref>. <ref type="table" target="#tab_3">Table 2</ref> summarizes the results of our approach applied to miniImageNet and CUB, compared to the state-of-theart results with and without using semantics. For both miniImageNet and CUB we used 3 semantic branches in our model. For CUB, the first branch input is label embedding, the second is attribute, and the third is description, the input to all branches' attention module is the visual features. For miniImageNet, the branches configuration used is discussed and analyzed in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance evaluation</head><p>As can be seen, in the most challenging 1-shot scenario, our multiple semantics based model improves the best previously reported result by 2.3% and 5.2% for miniImageNet and CUB, respectively. The highest result is achieved using multiple semantic branches receiving as inputs: category labels, more complex (than category labels) description based semantics, and for CUB also attributes. Please see section 4.3 for the description of the branches used to achieve the best result and for the examination of the different branch configurations alternatives.</p><p>As expected, the most significant gain from using multiple additional semantics comes when the fewest amount of training examples is available, that is in the 1-shot case. For the 5shot scenario, when more image examples become available for the novel categories and the visual features are more reliable, the importance of semantic information is decreasing. Yet, even in this case, the usage of semantics provide some improvement over using only the visual features (SimpleShot <ref type="bibr" target="#b32">[33]</ref>), +0.6% for miniImageNet and +1.6% for CUB. <ref type="table">Table 3</ref> summarizes the performance of different (multiple) semantic branch configuration alternatives and other aspects of the proposed approach evaluated using the 1-shot miniImageNet test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantics Effect</head><p>Combining visual features with labels provides 0.9% improvement over the SimpleShot <ref type="bibr" target="#b32">[33]</ref> baseline, this result supports the results reported in <ref type="bibr" target="#b36">[37]</ref> (Table 3a-b). As can be seen from the table, using the more complex description semantics instead of the labels used in <ref type="bibr" target="#b36">[37]</ref> does not by itself improve the performance by much, only +0.3% <ref type="table">(Table 3c</ref>). Next we tested the effect of 'semantic ensemble' by using multiple semantic branches relying only on the labels, without adding additional semantic information (descriptions). This leads to only a slight improvement over a single branch, +0.2% (Table 3d).</p><p>A more significant improvement of 0.8% over using just labels baseline is attained by incorporating additional semantic information (descriptions conditioned on the labels) in the second semantic branch <ref type="table">(Table 3e</ref>). Introducing intermediate supervision in the form of per branch task specific Cross Entropy losses brings even more significant improvement of 1.4% over the baseline <ref type="table">(Table 3f)</ref> underlining the importance of this component. <ref type="table">Table 3</ref>: Ablation study performed on 1-shot miniImageNet benchmark. With l = category label, d = category description, v = visual prototype. x/y (e.g. l/l) means x is the branch input and the convex combination parameter is conditioned on y. The 'Branch losses' column marks models that utilize the internal supervision of per branch task CE losses. a. Is the SimpleShot baseline. b. Is based on AM3 (but different backbone and training procedure). c. Using only description semantics we observe similar results as when using only labels. d. The effect of 'ensemble', i.e. adding another branch with no extra semantic, is minor (+0.2% over b.). e. Adding a second branch with extra semantics adds 0.8%. f-j. Utilizing branch losses with extra semantics adds another 0.6%. h-i. Third branch adds another 0.8%. j. Adding a forth branch does not help.</p><p>In further tests, all using the intermediate per branch loss, we see that conditioning the second (description) branch on itself does not bare improvements <ref type="table">(Table 3g</ref>), yet a substantial improvement of +2% over the baseline is obtained when adding the the self-attending description as the third semantic branch <ref type="table">(Table 3h</ref>). Changing the third semantic branch to use labels for attending to the added description semantics, and thus utilizing the most comprehensive conditioning strategy (attending using all prior inputs to the combination) leads to the maximal 2.2% improvement over the baseline <ref type="table">(Table 3i)</ref> and comprises our final method for miniImageNet. Finally, in additional experiments we have observed that adding additional semantic branches, while re-using the same semantic information, does not help the performance <ref type="table">(Table 3j</ref>, as an example). This is intuitive as this likely leads to increased over-fitting due to adding more trainable network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone Architecture</head><p>We also tested the effect of the backbone architecture on the performance of our approach. <ref type="table" target="#tab_4">Table 4</ref> presents the comparison between the relatively small ResNet-10 architecture and the larger DenseNet-121 architecture. Moving to a stronger backbone we observed substantial improvement for 5-shot but no improvement for 1-shot. It supports our observation that for the 1-shot case the model relies more heavily on the semantics branches while for 5-shot the visual features are more important.  In this work, we have proposed an extended approach for fewshot learning with additional semantic information. We suggest making few-shot learning with semantics closer to the setting used by human infants: we build on multiple semantic explanations (name, attributes and description) that accompany the few image examples and utilize more complex natural language based semantics rather than just the name of the category. In our experiments, we only touch the tip of the iceberg of the possible approaches for using descriptive and multiple semantics for few-shot learning. Many other ways for combining multiple semantic information with visual inputs are possible and are very interesting topics for the follow-up works. In particular, we offer to investigate the following possible future work directions:</p><p>â€¢ Using instance level instead of category level semantics.</p><p>â€¢ Attending to visual and semantic branches combining information from all the task categories. In the current experiments, the coefficient of each category semantic prototype is computed from the attention MLP input of the corresponding category (either semantic or visual prototype of the same category). A future work may learn to attend based on the entire task jointly. â€¢ Alternative non-linear (e.g. MLP) combination schemes for visual and semantic prototypes instead of the (linear) convex combination we use here. â€¢ Learning alternative metrics, conditioned on the semantics, for comparing prototypes and query image features (e.g. learned Mahalanobis distance, with covariance matrix computed from semantic prototypes). â€¢ Semantic ensembles: instead of combining prototypes, combine logits resulting from different semantic and visual branches. â€¢ Further exploring different semantic sources and prototype / attention combinations. E.g. using the categories hierarchy <ref type="bibr" target="#b0">[1]</ref> or investigating into multi-modal sources of semantics, such as audio / smell / touch / taste, to further approximate the human infant learning environment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pointing out to a new kind of object for a child is often accompanied by additional associated (multiple) semantic information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results for the 5-way miniImageNet and CUB benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effect of architecture on performance for miniImageNet</figDesc><table><row><cell>5 Summary &amp; conclusions</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Label-Embedding for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4247" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zeroshot visual recognition using semantics-preserving adversarial embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Closer Look At Few-Shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05298v2</idno>
		<title level="m">Semantic Feature Augmentation in Few-shot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00412</idno>
		<title level="m">Metadapt: Meta-learned taskadaptive architecture for few-shot classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<title level="m">Diversity with Cooperation: Ensemble Methods for Few-Shot Classification. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transductive Multi-View Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2332" to="2345" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Few-Shot Learning with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>Arxiv:1709.08878</idno>
		<title level="m">Generating Sentences by Editing Prototypes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-shot Visual Recognition by Shrinking and Hallucinating Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning to learn with conditional class dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Varno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">ImageNet Classification with Deep Convolutional Neural Networks. Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Services</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>San Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Amherst</surname></persName>
		</author>
		<title level="m">Meta-Learning with Differentiable Convex Optimization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Meta-SGD: Learning to Learn Quickly for Few-Shot Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer Learning by Borrowing Examples for Multiclass Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to Recognize Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="244" to="250" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Articulated pose estimation with tiny synthetic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<title level="m">Optimization As a Model for Few-Shot Learning. ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05939</idno>
		<title level="m">Metric Learning with Adaptive Density Discrimination</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meta-Learning with Latent Embedding Optimization. In ICLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<title level="m">Meta-Learning with Memory-Augmented Neural Networks. (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Delta-Encoder: an Effective Sample Synthesis Method for Few-Shot Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Prototypical Networks for Few-shot Learning. NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<title level="m">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to Learn: Model Regression Networks for Easy Small Sample Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Caltech-UCSD birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5571" to="5580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03596</idno>
		<title level="m">Deep Meta-Learning: Learning to Learn in the Concept Space</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
