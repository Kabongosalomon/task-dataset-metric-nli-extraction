<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Person Re-identification with Bias-controlled Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Iodice</surname></persName>
							<email>s.iodice16@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
							<email>k.mikolajczyk@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Person Re-identification with Bias-controlled Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by the effectiveness of adversarial training in the area of Generative Adversarial Networks we present a new approach for learning feature representations in person re-identification. We investigate different types of bias that typically occur in re-ID scenarios, i.e., pose, body part and camera view, and propose a general approach to address them. We introduce an adversarial strategy for controlling bias, named Bias-controlled Adversarial framework (BCA), with two complementary branches to reduce or to enhance bias-related features. The results and comparison to the state of the art on different benchmarks show that our framework is an effective strategy for person reidentification. The performance improvements are in both full and partial views of persons.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) with recent innovations such as GANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> and attention based models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> have fostered rapid development in human re-identification obtaining impressive results. However, these re-ID methods rely on CNN features which are susceptible to bias that the training data suffers from. Bias occurs when similarity between two data samples in re-ID problem is due to the same pose, body part or camera view, rather than to the ID-related cues. For example, <ref type="bibr" target="#b8">[9]</ref> recently demonstrated that popular CNN architectures, i.e., ResNet-50 focus on local textures rather than global shapes for recognizing objects in ImageNet. It was also observed <ref type="bibr" target="#b9">[10]</ref> that the background, i.e., camera view significantly affects feature representations in person re-ID when training on existing academic datasets. Other works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> deal with pose bias by generating multiple synthetic poses for training <ref type="bibr" target="#b1">[2]</ref> or by leveraging body part cues <ref type="bibr" target="#b13">[14]</ref>. In contrast to all these methods that focus on one specific bias only, we propose a general approach that can address different types of bias without additional training data. In <ref type="figure" target="#fig_0">figure 1</ref> we show examples of how pose, camera view, and body part affect the re-id rankings. These are top ranked returns given by a well performing TriNet <ref type="bibr" target="#b30">[31]</ref> on CUHK03 <ref type="bibr" target="#b0">[1]</ref>, Market-1501 <ref type="bibr" target="#b22">[23]</ref> and CropCUHK03 <ref type="bibr" target="#b14">[15]</ref> with partial views. Frames with different IDs are marked by a red x above the frame. The top row shows the correct ID ranked 4th after three different subjects in the same pose as the query; the second row illustrates camera view bias, where the correct ID is ranked next to similar subjects appearing in the same background as the query; finally, the bottom row shows body part bias where the same body crop/part is ranked higher than the correct ID, which is ranked 13th. Note that in person re-ID benchmarks the positive examples with the same bias are not included when calculating the performance score. This, however, can also be considered a certain type of bias as it does not exactly correspond to the practical scenario. Nonetheless, in re-ID settings, the bias shows with the topranked negative examples affecting re-ID performance as illustrated in figure 1. Its influence can be reduced by large and diverse training data. However, in existing benchmarks, the small number of training examples often reflects specific settings from data collection. In this paper, we present an adversarial framework for controlling the bias in re-ID representation. Specifically, our model includes a feature mapping combined with an identity and a bias discriminators, that exploit examples with the same ID or the same bias. Our framework contains two specialized branches: one for bias-reducing in the image representations, e.g., pose, body part, background, thus focusing on ID features, e.g., clothes details; and one for bias-enhancing required for correct separation from ID features. The two branches generate descriptors containing complementary information to produce the final image representation. The contributions of this work are the following: 1) We introduce the idea of bias discriminator that helps to extract robust descriptors in re-ID; specifically, we propose bias-controlled adversarial training with two network branches specialized in revealing biasrelated and bias-unrelated structures in data, which generalizes existing adversarial solutions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref> to other types of bias in human re-identification. 2) This is the first work that provides quantitative analysis on how bias factors, such as the pose, camera view and body parts affect re-ID performance. 3) We demonstrate experimentally that the two proposed branches effectively reduce distracting features and enhance essential re-ID cues. We show the importance of including both bias and ID features in the re-ID task leading to competitive results in several benchmarks with full and partial views. The two branches generate descriptors containing complementary information to produce the final image representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we review closely related methods and discuss the differences to our approach. Background and Pose Bias.</p><p>Despite many methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10]</ref> have proposed to leverage different regions/pixels to obtain descriptors robust to pose and background, the bias problem has not been explicitly analyzed and addressed. The only work investigating the influence of background is <ref type="bibr" target="#b12">[13]</ref> with the support of foreground masks. Instead, our framework relies on available annotations and low inference complexity compared to pixel based predictions. GANs based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> can potentially alleviate the bias problem by generating many synthetic samples with diversity, for example, in different poses <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>, camera views <ref type="bibr" target="#b4">[5]</ref>, and domains <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. However, none of these methods provides quantitative analyses of the bias problem in re-ID. Adversarial Training in Recognition. Adversarial learning was previously adopted for face and person re-ID <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref> with similar goal of disentangling representations from biased features. However, unlike our work, <ref type="bibr" target="#b24">[25]</ref> addressed the pose bias only with a specific solution relying on a pose landmark map as input for training. Furthermore, an unsupervised method <ref type="bibr" target="#b25">[26]</ref> learns ID related and unrelated features for face identification, but its generalization to articulated pedestrian bodies is not straightforward. Our model relies on training data from standard datasets only and leads to a better performance than <ref type="bibr" target="#b25">[26]</ref>, as shown in the experimental results. Adversarial Training in Domain Adaptation from labelled source to unlabelled target has been proved to be particularly effective. According to the domain adaptation principles <ref type="bibr" target="#b16">[17]</ref>, predictions should be made on features that can not discriminate between the source and the target domains. Thus <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> incorporate adversarial training to promote features that are discriminant for the main task in the source domain and non-discriminant with respect to the shift between the domains. Likewise, we adopt adversarial training to extract features that are discriminant for the re-ID task, and non-discriminant with respect to the bias classes. Partial re-ID is an emerging problem introduced in <ref type="bibr" target="#b21">[22]</ref>, which focused on recognizing or matching identities of people from frames containing only parts of human bodies. A local-to-local matching based on small patches and globalto-local with a sliding window search for partial templates was used in <ref type="bibr" target="#b21">[22]</ref>. Another approach <ref type="bibr" target="#b27">[28]</ref> reconstructs missing channels in the query feature maps from full observations in the gallery maps. However, <ref type="bibr" target="#b21">[22]</ref> was validated on small datasets, i.e., Partial Reid, i-LIDS, and Caviar, and may not scale well to real scenarios due to On 4 complexity during inference time, while <ref type="bibr" target="#b27">[28]</ref> relies on the assumption that gallery images always contain full body of persons. Recently, <ref type="bibr" target="#b14">[15]</ref> proposed a solution based on alignment and hallucination as well as a synthetic data generated from CUHK03 with partial views in both probe and gallery images. Similarly, in order to analyze partial view bias, we evaluate our approach on CropCUHK03 dataset <ref type="bibr" target="#b14">[15]</ref> and compare with their proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adversarial training</head><p>In this section, first we shortly summarise the main concepts our work is based on, and then present the proposed bias-controlled adversarial framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Adversarial Training Framework. The goal of the generative adversarial framework <ref type="bibr" target="#b23">[24]</ref> is to train a differentiable function G W parametrized by W , usually represented by deep neural networks, to generate fake samples as close as possible to the real data. G W is trained along with a discriminator D Φ , which estimates the likelihood that an input sample comes from the real distribution. D Φ is adversarial to G W in the sense that by recognizing fake from real samples, D Φ drives G W to generate more realistic examples. This is known as min max game with value function L(G, D):</p><formula xml:id="formula_0">min W max Φ L(G, D) = E x∼pr(x) [log D Φ (x)] + E z∼pz(z) [log(1 − D Φ (G W (z))]</formula><p>(1)</p><p>x and G W (·) are real and fake samples from input noise z, respectively. It results in the following adversarial loss:</p><formula xml:id="formula_1">L adv = −E x∼pr(x) [log D(x)] − E x∼g(x) [log(1 − D(x))]<label>(2)</label></formula><p>Many variants of GANs have been proposed with modifications to the original framework making it a powerful and flexible tool, e.g., <ref type="bibr" target="#b11">[12]</ref> introduced multiple discriminators:</p><formula xml:id="formula_2">L adv = n i=1 −E x∼pr(x) [log D i (x)] − E x∼g(x) [log(1 − D i (x))]</formula><p>(3) In domain adaptation problem, <ref type="bibr" target="#b19">[20]</ref> replaces the generator with the mapping f for making target samples X t indistinguishable from those in the source distribution X s :</p><formula xml:id="formula_3">L adv (f ) = − E x∼Xs [log D(f (x))] − E x∼Xt [log(1 − D(f (x))]<label>(4)</label></formula><p>A new objective, as an alternative to the binary cross entropy, was proposed in <ref type="bibr" target="#b20">[21]</ref>. It is based on similarity between distributions of the generated and real data:</p><formula xml:id="formula_4">L adv = ||E x∼pr(x) f (x) − E z∼g(z) f (G(z))|| 2 2<label>(5)</label></formula><p>Inspired by these methods, we 1) use multiple discriminators <ref type="bibr" target="#b11">[12]</ref> to recognize samples with the same identity or bias; 2) instead of employing a generator, we train feature mappings as in <ref type="bibr" target="#b19">[20]</ref> with controlled bias; 3) we train the feature mappings as <ref type="bibr" target="#b20">[21]</ref> to directly match the corresponding feature vector of the correct sample, instead of using the typical cross entropy loss. Triplet Loss. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> Training samples are selected and arranged in triplets x a , x p , x n , where x a , x p is the pair with the same identity therefore their distance is minimized and x a , x n is the pair with different identities therefore their distance is maximized. Batch Hard <ref type="bibr" target="#b30">[31]</ref> has lead to a superior performance compared to the triplet loss <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Positives and negatives are paired in a batch specifically designed to include C random classes, with randomly sampled K examples of each class, thus resulting in a batch of CK images. For each anchor in the batch the hardest positive and the hardest negative sample is selected when forming triplets. This leads to the following formulation of loss with mini-batch X and margin m:</p><formula xml:id="formula_5">L(X) = xa∈X [m + hard positive max xp ||x a − x p || 2 − hard negative min xn ||x a − x n || 2 ] + (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed model</head><p>We introduce our Bias-controlled Adversarial framework (BCA) with two main branches for bias-reducing (R) and bias-enhancing (E). The former suppresses features corresponding to the body part/pose/camera bias and selects ID features that are robust against their variations. The latter enhances features relevant for pose, camera or body part recognition. A overview of the training framework is presented in figure 2. Each branch includes two discriminators D r and D b and the feature mapping f . While the re-ID discriminator D r attempts the re-ID task, the bias discriminator D b drives the feature mapping f to reduce/enhance the bias features by minimizing the proposed adversarial triplet loss. Both branches are trained with adversarial loss for re-ID and for bias discriminator as presented below. Re-ID Adversarial loss. We define the re-identification objective L Dr as an adversarial triplet loss, which is a reinterpretation of batch hard triplet loss in equation 6 including GANs equations 4 and 5:</p><formula xml:id="formula_6">LDr(X) = ar ∈X [m + hard positive max pr ||E ar ∼p(ar ) f (ar) − E pr ∼p(pr ) f (pr)|| 2 − hard negative min nr ||E ar ∼p(ar ) f (ar) − E nr ∼p(nr ) f (nr) || 2 ]+ (7)</formula><p>where parameters of the feature mapping f are optimized so that the distribution of positive ID class samples p(p r ) matches the distribution of anchor ID class samples p(a r ), and also differs from the distribution of negative ID samples p(n r ). Bias Adversarial loss. Similarly to equation 7, bias adversarial loss is defined as:</p><formula xml:id="formula_7">L Db (X) = a b ∈X [m + easy positive min p b ||E a b ∼p(a b ) f (a b ) − E p b ∼p(p b ) f (p b )|| 2 − easy negative max n b ||E a b ∼p(a b ) f (a b ) − E n b ∼p(n b ) f (n b ) || 2 ]+<label>(8)</label></formula><p>In contrast to L Dr , training triplets {p b , a b , n b } for L D b are selected by considering the bias types and the easiest positive/negative samples as more meaningful to the bias term. In other words, for the re-ID task, it is more beneficial to include hard cases in the batch, e.g, same persons with a different pose, or similar pose of different persons. Instead, for the bias term, the easiest cases are those where the bias is more evident and therefore should be used for learning bias related characteristics. However, there are no specific constraints on the ID when pairing samples for the bias. Adversarial Triplet Loss. Our objective evaluates the loss for a mini-batch of samples X arranged in triplets with the batch hard strategy. It involves the bias discriminator D b , the ID discriminator D r and the mapping functions f • :</p><formula xml:id="formula_8">L adv (X) = λ Dr · L Dr (f • (X)) • λ Db · L D b (f • (X)) (9)</formula><p>L adv (X) incorporates re-ID adversarial loss L Dr and bias adversarial loss L Db for training D r and D b . Note that the loss corresponds to both the bias-reducing and biasenhancing branch, depending on the sign (•) between the two components. Maximizing L Db loss (-) reduces features related to bias in mapping images to features by f − , while minimizing L Db (+) leads to bias enhancing in f + . Parameters λ Dr and λ Db control the contributions from these two terms in each branch. Bias-reducing branch is obtained by setting sign (•) to (−) in <ref type="bibr">Equation 9</ref>, thus L Db is maximized, i.e., the distance between samples from the same bias class a b , p b , regardless their ID is maximized, while distance between different bias class samples a b , n b is minimized. This forces f − to reduce feature components encoding the bias and focus on unbiased patterns. Bias-enhancing branch. L Db is minimized by setting sign (•) to (+) in Equation 9, i.e., the distance between a b and p b is minimized, and between a b and n b is maximized. The network rewards biased samples, therefore emphasizes feature components encoding the bias. As we show in the experiments, it provides a complementary mapping branch f + that improves the representation for re-ID task. Re-ID loss L Dr is used in training in the same way in both scenarios to reduce the distance between positive ID pairs and increase for the negative ones. In contrast, the bias loss L Db differs for the two cases i.e., maximizing or minimizing the distance between positive bias pair for reducing and enhancing branches, respectively. Finally, during inference time the two embeddings -coming from the mappings f − and f + from the two independent branches -are concatenated to produce the final image representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In this section, we first describe datasets used for our evaluation and provide implementation details. We then analyze the effectiveness of our BCA framework for different types of bias. Finally, we report the performance on standard benchmarks and compare our approach against other methods on full and partial views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and bias annotation</head><p>CUHK03 <ref type="bibr" target="#b0">[1]</ref> includes 14, 096 images of 1, 467 pedestrians from 6 surveillance cameras. We adopt the new evaluation protocol from <ref type="bibr" target="#b33">[34]</ref>. In this dataset we experiment with the pose bias considering two class labels, i.e., frontal and profile views, which also correspond to the two disjoint camera views. MARKET-1501 <ref type="bibr" target="#b22">[23]</ref> contains 32, 668 annotated bound-ing boxes of 1, 501 identities. The images are acquired from six surveillance cameras including 5 high-resolution cameras and 1 low-resolution camera. In Market-1501 we investigate both the camera view and pose bias using the available camera annotation and the proposed three class pose labels, i.e., frontal, side and oblique. DUKE <ref type="bibr" target="#b28">[29]</ref> provides 36, 411 bounding boxes of 1, 501 identities. As in Market-1501, we analyze both the camera view and the pose bias from the existing camera annotation and our three pose classes. CropCUHK03 <ref type="bibr" target="#b14">[15]</ref> is a dataset derived from CUHK03 <ref type="bibr" target="#b0">[1]</ref> by cropping partial views but maintaining the same number of individuals and frames. We use CropCUHK03 settings s=0.5,omin=. <ref type="bibr" target="#b24">25</ref> where s is the fraction of area cropped from full labeled frame and o min is the minimum overlap between two views. The pose labels are frontal and side. In addition, we annotate each partial view with three body part labels, i.e., upper, central, bottom parts. Note that in all the datasets class labels are uniformly distributed. Our pose annotation for Market-1501 and Duke as well as body part annotation for CropCUHK03 are available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Baseline. We use an implementation of TriNet <ref type="bibr" target="#b30">[31]</ref> as a baseline for comparison in our experiment. We set a batch size of P · K = 64 samples with P = 16 randomly extracted persons IDs and K = 4 instances for each person. We train the network with Adam optimizer, with a learning rate of 0.0003 and linear decay to zero over 60 epochs. As common practice during the training, we perform data augmentation by using random horizontal flips and re-scaling input images to 384 × 128 pixels. Further implementation details can be found in <ref type="bibr" target="#b30">[31]</ref>. Bias-reducing/enhancing branches. ResNet50 is used as a backbone of the discriminators {D b , D r } and the mapping f in each of the two branches. In our implementation, these three components share the same weights within each of the ResNet50 branch. Each branch is trained independently and with the same hyper-parameters, optimizer and augmentation strategy as the baseline. In bias-reducing, we set λ D b = −0.01 for all types of bias. In bias-enhancing, we set λ D b = 0.05 for pose bias and λ D b = 0.01 for camera and body part bias. As a good practice, we kept consistent across different datasets the values of λ D b . During inference, mappings f − and f + output a concatenated descriptor of 2 × 2048 components from the-second-to-last layers of ResNet50. Note that no bias annotation is required during inference time.  <ref type="table">Table 1</ref>. Analysis of re-ID and bias classification task under different values of λ Db parameter for either reducing (R) or enhancing (E) bias features on CUHK03 (labeled split and new protocol), Market-1501 (single query), and CropCUHK03. Re-ID performance is measured in terms of Rank1 and mAP (mean average precision); Bias classification task is evaluated in terms of accuracy (acc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of individual components</head><p>We evaluate the effectiveness of our BCA framework for different bias, i.e., pose, camera, and body part. We first demonstrate that bias-reducing and bias-enhancing branches effectively suppress and boost the bias features by evaluating the generated re-ID descriptors for bias classification tasks. Furthermore, we show the importance of bias related features and how they affect the representations that are essential for the re-ID task. Finally, we statistically analyze the occurrence of bias during evaluation and prove that our method is able to control it. The bias classification experiment shows the extent to which the performance of a strong TriNet baseline <ref type="bibr" target="#b30">[31]</ref> is affected by the pose, camera and body parts. We extract features with pretrained mappings f • from bias-reducing (R) and bias-enhancing (E) branches on three different datasets. CUHK03 for front/side pose recognition; Market-1501 for six cameras; CropCUHK03 for top, central and bottom body parts. We use the extracted features as inputs to train a simple classification model based on a PReLU activation layer and a fully connected layer with a number of outputs corresponding to the bias class labels, i.e., 2 for pose, 6 for camera, and 3 for body part. Note that while training the bias classifier the feature extracting backbone is fixed. <ref type="table">Table 1</ref> shows the results which we discuss in detail below. Baseline. In order to measure the top performance the system can reach we evaluate the baseline disregarding biased pose, camera or body part samples. Specifically, during inference for each query we remove the samples from the gallery list that have the same bias label as the query and calculate the scores. Note that, re-ID protocols already discard positives with the same camera view to make the task challenging and in this experiment we additionally exclude the negatives with the same bias. The results show that the re-ID performance is significantly affected by the bias as we observe Rank1 improvement in each of the benchmarks by 2.8% on CUHK03, by 1.6% on Market-1501, and by 20.9% on CropCUHK03. The results also show that re-ID features from the baseline include information on specific bias as the bias classification accuracy is 0.78, 0.44 and 0.56 for pose, camera and body parts, respectively. Bias control. Next we demonstrate the ability of the proposed framework to control the bias. We train the two branches for different balance between the re-ID and the bias loss in equation 9, i.e., we vary the values of λ Db for pose, camera and body parts while keeping re-ID loss fixed λ Dr = 1. For example in table 1, by setting λ Db to negative weight of −0.01 i.e., the bias classification accuracy is reduced from 0.78 to 0.62 for the pose bias in CUHK03 and from 0.44 to 0.27 for the camera bias in Market-1501, with re-ID performance slightly increased. Further increasing negative λ Db deteriorates the re-ID performance. It is also evident that body part bias is more challenging to control than other types, however we are still able to reduce the bias classification accuracy in CropCUHK03 from 0.56 to 0.54 while increasing re-ID in both Rank1 and mAP metrics. Note that, re-ID performance is very sensitive to the value of λ Db and its behaviour is somewhat opposite in the two branches. Moreover, when re-ID loss λ Dr is deacti-vated and λ Db = 1, features from Bias-enhancing branch well recognizes all the bias types. The classification accuracy is 1 for pose, 0.82 for six cameras, and 0.66 for top, central and bottom body parts. These suggest that the bias features in re-ID descriptors and features learned from the bias labels are complementary which motivates us to include two different branches for controlling the bias in person representation. The reducing one for disentangling the bias from re-ID features in order to obtain robust representation. The enhancing one for boosting features from the available bias annotation. Note that the importance of including bias related features was previously highlighted by other works which incorporate shape features from background masks <ref type="bibr" target="#b13">[14]</ref> and body parts <ref type="bibr" target="#b9">[10]</ref> into re-ID representations as related to essential cues for re-ID. Our results further supports their observations.</p><p>Effect of bias on re-ID. An important observation under bias-reducing mode in table 1 is that by increasing the contribution from λ Db = 0.005 to λ Db = 0.1, Rank1 dramatically drops from 60.7% to 26.5% in CUHK03, from 87% to 79.2% in Market-1501 and from 39.6% to 15.6% with respect to the baseline. This suggests that pose, camera and body part features cannot be completely suppressed without affecting re-ID features. We found that effective bias reduction is with λ Db ≤ 0.01 resulting in an improvement from 60.7% to 62.5% in CUHK03 and 39.6% to 40.6% in CropCUHK03.</p><p>Another observation is that by emphasizing the opposite configuration, which enhances the bias features related to the annotation, Rank1 increases by nearly 2% in CUHK03 (λ Db = 0.05) and by 1.5% in Market-1501 (λ Db = 0.01).</p><p>Finally, concatenating the embeddings derived from both branches results in a considerable boost in Rank1 in all three benchmarks, i.e., from 60.7% to 65.1% in CUHK03, from 87% to 89.9% in Market-1501 and from 39.6% to 42.1% (case: R + E). This further suggests that biasreducing and bias-enhancing branches extract complementary features and form a better descriptor for re-ID when used jointly. Statistical analysis of bias in retrieval. Ranked gallery samples returned by the re-ID system include both positive and negative examples. Re-ID evaluation protocols discard only the positives from the same camera, however, the negatives are also biased and affect the re-ID scores. We analyze the influence of pose and camera bias in the ranked lists. In <ref type="figure" target="#fig_2">figure 3</ref> we report statistics under different configurations, i.e., baseline (Bas), bias-reducing (R), bias-enhancing (E) on datasets CUHK03 and Market-1501 for pose (top) and camera bias (bottom). The left and middle figures show the probability that a negative and positive sample, respectively, with the same bias appears at a certain rank position. We also report nauc 10 aggregating probability values over the first 10 rank positions. The bias is clearly visible for the baseline (black curve) significantly higher than the ideal case of no bias (grey dotted curve). Bias-reducing branch (R) leads to a drop in nauc(10) by 2% from 0.57 to 0.55 for pose and from 0.57 to 0.55 for camera bias, which further shows its effectiveness. In contrast, the bias-enhancing branch increases the probability of biased samples occurring higher in the rank, e.g., increasing values of nauc <ref type="bibr" target="#b9">(10)</ref>. Similarly, we analyze the behaviour of bias in positives. Although they are not included in the evaluation protocol, we perform this experiment to confirm that the proposed approach does not penalize them, which would have a negative effect in a practical scenario where all retrieved positive samples matter. This is clearly visible in figures 3 (middle) where the probability curves and nauc(10) values remains nearly the same for baseline as well as for R and E branches. Finally, in <ref type="figure" target="#fig_2">figure 3</ref> (right) we show that the influence of bias on the rankings can be controlled by varying the parameters λ Db in the bias-reducing and enhancing branches. For example, increasing λ Db from 5 · 10 −3 to 5 · 10 −2 , leads the bias-reducing branch (blue curves) to decrease the nauc(10) by 0.5 in same pose probability (top) and by 0.2 in same camera probability (bottom). On the other hand, decreasing λ Db from 5 · 10 −2 to 5 · 10 −3 (red curves) in bias-enhancing boosts the probability to retrieve a biased sample from 0.55 to 0.58 for the pose and from 0.22 to 0.24 for the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparative evaluation</head><p>We compare our approach against the state of art methods, in particular GAN, attention, background bias, and other adversarial learning techniques in tables 2, 3 and 4 for Market-1501 and Duke, CUHK03 as well as CropCUHK03, respectively. Our approach is referred to as R+E (bias re-ducing+enhancing) followed by bias type. In order to make it more complete we also indicate for each method if external data (D+), random erasing augmentation strategy (A+) and re-ranking (RR+) are adopted, as this practices positively affect the re-ID performance. <ref type="bibr">MARKET</ref>   the limitation of the method to generalize for articulated bodies. We also compare to FD-GAN <ref type="bibr" target="#b24">[25]</ref>, which addresses the pose bias. Our method shows comparable results in MARKET-1501 improving slightly the mAP from 77.7% to 77.9%, and giving Rank1 score of 82.2% in DukeMTCM, which is higher by 2.2% than FD-GAN. In addition, we outperform MaskG [10] by 6.6%, which also tries to address the background bias. These results show the advantage of our method which can deal with different types of bias and improves upon methods targeting specific bias. Moreover, our method performs as well as the attention based networks, that adopt external training data and random erasing augmentation technique. Finally, with re-ranking incorporated R+E (RR+) pose + cam our method with Rank1 93.1% in Market-1501 and 85.2% in Duke, outperforms most of the methods in this comparative evaluation. CUHK03 with new 700/767 protocol is used to obtain results in table 3 for both, labeled (L) and detected (D) splits. Our framework (R+E pose) shows very competitive scores and outperforms MaskG <ref type="bibr" target="#b9">[10]</ref> and Pose-transfer <ref type="bibr" target="#b1">[2]</ref>, which address background and pose bias, respectively. Incorporating re-ranking R+E pose (RR+) leads to Rank1 of 75.5% in labeled and 73.4% in detected splits, outperforming by a large margin all the compared approaches, except ACCN (D+), which uses external data for training. CropCUHK03. Finally, <ref type="table" target="#tab_4">table 4</ref> shows results for our method in partial re-ID problem on CropCUHK03 and compares it against other recent techniques. In particular, Partial Match Net <ref type="bibr" target="#b14">[15]</ref> was specifically designed to handle partial views by additional networks dealing with alignment and hallucination of missing parts. Our method (R+E body part + pose) significantly outperforms Partial Match Net in Rank1 from 31.3% to 43.5%, improving the state of art result on this dataset. The results are reported for the same evaluation settings. Moreover, incorporating re-ranking further boosts Rank1 performance to 54.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed an approach to person re-ID based on adversarial training to extract person representation focused on ID specific cues while allowing to control the influence of bias that many re-ID benchmarks suffer from. Via biasreducing and bias-enhancing branches we can optimize the balance of features related to re-ID and to the bias. The main novelty is in the training strategy that can be easily incorporated in other recognition problems. We have analyzed the impact of various type of bias in re-ID task, i.e., pose, body part, camera view presenting a methodology applicable to different bias factors. Furthermore, we demonstrate that leveraging two opposite training processes (bias-reducing and enhancing) which either suppress or emphasize bias related features, gives more robust ID representations due to the complementary content of the features. We carried out an extensive evaluation on four different benchmarks in comparison to a number of recently proposed methods. The proposed approach consistently improves the re-ID performance compared to other state of the art techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of how different types of bias, i.e., pose, camera view, and body part affect the re-ID rankings. We rank all gallery pictures by the distance to the query (from left to right). Frames containing different IDs are indicated with a red x above the frame. Top row examples are from CUHK03, middle from Market-1501 and bottom from CropCUHK03.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Bias-controlled Adversarial framework (BCA) includes two independent branches, i.e., bias-reducing (R) and bias-enhancing (E) sharing the same architecture but different weights. Each re-ID and bias branch includes a feature mapping f and discriminators {Dr, D b } sharing the weights of ResNet50 backbone. In each branch, the feature mapping f is driven by Dr and D b to reduce or enhance bias by changing the sign of L Db . Dr and D b are trained by triplets, where sr, dr are the positive/negatives pairs selected by considering their ID, and s b , d b by considering their bias type. During inference time, the final descriptor is obtained by concatenating the two embeddings given by the mappings f of the two independent branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Statistical analysis of bias under different configurations i.e. baseline (Bas), bias-reducing (R), bias-enhancing (E) on datasets CUHK03 (top) and Market-1501 (bottom) for pose and camera bias. Graphs illustrate the following statistics: left) the probability that a negative sample with the same bias appears at a certain rank position; middle) the probability that a positive sample with the same bias appears at a certain rank position; right) analysis of bias in negatives under different values of parameter λ Db .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison results in Market-1501 and Duke-reID. Our method with controlled pose and camera bias R+E pose+cam outperforms all GANs and adversarial learning methods. Yet, when combined with re-ranking R+E(RR+) pose+cam the performance further increases.Market-1501 and DukeMTCM-reID. In table 2 we observe that our proposed method that learns to reduce the pose and camera bias R+E pose+cam improves over the existing GAN based methods, with Rank1 of 90.4% on MARKET-1501 and 82.3% on DUKEMTCM-reID. Compared to the other adversarial learning solutions, we also outperform DR-GAN<ref type="bibr" target="#b26">[27]</ref> by a large margin, i.e. by 3.7% in MARKET-1501 and by 5.3% in DUKE.</figDesc><table><row><cell>Note that DR-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparative evaluation in CUHK03. Our framework R+E pose and with re-ranking incorporated R+E (RR+) pose report competitive results on this benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparative evaluation in CropCUHK03. Our approach outperforms the other methods evaluated on this dataset.</figDesc><table><row><cell>CropCUHK03</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>1, 2, 7, 8</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention-Aware Compositional Network for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mask-Guided Contrastive Attention Model for Person Re-Identification In</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<pubPlace>1, 2, 6, 7, 8</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative multi-adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eliminating Background-Bias for Robust Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Partial Person Reidentification with Alignment and Hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iodice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spindle net: Person reidentification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">T: Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<title level="m">Improved techniques for training gans In: Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">S: Partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scalable Person Re-identification: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring disentangled feature representation beyond face identification In</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Performance Measures and a Data Set for Multi-Target</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi-Camera Tracking In: ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pose-normalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Person reidentification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
