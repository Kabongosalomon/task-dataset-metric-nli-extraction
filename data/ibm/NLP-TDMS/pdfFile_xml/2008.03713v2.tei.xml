<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the previous image-based 3D human pose and mesh estimation methods estimate parameters of the human mesh model from an input image. However, directly regressing the parameters from the input image is a highly non-linear mapping because it breaks the spatial relationship between pixels in the input image. In addition, it cannot model the prediction uncertainty, which can make training harder. To resolve the above issues, we propose I2L-MeshNet, an image-to-lixel (line+pixel) prediction network. The proposed I2L-MeshNet predicts the per-lixel likelihood on 1D heatmaps for each mesh vertex coordinate instead of directly regressing the parameters. Our lixel-based 1D heatmap preserves the spatial relationship in the input image and models the prediction uncertainty. We demonstrate the benefit of the image-to-lixel prediction and show that the proposed I2L-MeshNet outperforms previous methods. The code is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human pose and mesh estimation aims to simultaneously recover 3D semantic human joint and 3D human mesh vertex locations. This is a very challenging task because of complicated human articulation and 2D-to-3D ambiguity. It can be used in many applications such as virtual/augmented reality and human action recognition.</p><p>SMPL <ref type="bibr" target="#b27">[28]</ref> and MANO <ref type="bibr" target="#b41">[42]</ref> are the most widely used parametric human body and hand mesh models, respectively, which can represent various human poses and identities. They produce 3D human joint and mesh coordinates from pose and identity parameters. Recent deep convolutional neural network (CNN)-based studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref> for the 3D human pose and mesh estimation are based on the model-based approach, which trains a network to estimate SMPL/MANO parameters from an input image. On the other hand, there have been few methods based on model-free approach <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>, which estimates mesh vertex coordinates directly. They obtain the 3D pose by multiplying a joint regression matrix, included in the human mesh model, to the estimated mesh.</p><p>input image output mesh input image output mesh input image output mesh <ref type="figure">Fig. 1</ref>. Qualitative results of the proposed I2L-MeshNet on MSCOCO <ref type="bibr" target="#b25">[26]</ref> and Frei-HAND <ref type="bibr" target="#b7">[8]</ref> datasets.</p><p>Although the recent deep CNN-based methods perform impressive, when estimating the target (i.e., SMPL/MANO parameters or mesh vertex coordinates), all of the previous 3D human pose and mesh estimation works break the spatial relationship among pixels in the input image because of the fullyconnected layers at the output stage. In addition, their target representations cannot model the uncertainty of the prediction. The above limitations can make training harder, and as a result, reduce the test accuracy as addressed in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45]</ref>. To address the limitations, recent state-of-the-art 3D human pose estimation methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44]</ref>, which localize 3D human joint coordinates without mesh vertex coordinates, utilize the heatmap as the target representation of their networks. Each value of one heatmap represents the likelihood of the existence of a human joint at the corresponding pixel positions of the input image and discretized depth value. Therefore, it preserves the spatial relationship between pixels in the input image and models the prediction uncertainty.</p><p>Inspired by the recent state-of-the-art heatmap-based 3D human pose estimation methods, we propose I2L-MeshNet, image-to-lixel prediction network that naturally extends heatmap-based 3D human pose to heatmap-based 3D human pose and mesh. Likewise voxel (volume+pixel) is defined as a quantized cell in three-dimensional space, we define lixel (line+pixel) as a quantized cell in one-dimensional space. Our I2L-MeshNet estimates per-lixel likelihood on 1D heatmaps for each mesh vertex coordinates, therefore it is based on the model-free approach. The previous state-of-the-art heatmap-based 3D human pose estimation methods predict 3D heatmap of each human joint. Unlike the number of human joints, which is around 20, the number of mesh vertex is much larger (e.g., 6980 for SMPL and 776 for MANO). As a result, predicting 3D heatmaps of all mesh vertices becomes computationally infeasible, which is be-yond the limit of modern GPU memory. In contrast, the proposed lixel-based 1D heatmap has an efficient memory complexity, which has a linear relationship with the heatmap resolution. Thus, it allows our system to predict heatmaps with sufficient resolution, which is essential for dense mesh vertex localization.</p><p>For more accurate 3D human pose and mesh estimation, we design the I2L-MeshNet as a cascaded network architecture, which consists of PoseNet and MeshNet. The PoseNet predicts the lixel-based 1D heatmaps of each 3D human joint coordinate. Then, the MeshNet utilizes the output of the PoseNet as an additional input along with the image feature to predict the lixel-based 1D heatmaps of each 3D human mesh vertex coordinate. As the locations of the human joints provide coarse but important information about the human mesh vertex locations, utilizing it for 3D mesh estimation is natural and can increase accuracy substantially.</p><p>Our I2L-MeshNet outperforms previous 3D human pose and mesh estimation methods on various 3D human pose and mesh benchmark datasets. <ref type="figure">Figure 1</ref> shows 3D human body and hand mesh estimation results on publicly available datasets.</p><p>Our contributions can be summarized as follows.</p><p>• We propose I2L-MeshNet, a novel image-to-lixel prediction network for 3D human pose and mesh estimation from a single RGB image. Our system predicts lixel-based 1D heatmap that preserves the spatial relationship in the input image and models the uncertainty of the prediction. • Our efficient lixel-based 1D heatmap allows our system to predict heatmaps with sufficient resolution, which is essential for dense mesh vertex localization. • We show that our I2L-MeshNet outperforms previous state-of-the-art methods on various 3D human pose and mesh datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>3D human body and hand pose and mesh estimation. Most of the current 3D human pose and mesh estimation methods are based on the model-based approach, which predict parameters of pre-defined human body and hand mesh models (i.e., SMPL and MANO, respectively). The model-based methods can be trained only from groundtruth human joint coordinates without mesh vertex coordinates because the model parameters are embedded in low dimensional space. Early model-based methods <ref type="bibr" target="#b3">[4]</ref> iteratively fit the SMPL parameters to estimated 2D human joint locations. More recent model-based methods regress the body model parameters from an input image using CNN. Kanazawa et al. <ref type="bibr" target="#b19">[20]</ref> proposed an end-to-end trainable human mesh recovery (HMR) system that uses the adversarial loss to make their output human shape is anatomically plausible. Pavlakos et al. <ref type="bibr" target="#b39">[40]</ref>  image space. Pavlakos et al. <ref type="bibr" target="#b37">[38]</ref> proposed a system that uses multi-view color consistency to supervise a network using multi-view geometry. Baek et al. <ref type="bibr" target="#b2">[3]</ref> trained their network to estimate the MANO parameters using a differentiable renderer. Boukhayma et al. <ref type="bibr" target="#b4">[5]</ref> trained their network that takes a single RGB image and estimates MANO parameters by minimizing the distance of the estimated hand joint locations and groundtruth. Kolotouros et al. <ref type="bibr" target="#b22">[23]</ref> introduced a self-improving system consists of SMPL parameter regressor and iterative fitting framework <ref type="bibr" target="#b3">[4]</ref>.</p><p>On the other hand, the model-free approach estimates the mesh vertex coordinates directly instead of regressing the model parameters. Due to the recent advancement of the iterative human body and hand model fitting frameworks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37]</ref>, pseudo-groundtruth mesh vertex annotation on large-scale datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> became available. Those datasets with mesh vertex annotation motivated several model-free methods that require mesh supervision. Kolotouros et al. <ref type="bibr" target="#b23">[24]</ref> designed a graph convolutional human mesh regression system. Their graph convolutional network takes a template human mesh in a rest pose as input and outputs mesh vertex coordinates using image feature from ResNet <ref type="bibr" target="#b11">[12]</ref>. Ge et al. <ref type="bibr" target="#b8">[9]</ref> proposed a graph convolution-based network which directly estimates vertices of hand mesh. Recently, Choi et al. <ref type="bibr" target="#b6">[7]</ref> proposed a graph convolutional network that recovers 3D human pose and mesh from a 2D human pose.</p><p>Unlike all the above model-based and model-free 3D human pose and mesh estimation methods, the proposed I2L-MeshNet outputs 3D human pose and mesh by preserving the spatial relationship between pixels in the input image and modeling uncertainty of the prediction. Those two main advantageous are brought by designing the target of our network to the lixel-based 1D heatmap. This can make training much stable, and the system achieves much lower test error. Heatmap-based 3D human pose estimation. Most of the recent state-ofthe-art 2D and 3D human pose estimation methods use heatmap as a prediction target, which preserves the spatial relationship in the input image and models the uncertainty of the prediction. Tompson et al. <ref type="bibr" target="#b44">[45]</ref> proposed to estimate the Gaussian heatmap instead of directly regressing coordinates of human body joints. Their heatmap representation helps their model to perform 2D human pose estimation more accurate and motivated many heatmap-based 2D human pose methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref>. Pavlakos et al. <ref type="bibr" target="#b38">[39]</ref> and Moon et al. <ref type="bibr" target="#b31">[32]</ref> firstly proposed to use 3D heatmaps as a prediction target for 3D human body pose and 3D hand pose estimation, respectively. Especially, Moon et al. <ref type="bibr" target="#b31">[32]</ref> demonstrated that under the same setting, changing prediction target from coordinates to heatmap significantly improves the 3D hand pose accuracy while requires much less amount of the learnable parameters. Recently, Moon et al. <ref type="bibr" target="#b32">[33]</ref> achieved significantly better 3D multi-person pose estimation accuracy using 3D heatmap compared with previous coordinate regression-based methods <ref type="bibr" target="#b40">[41]</ref>. <ref type="figure">Figure 2</ref> shows the overall pipeline of the proposed I2L-MeshNet. I2L-MeshNet consists of PoseNet and MeshNet, which will be described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">I2L-MeshNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PoseNet</head><p>The PoseNet estimates three lixel-based 1D heatmaps of all human joints P H = {P H,x , P H,y , P H,z } from the input image I. P H,x and P H,y are defined in x-and y-axis of the image space, while P H,z is defined in root joint (i.e., pelvis or wrist)relative depth space. For this, PoseNet extracts image feature F P ∈ R c×h×w from the input image by ResNet <ref type="bibr" target="#b11">[12]</ref>. Then, three upsampling modules increases the spatial size of F P by 8 times, while changing channel dimension from c = 2048 to c = 256. Each upsampling module consists of deconvolutional layer, 2D batch normalization layer <ref type="bibr" target="#b12">[13]</ref>, and ReLU function. The upsampled features are used to compute lixel-based 1D human pose heatmaps, as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref> (a).</p><p>We obtain x-and y-axis 1D human pose heatmaps as follows:</p><formula xml:id="formula_0">P H,x = f 1D,x P (avg y (f up P (F P ))) and P H,y = f 1D,y P (avg x (f up P (F P ))),<label>(1)</label></formula><p>where f up P denotes the three upsampling modules of the PoseNet. avg i and f 1D,i P denote i-axis marginalization by averaging and a 1-by-1 1D convolution that changes channel dimension from c to J for i-axis 1D human pose heatmap estimation, respectively. We obtain z-axis 1D human pose heatmaps as follows:</p><formula xml:id="formula_1">P H,z = f 1D,z P (ψ(f P (avg x,y (F P )))),<label>(2)</label></formula><p>where f P and ψ : R c D → R c ×D denote a building block and and reshape function, respectively. The building block consists of a fully-connected layer, 1D batch normalization layer, and ReLU function, and it changes the activation size from c to c D. D denotes depth discretization size and is equal to 8h = 8w. We convert the discretized heatmaps of P H to continuous coordinates</p><formula xml:id="formula_2">P C = [p C,x , p C,y , p C,z ] ∈ R J×3 by soft-argmax [44].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MeshNet</head><p>The MeshNet has a similar network architecture with that of the PoseNet. Instead of taking the input image I, MeshNet takes a pre-computed image feature from the PoseNetF P and 3D Gaussian heatmap P HG ∈ R J×D×8h×8w .F P is the input of the first residual block of the PoseNet whose spatial dimension is 8h × 8w. P HG is obtained from P C as follows:</p><formula xml:id="formula_3">P HG (j, z, y, x) = exp − (x − p C,x j ) 2 + (y − p C,y j ) 2 + (z − p C,z j ) 2 2σ 2 ,<label>(3)</label></formula><p>where p C,x j , p C,y j and p C,z j are jth joint x-, y-, and z-axis coordinates from P C , respectively. σ is set to 2.5.</p><p>From P HG andF P , we obtain image feature F M as follows:</p><formula xml:id="formula_4">F M = ResNet M (f M (ψ(P HG ) ⊕F P )),<label>(4)</label></formula><p>where ψ : R J×D×8h×8w → R JD×8h×8w and ⊕ denote reshape function and concatenation along the channel dimension, respectively. f M is a convolutional block that consists of a 3-by-3 convolutional layer, 2D batch normalization layer, and ReLU function. It changes the channel dimension of the input to the input channel dimension of the first residual block of the ResNet. ResNet M is the ResNet starting from the first residual block.</p><p>From the F M , MeshNet outputs three lixel-based 1D heatmaps of all mesh vertices M H = {M H,x , M H,y , M H,z } in an exactly the same manner with that of PoseNet, as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref> (a). Likewise heatmaps of PoseNet, M H,x and M H,y are defined in x-and y-axis of the image space, while M H,z is defined in root joint-relative depth space. We obtain x-and y-axis 1D human mesh heatmaps as follows:</p><formula xml:id="formula_5">M H,x = f 1D,x M (avg y (f up M (F M ))) and M H,y = f 1D,y M (avg x (f up M (F M ))),<label>(5)</label></formula><p>where f up M denotes the three upsampling modules of the MeshNet. f 1D,i M denote a 1-by-1 1D convolution that changes channel dimension from c to V for i-axis 1D human mesh heatmap estimation, respectively. <ref type="figure" target="#fig_0">Figure 3</ref> </p><formula xml:id="formula_6">(b) shows visualized f up M (F M ), M H,x , and M H,y .</formula><p>We obtain z-axis 1D human mesh heatmaps as follows:</p><formula xml:id="formula_7">M H,z = f 1D,z M (ψ(f M (avg x,y (F M )))),<label>(6)</label></formula><p>where f M and ψ : R c D → R c ×D denote a building block and and reshape function, respectively. The building block consists of a fully-connected layer, 1D batch normalization layer, and ReLU function, and it changes the activation size from c to c D. Likewise we did in the PoseNet, we convert the discretized heatmaps of M H to continuous coordinates</p><formula xml:id="formula_8">M C = [m C,x , m C,y , m C,z ] ∈ R V ×3 by soft-argmax [44].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Final 3D human pose and mesh</head><p>The final 3D human mesh M and pose P are obtained as follows:</p><formula xml:id="formula_9">M = Π(T −1 M C + R) and P = J M,<label>(7)</label></formula><p>where Π, T −1 , and R ∈ R 1×3 denote camera back-projection, inverse affine transformation (i.e., 2D crop and resize), and z-axis offset whose element is a depth of the root joint, respectively. R is obtained from RootNet <ref type="bibr" target="#b32">[33]</ref>. We use normalized camera intrinsic parameters if not available following Moon et al. <ref type="bibr" target="#b32">[33]</ref>. J ∈ R J×V is a joint regression matrix defined in SMPL or MANO model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss functions</head><p>PoseNet pose loss. To train the PoseNet, we use L1 loss function defined as follows:</p><formula xml:id="formula_10">L PoseNet pose = P C − P C * 1 ,<label>(8)</label></formula><p>where * indicates groundtruth. z-axis loss becomes zero if z-axis groundtruth is unavailable.</p><p>MeshNet pose loss. To train the MeshNet to predict mesh vertex aligned with body joint locations, we use L1 loss function defined as follows:</p><formula xml:id="formula_11">L MeshNet pose = J M C − P C * 1 ,<label>(9)</label></formula><p>where * indicates groundtruth. z-axis loss becomes zero if z-axis groundtruth is unavailable.</p><p>Mesh vertex loss. To train the MeshNet to output mesh vertex heatmaps, we use L1 loss function defined as follows:</p><formula xml:id="formula_12">L vertex = M C − M C * 1 ,<label>(10)</label></formula><p>where * indicates groundtruth. z-axis loss becomes zero if z-axis groundtruth is unavailable.</p><p>Mesh normal vector loss. Following Wang et al. <ref type="bibr" target="#b47">[48]</ref>, we supervise normal vector of predicted mesh to get visually pleasing mesh result. The L1 loss function for normal vector supervision is defined as follows:</p><formula xml:id="formula_13">L normal = f {i,j}⊂f m C i − m C j m C i − m C j 2 , n * f ,<label>(11)</label></formula><p>where f and n f indicate a mesh face and unit normal vector of face f , respectively. m C i and m C j denote ith and jth vertex coordinates of M C , respectively. n * f is computed from M C * , where * denotes groundtruth. The loss becomes zero if groundtruth 3D mesh is unavailable.</p><p>Mesh edge length loss. Following Wang et al. <ref type="bibr" target="#b47">[48]</ref>, we supervise edge length of predicted mesh to get visually pleasing mesh result. The L1 loss function for edge length supervision is defined as follows:</p><formula xml:id="formula_14">L edge = f {i,j}⊂f = | m C i − m C j 2 − m C * i − m C * j 2 |,<label>(12)</label></formula><p>where f and * indicate mesh face and groundtruth, respectively. m C i and m C j denote ith and jth vertex coordinates of M C , respectively. The loss becomes zero if groundtruth 3D mesh is unavailable. We train our I2L-MeshNet in an end-to-end manner using all the five loss functions as follows:</p><formula xml:id="formula_15">L = L PoseNet pose + L MeshNet pose + L vertex + λL normal + L edge ,<label>(13)</label></formula><p>where λ = 0.1 is a weight of L normal . For the stable training, we do not backpropagate gradients before P HG .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation details</head><p>PyTorch <ref type="bibr" target="#b35">[36]</ref> is used for implementation. The backbone part is initialized with the publicly released ResNet-50 <ref type="bibr" target="#b11">[12]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b42">[43]</ref>, and the weights of the remaining part are initialized by Gaussian distribution with σ = 0.001. The weights are updated by the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with a mini-batch size of 48. To crop the human region from the input image, we use groundtruth bounding box in both of training and testing stages following previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. When the bounding box is not available in the testing stage, we trained and tested Mask R-CNN <ref type="bibr" target="#b10">[11]</ref> to get the bounding box. The cropped human image is resized to 256×256, thus D = 64 and h = w = 8. Data augmentations including scaling (±25%), rotation (±60°), random horizontal flip, and color jittering (±20%) is performed in training. The initial learning rate is set to 10 −4 and reduced by a factor of 10 at the 10 th epoch. We train our model for 12 epochs with three NVIDIA RTX 2080Ti GPUs, which takes 36 hours for training. Our I2L-MeshNet runs at a speed of 25 frames per second (fps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and evaluation metrics</head><p>Human3.6M. Human3.6M <ref type="bibr" target="#b13">[14]</ref> contains 3.6M video frames with 3D joint coordinate annotations. Because of the license problem, previously used groundtruth SMPL parameters of the Human3.6M are inaccessible. Alternatively, we used SMPLify-X <ref type="bibr" target="#b36">[37]</ref> to obtain groundtruth SMPL parameters. Please see the supplementary material for a detailed description of SMPL parameters of the Hu-man3.6M. MPJPE and PA MPJPE are used for the evaluation <ref type="bibr" target="#b32">[33]</ref>, which is Euclidean distance (mm) between predicted and groundtruth 3D joint coordinates after root joint alignment and further rigid alignment, respectively. 3DPW. 3DPW <ref type="bibr" target="#b28">[29]</ref> contains 60 video sequences captured mostly in outdoor conditions. We use this dataset only for evaluation on its defined test set following Kolotouros et al. <ref type="bibr" target="#b22">[23]</ref>. The same evaluation metrics with Human3.6M (i.e., MPJPE and PA MPJPE) are used, following Kolotouros et al. <ref type="bibr" target="#b22">[23]</ref>. FreiHAND. FreiHAND <ref type="bibr" target="#b7">[8]</ref> contains real-captured 130K training images and 4K test images with MANO pose and shape parameters. The evaluation is performed at an online server. Following Zimmermann et al. <ref type="bibr" target="#b7">[8]</ref>, we report PA MPVPE, PA MPJPE, and F-scores. MSCOCO. MSCOCO <ref type="bibr" target="#b25">[26]</ref> contains large-scale in-the-wild images with 2D bounding box and human joint coordinates annotations. We fit SMPL using SMPLify-X [37] on the groundtruth 2D poses, and used the fitted meshes as groundtruth 3D meshes. This dataset is used only for the training. MuCo-3DHP. MuCo-3DHP <ref type="bibr" target="#b30">[31]</ref> is generated by compositing the existing MPI-INF-3DHP 3D <ref type="bibr" target="#b29">[30]</ref>. 200K frames are composited, and half of them have augmented backgrounds. We used images of MSCOCO dataset that do not include humans to augment the backgrounds following Moon et al. <ref type="bibr" target="#b32">[33]</ref>. This dataset is used only for the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation study</head><p>All models for the ablation study are trained and tested on Human3.6M. As Hu-man3.6M is the most widely used large-scale benchmark, we believe this dataset is suitable for the ablation study. Benefit of the heatmap-based mesh estimation. To demonstrate the benefit of the heatmap-based mesh estimation, we compare models with various target representations of the human mesh, such as SMPL parameters, vertex coordinates, and heatmap. <ref type="table">Table 1</ref> shows MPJPE, the number of parameters, and the GPU memory usage comparison between models with different targets. The table shows that our heatmap-based mesh estimation network achieves the lowest errors while using the smallest number of the parameters and consuming small GPU memory.</p><p>The superiority of our heatmap-based mesh estimation network is in two folds. First, it can model the uncertainty of the prediction. To validate this, we trained two models that estimate the camera-centered mesh vertex coordinates directly and estimates lixel-based 1D heatmap of the coordinates using two fullyconnected layers. Note that the targets of the two models are the same, but their representations are different. As the first network regresses the coordinates directly, it cannot model the uncertainty on the prediction, while the latter one can because of the heatmap target representation. However, both do not preserve the spatial relationship in the input image because of the global average pooling and the fully-connected layers. As the second and third row of the table show, modeling uncertainty on the prediction significantly decreases the errors while using a smaller number of parameters. In addition, it achieves lower errors than the SMPL parameter regression model, which is the most widely used target representation but cannot model the uncertainty. Second, it preserves the spatial relationship between pixels in the input image. The final model estimates the x-and y-axis heatmaps of each mesh vertex in a fully-convolutional way, thus preserves the spatial relationship. It achieves the best performance with the smallest number of the parameters while consuming similar GPU memory usage compared with SMPL parameter regression method that requires the least amount of GPU memory.</p><p>In <ref type="table">Table 1</ref>, all models have the same network architecture with our I2L-MeshNet except for the final output prediction part. We removed PoseNet from all models, and the remaining MeshNet directly estimates targets from the input image I. Except for the last row (ours), all settings output targets using two fully-connected layers. We followed the training details of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref> for the SMPL parameter estimation. Lixel-based vs. pixel-based vs. voxel-based heatmap. To demonstrate the effectiveness of the lixel-based 1D heatmap over other heatmap representations, we train three models that predict lixel-based, pixel-based, and voxel-based heatmap, respectively. We used the same network architecture (i.e., MeshNet of the I2L-MeshNet) for all settings except for the final prediction part. Their networks directly predict the heatmaps from the input image. x-, y-, and z-axis of each heatmap represents the same coordinates. <ref type="table">Table 2</ref> shows memory complexity, heatmap resolution, MPJPE and GPU memory usage comparison between models that predict different target representations of human mesh. The table shows that our lixel-based one achieves the lowest error while consuming small GPU memory usage.</p><p>Compared with the pixel-based and voxel-based heatmap, our lixel-based one consumes much less amount of GPU memory under the same resolution. The 8 × 8 × 8 voxel-based heatmap requires similar GPU memory usage with that of 64, 64, 64 lixel-based one, and we found that enlarging the voxel-based heatmap size from it is not allowed in current GPU memory limit (i.e., 12 GB). The pixelbased heatmap is more efficient than the voxel-based one; however still much inefficient than our lixel-based one, which makes enlarging from 32 × 32, 32 impossible. This inefficient memory usage limits the heatmap resolution; however, we found that the heatmap resolution is critical for dense mesh vertex localization. On the other hand, the memory complexity of our lixel-based heatmap is a linear function with respect to D; thus, we can predict high-resolution heatmap for each mesh vertex. The memory efficiency will be more important when a high-resolution human mesh model is used. Under the same resolution, the combination of pixel-based heatmap and lixelbased heatmap achieves the best performance. We think that estimating the voxel-based heatmap involves too many parameters at a single output layer, which makes it produce high errors. In addition, lixel-based heatmap inherently involves spatial ambiguity arises from marginalizing the 2D feature map to 1D, which can be a possible reason for worse performance than the combined one. Benefit of the cascaded PoseNet and MeshNet. To demonstrate the benefit of the cascaded PoseNet and MeshNet, we trained and tested three networks using various network cascading strategy. First, we removed PoseNet from the I2L-MeshNet. The remaining MeshNet directly predicts lixel-based 1D heatmap of each mesh vertex from the input image. Second, we trained I2L-MeshNet, which has cascaded PoseNet and MeshNet architecture. Third, to check the upper bound accuracy with respect to the output of the PoseNet, we fed the groundtruth 3D human pose instead of the output of the PoseNet to the Mesh-Net in both training and testing stage.  <ref type="table">Table 7</ref>. The PA MPVPE, PA MPJPE, and F-scores comparison between state-ofthe-art methods and the proposed I2L-MeshNet on FreiHAND. The checkmark denotes a method use groundtruth information during inference time.</p><p>that improving the 3D human pose estimation network can be one important way to improve 3D human mesh estimation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with state-of-the-art methods</head><p>Human3.6M and 3DPW. We compare the MPJPE and PA MPJPE of our I2L-MeshNet with previous state-of-the-art 3D human body pose and mesh estimation methods on Human3.6M and 3DPW test set. As each previous work trained their network on different training sets, we report the 3D errors in two ways.</p><p>First, we train all methods on Human3.6M and MSCOCO and report the errors in <ref type="table">Table 4</ref>. The previous state-of-the-art methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> are trained from their officially released codes. The table shows that our I2L-MeshNet significantly outperforms previous methods by a large margin on both datasets.</p><p>Second, we report the 3D errors of previous methods from their papers and ours in <ref type="table">Table 5</ref> and <ref type="table">Table 6</ref>. Each network of the previous method is trained on the different combinations of datasets, which include Human3.6M, MSCOCO, MPII <ref type="bibr" target="#b0">[1]</ref>, LSP <ref type="bibr" target="#b16">[17]</ref>, LSP-Extended <ref type="bibr" target="#b17">[18]</ref>, UP <ref type="bibr" target="#b24">[25]</ref>, and MPI-INF-3DHP <ref type="bibr" target="#b29">[30]</ref>. We used MuCo-3DHP for the additional training dataset for the evaluation on 3DPW dataset. We also report the 3D errors from a additional SMPL parameter regression module following Kolotouros et al. <ref type="bibr" target="#b23">[24]</ref>. The tables show that the performance gap between ours and the previous state-of-the-art method <ref type="bibr" target="#b22">[23]</ref> is significantly reduced.</p><p>The reason for the reduced performance gap is that previous model-based state-of-the-art methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref> can get benefit from many in-the-wild 2D human pose datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref> by a 2D pose-based weak supervision. As the human body or hand model assumes a prior distribution between the human model parameters (i.e., 3D joint rotations and identity vector) and 3D joint/mesh coordinates, the 2D pose-based weak supervision can provide gradients in depth axis, calculated from the prior distribution. Although the weak supervision still suffers from the depth ambiguity, utilizing in-the-wild images can be highly beneficial because the images have diverse appearances compared with those of the lab-recorded 3D datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. On the other hand, model-free approaches, including the proposed I2L-MeshNet, do not assume any prior distri-bution, therefore hard to get benefit from the weak supervision. Based on the two comparisons, we can draw two important conclusions.</p><p>• I2L-MeshNet achieve much higher accuracy than the model-based methods when trained on the same datasets that provide groundtruth 3D human poses and meshes. • The model-based approaches can achieve comparable or higher accuracy by utilizing additional in-the-wild 2D pose data without requiring the 3D supervisions.</p><p>We think that a larger number of accurately aligned in-the-wild image-3D mesh data can significantly boost the accuracy of I2L-MeshNet. The iterative fitting <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>, neural network <ref type="bibr" target="#b18">[19]</ref>, or their combination <ref type="bibr" target="#b22">[23]</ref> can be used to obtain more data. This can be an important future research direction, and we leave this as future work. FreiHAND. We compare MPVPE and F-scores of our I2L-MeshNet with previous state-of-the-art 3D human hand pose and mesh estimation methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. We trained Mask R-CNN <ref type="bibr" target="#b10">[11]</ref> on FreiHAND train images to get the hand bounding box of test images. <ref type="table">Table 7</ref> shows that the proposed I2L-MeshNet significantly outperforms all previous works without groundtruth scale information during the inference time. We additionally report MPJPE in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a I2L-MeshNet, image-to-lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image. We convert the output of the network to the lixel-based 1D heatmap, which preserves the spatial relationship in the input image and models uncertainty of the prediction. Our lixel-based 1D heatmap requires much less GPU memory usage under the same heatmap resolution while producing better accuracy compared with a widely used voxel-based 3D heatmap. Our I2L-MeshNet outperforms previous 3D human pose and mesh estimation methods on various 3D human pose and mesh datasets. We hope our method can give useful insight to the following model-free 3D human pose and mesh estimation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of "I2L-MeshNet: Image-to-Lixel</head><p>Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image"</p><p>In this supplementary material, we present more experimental results that could not be included in the main manuscript due to the lack of space. 7 Various settings of I2L-MeshNet 7.1 When to marginalize 2D to 1D?</p><p>We report how the MPJPE, PA MPJPE, and GPU memory usage change when the marginalization takes place on the ResNet output (i.e., F P or F M ), which is the input of the first upsampling module, instead of the output of the last upsampling module (i.e., f P up (F P ) or f M up (F M )) in <ref type="table" target="#tab_5">Table 8</ref>. For the convenience, we removed PoseNet from our I2L-MeshNet and changed MeshNet to take the input image. The table shows that the early marginalization increases the errors while requiring less amount of GPU memory. This is because the marginalized two 1D feature maps can be generated from multiple 2D feature map, which results in spatial ambiguity. To reduce the effect of this spatial ambiguity, we designed our I2L-MeshNet to extract a sufficient amount of 2D information and then apply the marginalization at the last part of the network instead of applying it in the early stage.</p><p>When the marginalization is applied on the ResNet output F M , all 2D layers (i.e., deconvolutional layers and batch normalization layers) in the upsampling modules are converted to the 1D layers. All models are trained on Human3.6M dataset. The z-axis heatmap prediction part is not changed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">How to marginalize 2D to 1D?</head><p>We report how the MPJPE and PA MPJPE change when different marginalization methods are used in <ref type="table" target="#tab_6">Table 9</ref>. For the convenience, we removed PoseNet from our I2L-MeshNet and changed MeshNet to take the input image. The table shows that our average pooling achieves the lowest errors. Compared with the max pooling that provides the gradients to one pixel position per one x or y position, our average pooling provides the gradients to all pixel positions, which is much richer ones. We implemented the weighted sum by constructing a convolutional layer whose kernel size is (8h, 1) and <ref type="bibr">(1, 8w</ref>) for x-and y-axis lixel-based 1D heatmap prediction, respectively, without padding. The weighted sum provides lower error than that of the max pooling, however still worse than our average pooling. We believe the large size of a kernel of the convolutional layer (i.e., (8h, 1) and <ref type="bibr">(1, 8w)</ref>) is hard to be optimized, which results in higher error than ours. For all settings, models are trained on Human3.6M dataset, and the z-axis heatmap prediction part is not changed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Comparison with previous 2.5D heatmap regression</head><p>We compare the MPJPE and GPU memory usage between a model that predicts our lixel-based 1D heatmap and a model that predicts the 2.5D heatmap <ref type="bibr" target="#b14">[15]</ref> in <ref type="table" target="#tab_8">Table 10</ref>. The 2.5D heatmap <ref type="bibr" target="#b14">[15]</ref> consists of xy heatmap and z heatmap, where xy one is the pixel-based 2D heatmap and z one has the same spatial size with that of xy heatmap and contains root joint-relative depth on the activated xy position for all mesh vertices. They predict the depth values on z heatmap, not the likelihood, thus cannot model uncertainty of the z-axis prediction. As the table shows, our lixel-based one achieves significantly lower error under the same resolution while requiring a much smaller amount of GPU memory. We think that this is because the 2.5D heatmap of Iqbal et al. <ref type="bibr" target="#b14">[15]</ref> cannot model uncertainty of the prediction in z-axis, while ours can. For all settings, models are trained on Human3.6M dataset, and we removed PoseNet and changed MeshNet to take an input image and predict the heatmap.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Effect of each loss function</head><p>We show the effectiveness of the MeshNet pose loss L MeshNet pose in <ref type="table">Table 11</ref>. Although we supervise mesh vertices by the mesh vertex loss L MeshNet pose , additional L MeshNet pose is helpful for human joint-aligned mesh prediction. Both models are trained on Human3.6M dataset.</p><p>For visually pleasant mesh estimation, we use normal vector loss L normal and edge length loss L edge . We show the effectiveness of the two loss functions in <ref type="figure">Figure 4</ref>. As the figure shows, the two loss functions improves visual quality of output meshes. We checked that L normal and L edge marginally affect the MPJPE and PA MPJPE. For all settings, all models are trained on Human3.6M dataset and MSCOCO dataset. All the previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref> used SMPL parameters obtained by applying Mosh <ref type="bibr" target="#b26">[27]</ref> on the marker data of Human3.6M dataset as the groundtruth parameters. However, currently, the distribution of the SMPL parameters from Mosh is disallowed because of the license problem. In addition, the source code of Mosh is not publicly released. Alternatively, we obtain groundtruth SMPL parameters by applying SMPLify-X <ref type="bibr" target="#b36">[37]</ref> on the groundtruth 3D joint coordinates of Human3.6M dataset. Although the obtained SMPL parameters are not perfectly aligned to the groundtruth 3D joint coordinates, we checked that the error of the SMPLify-X is much less than those of current state-of-the-art 3D human pose estimation methods, as shown in <ref type="table" target="#tab_2">Table 13</ref>. Therefore, we think using SMPL parameters from SMPLify-X as groundtruth is reasonable. Note that for a fair comparison, all the experimental results of previous works are reported by training and testing them on our SMPL parameters from SMPLify-X. When fitting, we used neutral gender SMPL body model. However, we found that it produces gender-specific body shapes, although we did not specify gender for each subject.</p><p>As most of the subjects of the training set in Human3.6M dataset are female, we found that our I2L-MeshNet trained on Human3.6M dataset tends to produce female body shape meshes. We tried to fix the identity code of the SMPL body model obtained from the T-pose; however it produces higher errors. Thus, we did not fix the identity code for each subject.</p><p>methods MPJPE Moon et al. <ref type="bibr" target="#b32">[33]</ref> 53.3 Sun et al. <ref type="bibr" target="#b43">[44]</ref> 49.6 Iskakov et al. <ref type="bibr" target="#b15">[16]</ref>* 20.8 SMPLify-X from GT 3D pose 13. <ref type="table" target="#tab_2">1  Table 13</ref>. The MPJPE comparison between SMPLify-X fitting results and state-ofthe-art 3D human pose estimation methods. "*" takes multi-view RGB images as inputs.</p><p>methods MPVPE MPJPE SMPLify <ref type="bibr" target="#b3">[4]</ref> 75.3 -BodyNet <ref type="bibr" target="#b45">[46]</ref> 65.8 40.8 I2L-MeshNet (Ours) 44.7 37. <ref type="table">7  Table 14</ref>. The MPVPE and MPJPE comparison between state-of-the-art methods and the proposed I2L-MeshNet on SURREAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Evaluation on SURREAL</head><p>We additionally provide evaluation results on SURREAL <ref type="bibr" target="#b46">[47]</ref> that contains 67K clips synthesized by animating SMPL body model. We followed the same training and test set split of BodyNet <ref type="bibr" target="#b45">[46]</ref>. For evaluation, mean per-vertex position error (MPVPE), which is averaged per-vertex Euclidean distance error (mm) between predicted and groundtruth 3D mesh coordinates, and MPJPE are used after root joint alignment. We compare MPVPE and MPJPE of our I2L-MeshNet with previous state-of-the-art 3D human body pose and mesh estimation methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46]</ref> on the SURREAL test set. To this end, we reduced the clips in the training set to 1 fps to make the training image set. <ref type="table">Table 14</ref> shows that the proposed I2L-MeshNet significantly outperforms all previous state-of-the-art methods. Especially, it achieves much lower test error compared with BodyNet <ref type="bibr" target="#b45">[46]</ref>, model-free approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Qualitative results</head><p>We provide qualitative results comparison between ours and previous state-ofthe-art model-free method (i.e., GraphCMR <ref type="bibr" target="#b23">[24]</ref>) in <ref type="figure">Figure 5</ref>. As the figure shows, our I2L-MeshNet provides much more visually pleasant mesh results than GraphCMR. We think this is because the graph convolutional network (GraphCNN) often tends to smooth the meshes by averaging the vertex feature with that of neighboring vertices. input image I2L-MeshNet (ours) GraphCMR <ref type="figure">Fig. 5</ref>. Estimated meshes comparisons between our I2L-MeshNet and GraphCMR <ref type="bibr" target="#b23">[24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Network architecture to predict lixel-based 1D heatmaps and visualized examples of feature maps and the 1D heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>used 2D joint heatmaps and silhouette as cues for predicting accurate SMPL parameters. Omran et al.<ref type="bibr" target="#b34">[35]</ref> proposed a similar system, which exploits human part segmentation as a cue for regressing SMPL parameters. Xu et al.<ref type="bibr" target="#b49">[50]</ref> used differentiable rendering to supervise human mesh in the 2D</figDesc><table><row><cell>input image</cell><cell>PoseNet</cell><cell>3D pose ( )</cell></row><row><cell>( )</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MeshNet</cell><cell>3D mesh (</cell><cell>)</cell></row><row><cell cols="4">Fig. 2. Overall pipeline of the proposed I2L-MeshNet.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>targets spatial uncertainty MPJPE no. param. GPU mem. The MPJPE, the number of parameters, and the GPU memory usage comparison between various target representations on Human3.6M. The MPJPE and the GPU memory usage comparison between various heatmap representations on Human3.6M.</figDesc><table><row><cell>SMPL param.</cell><cell></cell><cell>100.3</cell><cell>91M</cell><cell>4.3 GB</cell></row><row><cell>xyz coord.</cell><cell></cell><cell>114.3</cell><cell>117M</cell><cell>5.4 GB</cell></row><row><cell>xyz lixel hm. wo. spatial</cell><cell></cell><cell>92.6</cell><cell>82M</cell><cell>4.5 GB</cell></row><row><cell>xyz lixel hm. (ours)</cell><cell></cell><cell>86.2</cell><cell>73M</cell><cell>4.6 GB</cell></row><row><cell>targets</cell><cell cols="4">mem. complx. resolution MPJPE GPU mem.</cell></row><row><cell>xyz voxel hm.</cell><cell>O(V D 3 )</cell><cell>8×8×8 16×16×16</cell><cell>102.8 -</cell><cell>4.3 GB OOM</cell></row><row><cell></cell><cell></cell><cell>8×8, 8</cell><cell>97.9</cell><cell>3.5 GB</cell></row><row><cell>xy pixel hm. + z lixel hm.</cell><cell>O(V D 2 )</cell><cell>32×32, 32</cell><cell>89.4</cell><cell>5.7 GB</cell></row><row><cell></cell><cell></cell><cell>64×64, 64</cell><cell>-</cell><cell>OOM</cell></row><row><cell></cell><cell></cell><cell>8, 8, 8</cell><cell>100.2</cell><cell>3.4 GB</cell></row><row><cell>xyz lixel hm. (ours)</cell><cell>O(V D)</cell><cell>32, 32 ,32</cell><cell>94.8</cell><cell>4.0 GB</cell></row><row><cell></cell><cell></cell><cell>64, 64, 64</cell><cell>86.2</cell><cell>4.6 GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The MPJPE and PA MPJPE comparison between various network cascading strategies on Human3.6M.</figDesc><table><row><cell>settings</cell><cell cols="3">3D pose MPJPE PA MPJPE</cell></row><row><cell>MeshNet</cell><cell></cell><cell>86.2</cell><cell>59.8</cell></row><row><cell>PoseNet+MeshNet (ours)</cell><cell></cell><cell>81.8</cell><cell>58.0</cell></row><row><cell>MeshNet</cell><cell>GT</cell><cell>25.5</cell><cell>17.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 6 .</head><label>46</label><figDesc>The MPJPE and PA MPJPE comparison on Human3.6M and 3DPW. All methods are trained on Human3.6M and MSCOCO. The MPJPE and PA MPJPE comparison on 3DPW. Each method is trained on different datasets.</figDesc><table><row><cell cols="2">methods</cell><cell cols="5">Human3.6M MPJPE PA MPJPE MPJPE PA MPJPE 3DPW</cell></row><row><cell cols="2">HMR [20]</cell><cell>153.2</cell><cell>85.5</cell><cell cols="2">300.4</cell><cell>137.2</cell></row><row><cell cols="2">GraphCMR [24]</cell><cell>78.3</cell><cell>59.5</cell><cell cols="2">126.5</cell><cell>80.1</cell></row><row><cell cols="2">SPIN [23]</cell><cell>72.9</cell><cell>51.9</cell><cell cols="2">113.1</cell><cell>71.7</cell></row><row><cell cols="2">I2L-MeshNet (Ours)</cell><cell>55.7</cell><cell>41.7</cell><cell cols="2">95.4</cell><cell>60.8</cell></row><row><cell>methods</cell><cell cols="2">MPJPE PA MPJPE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SMPLify [4]</cell><cell>-</cell><cell>82.3</cell><cell>methods</cell><cell></cell><cell cols="2">MPJPE PA MPJPE</cell></row><row><cell>Lassner [25]</cell><cell>-</cell><cell>93.9</cell><cell>HMR [20]</cell><cell></cell><cell>-</cell><cell>81.3</cell></row><row><cell>HMR [20]</cell><cell>88.0</cell><cell>56.8</cell><cell cols="2">Kanazawa [21]</cell><cell>-</cell><cell>72.6</cell></row><row><cell>NBF [35]</cell><cell>-</cell><cell>59.9</cell><cell cols="2">GraphCMR [24]</cell><cell>-</cell><cell>70.2</cell></row><row><cell>Pavlakos [40]</cell><cell>-</cell><cell>75.9</cell><cell>Arnab [2]</cell><cell></cell><cell>-</cell><cell>72.2</cell></row><row><cell>Kanazawa [21]</cell><cell>-</cell><cell>56.9</cell><cell>SPIN [23]</cell><cell></cell><cell>-</cell><cell>59.2</cell></row><row><cell>GraphCMR [24] Arnab [2]</cell><cell>-77.8</cell><cell>50.1 54.3</cell><cell cols="2">I2L-MeshNet (Ours)</cell><cell>93.2</cell><cell>57.7</cell></row><row><cell>SPIN [23]</cell><cell>-</cell><cell>41.1</cell><cell cols="2">I2L-MeshNet</cell><cell></cell></row><row><cell>I2L-MeshNet (Ours)</cell><cell>55.7</cell><cell>41.1</cell><cell cols="2">SMPL regress (Ours) +</cell><cell>100.0</cell><cell>60.0</cell></row><row><cell cols="3">Table 5. The MPJPE and PA</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MPJPE comparison on Human3.6M.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Each method is trained on different</cell><cell></cell><cell></cell><cell></cell></row><row><cell>datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>shows utilizing the output of the PoseNet (the second row) achieves better accuracy compared with using only MeshNet (the first row) to estimate the human mesh. Interestingly, passing the groundtruth 3D human pose to the MeshNet (the last row) significantly improves the performance compared with all the other settings. This indicates</figDesc><table><row><cell>methods</cell><cell cols="4">PA MPVPE PA MPJPE F@5 mm F@15 mm GT scale</cell></row><row><cell>Hasson et al. [10]</cell><cell>13.2</cell><cell>-</cell><cell>0.436</cell><cell>0.908</cell></row><row><cell>Boukhayma et al. [5]</cell><cell>13.0</cell><cell>-</cell><cell>0.435</cell><cell>0.898</cell></row><row><cell>FreiHAND [8]</cell><cell>10.7</cell><cell>-</cell><cell>0.529</cell><cell>0.935</cell></row><row><cell>I2L-MeshNet (Ours)</cell><cell>7.6</cell><cell>7.4</cell><cell>0.681</cell><cell>0.973</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>The MPJPE, PA MPJPE, and GPU memory usage comparison between various marginalization settings on Human3.6M dataset.</figDesc><table><row><cell>settings</cell><cell>MPJPE</cell><cell cols="2">PA MPJPE GPU mem.</cell></row><row><cell>avg on FM</cell><cell>93.5</cell><cell>64.1</cell><cell>4.4 GB</cell></row><row><cell>avg on f M up (F M ) (ours)</cell><cell>86.2</cell><cell>59.8</cell><cell>4.6 GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>The MPJPE and PA MPJPE comparison between various marginalization settings on Human3.6M dataset.</figDesc><table><row><cell>settings</cell><cell>MPJPE</cell><cell>PA MPJPE</cell></row><row><cell>max pooling</cell><cell>93.5</cell><cell>64.1</cell></row><row><cell>weighted sum</cell><cell>89.4</cell><cell>61.4</cell></row><row><cell>avg pooling (ours)</cell><cell>86.2</cell><cell>59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>The MPJPE and GPU memory usage comparison between various marginalization settings on Human3.6M dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .Table 12 .</head><label>1112</label><figDesc>The MPJPE and PA MPJPE comparison between models trained with and without L MeshNet pose on Human3.6M dataset.We provide the MPJPE and PA MPJPE of PoseNet from I2L-MeshNet in Table 12. The PoseNet is trained with MeshNet by minimizing the loss function L. As our PoseNet predicts 3D joint coordinates of the SMPL body joint set or MANO hand joint set, we calculate the errors using groundtruth SMPL or MANO 3D joint coordinates. We could not calculate the MPJPE on FreiHAND dataset because the official evaluation server does not support it. The MPJPE and PA MPJPE of PoseNet on each dataset.11 Pseudo-groundtruth SMPL parameters ofHuman3.6M dataset</figDesc><table><row><cell></cell><cell>settings</cell><cell></cell><cell>MPJPE</cell><cell cols="2">PA MPJPE</cell></row><row><cell></cell><cell>wo. L MeshNet pose</cell><cell></cell><cell>84.5</cell><cell></cell><cell>58.5</cell></row><row><cell></cell><cell>w. L MeshNet pose</cell><cell></cell><cell>81.8</cell><cell></cell><cell>58.0</cell></row><row><cell>input image</cell><cell>without</cell><cell>with</cell><cell cols="2">input image</cell><cell>without</cell><cell>with</cell></row><row><cell cols="7">Fig. 4. Estimated meshes from models trained with different combinations of loss</cell></row><row><cell>functions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">10 Accuracy of PoseNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>datasets</cell><cell></cell><cell>MPJPE</cell><cell cols="2">PA MPJPE</cell></row><row><cell></cell><cell>Human3.6M</cell><cell></cell><cell>62.2</cell><cell>47.2</cell><cell></cell></row><row><cell></cell><cell>3DPW</cell><cell></cell><cell>112.2</cell><cell>72.3</cell><cell></cell></row><row><cell></cell><cell>SURREAL</cell><cell></cell><cell>40.0</cell><cell>29.5</cell><cell></cell></row><row><cell></cell><cell>FreiHAND</cell><cell></cell><cell>n/a</cell><cell>8.0</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">I2L-MeshNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pushing the envelope for RGB-based dense 3D hand pose estimation via neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3D hand shape and pose from images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pose2Mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">FreiHAND: A dataset for markerless capture of hand pose and shape from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y B R M A</forename><surname>Duygu Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">3D hand shape and pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning joint reconstruction of hands and manipulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kalevatykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments. TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hand pose estimation via latent 2.5 d heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuel Juergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human pose fitting towards in-the-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03686</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3D pose estimation from monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">V2V-PoseNet: Voxel-to-voxel prediction network for accurate 3D hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural Body Fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>3DV. IEEE</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">TexturePose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">LCR-Net: Localization-classificationregression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">DenseRaC: Joint 3D pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
