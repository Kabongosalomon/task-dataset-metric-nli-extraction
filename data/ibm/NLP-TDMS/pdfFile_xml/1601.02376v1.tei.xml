<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning over Multi-field Categorical Data -A Case Study on User Response Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-01-11">11 Jan 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
							<email>2dutianming@quicloud.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">RayCloud Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>j.wang@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning over Multi-field Categorical Data -A Case Study on User Response Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-01-11">11 Jan 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users' ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>User response (e.g., click-through or conversion) prediction plays a critical part in many web applications including web search, recommender systems, sponsored search, and display advertising. In online advertising, for instance, the ability of targeting individual users is the key advantage compared to traditional offline advertising. All these targeting techniques, essentially, rely on the system function of predicting whether a specific user will think the potential ad is "relevant", i.e., the probability that the user in a certain context will click a given ad <ref type="bibr" target="#b6">[6]</ref>. Sponsored search, contextual advertising, and the recently emerged realtime bidding (RTB) display advertising all heavily rely on the ability of learned models to predict ad click-through rates (CTR) <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b41">41]</ref>. The applied CTR estimation models today are mostly linear, ranging from logistic regression <ref type="bibr" target="#b32">[32]</ref> and naive Bayes <ref type="bibr" target="#b14">[14]</ref> to FTRL logistic regression <ref type="bibr" target="#b28">[28]</ref> and Bayesian probit regression <ref type="bibr" target="#b12">[12]</ref>, all of which are based on a huge number of sparse features with one-hot encoding <ref type="bibr" target="#b1">[1]</ref>. Linear models have advantages of easy implementation, efficient learning but relative low performance because of the failure of learning the nontrivial patterns to catch the interactions between the assumed (conditionally) independent raw features <ref type="bibr" target="#b12">[12]</ref>. Non-linear models, on the other hand, are able to utilise different feature combinations and thus could potentially improve estimation performance. For example, factorisation machines (FMs) <ref type="bibr" target="#b29">[29]</ref> map the user and item binary features into a low dimensional continuous space. And the feature interaction is automatically explored via vector inner product. Gradient boosting trees <ref type="bibr" target="#b38">[38]</ref> automatically learn feature combinations while growing each decision/regression tree. However, these models cannot make use of all possible combinations of different features <ref type="bibr" target="#b20">[20]</ref>. In addition, many models require feature engineering that manually designs what the inputs should be. Another problem of the mainstream ad CTR estimation models is that most prediction models have shallow structures and have limited expression to model the underlying patterns from complex and massive data <ref type="bibr" target="#b15">[15]</ref>. As a result, their data modelling and generalisation ability is still restricted.</p><p>Deep learning <ref type="bibr" target="#b25">[25]</ref> has become successful in computer vision <ref type="bibr" target="#b22">[22]</ref>, speech recognition <ref type="bibr" target="#b13">[13]</ref>, and natural language processing (NLP) <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b33">33]</ref> during recent five years. As visual, aural, and textual signals are known to be spatially and/or temporally correlated, the newly introduced unsupervised training on deep structures <ref type="bibr" target="#b18">[18]</ref> would be able to explore such local dependency and establish a dense representation of the feature space, making neural network models effective in learning high-order features directly from the raw feature input. With such learning ability, deep learning would be a good candidate to estimate online user response rate such as ad CTR. However, most input features in CTR estimation are of multi-field and are discrete categorical features, e.g., the user location city (London, Paris), device type (PC, Mobile), ad category (Sports, Electronics) etc., and their local dependencies (thus the sparsity in the feature space) are unknown. Therefore, it is of great interest to see how deep learning improves the CTR estimation via learning feature representation on such large-scale multifield discrete categorical features. To our best knowledge, there is no previous literature of ad CTR estimation using deep learning methods thus far 1 . In addition, training deep neural networks (DNNs) on a large input feature space requires tuning a huge number of parameters, which is computationally expensive. For instance, unlike image and audio cases, we have about 1 million binary input features and 100 hidden units in the first layer; then it requires 100 million links to build the first layer neural network.</p><p>In this paper, we take ad CTR estimation as a working example to study deep learning over a large multi-field categorical feature space by using embedding methods in both supervised and unsupervised fashions. We introduce two types of deep learning models, called Factorisation Machine supported Neural Network (FNN) and Sampling-based Neural Network (SNN). Specifically, FNN with a supervised-learning embedding layer using factorisation machines <ref type="bibr" target="#b31">[31]</ref> is proposed to efficiently reduce the dimension from sparse features to dense continuous features. The second model SNN is a deep neural network powered by a sampling-based restricted Boltzmann machine (SNN-RBM) or a samplingbased denoising auto-encoder (SNN-DAE) with a proposed negative sampling method. Based on the embedding layer, we build multiple layers neural nets with full connections to explore non-trivial data patterns. Our experiments on multiple real-world advertisers' ad click data have demonstrated the consistent improvement of CTR estimation from our proposed models over the state-ofthe-art ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Click-through rate, defined as the probability of the ad click from a specific user on a displayed ad, is essential in online advertising <ref type="bibr" target="#b39">[39]</ref>. In order to maximise revenue and user satisfaction, online advertising platforms must predict the expected user behaviour for each displayed ad and maximise the expectation that users will click. The majority of current models use logistic regression based on a set of sparse binary features converted from the original categorical features via one-hot encoding <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b32">32]</ref>. Heavy engineering efforts are needed to design features such as locations, top unigrams, combination features, etc. <ref type="bibr" target="#b15">[15]</ref>.</p><p>Embedding very large feature vector into low-dimensional vector spaces is useful for prediction task as it reduces the data and model complexity and improves both the effectiveness and the efficiency of the training and prediction. Various methods of embedding architectures have been proposed <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b23">23]</ref>. Factorisation machine (FM) <ref type="bibr" target="#b31">[31]</ref>, originally proposed for collaborative filtering recommendation, is regarded as one of the most successful embedding models. FM naturally has the capability of estimating interactions between any two features via mapping them into vectors in a low-rank latent space.</p><p>Deep Learning <ref type="bibr" target="#b2">[2]</ref> is a branch of artificial intelligence research that attempts to develop the techniques that will allow computers to handle complex tasks such as recognition and prediction at high performance. Deep neural networks (DNNs) are able to extract the hidden structures and intrinsic patterns at different levels of abstractions from training data. DNNs have been successfully applied in computer vision <ref type="bibr" target="#b40">[40]</ref>, speech recognition <ref type="bibr" target="#b8">[8]</ref> and natural language processing (NLP) <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b33">33]</ref>. Furthermore, with the help of unsupervised pre-training, we can get good feature representation which guides the learning towards basins of attraction of minima that support better generalisation from the training data <ref type="bibr" target="#b10">[10]</ref>. Usually, these deep models have two stages in learning <ref type="bibr" target="#b18">[18]</ref>: the first stage performs model initialisation via unsupervised learning (i.e., the restricted Boltzmann machine or stacked denoising auto-encoders) to make the model catch the input data distribution; the second stage involves a fine tuning of the initialised model via supervised learning with back-propagation. The novelty of our deep learning models lies in the first layer initialisation, where the input raw features are high dimensional and sparse binary features converted from the raw categorical features, which makes it hard to train traditional DNNs in large scale. Compared with the word-embedding techniques used in NLP <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b33">33]</ref>, our models deal with more general multi-field categorical features without any assumed data structures such as word alignment and letter-n-gram etc. Neural Networks (SNN). The input categorical features are field-wise one-hot encoded. For each field, e.g., city, there are multiple units, each of which represents a specific value of this field, e.g., city=London, and there is only one positive (1) unit, while all others are negative (0). The encoded features, denoted as x, are the input of many CTR estimation models <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b26">26]</ref> as well as our DNN models, as depicted at the bottom layer of <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Factorisation-machine supported Neural Networks (FNN)</head><p>Our first model FNN is based on the factorisation machine as the bottom layer. The network structure is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. With a top-down description, the output unit is a real numberŷ ∈ (0, 1) as predicted CTR, i.e., the probability of a specific user clicking a given ad in a certain context:</p><formula xml:id="formula_0">y = sigmoid(W 3 l 2 + b 3 ),<label>(1)</label></formula><p>where sigmoid(x) = 1/(1 + e −x ) is the logistic activation function, W 3 ∈ R 1×L , b 3 ∈ R and l 2 ∈ R L as input for this layer. The calculation of l 2 is</p><formula xml:id="formula_1">l 2 = tanh(W 2 l 1 + b 2 ),<label>(2)</label></formula><p>where tanh(</p><formula xml:id="formula_2">x) = (1 − e −2x )/(1 + e −2x ), W 2 ∈ R L×M , b 2 ∈ R L and l 1 ∈ R M .</formula><p>We choose tanh(·) as it has optimal empirical learning performance than other activation functions, as will be discussed in Section 4.3. Similarly,</p><formula xml:id="formula_3">l 1 = tanh(W 1 z + b 1 ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">W 1 ∈ R M×J , b 1 ∈ R M and z ∈ R J . z = (w 0 , z 1 , z 2 , ...z i , ..., z n ),<label>(4)</label></formula><p>where w 0 ∈ R is a global scalar parameter and n is the number of fields in total. z i ∈ R K+1 is a parameter vectors for the i-th field in factorisation machines:</p><formula xml:id="formula_5">z i = W i 0 · x[start i : end i ] = (w i , v 1 i , v 2 i , . . . , v K i ),<label>(5)</label></formula><p>where start i and end i are starting and ending feature indexes of the i-th field,</p><formula xml:id="formula_6">W i 0 ∈ R (K+1)×(endi−starti+1</formula><p>) and x is the input vector as described at beginning. All weights W i 0 are initialised with the bias term w i and vector v i respectively (e.g.,</p><formula xml:id="formula_7">W i 0 [0] is initialised by w i , W i 0 [1] is initialised by v 1 i , W i 0 [2] is initialised by v 2 i , etc.</formula><p>). In this way, z vector of the first layer is initialised as shown in <ref type="figure" target="#fig_0">Figure 1</ref> via training a factorisation machine (FM) <ref type="bibr" target="#b31">[31]</ref>:</p><formula xml:id="formula_8">y FM (x) := sigmoid w 0 + N i=1 w i x i + N i=1 N j=i+1 v i , v j x i x j ,<label>(6)</label></formula><p>where each feature i is assigned with a bias weight w i and a K-dimensional vector v i and the feature interaction is modelled as their vectors' inner product v i , v j . In this way, the above neural nets can learn more efficiently from factorisation machine representation so that the computational complexity problem of the high-dimensional binary inputs has been naturally bypassed. Different hidden layers can be regarded as different internal functions capturing different forms of representations of the data instance. For this reason, this model has more abilities of catching intrinsic data patterns and leads to better performance. The idea using FM in the bottom layer is ignited by Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b11">[11]</ref>, which exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers. Similarly, the inputs of hidden layer 1 are connected to the input units of a specific field. Also, the bottom layer is not fully connected as FM performs a field-wise training for one-hot sparse encoded input, allowing local sparsity, illustrated as the dash lines in <ref type="figure" target="#fig_0">Figure 1</ref>. FM learns good structural data representation in the latent space, helpful for any further model to build on. A subtle difference, though, appears between the product rule of FM and the sum rule of DNN for combination. However, according to <ref type="bibr" target="#b21">[21]</ref>, if the observational discriminatory information is highly ambiguous (which is true in our case for ad click behaviour), the posterior weights (from DNN) will not deviate dramatically from the prior (FM).</p><p>Furthermore, the weights in hidden layers (except the FM layer) are initialised by layer-wise RBM pre-training <ref type="bibr" target="#b3">[3]</ref> using contrastive divergence <ref type="bibr" target="#b17">[17]</ref>, which effectively preserves the information in input dataset as detailed in <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b16">16]</ref>. The initial weights for FMs are trained by stochastic gradient descent (SGD), as detailed in <ref type="bibr" target="#b31">[31]</ref>. Note that we only need to update weights which connect to the positive input units, which largely reduces the computational complexity. After pre-training of the FM and upper layers, supervised fine-tuning (back propagation) is applied to minimise loss function of cross entropy:</p><formula xml:id="formula_9">L(y,ŷ) = −y logŷ − (1 − y) log(1 −ŷ),<label>(7)</label></formula><p>whereŷ is the predicted CTR in Eq. (1) and y is the binary click ground-truth label. Using the chain rule of back propagation, the FNN weights including FM weights can be efficiently updated. For example, we update FM layer weights via</p><formula xml:id="formula_10">∂L(y,ŷ) ∂W i 0 = ∂L(y,ŷ) ∂z i ∂z i ∂W i 0 = ∂L(y,ŷ) ∂z i x[start i : end i ]<label>(8)</label></formula><formula xml:id="formula_11">W i 0 ← W i 0 − η · ∂L(y,ŷ) ∂z i x[start i : end i ].<label>(9)</label></formula><p>Due to the fact that the majority entries of x[start i : end i ] are 0, we can accelerate fine-tuning by updating weights linking to positive units only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sampling-based Neural Networks (SNN)</head><p>The structure of the second model SNN is shown in <ref type="figure">Figure 2</ref> </p><p>To initialise the weights of the bottom layer, we tried both restricted Boltzmann machine (RBM) <ref type="bibr" target="#b16">[16]</ref> and denoising auto-encoder (DAE) <ref type="bibr" target="#b4">[4]</ref> in the pretraining stage. In order to deal with the computational problem of training large sparse one-hot encoding data, we propose a sampling-based RBM <ref type="figure">(Figure 2(b)</ref>, denoted as SNN-RBM) and a sampling-based DAE in <ref type="figure">(Figure 2(c)</ref>, denoted as SNN-DAE) to efficiently calculate the initial weights of the bottom layer.</p><p>Instead of modelling the whole feature set for each training instance set, for each feature field, e.g., city, there is only one positive value feature for each training instance, e.g., city=London, we sample m negative units, e.g., city=Paris when m = 1, randomly with value 0. Black units in <ref type="figure">Figure 2</ref>(b) and 2(c) are unsampled and thus ignored when pre-training the data instance. With the sampled units, we can train an RBM via contrastive divergence <ref type="bibr" target="#b17">[17]</ref> and a DAE via SGD with unsupervised approaches to largely reduce the data dimension with high recovery performance. The real-value dense vector is used as the input of the further layers in SNN.</p><p>In this way, computational complexity can be dramatically reduced and, in turn, initial weights can be calculated quickly and back-propagation is then performed to fine-tune SNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularisation</head><p>To prevent overfitting, the widely used L2 regularisation term is added to the loss function. For example, the L2 regularisation for FNN in <ref type="figure" target="#fig_0">Figure 1</ref> is</p><formula xml:id="formula_13">Ω(w) = ||W 0 || 2 2 + 3 l=1 ||W l || 2 2 + ||b l || 2 2 .<label>(11)</label></formula><p>On the other hand, dropout <ref type="bibr" target="#b35">[35]</ref> is a technique which becomes a popular and effective regularisation technique for deep learning during the recent years. We also implement this regularisation and compare them in our experiment. Our experiment code 2 of both FNN and SNN is implemented with Theano 3 . Metric. To measure the CTR estimation performance of each model, we employ the area under ROC curve (AUC) 4 . The AUC <ref type="bibr" target="#b12">[12]</ref> metric is a widely used measure for evaluating the CTR performance. <ref type="table">Table 1</ref> shows the results that compare LR, FM, FNN and SNN with RBM and DAE on 5 different advertisers and the whole dataset. We observe that FM is not significantly better than LR, which means 2-order combination features might not be good enough to catch the underlying data patterns. The AUC performance of the proposed FNN and SNN is better than the performance of LR and FM on all tested datasets. Based on the latent structure learned by FM, FNN further learns effective patterns between these latent features and provides a consistent improvement over FM. The performance of SNN-DAE and SNN-RBM is generally consistent, i.e., the relative order of the results of the SNN are almost the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameter Tuning</head><p>Due to the fact that deep neural networks involve many implementation details and need to tune a fairly large number of hyper-parameters, following details show how we implement our models and tune hyperparameters in the models. We use stochastic gradient descent to learn most of our parameters for all proposed models. Regarding selecting the number of training epochs, we use early stopping <ref type="bibr" target="#b30">[30]</ref>, i.e., the training stops when the validation error increases. We try different learning rate from 1, 0.1, 0.01, 0.001 to 0.0001 and choose the one with optimal performance on the validation dataset.</p><p>For negative unit sampling of SNN-RBM and SNN-DAE, we try the negative sample number m = 1, 2 and 4 per field as described in Section 3.2, and find m = 2 produces the best results in most situations. For the activation functions in both models on the hidden layers (as Eqs. <ref type="formula" target="#formula_3">(3)</ref> and <ref type="formula" target="#formula_1">(2)</ref>), we try linear function, sigmoid function and tanh function, and find the result of tanh function is optimal. This might be because the hyperbolic tangent often converges faster than the sigmoid function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Architecture Selection</head><p>In our models, we investigate architectures with 3, 4 and 5 hidden layers by fixing all layer sizes and find the architecture with 3 hidden layers (i.e., 5 layers in total) is the best in terms of AUC performance. However, the range of choosing their layer sizes is exponential in the number of hidden layers. Suppose there is a deep neural network with L hidden layers and each of the hidden layers is trained with a range of hidden units from 100 to 500 with increments of 100, thus there are 5 L models in total to compare. Instead of trying all combinations of hidden units, in our experiment we use another strategy by starting tuning the different hidden layer sizes with the same number of hidden units in all three hidden layers 5 since the architecture with equal-size hidden layers is empirically better than the architecture with increasing width or decreasing width in <ref type="bibr" target="#b24">[24]</ref>. For this reason, we start tuning layer sizes with equal hidden layer sizes. In fact, apart from increasing, constant, decreasing layer sizes, there is a more effective structure, which is the diamond shape of neural networks, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>(a). We compare our diamond shape network with other three shapes of networks and tune the total number of total hidden units on two different datasets shown in Figures 3(b) and 3(c). The diamond shape architecture outperforms others in almost all layer size settings. The reason why this diamond shape works might be because this special shape of neural network has certain constraint to the capacity of the neural network, which provides better generalisation on test sets. On the other hand, the performance of diamond architecture picks at the total hidden unit size of 600, i.e., the combination of (200, 300, 100). This depends on the training data observation numbers. Too many hidden units against a limited dataset could cause overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Regularisation Comparison</head><p>Neural network training algorithms are very sensitive to the overfitting problem since deep networks have multiple non-linear layers, which makes them very expressive models that can learn very complicated functions. For DNN models, we compared L2 regularisation (Eq. (11)) and dropout <ref type="bibr" target="#b35">[35]</ref> for preventing complex co-adaptations on the training data. The dropout rate implemented in this experiment refers to the probability of each unit being active.  <ref type="figure" target="#fig_4">Figure 4(a)</ref> shows the compared AUC performance of SNN-RBM regularised by L2 norm and dropout. It is obvious that dropout outperforms L2 in all compared settings. The reason why dropout is more effective is that when feeding each training case, each hidden unit is stochastically excluded from the network with a probability of dropout rate, i.e., each training case can be regarded as a new model and these models are averaged as a special case of bagging <ref type="bibr" target="#b5">[5]</ref>, which effectively improves the generalisation ability of DNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of Parameters</head><p>As a summary of Sections 4.4 and 4.5, for both FNN and SNN, there are two important parameters which should be tuned to make the model more effective: (i) the parameters of layer size decide the architecture of the neural network and (ii) the parameter of dropout rate changes generalisation ability on all datasets compared to neural networks just with L2 regularisation. <ref type="figure" target="#fig_4">Figures 4(b)</ref> and 4(c) show how the AUC performance changes with the increasing of dropout in both FNN and SNN. We can find that there is an upward trend of performance in both models at the beginning and then drop sharply with continuous decreasing of dropout rate. The distinction between two models is the different sensitivities of the dropout. From <ref type="figure" target="#fig_4">Figure 4</ref>(c), we can see the model SNN is sensitive to the dropout rate. This might be caused by the connectivities in the bottom layer. The bottom layer of the SNN is fully connected with the input vector while the bottom layer for FNN is partially connected and thus the FNN is more robust when some hidden units are dropped out. Furthermore, the sigmoid activation function tend to more effective than the linear activation function in terms of dropout. Therefore, the dropout rates at the best performance of FNN and SNN are quite different. For FNN the optimal dropout rate is around 0.8 while for SNN is about 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigated the potential of training deep neural networks (DNNs) to predict users' ad click response based on multi-field categorical features. To deal with the computational complexity problem of high-dimensional discrete categorical features, we proposed two DNN models: field-wise feature embedding with supervised factorisation machine pre-training, and fully connected DNN with field-wise sampling-based RBM and DAE unsupervised pretraining. These architectures and pre-training algorithms make our DNNs trained very efficiently. Comprehensive experiments on a public real-world dataset verifies that the proposed DNN models successfully learn the underlying data patterns and provide superior CTR estimation performance than other compared models. The proposed models are very general and could enable a wide range of future works. For example, the model performance can be improved by momentum methods in that it suffices for handling the curvature problems in DNN training objectives without using complex second-order methods <ref type="bibr" target="#b36">[36]</ref>. In addition, the partial connection in the bottom layer could be extended to higher hidden layers as partial connectivities have many advantages such as lower complexity, higher generalisation ability and more similar to human brain <ref type="bibr" target="#b9">[9]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A 4-layer FNN model structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Fig. 2 .</head><label>22</label><figDesc>SNN First Layer Pre-Trained with Sampling-based RBM (c) SNN First Layer Pre-Trained with Sampling-based DAE Field A 4-layer SNN architecture and two first-layer pre-training methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a). The difference between SNN and FNN lies in the structure and training method in the bottom layer. SNN's bottom layer is fully connected with sigmoid activation function: z = sigmoid(W 0 x + b 0 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>AUC Performance with different architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>AUC performance w.r.t difference regularisation settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We evaluate our models based on iPinYou dataset<ref type="bibr" target="#b27">[27]</ref>, a public realworld display ad dataset with each ad display information and corresponding user click feedback. The data logs are organised by different advertisers and in a row-per-record format. There are 19.50M data instances with 14.79K positive label (click) in total. The features for each data instance are all categorical. Feature examples in the ad log data are user agent, partially masked IP, region, city, ad exchange, domain, URL, ad slot ID, ad slot visibility, ad slot size, ad slot format, creative ID, user tags, etc. After one-hot encoding, the number of binary features is 937.67K in the whole dataset. We feed each compared model with these binary-feature data instances and the user click (1) and non-click (0) feedback as the ground-truth labels. In our experiments, we use training data from advertiser 1458, 2259, 2261, 2997, 3386 and the whole dataset, respectively. Models. We compare the performance of the following CTR estimation models:LR: Logistic Regression<ref type="bibr" target="#b32">[32]</ref> is a linear model with simple implementation and fast training speed, which is widely used in online advertising estimation. FM: Factorisation Machine<ref type="bibr" target="#b31">[31]</ref> is a non-linear model able to estimate feature interactions even in problems with huge sparsity. FNN: Factorisation-machine supported Neural Network is our proposed model as described in Section 3.1. SNN: Sampling-based Neural Network is also our proposed model with samplingbased RBM and DAE pre-training methods for the first layer in Section 3.2, denoted as SNN-RBM and SNN-DAE respectively.</figDesc><table><row><cell>4 Experiment</cell></row><row><cell>4.1 Experiment Setup</cell></row><row><cell>Data.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although the leverage of deep learning models on ad CTR estimation has been claimed in industry (e.g.,<ref type="bibr" target="#b42">[42]</ref>), there is no detail of the models or implementation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The source code with demo data: https://github.com/wnzhang/deep-ctr 3 Theano: http://deeplearning.net/software/theano/ 4 Besides AUC, root mean square error (RMSE) is also tested. However, positive/negative examples are largly unbalanced in ad click scenario, and the empirically best regression model usually provides the predicted CTR close to 0, which results in very small RMSE values and thus the improvement is not well captured.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Some advanced Bayesian methods for hyperparameter tuning<ref type="bibr" target="#b34">[34]</ref> are not considered in this paper and may be investigated in the future work.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lr Fm Fnn Snn-Dae</forename><surname>Snn-Rbm</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-level student modeling with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Woolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent tutoring systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="584" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalized denoising auto-encoders as generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="899" to="907" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computational advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: SODA</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="992" to="992" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="6669" to="6673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A survey of partially connected neural networks. International journal of neural systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elizondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="535" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft&apos;s bing search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP. pp</title>
		<imprint>
			<biblScope unit="page" from="6645" to="6649" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Idiot&apos;s bayesnot so stupid after all?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International statistical review</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="398" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ADKDD. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Momentum</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">926</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3 idiots approach for display advertising challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet and Network Economics</title>
		<imprint>
			<biblScope unit="page" from="254" to="265" />
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Probabilistic latent network visualization: inferring and embedding diffusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1236" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Estimating conversion rate in display advertising from past performance data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Orten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="768" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ipinyou global rtb bidding algorithm competition dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ADKDD. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Predicting response in mobile advertising with hierarchical importance-aware factorization machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J W</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finegold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>WSDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic early stopping using cross validation: quantifying the criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Predicting clicks: estimating the clickthrough rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>WWW</publisher>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using boosted trees for click-through rate prediction for sponsored search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trofimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornetova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Topinskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WINE. p</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2012" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Click-through rate estimation for rare events in online advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online Multimedia Advertising: Techniques and Technologies</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. pp</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Optimal real-time bidding for display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mariana: Tencent deep learning platform and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1772" to="1777" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
