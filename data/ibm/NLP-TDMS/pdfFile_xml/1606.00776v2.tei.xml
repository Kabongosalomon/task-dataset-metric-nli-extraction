<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-14">14 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><forename type="middle">Vlad</forename><surname>Serban</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<email>zhou@us.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff7">
								<orgName type="laboratory">CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
							<email>aaron.courville@umontreal.caemail:tklinger</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">IBM Research T. J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">IBM Research T. J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">IBM Research T. J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">IBM Research T. J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-14">14 Jun 2016</date>
						</imprint>
					</monogr>
					<note>* This work was carried out while the first author was at IBM Research. •</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard loglikelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks (RNNs) have been gaining popularity in the machine learning community due to their impressive performance on tasks such as machine translation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5]</ref> and speech recognition <ref type="bibr" target="#b9">[10]</ref>. These results have spurred a cascade of novel neural network architectures <ref type="bibr" target="#b14">[15]</ref>, including attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, memory <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref> and pointer-based mechanisms <ref type="bibr" target="#b18">[19]</ref>.</p><p>The majority of the previous work has focused on developing new neural network architectures within the deterministic sequence-to-sequence framework. In other words, it has focused on changing the parametrization of the deterministic function mapping input sequences to output sequences, trained by maximizing the log-likelihood of the observed output sequence. Instead, we pursue a complimentary research direction aimed at generalizing the sequence-to-sequence framework to multiple input and output sequences, where each sequence exhibits its own stochastic process. We propose a new class of RNN models, called multiresolution recurrent neural networks (MrRNNs), which model multiple parallel sequences by factorizing the joint probability over the sequences. In particular, we impose a hierarchical structure on the sequences, such that information from high-level (abstract) sequences flows to low-level sequences (e.g. natural language sequences). This architecture exhibits a new objective function for training: the joint log-likelihood over all observed parallel sequences (as opposed to the log-likelihood over a single sequence), which biases the model towards modeling high-level abstractions. At test time, the model generates first the high-level sequence and afterwards the natural language sequence. This hierarchical generation process enables it to model complex output sequences with long-term dependencies.</p><p>Researchers have recently observed critical problems applying end-to-end neural network architectures for dialogue response generation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b15">16]</ref>. The neural networks have been unable to generate meaningful responses taking dialogue context into account, which indicates that the models have failed to learn useful high-level abstractions of the dialogue. Motivated by these shortcomings, we apply the proposed model to the task of dialogue response generation in two challenging domains: the goal-oriented Ubuntu technical support domain and non-goal-oriented Twitter conversations. In both domains, the model outperforms competing approaches. In particular, for Ubuntu, the model outperforms competing approaches by a substantial margin according to both a human evaluation study and automatic evaluation metrics achieving a new state-of-the-art result.</p><p>2 Model Architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recurrent Neural Network Language Model</head><p>We start by introducing the well-established recurrent neural network language model (RNNLM) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3]</ref>. RNNLM variants have been applied to diverse sequential tasks, including dialogue modeling <ref type="bibr" target="#b27">[28]</ref>, speech synthesis <ref type="bibr" target="#b6">[7]</ref>, handwriting generation <ref type="bibr" target="#b7">[8]</ref> and music composition <ref type="bibr" target="#b3">[4]</ref>. Let w 1 , . . . , w N be a sequence of discrete variables, called tokens (e.g. words), such that w n ∈ V for vocabulary V . The RNNLM is a probabilistic generative model, with parameters θ, which decomposes the probability over tokens: P θ (w 1 , . . . , w N ) = N n=1 P θ (w n |w 1 , . . . , w n−1 ).</p><p>(</p><p>where the parametrized approximation of the output distribution uses a softmax RNN: P θ (w n+1 = v|w 1 , . . . , w n ) = exp(g(h n , v)) v ∈V exp(g(h n , v ))</p><p>,</p><p>h n = f (h n−1 , w n ), g(h n , v)</p><formula xml:id="formula_2">= O T v h n ,<label>(3)</label></formula><p>where f is the hidden state update function, which we will assume is either the LSTM gating unit <ref type="bibr" target="#b10">[11]</ref> or GRU gating unit <ref type="bibr" target="#b4">[5]</ref> throughout the rest of the paper. For the LSTM gating unit, we consider the hidden state h m to be the LSTM cell and cell input hidden states concatenated. The matrix I ∈ R d h ×|V | is the input word embedding matrix, where column j contains the embedding for word index j and d h ∈ N is the word embedding dimensionality. Similarly, the matrix O ∈ R d h ×|V | is the output word embedding matrix. According to the model, the probability of observing a token w at position n + 1 increases if the context vector h n has a high dot-product with the word embedding corresponding to token w. Most commonly the model parameters are learned by maximizing the log-likelihood (equivalent to minimizing the cross-entropy) on the training set using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Recurrent Encoder-Decoder</head><p>Our work here builds upon that of Sordoni et al. <ref type="bibr" target="#b30">[31]</ref>, who proposed the hierarchical recurrent encoder-decoder model (HRED). Their model exploits the hierarchical structure in web queries in order to model a user search session as two hierarchical sequences: a sequence of queries and a sequence of words in each query. Serban et al. <ref type="bibr" target="#b27">[28]</ref> continue in the same direction by proposing to exploit the temporal structure inherent in natural language dialogue. Their model decomposes a dialogue into a hierarchical sequence: a sequence of utterances, each of which is a sequence of words. More specifically, the model consists of three RNN modules: an encoder RNN, a context RNN and a decoder RNN. A sequence of tokens (e.g. words in an utterance) are encoded into a real-valued vector by the encoder RNN. This in turn is given as input to the context RNN, which updates its internal hidden state to reflect all the information up to that point in time. It then produces a real-valued output vector, which the decoder RNN conditions on to generate the next sequence of tokens (next utterance). Due to space limitations, we refer the reader to <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28]</ref> for additional information on the model architecture. The HRED model for modeling structured discrete sequences is appealing for three reasons. First, it naturally captures the hierarchical structure we want to model in the data. Second, the context RNN acts like a memory module which can remember things at longer time scales. Third, the structure makes the objective function more stable w.r.t. the model parameters, and helps propagate the training signal for first-order optimization methods <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multiresolution RNN (MrRNN)</head><p>We consider the problem of generatively modeling multiple parallel sequences. Each sequence is hierarchical with the top level corresponding to utterances and the bottom level to tokens. Formally, let w 1 , . . . , w N be the first sequence of length N where w n = (w n,1 , . . . , w n,Kn ) is the n'th constituent sequence consisting of K n discrete tokens from vocabulary V w . Similarly, let z 1 , . . . , z N be the second sequence, also of length N , where z n = (z n,1 , . . . , z n,Ln ) is the n'th constituent sequence consisting of L n discrete tokens from vocabulary V z . In our experiments, each sequence w n will consist of the words in a dialogue utterance, and each sequence z n will contain the coarse tokens w.r.t. the same utterance (e.g. the nouns in the utterance).</p><p>Our aim is to build a probabilistic generative model over all tokens in the constituent sequences w 1 , . . . , w N and z 1 , . . . , z N . Let θ be the parameters of the generative model. We assume that w n is independent of z n conditioned on z 1 , . . . , z n for n &gt; n, and factor the probability over sequences:</p><formula xml:id="formula_3">P θ (w 1 , . . . , w N , z 1 , . . . , z N ) = N n=1 P θ (z n |z 1 , . . . , z n−1 ) N n=1 P θ (w n |w 1 , . . . , w n−1 , z 1 , . . . , z n ) = N n=1 P θ (z n |z 1 , . . . , z n−1 )P θ (w n |w 1 , . . . , w n−1 , z 1 , . . . , z n ),<label>(4)</label></formula><p>where we define the conditional probabilities over the tokens in each constituent sequence:</p><formula xml:id="formula_4">P θ (z n |z 1 , . . . , z n−1 ) = Ln m=1</formula><p>P θ (z n,m |z n,1 , . . . , z n,m−1 , z 1 , . . . , z n−1 ) P θ (w n |w 1 , . . . , w n−1 , z 1 , . . . , z n ) = Kn m=1 P θ (w n,m |w n,1 , . . . , w n,m−1 , w 1 , . . . , w n−1 , z 1 , . . . , z n )</p><p>We refer to the distribution over z 1 , . . . , z N as the coarse sub-model, and to the distribution over w 1 , . . . , w N as the natural language sub-model. For the coarse sub-model, we parametrize the conditional distribution P θ (z n |z 1 , . . . , z n−1 ) as the HRED model described in subsection 2.2, applied to the sequences z 1 , . . . , z N . For the natural language sub-model, we parametrize P θ (w n |w 1 , . . . , w n−1 , z 1 , . . . , z n ) as the HRED model applied to the sequences w 1 , . . . , w N , but with one difference. The coarse prediction encoder GRU-gated RNN encodes all the previously generated tokens z 1 , . . . , z n into a real-valued vector, which is concatenated with the context RNN <ref type="figure">Figure 1</ref>: Computational graph for the multiresolution recurrent neural network (MrRNN). The lower part models the stochastic process over coarse tokens, and the upper part models the stochastic process over natural language tokens. The rounded boxes represent (deterministic) real-valued vectors, and the variables z and w represent the coarse tokens and natural language tokens respectively. and given as input to the natural language decoder RNN. The coarse prediction encoder RNN is important because it encodes the high-level information, which is transmitted to the natural language sub-model. Unlike the encoder for the coarse-level sub-model, this encoding will be used to generate natural language and therefore the RNN uses different word embedding parameters. At generation time, the coarse sub-model generates a coarse sequence (e.g. a sequence of nouns), which corresponds to a high-level decision about what the natural language sequence should contain (e.g. nouns to include in the natural language sequence). Conditioned on the coarse sequence, through the coarse prediction encoder RNN, the natural language sub-model then generates a natural language sequence (e.g. dialogue utterance). The model is illustrated in <ref type="figure">Figure 1</ref>.</p><p>We will assume that both z 1 , . . . , z N and w 1 , . . . , w N are observed and optimize the parameters w.r.t. the joint log-likelihood over both sequences. At test time, to generate a response for sequence n we exploit the probabilistic factorization to approximate the maximum a posteriori (MAP) estimate:</p><p>arg max wn,zn P θ (w n , z n |w 1 , . . . , w n−1 , z 1 , . . . , z n−1 )</p><p>≈ arg max wn P θ (w n |w 1 , . . . , w n−1 , z 1 , . . . , z n−1 , z n ) arg max zn P θ (z n |z 1 , . . . , z n−1 ),</p><p>where we further approximate the MAP for each constituent sequence using beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks</head><p>We consider the task of natural language response generation for dialogue. Dialogue systems have been developed for applications ranging from technical support to language learning and entertainment <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>. Dialogue systems can be categorized into two different types: goal-driven dialogue systems and non-goal-driven dialogue systems <ref type="bibr" target="#b26">[27]</ref>. To demonstrate the versatility of the MrRNN, we apply it to both goal-driven and non-goal-driven dialogue tasks. We focus on the task of conditional response generation. Given a dialogue context consisting of one or more utterances, the model must generate the next response in the dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ubuntu Dialogue Corpus</head><p>The goal-driven dialogue task we consider is technical support for the Ubuntu operating system, where we use the Ubuntu Dialogue Corpus <ref type="bibr" target="#b17">[18]</ref>. The corpus consists of about 0.5 million natural language dialogues extracted from the #Ubuntu Internet Relayed Chat (IRC) channel. Users entering the chat channel usually have a specific technical problem. The users first describe their problem and afterwards other users try to help them resolve it. The technical problems range from software-related issues (e.g. installing or upgrading existing software) and hardware-related issues (e.g. fixing broken drivers or partitioning hard drives) to informational needs (e.g. finding software with specific functionality). Additional details are given in appendix 8.</p><p>Twitter Dialogue Corpus The next task we consider is the non-goal-driven task of generating responses to Twitter conversations. We use a Twitter dialogue corpus extracted in the first half of 2011 using a procedure similar to Ritter et al. <ref type="bibr" target="#b23">[24]</ref>. Unlike the Ubuntu domain, Twitter conversations are often more noisy and do not necessarily center around a single topic. We perform a minimal preprocessing on the dataset to remove irregular punctuation marks and afterwards tokenize it. The dataset is split into training, validation and test sets containing respectively 749, 060, 93, 633 and 10, 000 dialogues. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Coarse Sequence Representations</head><p>We experiment with two procedures for extracting the coarse sequence representations:</p><p>Noun Representation This procedure aims to exploit the basic high-level structure of natural language discourse.It is based on the hypothesis that dialogues are topic-driven and that these topics may be characterized by nouns. In addition to a tokenizer, used by both the HRED and RNNLM model, it requires a part-of-speech (POS) tagger to identify the nouns in the dialogue. The procedure uses a set of 84 and 795 predefined stop words for Ubuntu and Twitter respectively. It maps a natural language utterance to its coarse representation by extracting all the nouns using the POS tagger and then removing all stop words and repeated words (keeping only the first occurrence of a word). Dialogue utterances without nouns are assigned the "no_nouns" token. The procedure also extracts the tense of each utterance and adds it to the beginning of the coarse representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activity-Entity Representation</head><p>This procedure is specific to the Ubuntu technical support task, for which it aims to exploit domain knowledge related to technical problem solving. It is motivated by the observation that most dialogues are centered around activities and entities. For example, it is very common for users to state a specific problem they want to resolve, e.g. how do I install program X? or My driver X doesn't work, how do I fix it? In response to such questions, other users often respond with specific instructions, e.g. Go to website X to download software Y or Try to execute command X. In such cases, it is clear that the principal information resides in the technical entities and in the verbs (e.g. install, fix, download), and therefore that it will be advantageous to explicitly model this structure. Motivated by this observation, the procedure uses a set of 192 activities (verbs), created by manual inspection, and a set of 3115 technical entities and 230 frequent terminal commands, extracted automatically from available package managers and from the web. The procedure uses the POS tagger to extract the verbs from the each natural language utterance. It maps the natural language to its coarse representation by keeping only verbs from the activity set, as well as entities from the technical entity set (irrespective of their POS tags). If no activity is found in an utterance, the representation is assigned the "none_activity" token. The procedure also appends a binary variable to the end of the coarse representation indicating if a terminal command was detected in the utterance. Finally, the procedure extracts the tense of each utterance and adds it to the beginning of the coarse representation.</p><p>Both extraction procedures are applied at the utterance level, therefore there exists a one-to-one alignment between coarse sequences and natural language sequences (utterances). There also exists a one-to-many alignment between the coarse sequence tokens and the corresponding natural language tokens, with the exception of a few special tokens. Further details are given in appendix 9. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The models are implemented in Theano <ref type="bibr" target="#b32">[33]</ref>. We optimize all models based on the training set joint log-likelihood over coarse sequences and natural language sequences using the first-order stochastic gradient optimization method Adam <ref type="bibr" target="#b12">[13]</ref>. We train all models using early stopping with patience on the joint-log-likelihood <ref type="bibr" target="#b1">[2]</ref>. We choose our hyperparameters based on the joint log-likelihood of the validation set. We define the 20K most frequent words as the vocabulary and the word embedding dimensionality to size 300 for all models, with the exception of the RNNLM and HRED on Twitter, where we use embedding dimensionality of size 400. We apply gradient clipping to stop the parameters from exploding <ref type="bibr" target="#b22">[23]</ref>. At test time, we use a beam search of size 5 for generating the model responses. Further details are given in appendix 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Models</head><p>We compare our models to several baselines used previously in the literature. The first is the standard RNNLM with LSTM gating function <ref type="bibr" target="#b19">[20]</ref> (LSTM), which at test time is similar to the Seq2Seq LSTM model <ref type="bibr" target="#b31">[32]</ref>. The second baseline is the HRED model with LSTM gating function for the decoder RNN and GRU gating function for the encoder RNN and context RNN, proposed for dialogue response generation by Serban et al. <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b30">[31]</ref>. Source code for both baseline models will be made publicly available upon acceptance for publication. For both Ubuntu and Twitter, we specify the RNNLM model to have 2000 hidden units with the LSTM gating function. For Ubuntu, we specify the HRED model to have 500, 1000 and 500 hidden units respectively for the encoder RNN, context RNN and decoder RNN. For Twitter, we specify the HRED model to have 2000, 1000 and 1000 hidden units respectively for the encoder RNN, context RNN and decoder RNN. The third baseline is the latent variable latent variable hierarchical recurrent encoder-decoder (VHRED) proposed by Serban et al. <ref type="bibr" target="#b28">[29]</ref>. We use the exact same VHRED models as Serban et al. <ref type="bibr" target="#b28">[29]</ref>.</p><p>For Ubuntu, we introduce a fourth baseline, called HRED + Activity-Entity Features, which has access to the past activity-entity pairs. This model is similar to to the natural language sub-model of the MrRNN model, with the difference that the natural language decoder RNN is conditioned on a real-valued vector, produced by a GRU RNN encoding only the past coarse-level activity-entity sub-sequences. This baseline helps differentiate between a model which observes the coarse-level sequences only as as additional features and a model which explicitly models the stochastic process of the coarse-level sequences. We specify the model to have 500, 1000, 2000 hidden units respectively for the encoder RNN, context RNN and decoder RNN. We specify the GRU RNN encoding the past coarse-level activity-entity sub-sequences to have 500 hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multiresolution RNN</head><p>The coarse sub-model is parametrized as the Bidirectional-HRED model <ref type="bibr" target="#b27">[28]</ref> with 1000, 1000 and 2000 hidden units respectively for the coarse-level encoder, context and decoder RNNs. The natural language sub-model is parametrized as a conditional HRED model with 500, 1000 and 2000 hidden units respectively for the natural language encoder, context and decoder RNNs. The coarse prediction encoder RNN GRU RNN is parametrized with 500 hidden units. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ubuntu</head><p>Evaluation Methods It has long been known that accurate evaluation of dialogue system responses is difficult <ref type="bibr" target="#b25">[26]</ref>. Liu et al. <ref type="bibr" target="#b16">[17]</ref> have recently shown that all automatic evaluation metrics adapted for such evaluation, including word overlap-based metrics such as BLEU and METEOR, have either very low or no correlation with human judgment of the system performance. We therefore carry out an in-lab human study to evaluate the Ubuntu models. We recruit 5 human evaluators, and show them each 30 − 40 dialogue contexts with the ground truth response and 4 candidate responses (HRED, HRED + Activity-Entity Features and MrRNNs). For each context example, we ask them to compare the candidate responses to the ground truth response and dialogue context, and rate them for fluency and relevancy on a scale 0 − 4. Our setup is very similar to the evaluation setup used by Koehn and Monz <ref type="bibr" target="#b13">[14]</ref>, and comparable to Liu et al <ref type="bibr" target="#b16">[17]</ref>. Further details are given in appendix 11.</p><p>We further propose a new set of metrics for evaluating model responses on Ubuntu, which compare the activities and entities in the model generated response with those of the ground truth response. That is, the ground truth and model responses are mapped to their respective activity-entity representations, using the automatic procedure discussed in section 4, and then the overlap between their activities and entities are measured according to precision, recall and F1-score. Based on a careful manual inspection of the extracted activities and entities, we believe that these metrics are particularly suited for the goal-oriented Ubuntu Dialogue Corpus. The activities and entities reflect the principal instructions given in the responses, which are key to resolving the technical problems. Therefore, a model able to generate responses with actions and entities similar to the ground truth human responses -which often do lead to solving the users problem -is more likely to yield a successful dialogue system. The reader is encouraged to verify the details and completeness of the activity-entity representations in appendix 9. Scripts to generate the noun and activity-entity representations, and to evaluate the dialogue responses w.r.t. activity-entity pairs are available online. <ref type="bibr" target="#b2">3</ref> Results The results on Ubuntu are given in table 1. The MrRNNs clearly perform substantially better than the baseline models both w.r.t. human evaluation and automatic evaluation metrics. The MrRNN with noun representations achieves 2x − 3x higher scores w.r.t. entities compared to other models, and the human evaluators also rate its fluency and relevancy substantially higher than other models. The MrRNN with activity representations achieves 2x − 3x higher scores w.r.t. activities compared to other models and nearly 2x higher scores w.r.t. entities compared to all baselines. Human evaluators also rate its fluency substantially higher than the baseline models. However,its relevancy is rated only slightly higher compared to the HRED model, which we believe is caused by human evaluators being more likely to noticing software entities than actions in the dialogue responses (even though actions are critical to solving the actual technical problem). Overall, the results demonstrate that the MrRNNs have learned to model high-level goal-oriented sequential structure on Ubuntu.  Model responses are shown in <ref type="table" target="#tab_1">Table 2</ref>. In general, the MrRNN responses are more coherent and topic-oriented compared to the other model responses, which usually produce very generic responses <ref type="bibr" target="#b27">[28]</ref>. In particular, the MrRNN with activity-entity representation appears to give more goal-oriented instructions compared to the MrRNN with noun representation (see examples 2-4 in <ref type="table" target="#tab_1">Table 2</ref>). Additional examples are shown in appendix 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Twitter</head><p>Evaluation Methods For Twitter, similar to the Ubuntu metrics, we use the precision, recall and F1 metrics between the model responses and ground truth responses w.r.t. the noun representation. The reason we propose to use these metrics is similar to the reason given for the Ubuntu metrics related to entities: a good model response is one which includes the same nouns as the ground truth response. We also compute the tense accuracy, as we did for Ubuntu. Furthermore, we use the three embedding-based textual similarity metrics proposed by Liu et al. <ref type="bibr" target="#b16">[17]</ref>: Embedding Average (Average), Embedding Extrema (Extrema) and Embedding Greedy (Greedy). All three metrics are based on computing the textual similarity between the ground truth response and the model response using word embeddings. All three metrics measure topic similarity: if a model-generated response is on the same topic as the ground truth response (e.g. contain paraphrases of the same words), the metrics will yield a high score. This is a highly desirable property for dialogue systems on an open platform such as Twitter, however it is also substantially different from measuring the overall dialogue system performance, or the appropriateness of a single response, which would require human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results on Twitter are given in <ref type="table" target="#tab_2">Table 3</ref>. The responses of the MrRNN with noun representation are better than all other models on precision, recall and F1 w.r.t nouns. MrRNN is also better than all other models w.r.t. tense accuracy, and it is on par with VHRED on the embeddingbased metrics. In accordance with our previous results, this indicates that the model has learned to generate more on-topic responses and, thus, that explicitly modeling the stochastic process over nouns helps learn the high-level structure. This is confirmed by qualitative inspection of the generated responses, which are clearly more topic-oriented. See <ref type="table" target="#tab_0">Table 10</ref> in appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Closely related to our work is the model proposed by Ji et al. <ref type="bibr" target="#b11">[12]</ref>, which jointly models natural language text and high-level discourse phenomena. However, it only models a discrete class per sentence at the high level, which must be manually annotated by humans. On the other hand, MrRNN models a sequence of automatically extracted high-level tokens. Recurrent neural network models with stochastic latent variables, such as the Variational Recurrent Neural Networks by Chung et al. <ref type="bibr" target="#b6">[7]</ref>, are also closely related to our work. These models face the more difficult task of learning the high-level representations, while simultaneously learning to model the generative process over high-level sequences and low-level sequences, which is a more difficult optimization problem. In addition to this, such models assume the high-level latent variables to be continuous, usually Gaussian, distributions.</p><p>Recent dialogue-specific neural network architectures, such as the model proposed by Wen et al. <ref type="bibr" target="#b33">[34]</ref>, are also relevant to our work. Different from the MrRNN, they require domain-specific hand-crafted high-level (dialogue state) representations with human-labelled examples, and they usually consist of several sub-components each trained with a different objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We have proposed the multiresolution recurrent neural network (MrRNN) for generatively modeling sequential data at multiple levels of abstraction. It is trained by optimizing the joint log-likelihood over the sequences at each level. We apply MrRNN to dialog response generation on two different tasks, Ubuntu technical support and Twitter conversations, and evaluate it in a human evaluation study and via automatic evaluation metrics. On Ubuntu, MrRNN demonstrates dramatic improvements compared to competing models. On Twitter, MrRNN appears to generate more relevant and on-topic responses. Even though abstract information is implicitly present in natural language dialogues, by explicitly representing information at different levels of abstraction and jointly optimizing the generation process across abstraction levels, MrRNN is able to generate more fluent, relevant and goal-oriented responses. The results suggest that the fine-grained abstraction (low-level) provides the architecture with increased fluency for predicting natural utterances, while the coarse-grained (high-level) abstraction gives it the semantic structure necessary to generate more coherent and relevant utterances. The results also imply that it is not simply a matter of adding additional features for prediction -MrRNN outperforms a competitive baseline augmented with the coarse-grained abstraction sequences as features -rather, it is the combination of representation and generation at multiple levels that yields the improvements. Finally, we observe that the architecture provides a general framework for modeling discrete sequences, as long as a coarse abstraction is available. We therefore conjecture that the architecture may successfully be applied to broader natural language generation tasks, such as generating prose and persuasive argumentation, and other tasks involving discrete sequences, such as music composition. We leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 8 Task Details</head><p>Ubuntu We use the Ubuntu Dialogue Corpus v2.0 extracted Jamuary, 2016: http://cs.mcgill.ca/ jpineau/datasets/ubuntu-corpus-1.0/.</p><p>Twitter We preprocess the dataset using the Moses tokenizer extracted June, 2015: https://github.com/ moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Coarse Sequence Representations Nouns</head><p>The noun-based procedure for extracting coarse tokens aims to exploit high-level structure of natural language discourse. More specifically, it builds on the hypothesis that dialogues in general are topic-driven and that these topics may be characterized by the nouns inside the dialogues. At any point in time, the dialogue is centered around one or several topics. As the dialogue progresses, the underlying topic evolves as well. In addition to the tokenizer required by the previous extraction procedure, this procedure also requires a part-of-speech (POS) tagger to identify the nouns in the dialogue suitable for the language domain.</p><p>For extracting the noun-based coarse tokens, we define a set of 795 stop words for Twitter and 84 stop words for Ubuntu containing mainly English pronouns, punctuation marks and prepositions (excluding special placeholder tokens). We then extract the coarse tokens by applying the following procedure to each dialogue:</p><p>1. We apply the POS tagger version 0.3.2 developed by Owoputi and colleagues <ref type="bibr" target="#b21">[22]</ref> to extract POS. <ref type="bibr" target="#b4">5</ref> For Twitter, we use the parser trained on the Twitter corpus developed by Ritter et al. <ref type="bibr" target="#b24">[25]</ref>. For Ubuntu, we use the parser trained on the NPS Chat Corpus developed by Forsyth and Martellwhich was extracted from IRC chat channels similar to the Ubuntu Dialogue Corpus. 67 2. Given the POS tags, we remove all words which are not tagged as nouns and all words containing non-alphabet characters. <ref type="bibr" target="#b7">8</ref> . We keep all urls and paths.</p><p>3. We remove all stop words and all repeated tokens, while maintaining the order of the tokens. 4. We add the "no_nouns" token to all utterances, which do not contain any nouns. This ensures that no coarse sequences are empty. It also forces the coarse sub-model to explicitly generate at least one token, even when there are no actual nouns to generate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>For each utterance, we use the POS tags to detect three types of time tenses: past, present and future tenses. We append a token indicating which of the 2 3 tenses are present at the beginning of each utterance. <ref type="bibr" target="#b8">9</ref> If no tenses are detected, we append the token "no_tenses".</p><p>As before, there exists a one-to-many alignment between the extracted coarse sequence tokens and the natural language tokens, since this procedure also maintains the ordering of all special placeholder tokens, with the exception of the "no_nouns" token.</p><p>We cut-off the vocabulary at 10000 coarse tokens for both the Twitter and Ubuntu datasets excluding the special placeholder tokens. On average a Twitter dialogue in the training set contains 25 coarse tokens, while a Ubuntu dialogue in the training set contains 36 coarse tokens. Model statistics for the unigram and bigram language models are presented in <ref type="table" target="#tab_3">Table 4</ref> for the noun representations on the Ubuntu and Twitter training sets. <ref type="bibr" target="#b9">10</ref> The table shows a substantial difference in bits per words between the unigram and bigram models, which suggests that the nouns are significantly correlated with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activity-Entity Pairs</head><p>The activity-entity-based procedure for extracting coarse tokens attempts to exploit domain specific knowledge for the Ubuntu Dialogue Corpus, in particular in relation to providing technical assistance with problem solving. Our manual inspection of the corpus shows that many dialogues are centered around activities. For example, it is very common for users to state a specific problem they want to resolve, e.g. how do I install program X? or My driver X doesn't work, how do I fix it?. In response to such queries, other users often respond with specific instructions, e.g. Go to website X to download software Y or Try to execute command X. In addition to the technical entities, the principle message conveyed by each utterance resides in the verbs, e.g. install, work, fix, go, to, download, execute. Therefore, it seems clear that a dialogue system must have a strong understanding of both the activities and technical entities if it is to effectively assist users with technical problem solving. It seems likely that this would require a dialogue system able to relate technical entities to each other, e.g. to understand that firefox depends on the GCC library, and conform to the temporal structure of activities, e.g. understanding that the install activity is often followed by download activity.</p><p>We therefore construct two word lists: one for activities and one for technical entities. We construct the activity list based on manual inspection yielding a list of 192 verbs. For each activity, we further develop a list of synonyms and conjugations of the tenses of all words. We also use Word2Vec word embeddings <ref type="bibr" target="#b20">[21]</ref>, trained on the Ubuntu Dialogue Corpous training set, to identify commonly misspelled variants of each activity. The result is a dictionary, which maps a verb to its corresponding activity (if such exists). For constructing the technical entity list, we scrape publicly available resources, including Ubuntu and Linux-related websites as well as the Debian package manager APT. Similar to the activities, we also use the Word2Vec word embeddings to identify misspelled and paraphrased entities. This results in another dictionary, which maps one or two words to the corresponding technical entity. In total there are 3115 technical entities. In addition to this we also compile a list of 230 frequent commands. Examples of the extracted activities, entities and commands can be found in the appendix.</p><p>Afterwards, we extract the coarse tokens by applying the following procedure to each dialogue:</p><p>1. We apply the technical entity dictionary to extract all technical entities.</p><p>2. We apply the POS tagger version 0.3.2 developed by Owoputi and colleagues, trained on the NPS Chat Corpus developed by Forsyth and Martell as before. As input to the POS tagger, we map all technical entities to the token "something". This transformation should improve the POS tagging accuracy, since The corpus the parser was trained on does not contain technical words.</p><p>3. Given the POS tags, we extract all verbs which correspond to activities. <ref type="bibr" target="#b10">11</ref> . If there are no verbs in an entire utterance and the POS tagger identified the first word as a noun, we will assume that the first word is in fact a verb. We do this, because the parser does not work well for tagging technical instructions in imperative form, e.g. upgrade firefox. If no activities are detected, we append the token "none_activity" to the coarse sequence. We also keep all urls and paths.</p><p>4. We remove all repeated activities and technical entities, while maintaining the order of the tokens.</p><p>5. If a command is found inside an utterance, we append the "cmd" token at the end of the utterance. Otherwise, we append the "no_cmd" token to the end of the utterance. This enables the coarse sub-model to predict whether or not an utterance contains executable commands.</p><p>6. As for the noun-based coarse representation, we also append the time tense to the beginning of the sequence.</p><p>As before, there exists a one-to-many alignment between the extracted coarse sequence tokens and the natural language tokens, with the exception of the "none_activity" and "no_cmd" tokens.  Since the number of unique tokens are smaller than 10000, we do not need to cut-off the vocabulary. On average a Ubuntu dialogue in the training set contains 43 coarse tokens.</p><p>Our manual inspection of the extracted coarse sequences, show that the technical entities are identified with very high accuracy and that the activities capture the main intended action in the majority of utterances. Due to the high quality of the extracted activities and entities, we are confident that they may be used for evaluation purposes as well.</p><p>Scripts to generate the noun and activity-entity representations, and to evaluate the dialogue responses w.r.t. activity-entity pairs are available online at: https://github.com/julianser/ Ubuntu-Multiresolution-Tools/tree/master/ActEntRepresentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stop Words for Noun-based Coarse Tokens</head><p>Ubuntu stop words for noun-based coarse representation: all another any anybody anyone anything both each each other either everybody everyone everything few he her hers herself him himself his I it its itself many me mine more most much myself neither no one nobody none nothing one one another other others ours ourselves several she some somebody someone something that their theirs them themselves these they this those us we what whatever which whichever who whoever whom whomever whose you your yours yourself yourselves . , ? ' --! Twitter stop words for noun-based coarse representation: 12 all another any anybody anyone anything both each each other either everybody everyone everything few he her hers herself him himself his I it its itself many me mine more most much myself neither no one nobody none nothing one one another other others ours ourselves several she some somebody someone something that their theirs them themselves these they this those us we what whatever which whichever who whoever whom whomever whose you your yours yourself yourselves . , ? ' --!able about above abst accordance according accordingly across act actually added adj adopted affected affecting affects after afterwards again against ah all almost alone along already also although always am among amongst an and announce another any anybody anyhow anymore anyone anything anyway anyways anywhere apparently approximately are aren arent arise around as aside ask asking at auth available away awfully b back bc be became because become becomes becoming been before beforehand begin beginning beginnings begins behind being believe below beside besides between beyond biol bit both brief briefly but by c ca came can cannot can't cant cause causes certain certainly co com come comes contain containing contains cos could couldnt d date day did didn didn't different do does doesn doesn't doing don done don't dont down downwards due during e each ed edu effect eg eight eighty either else elsewhere end ending enough especially et et-al etc even ever every everybody everyone everything everywhere ex except f far few ff fifth first five fix followed following follows for former formerly forth found four from further furthermore g game gave get gets getting give given gives giving go goes going gone gonna good got gotten great h had happens hardly has hasn hasn't have haven haven't having he hed hence her here hereafter hereby herein heres hereupon hers herself hes hey hi hid him himself his hither home how howbeit however hundred i id ie if i'll im immediate immediately importance important in inc indeed index information instead into invention inward is isn isn't it itd it'll its itself i've j just k keep keeps kept keys kg km know known knows l ll largely last lately later latter latterly least less lest let lets like liked likely line little ll 'll lol look looking looks lot ltd m made mate mainly make makes many may maybe me mean means meantime meanwhile merely mg might million miss ml more moreover most mostly mr mrs much mug must my myself n na name namely nay nd near nearly necessarily necessary need needs neither never nevertheless new next nine ninety no nobody non none nonetheless noone nor normally nos not noted nothing now nowhere o obtain obtained obviously of off often oh ok okay old omitted omg on once one ones only onto or ord other others otherwise ought our ours ourselves out outside over overall owing own p page pages part particular particularly past people per perhaps placed please plus poorly possible possibly potentially pp predominantly present previously primarily probably promptly proud provides put q que quickly quite qv r ran rather rd re readily really recent recently ref refs regarding regardless regards related relatively research respectively resulted resulting results right rt run s said same saw say saying says sec section see seeing seem seemed seeming seems seen self selves sent seven several shall she shed she'll shes should shouldn shouldn't show showed shown showns shows significant significantly similar similarly since six slightly so some somebody somehow someone somethan something sometime sometimes somewhat somewhere soon sorry specifically specified specify specifying state states still stop strongly sub substantially successfully such sufficiently suggest sup sure t take taken taking tbh tell tends th than thank thanks thanx that that'll thats that've the their theirs them themselves then thence there thereafter thereby thered therefore therein there'll thereof therere theres thereto thereupon there've these they theyd they'll theyre they've thing things think this those thou though thoughh thousand throug through throughout thru thus til time tip to together too took toward towards tried tries truly try trying ts tweet twice two u un under unfortunately unless unlike unlikely until unto up upon ups ur us use used useful usefully usefulness uses using usually v value various ve 've very via viz vol vols vs w wanna want wants was wasn wasn't way we wed welcome well we'll went were weren weren't we've what whatever what'll whats when whence whenever where whereafter whereas whereby wherein wheres whereupon wherever whether which while whim whither who whod whoever whole who'll whom whomever whos whose why widely will willing wish with within without won won't words world would wouldn wouldn't www x y yeah yes yet you youd you'll your youre yours yourself yourselves you've z zero All models were trained with a learning rate of 0.0002 or 0.0001, batches of size either 40 or size 80 and gradients are clipped at 1.0. We truncate the backpropagation to batches with 80 tokens We validate on the entire validation set every 5000 training batches. We choose almost identical hyperparameters for the Ubuntu and Twitter models, since the models appear to perform similarly w.r.t. different hyperparameters and since the statistics of the two datasets are comparable. We use the 20K most frequent words on Twitter and Ubuntu as the natural language vocabulary for all the models, and assign all words outside the vocabulary to a special unknown token symbol. For MrRNN, we use a coarse token vocabulary consisting of the 10K most frequent tokens in the coarse token sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activities and Entities for Ubuntu Dialogue Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation</head><p>We compute the cost of each beam search (candidate response) as the log-likelihood of the tokens in the beam divided by the number of tokens it contains. The LSMT model performs better when the beam search is not allowed to generate the unknown token symbol, however even then it still performs worse than the HRED model across all metrics except for the command accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>Based on preliminary experiments, we found that a slightly different parametrization of the HRED baseline model worked better on Twitter. The encoder RNN has a bidirectional GRU RNN encoder, with 1000 hidden units for the forward and backward RNNs each, and a context RNN and a decoder RNN with 1000 hidden units each. Furthermore, the decoder RNN computes a 1000 dimensional real-valued vector for each hidden time step, which is multiplied with the output context RNN. The output is feed through a one-layer feed-forward neural network with hyperbolic tangent activation function, which the decoder RNN then conditions on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Human Evaluation</head><p>All human evaluators either study or work in an English speaking environment, and have indicated that they have some experience using a Linux operating system. To ensure a high quality of the ground truth responses, human evaluators were only asked to evaluate responses, where the ground truth contained at least one technical entity. Before starting evaluators, were shown one short annotated example with a brief explanation of how to give annotations. In particular, the evaluators were instructed to use the following reference in <ref type="figure" target="#fig_1">Figure 2</ref>.  <ref type="table" target="#tab_7">Table 7</ref> shows the scores by category.  Context Response seriously... 50% cpu from xorg running just two apps(terminal and system monitor ) i've been seeing this ever since i upgraded to 12.10 anyone else seeing this → I would do such kind of tests with ' top ' or ' something Enjoy the difference... → that is top saying 50% cpu from xorg → Ok. In that case I would try a window manager without composite for a try. 50% is too much.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>MrRNN Act. -Ent.: try a different mirror MrRNN Noun: something VHRED: but I dont know how to add it HRED: how do I do that ? hi community.. is there any difference between " /.bashrc " " /.bashrc. local " → yes, they're different files :-) the default /.bashrc sources /.bashrc. local → sorry I could undersatnd... If I write on terminal " gedit /.bashrc " it opens a file but how I open " /.bashrc. local " → " gedit /.bashrc. local "... → but it just open blank file nothing inside it MrRNN Act. -Ent.: open up gedit /.bashrc, there is a file called " something " MrRNN Noun: the file is empty.. VHRED: it's in /.bashrc or /.bashrc HRED: what are you trying to do ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Fluency and relevancy reference table presented to human evaluators. The 5 evaluators gave 1069 ratings in total.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>MrRNN Act. -Ent.: Are you using XGL ? MrRNN Noun: What window manager are you using ? HRED + Act.-Ent.: I don't know how to help you with that, sorry. HRED: i have no idea how to do that, i just want to know how to do it LSTM: something ? my linux seems to be stuck at rev 18... other systems in the house are as high as rev 34... any suggestions ? → what 's rev ? → the linux info says kernel linux says 3.5. something generic MrRNN Act. -Ent.: how do i compile the kernel ? MrRNN Noun: the linux kernel is 2.6. something HRED + Act.-Ent.: how do i find out what version of ubuntu i have ? HRED: that 's not what you want to do LSTM:? what do you mean ? i need some SERIOUS help with fullscreen video playback. i already asked in #lubuntu, but i might have a better chance here : most video files either let the player hang or audio/video goes badly out of sync right away. i know that it does work with the same files and it even did so like two hours ago, but after some browsing and man page reading, the problem comes up again. i have NO idea where to look anymore nor what I did to ' reproduce ' the → even with vlcMrRNN Act. -Ent.: works fine in vlc MrRNN Noun: do you use vlc ? HRED + Act.-Ent.: what video card do you have ? HRED: yeah, i know, but i don't know how to do that, but i don't know how to do that LSTM: ? what do you mean ? I'm in the process of building a new desktop and I was wondering if there was any way to, using KVM, start or restart a Windows session when my girlfriend logs in. I have plenty of memory and my i5 3570 supports all the necessary virtualisation technologies. → you can install VirtualBox, and I'm sure there 's a command line method to run VB with a specified installed VM operating system, and you can make that command a startup command ending with " &amp; " MrRNN Act. -Ent.: have you installed virtualbox ? MrRNN Noun: are you trying to install ubuntu ? HRED + Act.-Ent.: I don't know how to do that, sorry. I don't know how to do that. HRED: thanks, I'll give it a shot. LSTM: thanks, I'll try that. hello all, how can i setup two default gateways graphically in network manager ? → you can't have 2 default gateway on one device...? → well.. i can → hows that gonna work ? MrRNN Act. -Ent.: what are you looking for ? MrRNN Noun: do you have a default gateway ? HRED + Act.-Ent.: what are you trying to do ? HRED: i want to set up a static ip, but i dont know how to set it up LSTM: ? They are mounted via fstab → sure they are mounted ? → I've been using them all day :[ p :P * MrRNN Act. -Ent.: I see MrRNN Noun: something :/ something ls -ld something HRED + Act.-Ent.: I can mount it, but when I try to mount it, it says " mount : special device /dev/sda1 does not exist HRED: what do you mean by mount ? LSTM: ? what are you talking about ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ubuntu evaluation using precision (P), recall (R), F1 and accuracy metrics w.r.t. activity, entity, tense and command (Cmd) on ground truth utterances, and human fluency and relevancy scores given on a scale 0-4 ( * indicates scores significantly different from baseline models at 90% confidence)</figDesc><table><row><cell></cell><cell></cell><cell>Activity</cell><cell></cell><cell></cell><cell>Entity</cell><cell></cell><cell>Tense</cell><cell>Cmd</cell><cell cols="2">Human Eval.</cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Acc.</cell><cell>Acc.</cell><cell cols="2">Fluency Relevancy</cell></row><row><cell>LSTM</cell><cell>1.7</cell><cell>1.03</cell><cell>1.18</cell><cell>1.18</cell><cell>0.81</cell><cell>0.87</cell><cell>14.57</cell><cell>94.79</cell><cell>-</cell><cell>-</cell></row><row><cell>HRED</cell><cell>5.93</cell><cell>4.05</cell><cell>4.34</cell><cell>2.81</cell><cell>2.16</cell><cell>2.22</cell><cell>22.2</cell><cell>92.58</cell><cell>2.98</cell><cell>1.01</cell></row><row><cell>VHRED</cell><cell>6.43</cell><cell>4.31</cell><cell>4.63</cell><cell>3.28</cell><cell>2.41</cell><cell>2.53</cell><cell>20.2</cell><cell>92.02</cell><cell>-</cell><cell>-</cell></row><row><cell>HRED + Act.-Ent.</cell><cell>7.15</cell><cell>5.5</cell><cell>5.46</cell><cell>3.03</cell><cell>2.43</cell><cell>2.44</cell><cell>28.02</cell><cell>86.69</cell><cell>2.96</cell><cell>0.75</cell></row><row><cell>MrRNN Noun</cell><cell>5.81</cell><cell>3.56</cell><cell>4.04</cell><cell cols="4">8.68 5.55 6.31 24.03</cell><cell>90.66</cell><cell>3.48  *</cell><cell>1.32  *</cell></row><row><cell>MrRNN Act.-Ent.</cell><cell cols="4">16.84 9.72 11.43 4.91</cell><cell>3.36</cell><cell cols="3">3.72 29.01 95.04</cell><cell>3.42  *</cell><cell>1.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ubuntu model examples. The arrows indicate change of turn. Response Hey guys what do you in general use for irc something ubuntu xchat or xchatgnome ? → without -gnome. that is just cut down → you mean drop xchatgnome and go with xchat ? MrRNN Act. -Ent.: im using xchat right now MrRNN Noun: what is xchat-gnome ?VHRED: correct HRED: yes when setting up rules with iptables command only writes changes this file " etciptables. rules "? i ask this so i can backup before messing anything → sudo iptables-save something . dat to backup your rules restore with sudo iptables-restore &lt; something . dat MrRNN Act. -Ent.: I don't see any reason why you need iptables to do that MrRNN Noun: are you using ubuntu ? VHRED: thx HRED: thanks Hello I tried to install ubuntu studio but I get W : Failed to fetch &lt;url &gt;404 Not Found when apt-get update → the ppa you added is not online</figDesc><table><row><cell>Context</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Twitter evaluation using precision (P), recall (R), F1 and accuracy metrics w.r.t. noun representation, tense accuracy and embedding-based evaluation metrics on ground truth utterances.</figDesc><table><row><cell></cell><cell></cell><cell>Noun</cell><cell></cell><cell>Tense</cell><cell cols="3">Embedding Metrics</cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Acc.</cell><cell cols="3">Average Greedy Extrema</cell></row><row><cell>LSTM</cell><cell>0.71</cell><cell>0.71</cell><cell>0.65</cell><cell>27.06</cell><cell>51.24</cell><cell>38.9</cell><cell>36.58</cell></row><row><cell>HRED</cell><cell>0.31</cell><cell>0.31</cell><cell>0.29</cell><cell>26.47</cell><cell>50.1</cell><cell>37.83</cell><cell>35.55</cell></row><row><cell>VHRED</cell><cell>0.5</cell><cell>0.51</cell><cell>0.46</cell><cell>26.66</cell><cell>53.26</cell><cell>39.64</cell><cell>37.98</cell></row><row><cell>MrRNN Noun</cell><cell cols="4">4.82 5.22 4.63 34.48</cell><cell>49.77</cell><cell>40.44</cell><cell>37.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Unigram and bigram models bits per word on noun representations.</figDesc><table><row><cell>Model</cell><cell cols="2">Ubuntu Twitter</cell></row><row><cell>Unigram</cell><cell>10.16</cell><cell>12.38</cell></row><row><cell>Bigram</cell><cell>7.26</cell><cell>7.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Twitter Coarse Sequence Examples</figDesc><table><row><cell>Natural Language Tweets</cell><cell></cell><cell>Noun Representation</cell></row><row><cell cols="2">&lt;first_speaker&gt; at pinkberry</cell><cell>present_tenses</cell><cell>pinkberry</cell></row><row><cell cols="2">with my pink princess enjoying</cell><cell>princess moment</cell></row><row><cell>a precious moment &lt;url&gt;</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">present_tenses alma emma</cell></row><row><cell>&lt;second_speaker&gt;-</cell><cell>they</cell><cell>bif sis hugs</cell></row><row><cell cols="2">are adorable, alma still speaks</cell><cell></cell></row><row><cell>about emma bif sis . hugs</cell><cell></cell><cell></cell></row><row><cell cols="2">&lt;first_speaker&gt; &lt;at&gt; when you</cell><cell cols="2">present_tenses spray painting</cell></row><row><cell cols="2">are spray painting, where are</cell><cell>apartment</cell></row><row><cell cols="2">you doing it ? outside ? in your</cell><cell></cell></row><row><cell>apartment ? where ?</cell><cell></cell><cell cols="2">present_tenses spray stuff</cell></row><row><cell></cell><cell></cell><cell>bathroom</cell></row><row><cell cols="2">&lt;second_speaker&gt; &lt;at&gt; mostly</cell><cell></cell></row><row><cell cols="2">spray painting outside but some</cell><cell></cell></row><row><cell cols="2">little stuff in the bathroom .</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ubuntu Coarse Sequence Examples</figDesc><table><row><cell>Natural Language Dialogues</cell><cell cols="2">Activity-Entity Coarse Dialogues</cell><cell></cell></row><row><cell>if you can get a hold of the logs, there 's stuff</cell><cell cols="3">future_tenses get_activity install_activity</cell></row><row><cell>from **unknown** about his inability to install</cell><cell cols="2">amd64_entity no_cmd</cell><cell></cell></row><row><cell>amd64</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>no_tenses</cell><cell>check_activity</cell><cell>no_cmd</cell></row><row><cell>I'll check fabbione 's log, thanks sounds</cell><cell cols="3">past_present_tenses none_activity no_cmd</cell></row><row><cell>like he had the same problem I did ew, why ? ...</cell><cell cols="3">no_tenses none_activity no_cmd ...</cell></row><row><cell>upgrade lsb-base and acpid</cell><cell>no_tenses</cell><cell>upgrade_activity</cell><cell>lsb_entity</cell></row><row><cell></cell><cell cols="2">acpid_entity no_cmd</cell><cell></cell></row><row><cell>i'm up to date</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">no_tenses none_activity no_cmd</cell><cell></cell></row><row><cell>what error do you get ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">present_tenses get_activity no_cmd</cell></row><row><cell>i don't find error :/ where do i search</cell><cell></cell><cell></cell><cell></cell></row><row><cell>from ? acpid works, but i must launch it</cell><cell cols="3">present_tenses discover_activity no_cmd</cell></row><row><cell>manually in a root sterm ...</cell><cell cols="2">present_future_tenses</cell><cell>work_activity</cell></row><row><cell></cell><cell cols="3">acpid_entity root_entity no_cmd ...</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>accept, activate, add, ask, appoint, attach, backup, boot, check, choose, clean, click, comment, compare, compile, compress, change, affirm, connect, continue, administrate, copies, break, create, cut, debug, decipher, decompress, define, describe, debind, deattach, deactivate, download, adapt, eject, email, conceal, consider, execute, close, expand, expect, export, discover, correct, fold, freeze, get, deliver, go, grab, hash, import, include, install, interrupt, load, block, log, log-in, log-out, demote, build, clock, bind, more, mount, move, navigate, open, arrange, partition, paste, patch, plan, plug, post, practice, produce, pull, purge, push, put, queries, quote, look, reattach, reboot, receive, reject, release, remake, delete, name, replace, request, reset, resize, restart, retry, return, revert, reroute, scroll, send, set, display, shutdown, size, sleep, sort, split, come-up, store, signup, get-ahold-of, say, test, transfer, try, uncomment, de-expand, uninstall, unmount, unplug, unset, sign-out, update, upgrade, upload, use, delay, enter, support, prevent, loose, point, contain, access, share, buy, sell, help, work, mute, restrict, play, call, thank, burn, advice, force, repeat, stream, respond, browse, scan, restore, design, refresh, bundle, implement, programming, compute, touch, overheat, cause, affect, swap, format, rescue, zoomed, detect, dump, simulate, checkout, unblock, document, troubleshoot, convert, allocate, minimize, maximize, redirect, maintain, print, spam, throw, sync, contact, destroy Ubuntu entities (excerpt): ubuntu_7.04, dmraid, vnc4server, tasksel, aegis, mirage, system-config-audit, uif2iso, aumix, unrar, dell, hibernate, ucoded, finger, zoneminder, ucfg, macaddress, ia32-libs, synergy, aircrack-ng, pulseaudio, gnome, kid3, bittorrent, systemsettings, cups, finger, xchm, pan, uwidget, vnc-java, linux-source, ucommand.com, epiphany, avanade, onboard, uextended, substance, pmount, lilypond, proftpd, unii, jockey-common, aha, units, xrdp, mp3check, cruft, uemulator, ulivecd, amsn, ubuntu_5.10, acpidump, uadd-on, gpac, ifenslave, pidgin, soundconverter, kdelibs-bin, esmtp, vim, travel, smartdimmer, uactionscript, scrotwm, fbdesk, tulip, beep, nikto, wine, linux-image, azureus, vim, makefile, uuid, whiptail, alex, junior-arcade, libssl-dev, update-inetd, uextended, uaiglx, sudo, dump, lockout, overlayscrollbar, xubuntu, mdk, mdm, mdf2iso, linux-libc-dev, sms, lm-sensors, dsl, lxde, dsh, smc, sdf, install-info, xsensors, gutenprint, sensors, ubuntu_13.04, atd, ata, fatrat, fglrx, equinix, atp, atx, libjpeg-dbg, umingw, update-inetd, firefox, devede, cd-r, tango, mixxx, uemulator, compiz, libpulse-dev, synaptic, ecryptfs, crawl, ugtk+, tree, perl, tree, ubuntu-docs, libsane, gnomeradio, ufilemaker, dyndns, libfreetype6, daemon, xsensors, vncviewer, vga, indicator-applet, nvidia-173, rsync, members, qemu, mount, rsync, macbook, gsfonts, synaptic, finger, john, cam, lpr, lpr, xsensors, lpr, lpr, screen, inotify, signatures, units, ushareware, ufraw, bonnie, nec, fstab, nano, bless, bibletime, irssi, ujump, foremost, nzbget, ssid, onboard, synaptic, branding, hostname, radio, hotwire, xebia, netcfg, xchat, irq, lazarus, pilot, ucopyleft, java-common, vm, ifplugd, ncmpcpp, irc, uclass, gnome, sram, binfmt-support, vuze, java-common, sauerbraten, adapter, login Ubuntu commands: alias, apt-get, aptitude, aspell, awk, basename, bc, bg, break, builtin, bzip2, cal, case, cat, cd, cfdisk, chgrp, chmod, chown, chroot, chkconfig, cksum, cmp, comm, command, continue, cp, cron, crontab, csplit, curl, cut, date, dc, dd, ddrescue, declare, df, diff, diff3, dig, dir, dircolors, dirname, dirs, dmesg, du, echo, egrep, eject, enable, env, eval, exec, exit, expect, expand, export, expr, false, fdformat, fdisk, fg, fgrep, file, find, fmt, fold, for, fsck, ftp, function, fuser, gawk, getopts, grep, groupadd, groupdel, groupmod, groups, gzip, hash, head, history, hostname, htop, iconv, id, if, ifconfig, ifdown, ifup, import, install, ip, jobs, join, kill, killall, less, let, link, ln, local, locate, logname, logout, look, lpc, lpr, lprm, ls, lsof, man, mkdir, mkfifo, mknod, more, most, mount, mtools, mtr, mv, mmv, nc, nl, nohup, notify-send, nslookup, open, op, passwd, paste, ping, pkill, popd, pr, printf, ps, pushd, pv, pwd, quota, quotacheck, quotactl, ram, rar, rcp, read, readonly, rename, return, rev, rm, rmdir, rsync, screen, scp, sdiff, sed, select, seq, set, shift, shopt, shutdown, sleep, slocate, sort, source, split, ssh, stat, strace, su, sudo, sum, suspend, sync, tail, tar, tee, test, time, timeout, times, touch, top, tput, traceroute, tr, true, tsort, tty, type, ulimit, umask, unalias, uname, unexpand, uniq, units, unrar, unset, unshar, until, useradd, userdel, usermod, users, uuencode, uudecode, vi, vmstat, wait, watch, wc, whereis, which, while, who, whoami, write, xargs, xdg-open, xz, yes, zip, admin, purge 10 Model Details</figDesc><table><row><cell>Training</cell></row><row><cell>Ubuntu activities:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ubuntu human fluency and relevancy scores by rating category Model response examples are given in this section. All the model responses can be downloaded at www.iulianserban.com/Files/TwitterDialogueCorpus.zip and www.iulianserban.com/Files/ UbuntuDialogueCorpus.zip.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Fluency</cell><cell></cell><cell></cell><cell cols="4">Relevancỳ````````M</cell></row><row><cell>odel</cell><cell>Rating Level</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>HRED</cell><cell></cell><cell cols="5">3 11 21 50 49</cell><cell cols="5">68 22 19 19 4</cell></row><row><cell cols="2">HRED + Act.-Ent.</cell><cell cols="5">3 17 19 37 57</cell><cell cols="3">69 39 18</cell><cell>6</cell><cell>2</cell></row><row><cell cols="2">MrRNN Noun</cell><cell>1</cell><cell>2</cell><cell>8</cell><cell cols="2">52 71</cell><cell cols="5">51 45 24 10 4</cell></row><row><cell cols="2">MrRNN Act.-Ent</cell><cell>0</cell><cell>2</cell><cell>6</cell><cell cols="2">52 74</cell><cell cols="5">27 53 39 14 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ubuntu model examples. The → token indicates a change of turn.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Ubuntu model examples. The → token indicates a change of turn.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Twitter model examples. The → token indicates a change of turn.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Due to Twitter's terms of service we are not allowed to redistribute Twitter content. Therefore, only the tweet IDs can be made public. These are available at: www.iulianserban.com/Files/TwitterDialogueCorpus. zip.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The pre-processed Ubuntu Dialogue Corpus used, as well as the noun representations and activity-entity representations, are available at www.iulianserban.com/Files/UbuntuDialogueCorpus.zip.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/julianser/Ubuntu-Multiresolution-Tools/tree/master/ ActEntRepresentation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Due to Twitter's Terms and Conditions we are unfortunately not allowed to publish the preprocessed dataset. 5 www.cs.cmu.edu/~ark/TweetNLP/ 6 As input to the POS tagger, we replace all unknown tokens with the word "something" and remove all special placeholder tokens (since the POS tagger was trained on a corpus without these words). We further reduce any consecutive sequence of spaces to a single space. For Ubuntu, we also replace all commands and entities with the word "something". For Twitter, we also replace all numbers with the word "some", all urls with the word "somewhere" and all heart emoticons with the word "love".<ref type="bibr" target="#b6">7</ref> Forsyth, E. N. and Martell, C. H. (2007). Lexical and discourse analysis of online chat dialog. In Semantic Computing, 2007. ICSC 2007. International Conference on, pages 19-26. IEEE.<ref type="bibr" target="#b7">8</ref> We define nouns as all words with tags containing the prefix "NN" according to the PTB-style tagset.<ref type="bibr" target="#b8">9</ref> Note that an utterance may contain several sentences. It therefore often happens that an utterance contains several time tenses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">The models were trained using maximum log-likelihood on the noun representations excluding all special tokens.<ref type="bibr" target="#b10">11</ref> We define verbs as all words with tags containing the prefix "VB" according to the PTB-style tagset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Part of these were extracted from https://github.com/defacto133/twitter-wordcloud-bot/ blob/master/assets/stopwords-en.txt.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Ryan Lowe, Michael Noseworthy, Caglar Gulcehre, Sungjin Ahn, Harm de Vries, Song Feng and On Yi Ching for participating and helping with the human study. The authors thank Orhan Firat and Caglar Gulcehre for constructive feedback, and thank Ryan Lowe, Nissan Pow and Joelle Pineau for making the Ubuntu Dialogue Corpus available to the public.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>Response can you have gnome and KDE installed and use one or the other ? cough erin brainwaves MrRNN Act.-Ent.: what are you looking for ? MrRNN Noun: gconf-editor HRED + Act.-Ent.: what do you want me to do ? HRED: yeah, i just did a fresh install of ubuntu, and it worked fine. LSTM: ? Ugh so my mount points need to be in /media rather than /mnt to make them appear in the nautilus side-pane : → you could symlink /mnt to /media afaik MrRNN Noun: what version of ubuntu are you using ? HRED + Act.-Ent.: how do i install the latest version of wine ? i have the latest version, but i dont know how to install it HRED: yes LSTM: yes Hey guys, I'm trying to install libreoffice from the repositories, but it 's throwing a wobbly. Can anyone help ? Looks like dependency problems, but when I attempt to resolve them it throws a wobbly at me. &lt;url &gt;→ try ' apt-get install -f ' yeah, it looks like you don't have java install You need to try to run ' apt-get install -f', just that string try ' apt-get purge libreoffice ' it looks like you had it installed previously → update : just realized I removed it earlier because it wasn't working. let me re-install. when I run ' apt-get purge libreoffice ' it just complains of unmet dependencies/ → but you were able to reinstall fine ? → no, it throws the errors seen in the pastebin at me → jwrigley, check to see if /var/lib/libreoffice/share/prereg exists → doesn't appear to exist when i cd to it in terminal → use apt-get via the command line tab completion helps a lot run ' apt-get purge libreoffice ' → oh, yeah i see now. sorry. now it is telling me to fix dependencies with apt-get install -f → awesome, so are you all set now ? → well libreoffice is still not working, so no. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janvin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A latent variable recurrent neural network for discourse relation language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Manual and automatic evaluation of machine translation between european languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Machine Translation, ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="102" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGDIAL-2015</title>
		<meeting>of SIGDIAL-2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Proceedings of INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Owoputi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quantitative evaluation of user simulation techniques for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th SIGdial Workshop on DISCOURSE and DIALOGUE</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of available corpora for building data-driven dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno>abs/1512.05742</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06069</idno>
		<title level="m">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chatbots: are they really useful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Shawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Atwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LDV Forum</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A hierarchical recurrent encoder-decoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM-2015</title>
		<meeting>of CIKM-2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
	</analytic>
	<monogr>
		<title level="m">Theano Development Team</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A network-based end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">Memory networks. ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">POMDP-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
