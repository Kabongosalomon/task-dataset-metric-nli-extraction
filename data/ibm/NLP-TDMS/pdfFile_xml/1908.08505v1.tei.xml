<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COLORNET -ESTIMATING COLORFULNESS IN NATURAL IMAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emin</forename><surname>Zerman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">V-SENSE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Rana</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">V-SENSE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">V-SENSE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">COLORNET -ESTIMATING COLORFULNESS IN NATURAL IMAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Colourfulness</term>
					<term>CNN</term>
					<term>Color metric</term>
					<term>Deep learn- ing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Measuring the colorfulness of a natural or virtual scene is critical for many applications in image processing field ranging from capturing to display. In this paper, we propose the first deep learning-based colorfulness estimation metric. For this purpose, we develop a color rating model which simultaneously learns to extracts the pertinent characteristic color features and the mapping from feature space to the ideal colorfulness scores for a variety of natural colored images. Additionally, we propose to overcome the lack of adequate annotated dataset problem by combining/aligning two publicly available colorfulness databases using the results of a new subjective test which employs a common subset of both databases. Using the obtained subjectively annotated dataset with 180 colored images, we finally demonstrate the efficacy of our proposed model over the traditional methods, both quantitatively and qualitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Color is a crucial factor in human visual perception. It affects human behavior and decision processes both in nature and in society, as color conveys pivotal information about the surroundings. Thus, its accurate acquisition and display are necessary for multimedia, entertainment, and image processing systems.</p><p>Within the imaging pipeline, the color or brightness information of the real (or virtual) scene needs to be processed for various reasons such as color grading <ref type="bibr" target="#b0">[1]</ref>, tone-mapping <ref type="bibr" target="#b1">[2]</ref> for high dynamic range (HDR) scenes, gamut mapping <ref type="bibr" target="#b2">[3]</ref>, color correction <ref type="bibr" target="#b3">[4]</ref>. During the acquisition or the processing, color of the scene can be affected by color cast or colorfulness changes <ref type="bibr" target="#b4">[5]</ref>. Within the scope of this study, only the colorfulness aspect of the natural images is considered.</p><p>Colorfulness is generally defined as the amount, intensity, and saturation of colors in an image <ref type="bibr" target="#b5">[6]</ref>. Understanding and estimating colorfulness is quintessential for a variety of applications, e.g., HDR tone-mapping <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>, aesthetics image analysis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, colorreproduction in cameras <ref type="bibr" target="#b12">[13]</ref>, image/video quality assessment <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, virtual reality <ref type="bibr" target="#b17">[18]</ref> etc. For colorfulness estimation, several methods have been proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> in the literature, in addition to the techniques used in the color appearance models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> (see Section 2 for more discussion).</p><p>To explore the competence of learning-based approaches and to create a stepping stone for further analysis of human perception with deep learning methods, in this paper, we propose a novel deep *These authors contributed equally to this work. This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under the Grant Number 15/RP/2776. We also gratefully acknowledge the support of NVIDIA Corporation with the donated GPU used for this research. learning-based objective metric 'ColorNet' for the estimation of colorfulness in natural images. Based on a convolutional neural network (CNN), our proposed ColorNet is a two-stage color rating model, where at stage I, a feature network extracts the characteristics features from the natural images and at stage II, a rating network estimates the colorfulness rating. To design our feature network, we explore the designs of the popular high-level CNN based feature models such as VGG <ref type="bibr" target="#b21">[22]</ref>, ResNet <ref type="bibr" target="#b22">[23]</ref>, and MobileNet <ref type="bibr" target="#b23">[24]</ref> architectures which we finally alter and tune for our colorfulness metric problem at hand. We also propose a rating network which is simultaneously learned to estimate the relationship between the characteristic features and ideal colorfulness scores.</p><p>In this paper, we additionally overcome the challenge of the absence of a well-annotated dataset for training and validating Col-orNet model in a supervised manner. To this end, we combine two publicly available colorfulness databases <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> using the results of a new subjective test which employs a common subset of both databases. The resulting 'Combined' dataset contains 180 color images and corresponding subjective colorfulness scores. Finally, we compare and showcase how our ColorNet model outperforms the state-of-the-art traditional colorfulness estimation models both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Several studies have attempted to understand and estimate the colorfulness in visual content. Color appearance models (CAMs) utilize some form of chroma and colorfulness estimation to estimate the local visual perception and to reproduce the colors considering the lighting conditions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. However, such estimations are valid mostly for simple uniform image patches. To estimate the overall colorfulness values of complex natural images, several studies have been conducted <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> in the literature.</p><p>Yendrikhovskij et al. <ref type="bibr" target="#b15">[16]</ref> developed a model to estimate the quality of natural color images by computing their naturalness and colorfulness indices. The proposed colorfulness index of an image is given as a summation of the average saturation (saturation as defined in CIE L*u*v* color space) and the standard deviation of the saturation. In another study, Datta et al. <ref type="bibr" target="#b10">[11]</ref> proposed to first partition the RGB cube and estimate the colorfulness by calculating the Earth Mover's Distance between the pixel distribution of the tested image and that of an "ideal" colorful image. However, compared to others, the model failed in perceptual validation <ref type="bibr" target="#b18">[19]</ref>.</p><p>Hasler and Süsstrunk <ref type="bibr" target="#b4">[5]</ref>, on the other hand, studied the colorfulness in natural images by conducting a subjective user study over a relatively complex set of images. Their proposed method estimates the colorfulness by computing the first and second order statistics between the opponent color channels i.e., yellow-blue and red-green color channels of the given RGB image. Using the same opponent color space strategy, Panetta et al. <ref type="bibr" target="#b16">[17]</ref> proposed a colorfulness metric as a part of their no-reference image quality measure. Their pro- posed model involved statistical computations in logarithmic space assuming that human visual perception works logarithmically. The performances of these colorfulness methods have been compared in a couple of studies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. Amati et al. <ref type="bibr" target="#b18">[19]</ref> analyzed the relationship between the colorfulness and the aesthetics of the images by conducting a subjective study with 100 images. They also proposed a contrast-based colorfulness metric which, however, was not better than Hasler and Süsstrunk <ref type="bibr" target="#b4">[5]</ref>. Hence, it was not considered in this study.</p><p>Considering a tone mapping scenario, Krasula et al. <ref type="bibr" target="#b24">[25]</ref> compared three different colorfulness methods (namely CQE CF 1 , CQE CF 2 <ref type="bibr" target="#b16">[17]</ref>, and CIQIc -a colorfulness metric inspired from Hasler and Süsstrunk <ref type="bibr" target="#b4">[5]</ref>) along with three naturalness and six contrast metrics. To this end, they employed the high dynamic range (HDR) image tone mapping methods database ofČadík et al. <ref type="bibr" target="#b25">[26]</ref> with 42 images. As a result of this comparison, the CQE CF 1 metric was found to be the most consistent and most correlated colorfulness metric.</p><p>In addition to the traditional image processing methods, several deep learning techniques have been recently explored for designing learning-based image quality metrics <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. An early attempt was made by <ref type="bibr" target="#b27">[28]</ref> for no-reference image quality assessment task where the efficacy of utilizing the high-level CNN features was explored. In <ref type="bibr" target="#b26">[27]</ref>, a neural metric has been proposed to predict the aesthetic and pixel-level quality distributions. However, no work has been done to estimate colorfulness in images. In this study, we, therefore, first gather a 180-image dataset with subjective colorfulness scores and then propose a color quality estimation model by exploring various state-of-the-art high-level CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SUBJECTIVE DATA</head><p>In this study, we use the subjective colorfulness scores collected from participants for two different colorfulness databases: EPFL Dataset <ref type="bibr" target="#b4">[5]</ref> with 84 images and UCL Dataset 1 <ref type="bibr" target="#b18">[19]</ref> with 96 images 2 .</p><p>Both datasets provide the collected subjective scores and corresponding images. The UCL Dataset provides pairwise comparison scores (also with the user confidence). The numerical colorfulness scores, for this database, are obtained through a Thurstone Case V scaling 3 <ref type="bibr" target="#b28">[29]</ref>. The quality scores for the EPFL Dataset are already scaled by the respective authors, using one of the methods proposed by Engeldrum <ref type="bibr" target="#b29">[30]</ref>.</p><p>Even though a psychometric scaling algorithm has been employed, the EPFL Dataset scores have been collected using a rating methodology. Whereas, UCL Dataset scores have been collected using a ranking (i.e. pairwise comparison) methodology. To bring these scores to the same scale, a third subjective test is conducted as an anchor, using a common subset of these two datasets. The two databases are then combined (i.e. aligned) using the scores from this third subjective experiment <ref type="bibr" target="#b30">[31]</ref> as explained below.</p><p>Selection of Images from Each Dataset: To create a common subset, we first selected 12 representative images from each database. These selected images cover the whole quality scale, starting from not colorful at all to very colorful. The distribution of subjective colorfulness scores for all of the images and the selected images is shown in <ref type="figure" target="#fig_0">Fig. 1.</ref>(a) and <ref type="figure" target="#fig_0">Fig. 1</ref>.(c) for EPFL and UCL, respectively. These selected images are used in a new subjective experiment. The new subjective test and the subjective scores collected are referred to as 'Anchor' experiment and dataset, respectively.</p><p>Subjective Test for Anchoring the Two Datasets: To merge EPFL and UCL datasets on the same scale, we have conducted a third subjective experiment with a common subset of images <ref type="bibr" target="#b30">[31]</ref>. The pairwise comparisons (PWC) methodology is chosen in order to keep the cognitive load for the participants lighter and the experiment process easier. Although PWC is easier for subjects to decide, the full PWC design (with n(n − 1)/2 pairs) is time-consuming. Instead, in this study, we use an adaptive pair selection process, known as adaptive square design (ASD) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. With this pair selection process (and also test methodology), the pair selection and scaling are done iteratively, making sure that similar stimuli are compared more than different stimuli (for which the difference will be clearer with fewer comparisons). The ASD is implemented in Matlab, and a Thurstone Case V based psychometric scaling method is used for this experiment. For the presentation and interactive voting, a Mat-lab toolbox called Psychtoolbox is used <ref type="bibr" target="#b33">[34]</ref>.</p><p>To obtain the subjective scores, the 24 selected images are used in the 'Anchor' experiment, to which three expert viewers have attended. Instead of updating the rank matrix of ASD for each observer, a different approach is used. The rank matrix of ASD is randomly initialized for each expert observer, and throughout the 5 loops of the test, this rank matrix is updated, and the new pairs are selected considering this updated rank matrix. This ensured that different images have been compared for each different expert viewer.</p><p>The obtained PWC results are then scaled to quality values which are later used to merge the databases. The scaling results are generally arbitrary (i.e., relative to each other without an origin point), and hence, hard to understand. Therefore, these subjective quality scores are mapped to <ref type="bibr">[1 9</ref>] scale, 1 being the least colorful and 9 being the most colorful.</p><p>Combining the Datasets: The relationship between the new 'Anchor' colorfulness scores and those of EPFL and UCL are shown in <ref type="figure" target="#fig_0">Fig. 1.(b) and 1.(d)</ref>, respectively, for the selected images. The relationship between these scores is linear, with very high correlation scores.</p><p>To merge the databases linearly with corresponding 'Anchor' scores, the parameters a and b are found by solving y = ax + b relationship. Here, y is the 'Anchor' score and x is the source (either EPFL or UCL) database score. These parameters are found as: aEP F L = 0.8748 and bEP F L = 1.4350 for EPFL and aUCL = 1.1388 and bUCL = 6.8759 for UCL databases.</p><p>Then, the two databases are brought to the same scale as:</p><formula xml:id="formula_0">QDB = aDB × QDB + bDB<label>(1)</label></formula><p>Finally, the mapped quality scores are concatenated in a single dataset. In <ref type="figure" target="#fig_0">Fig. 1.(e)</ref>, the merged (denoted as 'Combined') subjective quality scores are plotted again vs the 'Anchor' scores to validate the merging operation, where these two databases are observed to be on the same scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROPOSED COLOR RATING MODEL</head><p>The ColorNet architecture is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The proposed model has two major building blocks, 1) the feature network Φ f and 2) the rating network Φr. We base our feature network Φ f on the state-of-the-art deep learning models, namely, VGG <ref type="bibr" target="#b21">[22]</ref>, ResNet <ref type="bibr" target="#b22">[23]</ref> and MobileNet <ref type="bibr" target="#b23">[24]</ref>. For all these models, we specifically removed the last layers originally meant for classification and fine-tuned the remaining feature layers in an end-to-end fashion. These features are fed to the rating network Φr which has an objective of mapping the characteristic features to the colorfulness rating domain. The three variants of our ColorNet model are named as ColorNet-VGG, ColorNet-ResNet, and ColorNet-Mobile. The rating network is the same for all three variants. It consists of a dropout regularization layer, two fully connected layers F C − 10 and F C − 1 with 10 and 1 channels respectively. An added non-linearity is introduced by using the ReLU unit in between the fully connected layers to learn the desired colorfulness mapping. Further details regarding the three variants are as follows:</p><p>1. ColorNet-VGG has a 13 convolutional layered feature network Φ vgg f adopted from the VGG16 <ref type="bibr" target="#b21">[22]</ref> architecture, with small 3 × 3 convolutions, resulting in a feature vector of dimension 512×7×7. In our paper, we removed the three fully connected layers of the VGG16 architecture and simply fed the resulting feature vector into the proposed rating network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ColorNet-ResNet consists of an 18 layer deep residual feature</head><p>network Φ resnet f adopted from the ResNet architecture <ref type="bibr" target="#b22">[23]</ref>. Similar to VGG <ref type="bibr" target="#b21">[22]</ref>, the ResNet architecture has small 3 × 3 convolutions, however with an additional concept of residual learning applied to every few stacked layers. In our paper, we removed the last fully connected layer of 1000 channels and fed the resulting feature of size 512 × 7 × 7 into the rating work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ColorNet-Mobile consists of a 28 convolution layers fea-</head><p>ture network Φ mobile f adopted from the MobileNet architecture <ref type="bibr" target="#b23">[24]</ref>, which comprises of both depth-wise and point-wise convolutions. MobileNet has been a widely adopted network for many mobile and embedded applications. In this paper, we removed its last fully connected and soft-max layer to obtain a feature of size 1024 × 7 × 7 which is finally fed into the rating network.</p><p>Note that we choose these feature models mainly due to ease in training and faster convergence over small dataset size. For further details regarding the feature network architectures, we refer the reader to <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. For each ColorNet variant, the receptive input for the first fully-connected is altered as per the resulting feature size of the feature network.</p><p>Loss Function: The ultimate goal of ColorNet models is to rate the colorfulness in images. To this end, we train these models in an end-to-end fashion using a fully supervised setting. For training, we define a set of an image and its corresponding rating as a pair, given as (Xi, yi), where Xi is an image and yi is its corresponding colorfulness rating, i = [1, n], n = number of training image pairs. To train the ColorNet models, we used the following objective function:</p><formula xml:id="formula_1">L j (y,ŷ) = 1 n n k=1 |y k − φ j r .φ j f (Xi)|<label>(2)</label></formula><p>whereŷ is the predicted colorfulness rating and j = {vgg, resnet, mobile}. We additionally experimented with the L2 norm as a loss function, however, we observe that practically the L1 loss term effectively penalizes the network to learn and converge at a faster rate using the ADAM <ref type="bibr" target="#b34">[35]</ref> optimizer technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>Training and Implementation Details: We split the dataset of 180 images into training, validation and test sets in an 80%, 10% and 10% setting. For training, the input image size is fixed at 600 × 600, and random crops of size 512 × 512 are applied to the image. Additional data augmentation techniques such as rotation and flipping are applied to scale up the training dataset. The ColorNet model is implemented using the Pytorch <ref type="bibr" target="#b35">[36]</ref> deep learning library. During training, the batch size is set to 4 and the baseline weights of the feature networks are initialized by training on the ImageNet <ref type="bibr" target="#b36">[37]</ref> dataset. The weights of the layers in the rating networks are initialized randomly. In other terms, we fine-tune the feature network and train the rating network layers for our task at hand. A dropout rate of 0.75 is set in the rating network for all the three models. We utilize an ADAM solver <ref type="bibr" target="#b34">[35]</ref> with an initial learning rate of 1 × 10 4 and 1 × 10 3 for training the feature and rating networks respectively. The learning rates are allowed to decay exponentially with a decay rate of 0.95, after every 10 epochs. The momentum rate is fixed at 0.9 for all epochs. All our models are trained in an end-to-end fashion for 200 epochs. Training is done using a 12 GB NVIDIA Titan-X GPU on an Intel Xeon E7 core i7 machine for 200 epochs which take approximately 2 hours. Inference time is 25 secs for each image.</p><p>Traditional Colorfulness Methods: We implemented four different colorfulness metrics: Hasler and Süsstrunk (CFHasler) <ref type="bibr" target="#b4">[5]</ref> , two versions (CQE CF 1 and CQE CF 2 ) of Panetta et al. <ref type="bibr" target="#b16">[17]</ref>, and Yendrikhovskij et al. (CFYendrikhovskij) <ref type="bibr" target="#b15">[16]</ref> to compare with the proposed ColorNet model.</p><p>For the Hasler and Süsstrunk <ref type="bibr" target="#b4">[5]</ref>, with a given RGB image, the metric uses the mean µ, and standard deviation σ of the opponent color space vectors vrg and v yb where vrg = IR − IG and v yb = (IR + IG)/2 − IB. Then, CFHasler is computed as:</p><formula xml:id="formula_2">CFHasler = σ 2 rg + σ 2 yb + 0.3 × µ 2 rg + µ 2 yb<label>(3)</label></formula><p>Panetta et al. <ref type="bibr" target="#b16">[17]</ref> use the similar color opponent space, but in logarithmic space. The metrics CQE CF 1 and CQE CF 2 are calculated as:</p><formula xml:id="formula_3">CQE CF 1 = 0.02 × log σ 2 rg |µrg| 0.2 × log σ 2 yb |µ yb | 0.2 CQE CF 2 = 0.02 × log(σ 2 rg ) × log(σ 2 yb ) log(σ 2 c ) × log(µ 2 rg ) × log(µ 2 yb ) log(µ 2 c )<label>(4)</label></formula><p>where vrg and v yb are concatenated to form vc, i.e., vc = [vrg, v yb ].</p><p>To compute CFYendrikhovskij, we use the following formula after the RGB to CIE L*u*v* color space transform, as specified in the paper <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_4">Su * v * = (u * 2 + v * 2 ) L * + , = 0 CFYendrikhovskij = µS u * v * + σS u * v *<label>(5)</label></formula><p>Quantitative Evaluation: Colorfulness metrics are evaluated by computing the Pearson correlation coefficient (PCC) and the Spearman rank-ordered correlation coefficient (SROCC). For testing, we used the 10-fold cross-validation strategy where the whole Qualitative Evaluation: For the qualitative evaluation of Col-orNet, we crafted a set of images that are not used during the training of the model, by considering two different scenarios: i) change in the dominant colours and ii) change in the color contrast. In <ref type="figure">Fig. 3</ref>, row I shows 4 different cases of the change in the dominant colors, and row II shows the change in the color contrast. We report the objective results for the proposed deep-learning-based colorfulness estimation method, in <ref type="figure">Fig. 3</ref>, and show also CF Hasler for completeness. The results showcase that in both cases i.e., by increasing color contrast and increasing the number of dominant hues, the proposed metric scores increase, thus, validating the understanding of colorfulness of our model. Overall, our results confirm that learning-based models bring huge potential to cater for various implicit aspects of color perception over a wide variety of natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this study, we propose a CNN based model for the estimation of colorfulness ratings. To prepare a well-annotated colored image dataset, we combine two colorfulness databases with subjective user scores, using the results of an anchor subjective experiment with a common subset of images. We compare the results of the proposed model to those of four other traditional colorfulness metrics quantitatively and qualitatively where we observe that our learningbased model effectively rates the colorfulness by catering for the wide variety of natural images. This study constitutes an initial step towards the exploration of color perception in natural images using the deep learning approach. In future work, we aim to delve deeper in learning-based color perception models and analyze the impacts of various associated factors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:1908.08505v1 [cs.MM] 22 Aug 2019 (a) Subset EPFL (b) EPFL vs. 'Anchor' (c) Subset UCL (d) UCL vs. 'Anchor' (e) 'Combined' vs. 'Anchor' Subjective data. To be representative, 12 images are selected each from EPFL (a) and UCL (c) datasets where blue circles indicate subjective colorfulness scores for all the images and red cross marks indicate those of the selected images. In (b), (d) and (e), we present the relationship between the colorfulness scores of the 'Anchor' experiment conducted and the colorfulness scores of EPFL (b), UCL (d), and 'Combined' (e) datasets. The diamonds indicate the scores for the selected subset images and the black dashed line indicates the best linear fit between the considered two scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The ColorNet Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Correlation Coefficient Results. Qualitative Evaluation. Row I depicts the change in dominant colors. Row II depicts the change in the saturation of the colors.dataset was divided into 10 non-overlapping pieces P, and for each iteration, one piece is used for the test, one piece used for validation, and the remaining pieces are used for training (as also described above). For each iteration itr, the unique piece Pitr is used for the test, where all the pieces (Pitr, itr ∈ [1, 10]) cover the whole dataset. Finally, the PCC and SROCC results for 10 different test cases are averaged. The performance results are presented inTable 1where PCC and SROCC for the 4 state-of-the-art colorfulness metrics are computed using the aligned subjective scores from 'Combined' dataset. Our proposed ColorNet model with VGG and ResNet based feature network outperforms over the other classical models. This is partly due to the model's ability to learn rich high-level color feature representation.</figDesc><table><row><cell cols="4">Colorfulness Metric PCC SROCC</cell></row><row><cell cols="2">CFHasler [5]</cell><cell>0.841</cell><cell>0.884</cell></row><row><cell>CQE CF 1</cell><cell>[17]</cell><cell>0.895</cell><cell>0.896</cell></row><row><cell>CQE CF 2</cell><cell>[17]</cell><cell>0.312</cell><cell>0.415</cell></row><row><cell cols="2">CFYendrikhovskij [16]</cell><cell>0.843</cell><cell>0.834</cell></row><row><cell cols="2">ColorNet-Mobile</cell><cell>0.841</cell><cell>0.774</cell></row><row><cell cols="2">ColorNet-ResNet</cell><cell>0.916</cell><cell>0.889</cell></row><row><cell cols="2">ColorNet-VGG</cell><cell>0.937</cell><cell>0.921</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">University College London Colourfulness Datasethttp:// reality.cs.ucl.ac.uk/projects/image-colourfulness/ image-colourfulness.html 2 Although the links are provided for 100 images the UCL Dataset, four out of these 100 images have been removed from Flickr.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">pwcmp softwarehttps://github.com/mantiuk/pwcmp</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated colour grading using colour distribution transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pitié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="123" to="137" />
			<date type="published" when="2007-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Display adaptive tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kerofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A gamut-mapping framework for coloraccurate reproduction of HDR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sikudová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Artusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Akyüz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mazlumoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="78" to="90" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Color correction for tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaszewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring colorfulness in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Electronic Imaging, Human Vision and Electronic Imaging VIII</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
		<title level="m">Color Appearance Models</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Calibrated image appearance reproduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ballestad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Damberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning-based tone mapping operator for efficient image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valenzise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="256" to="268" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning-based Adaptive Tone Mapping for Keypoint Detection</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo (ICME&apos;2017)</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning-Based Tone Mapping Operator for Image Matching</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated aesthetic analysis of photographic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydın</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving color reproduction accuracy on cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karaimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of public image and video databases for quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="616" to="625" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selecting scenes for 2D and 3D subjective video quality tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barkowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing color reproduction of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yendrikhovskij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Blommaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Ridder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Color and Imaging Conference. Society for Imaging Science and Technology</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">No reference color image contrast and quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Panetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agaian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="643" to="651" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards generating ambisonics using audio-visual cue for virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ozcinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A study of image colourfulness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Aesthetics</title>
		<meeting>the Workshop on Computational Aesthetics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Reproduction of Colour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W G</forename><surname>Hunt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note>5th ed</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revision of the chroma and hue scales of a nonlinear color-appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nayatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Color Research &amp; Application</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Objective evaluation of naturalness, contrast, and colorfulness of tone-mapped images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Krasula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fliegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klíma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Optical Engineering + Applications, Applications of Digital Image Processing XXXVII. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation of HDR tone mapping methods using essential perceptual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Čadík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Artusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="349" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NIMA: Neural image assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3998" to="4011" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A practical guide and software for analysing pairwise comparison experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03686</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">stat.AP -</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Psychometric Scaling: A Toolkit for Imaging Systems Development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Engeldrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Imcotek press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Aligning subjective tests using a low cost common set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pitrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barkowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pépion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in Euro ITV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting paired comparison methodology in measuring visual discomfort of 3DTV: performances of three different designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barkowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Electronic Imaging, Stereoscopic Displays and Applications XXIV. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Subjective assessment methods for 3D video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITU-R Recommendation P.915</title>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What&apos;s new in Psychtoolbox-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ingling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Broussard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
