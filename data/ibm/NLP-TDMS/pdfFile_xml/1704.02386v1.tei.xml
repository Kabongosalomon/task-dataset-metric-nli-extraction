<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixelwise Instance Segmentation with a Dynamically Instantiated Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
							<email>anurag.arnab@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<email>philip.torr@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pixelwise Instance Segmentation with a Dynamically Instantiated Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation and object detection research have recently achieved rapid progress. However, the former task has no notion of different instances of the same object, and the latter operates at a coarse, bounding-box level. We propose an Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label. Most approaches adapt object detectors to produce segments instead of boxes. In contrast, our method is based on an initial semantic segmentation module, which feeds into an instance subnetwork. This subnetwork uses the initial category-level segmentation, along with cues from the output of an object detector, within an end-to-end CRF to predict instances. This part of our model is dynamically instantiated to produce a variable number of instances per image. Our end-to-end approach requires no post-processing and considers the image holistically, instead of processing independent proposals. Therefore, unlike some related work, a pixel cannot belong to multiple instances. Furthermore, far more precise segmentations are achieved, as shown by our substantial improvements at high AP r thresholds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation and object detection are wellstudied scene understanding problems, and have recently witnessed great progress due to deep learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>. However, semantic segmentation -which labels every pixel in an image with its object class -has no notion of different instances of an object <ref type="figure">(Fig. 1</ref>). Object detection does localise different object instances, but does so at a very coarse, bounding-box level. Instance segmentation localises objects at a pixel level, as shown in <ref type="figure">Fig. 1</ref>, and can be thought of being at the intersection of these two scene understanding tasks. Unlike the former, it knows about different instances of the same object, and unlike the latter, it operates at a pixel level. Accurate recognition and localisation of objects enables many applications, such as autonomous driving <ref type="bibr" target="#b8">[9]</ref>, image-editing <ref type="bibr" target="#b52">[53]</ref> and robotics <ref type="bibr" target="#b16">[17]</ref>.</p><p>Many recent approaches to instance segmentation are based on object detection pipelines where objects are first localised with bounding boxes. Thereafter, each bounding box is refined into a segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30</ref>]. Another related approach <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b55">56]</ref> is to use segment-based region proposals <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> instead of box-based proposals. However, these methods do not consider the entire image, but rather independent proposals. As a result, occlusions between different objects are not handled. Furthermore, many of these methods cannot easily produce segmentation maps of the image, as shown in <ref type="figure">Fig. 1</ref>, since they process numerous proposals independently. There are typically far more proposals than actual objects in the image, and these proposals can overlap and be assigned different class labels. Finally, as these methods are based on an initial detection step, they cannot recover from false detections.</p><p>Our proposed method is inspired by the fact that instance segmentation can be viewed as a more complex form of semantic segmentation, since we are not only required to label the object class of each pixel, but also its instance identity. We produce a pixelwise segmentation of the image, where each pixel is assigned both a semantic class and instance label. Our end-to-end trained network, which outputs a variable number of instances per input image, begins with an initial semantic segmentation module. The following, dynamic part of the network, then uses information from an object detector and a Conditional Random Field (CRF) model to distinguish different instances. This approach is robust to false-positive detections, as well as poorly localised bounding boxes which do not cover the entire object, in contrast to detection-based methods to instance segmentation. Moreover, as it considers the entire image when making predictions, it attempts to resolve occlusions between different objects and can produce segmentation maps as in <ref type="figure">Fig. 1</ref> without any post-processing.</p><p>Furthermore, we note that the Average Precision (AP) metric <ref type="bibr" target="#b13">[14]</ref> used in evaluating object detection systems, and its AP r variant <ref type="bibr" target="#b18">[19]</ref> used for instance segmentation, considers individual, potentially overlapping, object predictions in isolation, as opposed to the entire image. To evaluate methods such as ours, which produce complete segmentation maps and reason about occlusions, we also evaluate using Our proposed method jointly produces both semantic and instance segmentations. Our method uses the output of an object detector as a cue to identify instances, but is robust to false positive detections, poor bounding box localisation and occlusions. Best viewed in colour.</p><p>the "Matching Intersection over Union" metric.</p><p>Our system, which is based on an initial semantic segmentation subnetwork, produces sharp and accurate instance segmentations. This is reflected by the substantial improvements we achieve over state-of-the-art methods at high AP r thresholds on the Pascal VOC and Semantic Boundaries datasets. Furthermore, our network improves on the semantic segmentation task while being trained for the related task of instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>An early work on instance segmentation was by Winn and Shotton <ref type="bibr" target="#b50">[51]</ref>. A per-pixel unary classifier was trained to predict parts of an object. These parts were then encouraged to maintain a spatial ordering, that is characteristic of an instance, using asymmetric pairwise potentials in a Conditional Random Field (CRF). Subsequent work <ref type="bibr" target="#b53">[54]</ref>, presented another approach where detection outputs of DPM <ref type="bibr" target="#b14">[15]</ref>, with associated foreground masks, were assigned a depth ordering using a generative, probabilistic model. This depth ordering resolved occlusions.</p><p>However, instance segmentation has become more common after the "Simultaneous Detection and Segmentation" (SDS) work of Hariharan et al. <ref type="bibr" target="#b18">[19]</ref>. This system was based on the R-CNN pipeline <ref type="bibr" target="#b15">[16]</ref>: Region proposals, generated by the method of <ref type="bibr" target="#b0">[1]</ref>, were classified into object categories with a Convolutional Neural Network (CNN) before applying bounding-box regression as post-processing. A classspecific segmentation was then performed in this bounding box to simultaneously detect and segment the object. Numerous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref> have extended this pipeline. However, approaches that segment instances by refining detections <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30]</ref> are inherently limited by the quality of the initial proposals. This problem is exacerbated by the fact that this pipeline consists of several different modules trained with different objective functions. Furthermore, numerous post-processing steps such as "superpixel projection" and rescoring are performed. Dai et al. <ref type="bibr" target="#b11">[12]</ref> addressed some of these issues by designing one end-to-end trained network that generates box-proposals, creates foreground masks from these proposals and then classifies these masks. This network can be seen as an extension of the end-to-end Faster-RCNN <ref type="bibr" target="#b43">[44]</ref> detection framework, which generates box-proposals and classifies them. Additionally, Liu et al. <ref type="bibr" target="#b36">[37]</ref> formulated an end-to-end version of the SDS network <ref type="bibr" target="#b18">[19]</ref>, whilst <ref type="bibr" target="#b31">[32]</ref> iteratively refined object proposals.</p><p>On a separate track, algorithms have also been developed that do not require object detectors. Zhang et al. <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> segmented car instances by predicting the depth ordering of each pixel in the image. Unlike the previous detectionbased approaches, this method reasoned globally about all instances in the image simultaneously (rather than individual proposals) with an MRF-based formulation. However, inference of this graphical model was not performed end-toend as shown to be possible in <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34]</ref>. Furthermore, although this method does not use object detections, it is trained with ground truth depth and assumes a maximum of nine cars in an image. Predicting all the instances in an image simultaneously (rather than classifying individual proposals) requires a model to be able to handle a variable number of output instances per image. As a result, <ref type="bibr" target="#b44">[45]</ref> proposed a Recurrent Neural Network (RNN) for this task. However, this model was only for a single object category. Our proposed method not only outputs a variable number of instances, but can also handle multiple object classes.</p><p>Liang et al. <ref type="bibr" target="#b32">[33]</ref> developed another proposal-free method based on the semantic segmentation network of <ref type="bibr" target="#b5">[6]</ref>. The category-level segmentation, along with CNN features, was used to predict instance-level bounding boxes. The number of instances of each class was also predicted to enable a final spectral clustering step. However, this additional information predicted by Liang's network could have been obtained from an object detector. Arnab et al. <ref type="bibr" target="#b2">[3]</ref> also started with an initial semantic segmentation network <ref type="bibr" target="#b1">[2]</ref>, and combined this with the outputs of an object detector using a CRF to reason about instances. This method was not trained endto-end though, and could not really recover from errors in bounding-box localisation or occlusion.</p><p>Our method also has an initial semantic segmentation subnetwork, and uses the outputs of an object detector. However, in contrast to <ref type="bibr" target="#b2">[3]</ref> it is trained end-to-end to improve on both semantic-and instance-segmentation performance (to our knowledge, this is the first work to achieve this). Furthermore, it can handle detector localisation errors and occlusions better due to the energy terms in our end-to-end CRF. In contrast to detection-based approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37]</ref>, our network requires no additional postprocessing to create an instance segmentation map as in <ref type="figure">Fig. 1</ref>(c) and reasons about the entire image, rather than independent proposals. This global reasoning allows our method to produce more accurate segmentations. Our proposed system also handles a variable number of instances per image, and thus does not assume a maximum number of instances like <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>Our network ( <ref type="figure">Fig. 2)</ref> contains an initial semantic segmentation module. We use the semantic segmentation result, along with the outputs of an object detector, to compute the unary potentials of a Conditional Random Field (CRF) defined over object instances. We perform mean field inference in this random field to obtain the Maximum a Posteriori (MAP) estimate, which is our labelling. Although our network consists of two conceptually different parts -a semantic segmentation module, and an instance segmentation network -the entire pipeline is fully differentiable, given object detections, and trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Semantic Segmentation subnetwork</head><p>Semantic Segmentation assigns each pixel in an image a semantic class label from a given set, L. In our case, this module uses the FCN8s architecture <ref type="bibr" target="#b37">[38]</ref> which is based on the VGG <ref type="bibr" target="#b46">[47]</ref> ImageNet model. For better segmentation results, we include mean field inference of a Conditional Random Field as the last layer of this module. This CRF contains the densely-connected pairwise potentials described in <ref type="bibr" target="#b25">[26]</ref> and is formulated as a recurrent neural network as in <ref type="bibr" target="#b59">[60]</ref>. Additionally, we include the Higher Order detection potential described in <ref type="bibr" target="#b1">[2]</ref>. This detection potential has two primary benefits: Firstly, it improves semantic segmentation quality by encouraging consistency between object detections and segmentations. Secondly, it also recalibrates detection scores. This detection potential is similar to the one previously proposed by <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b51">[52]</ref> and <ref type="bibr" target="#b54">[55]</ref>, but formulated for the differentiable mean field inference algorithm. We employ this potential as we are already using object detection information for identifying object instances in the next stage. We denote the output at the semantic segmentation module of our network as the tensor Q, where Q i (l) denotes the probability (obtained by applying the softmax function on the network's activations) of pixel i taking on the label l ∈ L.</p><p>(a) Semantic Segmentation (b) Instance Segmentation <ref type="figure">Figure 3</ref>: Instance segmentation using only the "Box" unary potential. This potential is effective when we have a good initial semantic segmentation (a). Occlusions between objects of the same class can be resolved by the pairwise term based on appearance differences. Note that we can ignore the confident, false-positive "bottle" detections (b). This is in contrast to methods such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref> which cannot recover from detection errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance Segmentation subnetwork</head><p>At the input to our instance segmentation subnetwork, we assume that we have two inputs available: The semantic segmentation predictions, Q, for each pixel and label, and a set of object detections. For each input image, we assume that there are D object detections, and that the i th detection is of the form (l i , s i , B i ) where l i ∈ L is the detected class label, s i ∈ [0, 1] is the confidence score and B i is the set of indices of the pixels falling within the detector's bounding box. Note that the number D varies for every input image.</p><p>The problem of instance segmentation can then be thought of as assigning every pixel to either a particular object detection, or the background label. This is based on the assumption that every object detection specifies a potential object instance. We define a multinomial random variable, V , at each of the N pixels in the image, and</p><formula xml:id="formula_0">V = [V 1 V 2 . . . V N ] T .</formula><p>Each variable at pixel i, V i , is assigned a label corresponding to its instance. This label set, {0, 1, 2, ..., D} changes for each image since D, the number of detections, varies for every image (0 is the background label). In the case of instance segmentation of images, the quality of a prediction is invariant to the permutations of the instance labelling. For example, labelling the "blue person" in <ref type="figure">Fig. 1</ref>(c) as "1" and the "purple person" as "2" is no different to labelling them as "2" and "1" respectively. This condition is handled by our loss function in Sec. 3.4.</p><p>Note that unlike works such as <ref type="bibr" target="#b56">[57]</ref> and <ref type="bibr" target="#b57">[58]</ref> we do not assume a maximum number of possible instances and keep a fixed label set. Furthermore, since we are considering object detection outputs jointly with semantic segmentation predictions, we have some robustness to high-scoring false positive detections unlike methods such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref> which refine object detections into segmentations.</p><p>We formulate a Conditional Random Field over our instance variables, V , which consists of unary and pairwise  <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> cannot cope with poorly localised detections. Note, the overlaid detection boxes are an additional input to our system. energies. The energy of the assignment v to all the variables, V, is</p><formula xml:id="formula_1">E(V = v) = i U (v i ) + i&lt;j P (v i , v j ).<label>(1)</label></formula><p>The unary energy is a sum of three terms, which take into account the object detection bounding boxes, the initial semantic segmentation and shape information,</p><formula xml:id="formula_2">U (v i ) = −ln[w 1 ψ Box (v i ) + w 2 ψ Global (v i )+ w 3 ψ Shape (v i )],<label>(2)</label></formula><p>and are described further in Sections 3.2.1 through 3.2.3. w 1 , w 2 and w 3 are all weighting co-efficients learned via backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Box Term</head><p>This potential encourages a pixel to be assigned to the instance corresponding to the k th detection if it falls within the detection's bounding box. This potential is proportional to the probability of the pixel's semantic class being equal to the detected class Q i (l k ) and the detection score, s k .</p><formula xml:id="formula_3">ψ Box (V i = k) = Q i (l k )s k if i ∈ B k 0 otherwise<label>(3)</label></formula><p>As shown in <ref type="figure">Fig. 3</ref>, this potential performs well when the initial semantic segmentation is good. It is robust to false positive detections, unlike methods which refine bounding boxes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> since the detections are considered in light of our initial semantic segmentation, Q. Together with the pairwise term (Sec. 3.2.4), occlusions between objects of the same class can be resolved if there are appearance differences in the different instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Global Term</head><p>This term does not rely on bounding boxes, but only the segmentation prediction at a particular pixel, Q i . It encodes the intuition that if we only know there are d possible instances of a particular object class, and have no further localisation information, each instance is equally probable, and this potential is proportional to the semantic segmentation confidence for the detected object class at that pixel:</p><formula xml:id="formula_4">ψ Global (V i = k) = Q i (l k ).<label>(4)</label></formula><p>As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, this potential overcomes cases where the bounding box does not cover the entire extent of the object, as it assigns probability mass to a particular instance label throughout all pixels in the image. This is also beneficial during training, as it ensures that the final output is dependent on the segmentation prediction at all pixels in the image, leading to error gradients that are more stable across batches and thus more amenable to backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Shape Term</head><p>We also incorporate shape priors to help us reason about occlusions involving multiple objects of the same class, which may have minimal appearance variation between them, as shown in <ref type="figure">Fig. 5</ref>. In such cases, a prior on the expected shape of an object category can help us to identify the foreground instance within a bounding box. Previous approaches to incorporating shape priors in segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">50]</ref> have involved generating "shape exemplars" from the training dataset and, at inference time, matching these exemplars to object proposals using the Chamfer distance <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b35">36]</ref>. We propose a fully differentiable method: Given a set of shape templates, T , we warp each shape template using bilinear interpolation intoT so that it matches the dimensions of the k th bounding box, B k . We then select the shape prior which matches the segmentation prediction for the detected class within the bounding box, Q B k (l k ), the best according to the normalised cross correlation. Our shape prior is then the Hadamard (elementwise) product (⊙) between the segmentation unaries and the matched shape prior:</p><formula xml:id="formula_5">t * = arg max t∈T Q B k (l k ) ⊙ t Q B k (l k ) t (5) ψ(V B k = k) = Q B k (l k ) ⊙ t * .<label>(6)</label></formula><p>Equations 5 and 6 can be seen as a special case of maxpooling, and the numerator of Eq. 5 is simply a convolution that produces a scalar output since the two arguments are of equal dimension. Additionally, during training, we can consider the shape priors T as parameters of our "shape term" layer and backpropagate through to the matched exemplar t * to update it. In practice, we initialised these parameters (a) Without shape term (b) With Shape term <ref type="figure">Figure 5</ref>: The "Shape" unary potential (b) helps us to distinguish between the green and purple sheep, which the other two unary potentials cannot. Input detections are overlaid on the images. with the shape priors described in <ref type="bibr" target="#b49">[50]</ref>. This consists of roughly 250 shape templates for each of five different aspect ratios. These were obtained by clustering foreground masks of object instances from the training set.</p><p>Here, we have only matched a single shape template to a proposed instance. This method could be extended in future to matching multiple templates to an instance, in which case each shape exemplar would correspond to a part of the object such as in DPM <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Pairwise term</head><p>The pairwise term consists of densely-connected Gaussian potentials <ref type="bibr" target="#b25">[26]</ref> and encourages appearance and spatial consistency. The weights governing the importance of these terms are also learnt via backpropagation, as in <ref type="bibr" target="#b59">[60]</ref>. We find that these priors are useful in the case of instance segmentation as well, since nearby pixels that have similar appearance often belong to the same object instance. They are often able to resolve occlusions based on appearance differences between objects of the same class ( <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference of our Dynamic Instance CRF</head><p>We use mean field inference to approximately minimise the Gibbs Energy in Eq. 1 which corresponds to finding the Maximum a Posteriori (MAP) labelling of the corresponding probability distribution,</p><formula xml:id="formula_6">P (V = v) = 1 Z exp (−E(v))</formula><p>where Z is the normalisation factor. Mean field inference is differentiable, and this iterative algorithm can be unrolled and seen as a recurrent neural network <ref type="bibr" target="#b59">[60]</ref>. Following this approach, we can incorporate mean field inference of a CRF as a layer of our neural network. This enables us to train our entire instance segmentation network end-to-end.</p><p>Because we deal with a variable number of instances for every image, our CRF needs to be dynamically instantiated to have a different number of labels for every image, as observed in <ref type="bibr" target="#b2">[3]</ref>. Therefore, unlike <ref type="bibr" target="#b59">[60]</ref>, none of our weights are class-specific. This weight-sharing not only allows us to deal with variable length inputs, but class-specific weights (a) Original ground truth, G (b) Prediction, P (c) "Matched" ground truth, G * <ref type="figure">Figure 6</ref>: Due to the problem of label permutations, we "match" the ground truth with our prediction before computing the loss when training.</p><p>also do not make sense in the case of instance segmentation since a class label has no particular semantic meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>When training for instance segmentation, we have a single loss function which we backpropagate through our instance-and semantic-segmentation modules to update all the parameters. As discussed previously, we need to deal with different permutations of our final labelling which could have the same final result. The works of <ref type="bibr" target="#b56">[57]</ref> and <ref type="bibr" target="#b57">[58]</ref> order instances by depth to break this symmetry. However, this requires ground-truth depth maps during training which we do not assume that we have. Proposal-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref> do not have this issue since they consider a single proposal at a time, rather than the entire image. Our approach is similar to <ref type="bibr" target="#b44">[45]</ref> in that we match the original ground truth to our instance segmentation prediction based on the Intersection over Union (IoU) <ref type="bibr" target="#b13">[14]</ref> of each instance prediction and ground truth, as shown in <ref type="figure">Fig. 6</ref>.</p><p>More formally, we denote the ground-truth labelling of an image, G, to be a set of r segments, {g 1 , g 2 , . . . , g r }, where each segment (set of pixels) is an object instance and has an associated semantic class label. Our prediction, which is the output of our network, P, is a set of s segments, {p 1 , p 2 , . . . , p s }, also where each segment corresponds to an instance label and also has an associated class label. Note that r and s may be different since we may predict greater or fewer instances than actually present. Let M denote the set of all permutations of the ground-truth, G. As can be seen in <ref type="figure">Fig. 6</ref>, different permutations of the ground-truth correspond to the same qualitative result. We define the "matched" ground-truth, G * , as the permutation of the original ground-truth labelling which maximises the IoU between the prediction, P, and ground truth:</p><formula xml:id="formula_7">G * = arg max m∈M IoU(m, P).<label>(7)</label></formula><p>Once we have the "matched" ground truth, G * , <ref type="figure">(Fig. 6)</ref> for an image, we can apply any loss function to train our network for segmentation. In our case, we use the common cross-entropy loss function. We found that this performed better than the approximate IoU loss proposed in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45]</ref>. Crucially, we do not need to evaluate all permutations of the ground truth to compute Eq. 7, since it can be formulated as a maximum-weight bipartite matching problem. The edges in our bipartite graph connect ground-truth and predicted segments. The edge weights are given by the IoU between the ground truth and predicted segments if they share the same semantic class label, and zero otherwise. Leftover segments are matched to "dummy" nodes with zero overlap.</p><p>Additionally, the ordering of the instances in our network are actually determined by the object detector, which remains static during training. As a result, the ordering of our predictions does not fluctuate much during training -it only changes in cases where there are multiple detections overlapping an object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Network Training</head><p>We first train a network for semantic segmentation with the standard cross-entropy loss. In our case, this network is FCN8s <ref type="bibr" target="#b37">[38]</ref> with a CRF whose inference is unrolled as an RNN and trained end-to-end, as described in <ref type="bibr" target="#b59">[60]</ref> and <ref type="bibr" target="#b1">[2]</ref>. To this pretrained network, we append our instance segmentation subnetwork, and finetune with instance segmentation annotations and only the loss detailed in Sec. 3.4. For the semantic segmentation subnetwork, we train with an initial learning rate of 10 −8 , momentum of 0.9 and batch size of 20. The learning rate is low since we do not normalise the loss by the number of pixels. This is so that images with more pixels contribute a higher loss. The normalised learning rate is approximately 2 × 10 −3 . When training our instance segmentation network as well, we lower the learning rate to 10 −12 and use a batch size of 1 instead. Decreasing the batch size gave empirically better results. We also clipped gradients (a technique common in training RNNs <ref type="bibr" target="#b39">[40]</ref>) with ℓ 2 norms above 10 9 . This threshold was set by observing "normal" gradient magnitudes during training. The relatively high magnitude is due to the fact that our loss is not normalised. In our complete network, we have two CRF inference modules which are RNNs (one each in the semantic-and instance-segmentation subnetworks), and gradient clipping facilitated successful training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Discussion</head><p>Our network is able to compute a semantic and instance segmentation of the input image in a single forward pass. We do not require any post-processing, such as the patch aggregation of <ref type="bibr" target="#b36">[37]</ref>, "mask-voting" of <ref type="bibr" target="#b11">[12]</ref>, "superpixel projection" of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref> or spectral clustering of <ref type="bibr" target="#b32">[33]</ref>. The fact that we compute an initial semantic segmentation means that we have some robustness to errors in the object detector <ref type="figure">(Fig. 3</ref>). Furthermore, we are not necessarily limited by poorly localised object detections either <ref type="figure" target="#fig_1">(Fig. 4</ref>). Our CRF model allows us to reason about the entire image at a time, rather than consider independent object proposals, as done in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30]</ref>. Although we do not train our object detector jointly with the network, it also means that our segmentation network and object detector do not succumb to the same failure cases. Moreover, it ensures that our instance labelling does not "switch" often during training, which makes learning more stable. Finally, note that although we perform mean field inference of a CRF within our network, we do not optimise the CRF's likelihood, but rather a cross-entropy loss (Sec 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>Sections 4.1 to 4.6 describe our evaluation on the Pascal VOC Validation Set <ref type="bibr" target="#b13">[14]</ref> and the Semantic Boundaries Dataset (SBD) <ref type="bibr" target="#b17">[18]</ref> (which provides per-pixel annotations to 11355 previously unlaballed images from Pascal VOC). Section 4.7 details results on Cityscapes <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Details</head><p>We first train a network for semantic segmentation, therafter we finetune it to the task of instance segmentation, as described in Sec. 3.5. Our training data for the semantic segmentation pretraining consists of images from Pascal VOC <ref type="bibr" target="#b13">[14]</ref>, SBD <ref type="bibr" target="#b17">[18]</ref> and Microsoft COCO <ref type="bibr" target="#b34">[35]</ref>. Finally, when finetuning for instance segmentation, we use only training data from either the VOC dataset, or from the SBD dataset. We train separate models for evaluating on the VOC Validation Set, and the SBD Validation Set. In each case, we remove validation set images from the initial semantic segmentation pretraining set. We use the publicly available R-FCN object detection framework <ref type="bibr" target="#b12">[13]</ref>, and ensure that the images used to train the detector do not fall into our test sets for instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We report the mean Average Precision over regions (AP r ) as defined by <ref type="bibr" target="#b18">[19]</ref>. The difference between AP r and the AP metric used in object detection <ref type="bibr" target="#b13">[14]</ref> is that the Intersection over Union (IoU) is computed over predicted and ground-truth regions instead of bounding boxes. Furthermore, the standard AP metric uses an IoU threshold of 0.5 to determine whether a prediction is correct or not. Here, we use a variety of IoU thresholds since larger thresholds require more precise segmentations. Additionally, we report the AP r vol which is the average of the AP r for 9 IoU thresholds ranging from 0.1 to 0.9 in increments of 0.1.</p><p>However, we also observe that the AP r metric requires an algorithm to produce a ranked list of segments and their object class. It does not require, nor evaluate, the ability of an algorithm to produce a globally coherent segmentation map of the image, for example <ref type="figure">Fig. 1c</ref>. To measure this, we propose the "Matching IoU" which matches the predicted image and ground truth, and then calculates the corresponding IoU as defined in <ref type="bibr" target="#b13">[14]</ref>. This matching procedure is the same as described in Sec. 3.4. This measure was originally proposed in <ref type="bibr" target="#b53">[54]</ref>, but has not been used since in evaluating instance segmentation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect of Instance Potentials and End-to-End training</head><p>We first perform ablation studies on the VOC 2012 Validation set. This dataset, consisting of 1464 training and 1449 validation images has very high-quality annotations with detailed object delineations which makes it the most suited for evaluating pixel-level segmentations.</p><p>In Tab. 1, we examine the effect of each of our unary potentials in our Instance subnetwork on overall performance. Furthermore, we examine the effect of end-to-end training the entire network as opposed to piecewise training. Piecewise training refers to freezing the pretrained semantic segmentation subnetwork's weights and only optimising the instance segmentation subnetwork's parameters. Note that when training with only the "Box" (Eq. 3) unary potential and pairwise term, we also have to add in an additional "Background" detection which encompasses the entire image. Otherwise, we cannot classify the background label.</p><p>We can see that each unary potential improves overall instance segmentation results, both in terms of AP r vol and the Matching IoU. The "Global" term (Eq. 4) shows particular improvement over the "Box" term at the high AP r threshold of 0.9. This is because it can overcome errors in bounding box localisation ( <ref type="figure" target="#fig_1">Fig. 4)</ref> and leverage our semantic segmentation network's accurate predictions to produce precise labellings. The "Shape" term's improvement in the AP r vol is primarily due to an improvement in the AP r at low thresholds. By using shape priors, we are able to recover instances which were occluded and missed out. End-to-end training also improves results at all AP r thresholds. Training with just the "Box" term shows a modest improvement in the AP r vol of 1.3%. Training with the "Global" and "Shape" terms shows larger improvements of 2.1% and 2.3% respectively. This may be because the "Box" term only considers the semantic segmentation at parts of the image covered by object detections. Once we include the "Global" term, we consider the semantic segmentation over the entire image for the detected class. Training makes more efficient use of images, and error gradients are more stable in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on VOC Validation Set</head><p>We then compare our best instance segmentation model to recent methods on the VOC Validation Set in Tab. 2. The fact that our algorithm achieves the highest AP r at thresholds above 0.7 indicates that our method produces more detailed and accurate segmentations.</p><p>At an IoU threshold of 0.9, our improvement over the previous state-of-the-art (MPA <ref type="bibr" target="#b36">[37]</ref>) is 6.6%, which is a relative improvement of 36%. Unlike <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8]</ref>, our network performs an initial semantic segmentation which may explain our more accurate segmentations. Other segmentation-based approaches, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref> are not fully endto-end trained. We also achieve the best AP r vol of 57.5%. The relatively small difference in AP r vol to MPA <ref type="bibr" target="#b36">[37]</ref> despite large improvements at high IoU thresholds indicates that MPA performs better at low IoU thresholds. Proposalbased methods, such as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref> are more likely to perform better at low IoU thresholds since they output more proposals than actual instances in an image (SDS evaluates 2000 proposals per image). Furthermore, note that whilst MPA takes 8.7s to process an image <ref type="bibr" target="#b36">[37]</ref>, our method requires approximately 1.5s on the same Titan X GPU. More detailed qualitative and quantitative results, including success and failure cases, are included in the supplementary material.  <ref type="bibr" target="#b19">[20]</ref> 56.5 37.0 --IIS <ref type="bibr" target="#b29">[30]</ref> 60.1 38.7 --CFM <ref type="bibr" target="#b10">[11]</ref> 60.7 39.6 --Hypercolumn rescore <ref type="bibr" target="#b19">[20]</ref> 60.0 40.4 --MPA 3-scale rescore <ref type="bibr" target="#b36">[37]</ref> 61.8 -52.0 -MNC <ref type="bibr" target="#b11">[12]</ref> 63. <ref type="bibr" target="#b4">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on SBD Dataset</head><p>We also evaluate our model on the SBD dataset, which consists of 5623 training and 5732 validation images, as shown in Tab. 3. Following other works, we only report AP r results at IoU thresholds of 0.5 and 0.7. However, we provide more detailed results in our supplementary material. Once again, we show significant improvements over other work at high AP r thresholds. Here, our AP r at 0.7 improves by 1.5% over the previous state-of-the-art <ref type="bibr" target="#b29">[30]</ref>. Note that <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b19">20]</ref> perform additional post-processing where their results are rescored using an additional object detector. In contrast, our results are obtained by a single forward pass through our network. We have also improved substantially on the AP r vol measure (3.4%) compared to other works which have reported it. We also used the publicly available source code 1 , model and default parameters of MNC <ref type="bibr" target="#b11">[12]</ref> to evaluate the "Matching IoU". Our method improves this by 8.3%. This metric is a stricter measure of segmentation performance, and our method, which is based on an initial semantic segmentation and includes a CRF as part of training therefore performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Improvement in Semantic Segmentation</head><p>Finetuning our network for instance segmentation, with the loss described in Sec. 3.4 improves semantic segmentation performance on both the VOC and SBD dataset, as shown in Tab. 4. The improvement is 0.9% on VOC, and 1% on SBD. The tasks of instance segmentation and semantic segmentation are highly related -in fact, instance segmentation can be thought of as a more specific case of semantic segmentation. As a result, finetuning for one task improves the other. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Results on Cityscapes</head><p>Finally, we evaluate our algorithm on the Cityscapes road-scene understanding dataset <ref type="bibr" target="#b8">[9]</ref>. This dataset consists of 2975 training images, and the held-out test set consisting of 1525 images are evaluated on an online server. None of the 500 validation images were used for training. We use an initial semantic segmentation subnetwork that is based on the ResNet-101 architecture <ref type="bibr" target="#b58">[59]</ref>, and all of the instance unary potentials described in Sec. 3.2.</p><p>As shown in Tab. 5, our method sets a new state-of-theart on Cityscapes, surpassing concurrent work <ref type="bibr" target="#b20">[21]</ref> and the best previous published work <ref type="bibr" target="#b48">[49]</ref> by significant margins. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We have presented an end-to-end instance segmentation approach that produces intermediate semantic segmentations, and shown that finetuning for instance segmentation improves our network's semantic segmentations. Our approach differs from other methods which derive their architectures from object detection networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b19">20]</ref> in that our approach is more similar to a semantic segmentation network. As a result, our system produces more accurate and detailed segmentations as shown by our substantial improvements at high AP r thresholds. Moreover, our system produces segmentation maps naturally, and in contrast to other published work, does not require any post-processing. Finally, our network produces a variable number of outputs, depending on the number of instances in the image. Our future work is to incorporate an object detector into the endto-end training of our system to create a network that performs semantic segmentation, object detection and instance segmentation jointly. Possible techniques for doing this are suggested by <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b38">[39]</ref>. <ref type="figure">Figures 7 and 8</ref> show success and failure cases of our algorithm. <ref type="figure">Figure 9</ref> compares the results of our algorithm to the publicly available model for MNC <ref type="bibr" target="#b11">[12]</ref>. <ref type="figure">Figure 10</ref> compares our results to those of FCIS <ref type="bibr" target="#b30">[31]</ref>, concurrent work which won the COCO 2016 challenge. <ref type="figure" target="#fig_0">Figure 11</ref> presents some qualitative results on the Cityscapes dataset.</p><p>Section A shows more detailed results on the VOC dataset. <ref type="figure">Figure 12</ref> shows a visualisation of our results at different AP r thresholds, and <ref type="table" target="#tab_8">Tables 7 to 9</ref> show per-class AP r results at thresholds of 0.5, 0.7 and 0.9.</p><p>Section B shows more detailed results on the SBD dataset. <ref type="table" target="#tab_7">Table 6</ref> shows our mean AP r results at thresholds from 0.5 to 0.9, whilst <ref type="table" target="#tab_1">Tables 10 and 11</ref> show per-class AP r results at thresholds of 0.7 and 0.5 respectively. <ref type="figure">Figure 7</ref>: Success cases of our method. First and second row: Our algorithm can leverage good initial semantic segmentations, and detections, to produce an instance segmentation. Third row: Notice that we have ignored three false-positive detections. Additionally, the red bounding box does not completely encompass the person, but our algorithm is still able to associate pixels "outside-the-box" with the correct detection (also applies to row 2). Fourth row: Our system is able to deal with the heavily occluded sheep, and ignore the false-positive detection. Fifth row: We have not been able to identify one bicycle on the left since it was not detected, but otherwise have performed well. Sixth row: Although subjective, the train has not been annotated in the dataset, but both our initial semantic segmentation and object detection networks have identified it. Note that the first three images are from the VOC dataset, and the last three from SBD. Annotations in the VOC dataset are more detailed, and also make more use of the grey "ignore" label to indicate uncertain areas in the image. The first column shows the input image, and the results of our object detector which are another input to our network. Best viewed in colour. <ref type="figure">Figure 8</ref>: Failure cases of our method. First row: Both our initial detector, and semantic segmentation system did not identify a car in the background. Additionally, the "brown" person prediction actually consists of two people that have been merged together. This is because the detector did not find the background person. Second row: Our initial semantic segmentation identified the table, but it is not there in the Instance Segmentation. This is because there was no "table detection" to associate these pixels with. Using heuristics, we could propose additional detections in cases like these. However, we have not done this in our work. Third row: A difficult case where we have segmented most of the people. However, sometimes two people instances are joined together as one person instance. This problem is because we do not have a detection for each person in the image. Fourth row: Due to our initial semantic segmentation, we have not been able to segment the green person and table correctly. Fifth row: We have failed to segment a bird although it was detected. Sixth row: The occluding cows, which all appear similar, pose a challenge, even with our shape priors. The first column shows the input image, and the results of our object detector which are another input to our network. Best viewed in colour.</p><p>MNC <ref type="bibr" target="#b11">[12]</ref> Ours Ground truth <ref type="figure">Figure 9</ref>: Comparison to MNC <ref type="bibr" target="#b11">[12]</ref> The above examples emphasise the advantages in our method over MNC <ref type="bibr" target="#b11">[12]</ref>. Unlike proposal-based approaches such as MNC, our method can handle false-positive detections, poor bounding box localisation, reasons globally about the image and also produces more precise segmentations due to the initial semantic segmentation module which includes a differentiable CRF. Row 1 shows a case where MNC, which scores segment-based proposals, is fooled by a false-positive detection and segments an imaginary human (yellow segment). Our method is robust to falsepositive detections due to the initial semantic segmentation module which does not have the same failure modes as the detector. Rows 2, 3 and 4 show how MNC <ref type="bibr" target="#b11">[12]</ref> cannot deal with poorly localised bounding boxes. The horizontal boundaries of the red person in Row 2, and light-blue person in Row 4 correspond to the limits of the proposal processed by MNC. Our method, in contrast, can segment "outside the detection bounding box" due to the global instance unary potential (Eq. 4). As MNC does not reason globally about the image, it cannot handle cases of overlapping bounding boxes well, and produces more instances than there actually are. The first column shows the input image, and the results of our object detector which are another input to our network. MNC does not use these detections, but does internally produce box-based proposals which are not shown. Best viewed in colour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>MNC <ref type="bibr" target="#b11">[12]</ref> Ours Ground truth <ref type="figure">Figure 9</ref> continued: Comparison to MNC <ref type="bibr" target="#b11">[12]</ref> The above examples show that our method produces more precise segmentations than MNC, that adhere to the boundaries of the objects. However, in Rows 3, 4 and 5, we see that MNC is able to segment instances that our method misses out. In Row 3, our algorithm does not segment the baby, although there is a detection for it. This suggests that our shape prior which was formulated to overcome such occlusions could be better. As MNC processes individual instances, it does not have a problem with dealing with small, occluding instances. In Row 4, MNC has again identified a person that our algorithm could not. However, this is because we did not have a detection for this person. In Row 5, MNC has segmented the horses on the right better than our method. The first column shows the input image, and the results of our object detector which are another input to our network. MNC does not use these detections, but does internally produce box-based proposals which are not shown. We used the publicly available code, models and default parameters of MNC to produce this figure. Best viewed in colour. <ref type="figure">Figure 10</ref>: Comparison to FCIS <ref type="bibr" target="#b30">[31]</ref> The above images compare our method to the concurrent work, FCIS <ref type="bibr" target="#b30">[31]</ref>, which was trained on COCO <ref type="bibr" target="#b34">[35]</ref> and won the COCO 2016 challenge. Unlike proposal-based methods such as FCIS, our method can handle false-positive detections and poor bounding-box localisation. Furthermore, as our method reasons globally about the image, one pixel can only be assigned to a single instance, which is not the case with FCIS. Our method also produces more precise segmentations, as it includes a differentiable CRF, and it is based off a semantic segmentation network. The results of FCIS are obtained from their publicly available results on the COCO test set (https://github.com/daijifeng001/ TA-FCN). Note that FCIS is trained on COCO, and our model is trained on Pascal VOC which does not have as many classes as COCO, such as "umbrella" and "suitcase" among others. As a result, we are not able to detect these objects. The first column shows the input image, and the results of our object detector which are another input to our network. FCIS does not use these detections, but does internally produce proposals which are not shown. Best viewed in colour. A. Detailed results on the VOC dataset <ref type="figure">Figure 12</ref> shows a visualisation of the AP r obtained by our method for each class across nine different thresholds. Each "column" of <ref type="figure">Fig. 12</ref> corresponds to the AP r for each class at a given IoU threshold. It is therefore an alternate representation for the results tables (Tables 7 to 9). We can see that our method struggles with classes such as "bicycle", "chair", "dining table" and "potted plant". This may be explained by the fact that current semantic segmentation systems (including ours) struggle with these classes. All recent methods on the Pascal VOC leaderboard 2 obtain an IoU for these classes which is lower than the mean IoU for all classes. In fact the semantic segmentation IoU for the "chair" class is less than half of the mean IoU for all the classes for 16 out of the 20 most recent submissions on the VOC leaderboard at the time of writing. <ref type="table" target="#tab_8">Tables 7 to 9</ref> show per-class instance segmentation results on the VOC dataset, at IoU thresholds of 0.9, 0.7 and 0.5 respectively. At an IoU threshold of 0.9, our method achieves the highest AP r for 16 of the 20 object classes. At the threshold of 0.7, we achieve the highest AP r in 15 classes. Finally, at an IoU threshold of 0.5, our method, MPA 3-scale <ref type="bibr" target="#b36">[37]</ref> and PFN <ref type="bibr" target="#b32">[33]</ref> each achieve the highest AP r for 6 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed results on the SBD dataset</head><p>Once again, we show a visualisation of the AP r obtained by our method for each class across nine different thresholds <ref type="figure">(Fig. 13</ref>). The trend is quite similar to the VOC dataset in that our algorithm struggles on the same object classes ("chair", "dining table", "potted plant", "bottle"). Note that our AP r for the "bicycle" class has improved compared to the VOC dataset. This is probably because the VOC dataset has more detailed annotations. In the VOC dataset, each spoke of a bicycle's wheel is often labelled, whilst in SBD, the entire wheel is labelled as a single circle with the "bicycle" label. Therefore, the SBD dataset's coarser labelling makes it easier for an algorithm to perform well on objects with fine details. <ref type="table" target="#tab_7">Table 6</ref> shows our mean AP r over all classes at thresholds ranging from 0.5 to 0.9. Our AP r at 0.9 is low compared to the result which we obtained on the VOC dataset. This could be for a number of reasons: As the SBD dataset is not as finely annotated as the VOC dataset, it might not be suited for measuring the AP r at such high thresholds. Additionally, the training data is not as good for training our system which includes a CRF and is therefore able to delineate sharp boundaries. Finally, as the SBD dataset has 5732 validation images (compared to the 1449 in VOC), it leaves less data for pretraining our initial semantic segmen-2 http://host.robots.ox.ac.uk:8080/leaderboard/ displaylb.php?challengeid=11&amp;compid=6 tation module. This may hinder our network in being able to produce precise segmentations.  <ref type="table" target="#tab_1">Tables 10 and 11</ref> show per-class instance segmentation results on the SBD dataset, at IoU thresholds of 0.7 and 0.5 respectively. We can only compare results at these two thresholds since these are the only thresholds which other work has reported. <ref type="figure">Figure 12</ref>: A visualisation of the AP r obtained for each of the 20 classes on the VOC dataset, at nine different IoU thresholds. The x-axis represents the IoU threshold, and the y-axis each of the Pascal classes. Therefore, each "column" of this figure corresponds to the AP r per class at a particular threshold, and is thus an alternate representation to the results tables. Best viewed in colour. <ref type="figure">Figure 13</ref>: A visualisation of the AP r obtained for each of the 20 classes on the SBD dataset, at nine different IoU thresholds. The x-axis represents the IoU threshold, and the y-axis each of the Pascal classes. Therefore, each "column" of this figure corresponds to the AP r per class at a particular threshold, and is thus an alternate representation to the results tables. Best viewed in colour.           </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 arXivFigure 1 :</head><label>11</label><figDesc>Object detection (a) localises the different people, but at a coarse, bounding-box level. Semantic segmentation (b) labels every pixel, but has no notion of instances. Instance segmentation (c) labels each pixel of each person uniquely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The "Global" unary potential (b) is particularly effective in cases where the input detection bounding box does not cover the entire extent of the object. Methods which are based on refining bounding-box detections such as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 11 :</head><label>11</label><figDesc>Sample results on the Cityscapes dataset The above images show how our method can handle the large numbers of instances present in the Cityscapes dataset. Unlike other recent approaches, our algorithm can deal with objects that are not continuous -such as the car in the first row which is occluded by a pole. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>[</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The effect of the different CRF unary potentials, and end-to-end training with them, on the VOC 2012 Validation Set.</figDesc><table><row><cell></cell><cell>0.5</cell><cell>AP r 0.7</cell><cell>0.9</cell><cell>AP r vol</cell><cell>match IoU</cell></row><row><cell>Box Term (piecewise) Box+Global (piecewise) Box+Global+Shape (piecewise)</cell><cell cols="3">60.0 47.3 21.2 59.1 46.1 23.4 59.5 46.4 23.3</cell><cell>54.9 54.6 55.2</cell><cell>42.6 43.0 44.8</cell></row><row><cell>Box Term (end-to-end) Box+Global (end-to-end) Box+Global+Shape (end-to-end)</cell><cell cols="3">60.7 47.4 24.6 60.9 48.1 25.5 61.7 48.6 25.1</cell><cell>56.2 56.7 57.5</cell><cell>46.9 47.1 48.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Instance Segmentation performance to recent methods on the VOC 2012 Validation Set</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell>0.6</cell><cell>AP r 0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>AP r vol</cell></row><row><cell cols="6">SDS [19] Chen et al. [8] PFN [33] Arnab et al. [3] MPA 1-scale [37] 60.3 54.6 45.9 34.3 17.3 43.8 34.5 21.3 8.7 0.9 46.3 38.2 27.0 13.5 2.6 58.7 51.3 42.5 31.2 15.7 58.3 52.4 45.4 34.9 20.1 MPA 3-scale [37] 62.1 56.6 47.4 36.1 18.5 Ours 61.7 55.5 48.6 39.5 25.1</cell><cell>--52.3 53.1 54.5 56.5 57.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Instance Segmentation performance on the SBD Dataset</figDesc><table><row><cell>Method</cell><cell>AP r 0.5 0.7</cell><cell>AP r vol</cell><cell>match IoU</cell></row><row><cell>SDS [19] MPA 1-scale [37] Hypercolumn</cell><cell>49.7 25.3 55.5 -</cell><cell>41.4 48.3</cell><cell>--</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Semantic Segmentation performance before and after finetuning for Instance Segmentation on the VOC and SBD Validation Sets</figDesc><table><row><cell>Dataset</cell><cell>Mean IoU [%] before Instance finetuning</cell><cell>Mean IoU [%] after Instance finetuning</cell></row><row><cell>VOC SBD</cell><cell>74.2 71.5</cell><cell>75.1 72.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results on Cityscapes Test Set. Evaluation metrics and results of competing methods obtained from the online server. The "AP" metric of Cityscapes is similar to our AP r vol metric.</figDesc><table><row><cell>Method</cell><cell cols="4">AP AP at 0.5 AP 100m AP 50m</cell></row><row><cell cols="2">Ours SAIS [21] DWT [4] InstanceCut [24] Graph Decomp. [29] 9.8 20.0 17.4 15.6 13.0 RecAttend [43] 9.5 Pixel Encoding [49] 8.9 R-CNN [9] 4.6</cell><cell>38.8 36.7 30.0 27.9 23.2 18.9 21.1 12.9</cell><cell>32.6 29.3 26.2 22.1 16.8 16.8 15.3 7.7</cell><cell>37.6 34.0 31.8 26.1 20.3 20.9 16.7 10.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of Instance Segmentation performance at multiple AP r thesholds on the VOC 2012 Validation Set</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell>0.6</cell><cell>AP r 0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>AP r vol</cell></row><row><cell cols="6">Ours (piecewise) Ours (end-to-end ) 62.0 54.0 44.8 32.3 13.8 59.1 51.9 42.1 29.4 12.0</cell><cell>52.3 55.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison of mean AP r , achieved by different published methods, at an IoU threshold of 0.9, for all twenty classes in the VOC dataset.</figDesc><table><row><cell>tv</cell><cell></cell></row><row><cell>train</cell><cell></cell></row><row><cell>sofa</cell><cell></cell></row><row><cell>plant sheep</cell><cell></cell></row><row><cell>per-son</cell><cell></cell></row><row><cell>horse mbike</cell><cell></cell></row><row><cell>dog</cell><cell></cell></row><row><cell>table</cell><cell></cell></row><row><cell>cow</cell><cell></cell></row><row><cell>chair</cell><cell></cell></row><row><cell>cat</cell><cell></cell></row><row><cell>car</cell><cell></cell></row><row><cell>bus</cell><cell></cell></row><row><cell>bot-tle</cell><cell></cell></row><row><cell>boat</cell><cell>14.4</cell></row><row><cell>bird</cell><cell>36.8</cell></row><row><cell>bike</cell><cell>0.03</cell></row><row><cell>aero-plane</cell><cell>56.6</cell></row><row><cell>Mean (%) AP r</cell><cell>25.1</cell></row><row><cell>Method</cell><cell>Our method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison of mean AP r , achieved by different published methods, at an IoU threshold of 0.7, for all twenty classes in the VOC dataset.</figDesc><table><row><cell>tv</cell><cell></cell></row><row><cell>train</cell><cell></cell></row><row><cell>sofa</cell><cell></cell></row><row><cell>plant sheep</cell><cell></cell></row><row><cell>per-son</cell><cell></cell></row><row><cell>horse mbike</cell><cell></cell></row><row><cell>dog</cell><cell></cell></row><row><cell>table</cell><cell></cell></row><row><cell>cow</cell><cell>56.3</cell></row><row><cell>chair</cell><cell>11.2</cell></row><row><cell>cat</cell><cell>72.1</cell></row><row><cell>car</cell><cell>38.7</cell></row><row><cell>bus</cell><cell>61.1</cell></row><row><cell>bot-tle</cell><cell>25.2</cell></row><row><cell>boat</cell><cell>45.1</cell></row><row><cell>bird</cell><cell>68.2</cell></row><row><cell>bike</cell><cell>1.4</cell></row><row><cell>aero-plane</cell><cell>69.6</cell></row><row><cell>Mean (%) AP r</cell><cell>48.6</cell></row><row><cell>Method</cell><cell>Our method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparison of mean AP r , achieved by different published methods, at an IoU threshold of 0.5, for all twenty classes in the VOC dataset.</figDesc><table><row><cell>tv</cell></row><row><cell>train</cell></row><row><cell>sofa</cell></row><row><cell>plant sheep</cell></row><row><cell>per-son</cell></row><row><cell>horse mbike</cell></row><row><cell>dog</cell></row><row><cell>table</cell></row><row><cell>cow</cell></row><row><cell>chair</cell></row><row><cell>cat</cell></row><row><cell>car</cell></row><row><cell>bus</cell></row><row><cell>bot-tle</cell></row><row><cell>boat</cell></row><row><cell>bird</cell></row><row><cell>bike</cell></row><row><cell>aero-plane</cell></row><row><cell>AP r (%) Mean</cell></row><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Comparison of mean AP r , achieved by different published methods, at an IoU threshold of 0.7, for all twenty classes in the SBD dataset.</figDesc><table><row><cell>tv</cell><cell>58.2</cell><cell>53.9</cell><cell>50.8</cell></row><row><cell>train</cell><cell>69.7</cell><cell>62.2</cell><cell>58.8</cell></row><row><cell>sofa</cell><cell>36.5</cell><cell>23.7</cell><cell>19.9</cell></row><row><cell>plant sheep</cell><cell>14.8 47.6</cell><cell>19.0 51.0</cell><cell>12.0 40.9</cell></row><row><cell>per-son</cell><cell>40.3</cell><cell>33.0</cell><cell>32.5</cell></row><row><cell>horse mbike</cell><cell>48.4 46.3</cell><cell>53.0 46.8</cell><cell>39.6 41.9</cell></row><row><cell>dog</cell><cell>71.6</cell><cell>64.0</cell><cell>60.4</cell></row><row><cell>table</cell><cell>16.9</cell><cell>13.6</cell><cell>10.4</cell></row><row><cell>cow</cell><cell>46.8</cell><cell>48.4</cell><cell>37.6</cell></row><row><cell>chair</cell><cell>9.6</cell><cell>10.9</cell><cell>7.2</cell></row><row><cell>cat</cell><cell>74.7</cell><cell>66.8</cell><cell>65.4</cell></row><row><cell>car</cell><cell>46.0</cell><cell>48.7</cell><cell>44.8</cell></row><row><cell>bus</cell><cell>70.3</cell><cell>74.0</cell><cell>72.4</cell></row><row><cell>bot-tle</cell><cell>22.4</cell><cell>29.6</cell><cell>22.7</cell></row><row><cell>boat</cell><cell>26.4</cell><cell>26.4</cell><cell>22.0</cell></row><row><cell>bird</cell><cell>52.7</cell><cell>44.4</cell><cell>42.0</cell></row><row><cell>bike</cell><cell>27.4</cell><cell>35.1</cell><cell>31.5</cell></row><row><cell>aero-plane</cell><cell>69.0</cell><cell>61.9</cell><cell>61.8</cell></row><row><cell>Mean (%) AP r</cell><cell>44.8</cell><cell>43.3</cell><cell>38.7</cell></row><row><cell>Method</cell><cell cols="3">Our method IIS sp, rescore [30] IIS raw [30]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Comparison of mean AP r , achieved by different published methods, at an IoU threshold of 0.5, for all twenty classes in the SBD dataset.</figDesc><table><row><cell>tv</cell><cell></cell></row><row><cell>train</cell><cell></cell></row><row><cell>sofa</cell><cell></cell></row><row><cell>plant sheep</cell><cell></cell></row><row><cell>per-son</cell><cell></cell></row><row><cell>horse mbike</cell><cell></cell></row><row><cell>dog</cell><cell></cell></row><row><cell>table</cell><cell></cell></row><row><cell>cow</cell><cell></cell></row><row><cell>chair</cell><cell></cell></row><row><cell>cat</cell><cell></cell></row><row><cell>car</cell><cell></cell></row><row><cell>bus</cell><cell></cell></row><row><cell>bot-tle</cell><cell></cell></row><row><cell>boat</cell><cell></cell></row><row><cell>bird</cell><cell></cell></row><row><cell>bike</cell><cell></cell></row><row><cell>aero-plane</cell><cell></cell></row><row><cell>AP r (%) Mean</cell><cell></cell></row><row><cell>Method</cell><cell>Our method</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/daijifeng001/MNC</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Bernardino Romera-Paredes and Stuart Golodetz for insightful discussions and feedback. This work was supported by the EPSRC, Clarendon Fund, ERC grant ERC-2012-AdG 321162-HELIOS, EPRSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this supplementary material, we include more detailed qualitative and quantitative results on the VOC and SBD datasets. Furthermore, we also show the runtime of our algorithm.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up instance segmentation with deep higher order crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1611.08303</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-instance object segmentation with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="page" from="3470" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>ECCV. 2014. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1920" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Shape-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1612.03129</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Exemplar-based CRF for Multiinstance Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Instancecut: from edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08272</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What, where and how many? combining object detectors and crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Joint graph decomposition and node labeling by local search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04399</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iterative Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reversible recursive instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast directional chamfer matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1696" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-scale patch aggregation (mpa) for simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crossstitch Networks for Multi-task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">End-to-end instance segmentation and counting with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Contour-based learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Relating things and stuff via object property interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1370" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scalpel: Segmentation cascades with localized priors and efficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2035" to="2042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The layout consistent random field for recognizing and segmenting partially occluded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A dynamic conditional random field model for joint labeling of object and scene classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="733" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Layered object models for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2614" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arnab</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
