<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAMNet: Multi-path Adaptive Modulation Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Hyuk</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<addrLine>85 Songdogwahak-ro, Yeonsu-gu</addrLine>
									<settlement>Incheon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ho</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<addrLine>85 Songdogwahak-ro, Yeonsu-gu</addrLine>
									<settlement>Incheon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manri</forename><surname>Cheon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<addrLine>85 Songdogwahak-ro, Yeonsu-gu</addrLine>
									<settlement>Incheon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Seok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<addrLine>85 Songdogwahak-ro, Yeonsu-gu</addrLine>
									<settlement>Incheon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MAMNet: Multi-path Adaptive Modulation Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Single image super-resolution</term>
					<term>feature modulation</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, single image super-resolution (SR) methods based on deep convolutional neural networks (CNNs) have made significant progress. However, due to the non-adaptive nature of the convolution operation, they cannot adapt to various characteristics of images, which limits their representational capability and, consequently, results in unnecessarily large model sizes. To address this issue, we propose a novel multi-path adaptive modulation network (MAMNet). Specifically, we propose a multi-path adaptive modulation block (MAMB), which is a lightweight yet effective residual block that adaptively modulates residual feature responses by fully exploiting their information via three paths. The three paths model three types of information suitable for SR: 1) channel-specific information (CSI) using global variance pooling, 2) inter-channel dependencies (ICD) based on the CSI, 3) and channel-specific spatial dependencies (CSD) via depth-wise convolution. We demonstrate that the proposed MAMB is effective and parameter-efficient for image SR than other feature modulation methods. In addition, experimental results show that our MAMNet outperforms most of the state-of-the-art methods with a relatively small number of parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SR) is the process of inferring a high-resolution (HR) image from a single low-resolution (LR) image. It is one of the computer vision problems progressing rapidly with the development of deep learning. Recently, convolutional neural network (CNN)-based SR methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> have shown better performance compared with previous hand-crafted methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Stacking an extensive amount of layers is a common practice to improve performance of deep networks <ref type="bibr" target="#b14">[15]</ref>. After Kim et al. <ref type="bibr" target="#b1">[2]</ref> first applying residual learning in their very deep CNN for SR (VDSR), this trend goes on for SR as well. Ledig et al. <ref type="bibr" target="#b2">[3]</ref> propose a deeper network (SRResNet) than VDSR based on the ResNet architecture. Lim et al. <ref type="bibr" target="#b3">[4]</ref> modify SRResNet and propose two very large networks having superior performance: wider one and deeper one, i.e., enhanced deep ResNet for SR (EDSR) and multi-scale deep SR (MDSR), respectively. In addition, there have been approaches adopting DenseNet <ref type="bibr" target="#b15">[16]</ref> for SR, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>While a huge size of CNN-based SR network tends to yield improved performance, it still has some limitations due to its non-adaptive nature, i.e., convolution is performed with fixed weights regardless of the input. First, most CNN-based methods internally treat all types of LR images equally, which may not effectively distinguish the detailed characteristics of the content (e.g., natural vs. computer-generated ones). Second, all regions are considered equally within an LR image, which may not effectively distinguish the detailed characteristics of each region (e.g., low vs. high frequency). These limitations restrict the representational capability of SR networks, which leads to inefficient parameter usage, i.e., excessively large model sizes. Therefore, designing flexible networks for various situations is required for effective and efficient SR.</p><p>A few recent SR methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> attempt to address this issue. They design adaptive SR networks by modulating convolutional feature responses utilizing their information. Zhang et al. <ref type="bibr" target="#b8">[9]</ref> propose the residual channel attention block (RCAB) that modulates channel-wise feature responses by exploiting inter-channel dependencies. Hu et al. <ref type="bibr" target="#b16">[17]</ref> propose the channelwise and spatial attention residual (CSAR) block to adaptively modulate feature responses by explicitly modelling channelwise and spatial interdependencies. However, these methods do not make full use of information from feature responses for imposing sufficient adaptability on SR networks. Specifically, although the CSAR block exploits two types of interdependencies between feature responses, channel-specific information is not exploited for feature modulation.</p><p>Furthermore, from a network design perspective, these methods do not fully take into account the characteristics of SR, which differ from those of high-level vision problems such as image classification. First, both RCAB and the CSAR block use the squeeze-and-excitation (SE) block <ref type="bibr" target="#b17">[18]</ref> for modelling the inter-channel dependencies, which uses global average pooling to extract channel-wise statistics. However, since image SR ultimately aims at restoring high-frequency components of images, it is more reasonable to exploit frequency-related information as statistics representing channels. Second, the CSAR block models spatial interdependencies without considering channel-specific characteristics, i.e., the same spatial modulation is performed for all channels. While such a feature modulation strategy is effective when certain spatial regions of an image are more important than others (for, e.g., image classification <ref type="bibr" target="#b18">[19]</ref>), it is not suitable for image SR where all areas of images have similar importance.</p><p>To address these issues, we propose a novel multi-path adaptive modulation network (MAMNet), whose overall architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, we design a novel multi-path adaptive modulation block (MAMB) ( <ref type="figure" target="#fig_1">Figure 2</ref>), a lightweight yet effective residual block, which adaptively modulates residual feature responses by fully exploiting their information via three paths in a SR-optimized manner. The three paths correspond to the three different types of information, i.e., channel-specific information (CSI), inter-channel dependencies (ICD), and channel-specific spatial dependencies (CSD). For modelling CSI, we extract a statistic representing each channel by performing global variance pooling that can reflect frequency-related information, which is a more reasonable approach to SR compared to global average pooling. To the best of our knowledge, this concept has not been adopted in existing image SR methods. Based on the extracted channel-wise variances, we model ICD using two fully-connected layers. Lastly, for modelling CSD, we generate a spatial modulation map for each channel via a depth-wise convolution layer. Compared to the previous methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref>, our method is effective for SR in that it models spatial dependencies with preserving the characteristics inherent to each channel.</p><p>In summary, our main contributions are as follows:</p><p>• We propose a novel multi-path adaptive modulation network (MAMNet) for effective and parameter-efficient image SR. The proposed MAMNet resolves the nonadaptivity inherent in most of the previous CNN-based SR networks, by adaptively modulating convolutional feature responses.</p><p>• As the key component of our MAMNet, we propose a multi-path adaptive modulation block (MAMB) to fully exploit information of the feature responses for their modulation. The information exploitation proceeds via three paths corresponding to the three types of information, i.e., channel-specific information (CSI), inter-channel dependencies (ICD), and channel-specific spatial dependencies (CSD).</p><p>• We model the three types of information in a SR-optimized manner. First, we extract CSI by performing a global variance pooling that reflects frequency-related information.</p><p>In addition, we model CSD via a depth-wise convolution, which not only exploits spatial dependencies but also preserves channel-specific characteristics.</p><p>The rest of this paper is organized as follows. Section 2 reviews the deep CNN-based image SR methods and attention mechanisms in CNNs. Section 3 introduces our proposed MAMNet in detail. We discuss the differences between relevant studies and the proposed method in Section 4. Section 5 analyzes the proposed method in detail and provides performance comparisons with other methods experimentally. Finally, we conclude our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Many CNN-based networks have been proposed to improve the performance of image SR <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. As mentioned in Section 1, they have evolved toward deepening networks. We first review deep CNN-based SR networks developed in previous studies.</p><p>Our proposed method is related to the attention mechanism <ref type="bibr" target="#b17">[18]</ref>, which is one of the notable network structures to recalibrate the feature responses so that more adaptive and efficient training is possible. We briefly review the methods in which attention mechanisms are applied to CNNs.</p><p>Deep CNN-based image SR. After Lim et al. <ref type="bibr" target="#b3">[4]</ref> proposing huge ResNet-based SR models, i.e., EDSR and MDSR, and Tong et al. <ref type="bibr" target="#b4">[5]</ref> adopting the DenseNet structure for image SR, i.e., SRDenseNet, there have been some approaches to further improve the performance of image SR <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Zhang et al. <ref type="bibr" target="#b6">[7]</ref> propose a residual dense network (RDN) to fully exploit hierarchical features from LR images. RDN consists of stacked residual dense blocks (RDBs), which combine the ResNet and DenseNet structures. Haris et al. <ref type="bibr" target="#b5">[6]</ref> propose deep back-projection networks (DBPNs), which exploit the mutual dependencies of LR and HR images. Inspired by <ref type="bibr" target="#b19">[20]</ref>, while most recent models use the post up-sampling approach, DBPNs consist of iterative up-and down-sampling layers to explicitly model an error feedback mechanism. Inspired by the inception block <ref type="bibr" target="#b20">[21]</ref>, Li et al. <ref type="bibr" target="#b7">[8]</ref> propose the multi-scale residual network (MSRN), which employs different convolution kernels with different sizes in its basic building block. The aforementioned deep CNN-based SR methods have achieved good performance through various network structures. However, they treat all types of information equally and are not adaptable to various situations.</p><p>Attention mechanism. In the cognitive process of human, the use of selective information, i.e., focusing on more important information, generally occurs <ref type="bibr" target="#b21">[22]</ref>. This process is referred to as the attention mechanism, which is widely used in various applications <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Recently, there are some approaches to apply attention mechanisms to ResNet-based networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17]</ref>. Wang et al. <ref type="bibr" target="#b24">[25]</ref> propose a residual attention network (RAN) to improve the performance of image classification. Since their attention module generates 3D attention maps using 3D feature maps directly, it is helpful to improve performance, but is relatively heavy. Hu et al. <ref type="bibr" target="#b17">[18]</ref> introduce a compact attention mechanism, i.e., the SE block, which adaptively recalibrates 3D feature maps by explicitly modelling inter-channel dependencies. The SE block generates 1D channel attention maps using only 1D global average pooled features. While the attention mechanism in the SE block uses only inter-channel relation for refining feature maps, the convolutional block attention module (CBAM) <ref type="bibr" target="#b18">[19]</ref> exploits both inter-channel and spatial relationships of feature maps through its channel and spatial attention modules, respectively, which are performed sequentially. The channel attention module in CBAM is different from the SE block in that global max pooling is additionally performed to extract multiple channel statistics.</p><p>As mentioned in Section 1, while Zhang et al. <ref type="bibr" target="#b8">[9]</ref> and Hu et al. <ref type="bibr" target="#b16">[17]</ref> try to apply the attention mechanism to image SR, they do not fully exploit feature responses and different characteristics between high-and low-level computer vision problems are not adequately considered. Differences between these methods and ours are explained in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>The overall architecture of our MAMNet is illustrated in <ref type="figure">Figure</ref> 1. It can be divided into two parts: 1) feature extraction part, and 2) upscaling part. Let I LR ∈ R H×W×3 and I S R denote the input LR image and the corresponding output image, respectively. At the beginning, one convolution layer is applied to I LR to extract initial feature maps, i.e.,</p><formula xml:id="formula_0">F 0 = f 0 (I LR ),<label>(1)</label></formula><p>where f 0 (⋅) denotes the first convolution and F 0 means the extracted feature maps to be fed into the first MAMB, which is described in detail in Section 3.2. F 0 is updated through R MAMBs and one convolution layer. Then, the updated feature maps are added to F 0 by using the global skip connection:</p><formula xml:id="formula_1">F f eat = F 0 + f f eat (F R ),<label>(2)</label></formula><p>where F R is the output feature maps of the R-th MAMB and f f eat (⋅) and F f eat are the last convolution layer and feature maps of the feature extraction part, respectively. For the upscaling part, we use the sub-pixel convolution layers <ref type="bibr" target="#b25">[26]</ref>, which are followed by one convolution layer for reconstruction:</p><formula xml:id="formula_2">I S R = f recon ( f up (F f eat )),<label>(3)</label></formula><p>where f up (⋅) and f recon (⋅) are the functions for upscaling and reconstruction, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-path Adaptive Modulation Block</head><p>The structure of MAMB is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Let F r−1 and F r be the input and output feature maps of the r-th MAMB. Then, F r can be formulated as where f MAMB,r (⋅) denotes the operations of the r-th MAMB, X r is the resultant feature maps having spatial dimensions H × W and a channel dimension C after sequentially applying convolution, ReLU, and convolution on F r−1 , and f MAM (⋅) means our multi-path adaptive modulation (MAM) that simultaneously exploits the three types of information, i.e., CSI, ICD, and CSD. The feature modulation is performed as follows:</p><formula xml:id="formula_3">F r = f MAMB,r (F r−1 ) = F r−1 + f MAM (X r ),<label>(4)</label></formula><formula xml:id="formula_4">X r = f MAM (X r ) = σ(M CS I r ⊕ M ICD r ⊕ M CS D r ) ⊗ X r ,<label>(5)</label></formula><p>where M CS I r , M ICD r , and M CS D r are the modulation maps using CSI, ICD, and CSD, respectively, ⊕ and ⊗ denote element-wise addition and multiplication, respectively, σ(⋅) is the sigmoid activation function, andX r is the modulated feature maps. In order to enable the element-wise addition, M CS I r and M ICD r are resized to the size of M CS D r via nearest-neighbor interpolation. Channel-specific information (CSI). Each channel of X r is the responses to a particular filter, which tend to vary depending on the characteristics of LR images. Therefore, we utilize the CSI to adaptively modulate each channel. It is important to extract a statistic that can effectively represent the characteristics of each channel. Since image SR ultimately aims at restoring high-frequency components of images, we choose to use the variance, a frequency-related measure, for modelling the CSI. Given X r = [x r,1 , x r,2 , ..., x r,C ], the c-th modulation map m CS I r,c of M CS I r is calculated by:</p><formula xml:id="formula_5">µ CS I r,c = 1 H × W H i=1 W j=1 x r,c (i, j),<label>(6)</label></formula><formula xml:id="formula_6">m CS I r,c = 1 H × W H i=1 W j=1 (x r,c (i, j) − µ CS I r,c ) 2 ,<label>(7)</label></formula><p>where the modulation map is used after standardization.</p><p>Inter-channel dependencies (ICD). An LR image shows different interdependencies between channels depending on the types of textures it contains <ref type="bibr" target="#b26">[27]</ref>. For example, an image with a repeated pattern shows high interdependencies among channels related to the pattern. For generating the modulation map M ICD r , we exploit this information, i.e., ICD, by employing two fullyconnected layers whose structure is the same to that in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>:</p><formula xml:id="formula_7">M ICD r = W 2 δ(W 1 M CS I r ),<label>(8)</label></formula><p>where <ref type="bibr" target="#b15">16</ref> are the parameters of the fully-connected layers, and δ(⋅) is the ReLU activation function.</p><formula xml:id="formula_8">W 1 ∈ R C 16 × C and W 2 ∈ R C× C</formula><p>Channel-specific spatial dependencies (CSD). Each channel in the feature maps X r has a different meaning depending on the role of the filter used. For example, some filters may extract the edge components in the horizontal direction, and some other filters may extract the edge components in the vertical direction. From the viewpoint of SR, where it is important to extract as much information as possible from LR images, it is expected that every channel plays its own important role. In addition, the importance of the channels varies spatially. For example, in the case of edges or complex textures, detailed information, i.e., those from complex filters, would be important. On the other hand, in the case of the region having almost no high-frequency components such as sky or homogeneous areas of comic images, relatively less detailed information would be more important and need to be attended. Therefore, it is necessary to model spatial interdependencies within each channel, i.e., CSD. To preserve channel-specific characteristics, we obtain a different 2D modulation map for each channel using depth-wise convolution <ref type="bibr" target="#b27">[28]</ref>, where independent convolution operations are performed for each channel:</p><formula xml:id="formula_9">M CS D r = f depth (X r ),<label>(9)</label></formula><p>where f depth (⋅) denotes the 3 × 3 depth-wise convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Difference from the SE block and RCAB. The residual channel attention network (RCAN) <ref type="bibr" target="#b8">[9]</ref> adopts the channel attention mechanism in RCAB, which is the same to the SE block <ref type="bibr" target="#b17">[18]</ref>. It relies on global average pooling to extract representative statistics. However, it is designed for high-level computer vision tasks such as image classification, and thus may not be optimal for image SR. We propose a variance-based channel modulation using ICD to improve the SR performance, as will be shown experimentally ( <ref type="table" target="#tab_1">Table 2</ref>). In addition, we exploit not only ICD but also CSI and CSD, which leads to fully utilize the information of residual feature maps.</p><p>Difference from CBAM. The channel attention module in CBAM <ref type="bibr" target="#b18">[19]</ref> uses both global average and max pooling, achieving performance improvement in high-level computer vision tasks. However, as will be shown later <ref type="table" target="#tab_1">(Table 2)</ref>, the additional use of max pooling lowers SR performance, indicating that it is important to choose appropriate statistics according to the application. We demonstrate that the frequency-related statistic, i.e., variance, is effective for image SR. In addition, it should be noted that the spatial attention module in CBAM and our method modelling CSD are largely different. After compressing the information via global average and max pooling in the channel direction, CBAM generates a single 2D spatial attention map through a convolution layer. This approach has two drawbacks in SR: Each channel has different information (e.g., frequency-related information) and plays a specific role. Therefore, it is not reasonable to squeeze information through pooling. In addition, it is not suitable for SR to apply a single 2D spatial attention map without reflecting channel-specific characteristics. As shown in <ref type="table" target="#tab_3">Table 3</ref>, this method causes performance degradation. On the other hand, our method is applied for each channel to modulate it in a spatially adaptive manner. Furthermore, CBAM does not fully exploit information of feature responses, because it does not consider CSI for the feature modulation.</p><p>Difference from the CSAR block. The CSAR block <ref type="bibr" target="#b16">[17]</ref> also employs the SE block for modelling ICD. In addition, it generates a single 2D map like CBAM for modelling spatial interdependencies, while in our MAMB, we model CSD instead. As shown in <ref type="table" target="#tab_3">Table 3</ref>, this approach degrades the SR performances. It should be noted that such results are obtained with employing overly large numbers of parameters because it does not adequately consider the characteristics of image SR. Moreover, the CSAR block also does not utilize CSI for feature modulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets and metrics. In our experiments, we follow the evaluation protocol commonly used in many previous studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. We train all our models using the training images from the DIV2K dataset <ref type="bibr" target="#b28">[29]</ref>. It contains 800 RGB HR training images and their corresponding LR training images. For evaluation, we use five datasets commonly used in SR benchmarks: Set5 <ref type="bibr" target="#b29">[30]</ref>, Set14 <ref type="bibr" target="#b30">[31]</ref>, BSD100 <ref type="bibr" target="#b31">[32]</ref>, Urban100 <ref type="bibr" target="#b13">[14]</ref>, Manga109 <ref type="bibr" target="#b32">[33]</ref>. The Set5, Set14, and BSD100 datasets consist of natural images. The Urban100 dataset includes images related to building structures with complex and repetitive patterns, which are challenging for SR. The Manga109 dataset consists of images taken from Japanese cartoon, which are computer-generated images and have different characteristics from natural ones. To evaluate SR performance, we calculate the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) index on the Y channel after converting to YCbCr channels.</p><p>Implementation Details. To construct an input mini-batch for training, we randomly crop a 48×48 patch from each of the randomly selected 16 LR training images. For data augmentation, the patches are randomly horizontal-flipped and rotated (90°, 180°, and 270°). Before feeding the mini-batch into our networks, we subtract the average value of the entire training  images for each RGB channel of the patches. We set the size and number of filters as 3×3 and 64 respectively in all convolution layers except those for the upscaling part. All our networks are optimized using the Adam optimizer <ref type="bibr" target="#b33">[34]</ref> to minimize the L1 loss function, where the parameters of the optimizer are set as β1 = 0.9, β2 = 0.999, and = 10 −8 . The learning rate is initially set to 10 −4 , which decreases by a half at every 2 × 10 5 iterations. We implement our networks using the Tensorflow framework with NVIDIA GeForce GTX 1080 GPU 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model Analysis</head><p>In this section, we analyze the three paths (i.e., CSI, ICD, and CSD) of our proposed MAMB. In MAMB, employing one or multiple path(s), except for CSI, increases the number of model parameters, and as the network becomes deeper, the number of such additional parameters becomes large. We want to minimize the possibility to obtain improved performance simply due to such an increased number of parameters and, as a result, analyze the effect of our methods fairly. To this end, we conduct experiments with networks having 64 filters (C=64) and 16 residual blocks (R=16), which are not too deep or wide. In addition, we analyze convergence of MAMNet with various configurations of R and C.</p><p>CSI. We examine the effectiveness of modelling CSI for image SR. For the experiment, we construct six networks, one without feature modulation (i.e., baseline) and the rest with only a CSI path using different global pooling methods for modelling CSI, which are the maximum, average, variance, standardized variance, and power. Here, "power" means the average of squared channel responses. <ref type="table" target="#tab_0">Table 1</ref> shows the result. Unlike the result in the image classification task <ref type="bibr" target="#b18">[19]</ref>, the max pooling rather degrades SR performance, which shows the impor- <ref type="bibr" target="#b0">1</ref> Our code is publicly available at https://github.com/junhyukk/ MAMNet-tensorflow   tance of using appropriate methods for modelling CSI depending on the nature of the problem being solved. For all datasets, our standardized variance-based method shows the best or second best SR performances, which demonstrates that using the frequency-related measure for CSI is effective for image SR. ICD. We analyze whether the proposed variance-based CSI is also effective in modelling ICD for image SR. We compare our method with the channel attention module (CAM) of CBAM and RCAB. To model ICD, all the methods employ two fully-connected layers having the same structure. The difference between them is that which CSI is used for modelling ICD. Similar to the result of <ref type="table" target="#tab_0">Table 1</ref>, the proposed method shows the best SR performance for all datasets in <ref type="table" target="#tab_1">Table 2</ref>, which strengthens the above conclusion that it is important to represent CSI appropriately. In addition, using both average and max pooling shows lower SR performance compared to the baseline, which means that the max pooling is not helpful for image SR.</p><p>CSD. We compare the proposed CSD with the previous methods, the spatial attention module (SAM) of CBAM and the spatial attention (SA) unit of the CSAR block, both of which model spatial feature interdependencies without considering channel-specific characteristics. <ref type="table" target="#tab_3">Table 3</ref> shows the result. CBAM shows lower SR performance than the baseline for all datasets. The CSAR block shows better SR performance than CBAM, but it is still worse than the baseline for all datasets except for Set5. On the other hand, our method maintains similar or better performance in comparison to the baseline, which demonstrates that it is more effective to consider channel-specific characteristics when modelling spatial interdependencies. Note that our method uses far fewer additional parameters than the CSAR block (0.7% vs. 9.9%).  MAMB. <ref type="table" target="#tab_4">Table 4</ref> shows the ablation study on the three paths (CSI, ICD, and CSD) of the proposed MAMB. First, without CSI, ICD and CSD, the network exhibits the worst performance on average for the five datasets (34.40 dB), which implies the non-adaptive SR network does not effectively extract features from LR images. This demonstrates that simply stacking residual blocks leads to limited representational power of deep networks for image SR.</p><p>Then, we add one of the three paths to the baseline (the second, third, and fourth columns of <ref type="table" target="#tab_4">Table 4</ref>). We confirm that CSI and ICD effectively lead to performance improvement (+0.15 dB and +0.14 dB, respectively) on average with no and negligible increase of the number of parameters (0% and 0.68%, respectively). CSD also yields the SR performances similar to or slightly better (+0.05 dB for Set5) than the baseline.</p><p>In addition, we experiment on three networks using two of the three paths (the fifth, sixth, and seventh columns of <ref type="table" target="#tab_4">Table 4</ref>). We observe that the networks perform better than those using only one path. Then, the best performance is achieved when all the three paths are used simultaneously, which is shown in the last column of <ref type="table" target="#tab_4">Table 4</ref>. The experimental results demonstrate that feature modulation exploiting sufficient information (CSI, ICD, and CSD) via multiple paths is effective for image SR. Moreover, the performance improvement is achieved in a parameter-efficient manner (+0.19 dB with only 1.43% additional parameters).</p><p>To further analyze the role of each path of the proposed MAMB, we visualize the modulation map of each path in <ref type="figure" target="#fig_3">Figure 3. Figure 3a</ref> shows those corresponding to CSI and ICD in the fourth, eighth, and last MAMBs, respectively. Here, we have two interesting observations. First, when the values of M CS I are similar across the channels, which means that the channels contain similar amounts of information, the values of M ICD vary significantly from channel to channel (the left and right panels of <ref type="figure" target="#fig_3">Figure 3a</ref>). Second, when M ICD is hardly activated differently across the channels, the values via M CS I are largely different depending on the channel and the information of the CSI is used dominantly for feature modulation, as shown in the middle panel of <ref type="figure" target="#fig_3">Figure 3a</ref>. These observations imply that although both CSI and ICD are derived from the channel-wise pooling, they have their own roles, which appear even complementary, for adaptive feature modulation. <ref type="figure" target="#fig_3">Figure 3b</ref> shows the 32nd and 64th channels of M CS D in each MAMB of <ref type="figure" target="#fig_3">Figure 3a</ref>. Each map has spatially varying values, which demonstrates that each map spatially modulates each channel adaptively. In addition, the distribution of the map is different for each channel. For example, the 64th channel of the 4th MAMB has a modulation map with relatively similar values in the spatial domain, which means that CSI and ICD can provide sufficient information for feature modulation. On the other hand, the 32nd channel of the 16th MAMB has a modulation map with largely different values depending on the spatial location. Specifically, the left area containing text has relatively large values, while the rest shows small values. Through these results, it is confirmed that each channel requires different spatial modulation depending on its characteristics, i.e., CSD.</p><p>Effect of R and C. The structure of our MAMNet is determined by the number of MAMB (R) and the number of channels (C) used in each MAMB. In this experiment, we examine the effect of these two variables on performance.  the case with R = 16 and C = 64 (R16C64), we increase R or C.</p><p>The convergence analysis of the networks with different configurations according to the number of training iterations is shown in <ref type="figure">Figure 4</ref>, where CARN <ref type="bibr" target="#b34">[35]</ref>, MSRN <ref type="bibr" target="#b7">[8]</ref>, and D-DBPN <ref type="bibr" target="#b5">[6]</ref> are compared as references. A larger value of R or C leads to performance improvement, which means that our proposed method allows deeper and wider structures through effective feature modulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Other Feature Modulation Methods</head><p>To demonstrate effectiveness and efficiency of our proposed MAMB, we evaluate our MAMB by comparing with other feature modulation strategies for image SR <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>. For fairness, we construct networks in the form of implementing the feature modulation methods in each residual block of the baseline network.</p><p>Quantitative comparison. We compare performances across networks of varying sizes (R), which are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. We can see that our method performs better for all network sizes than the others. In other words, our network needs only a relatively small number of parameters to achieve the same performance.</p><p>Qualitative comparison. We further provide qualitative comparison of the feature modulation methods for R=16 and C=64 in <ref type="figure">Figure 6</ref>. Our method shows superior performance particularly in difficult cases <ref type="figure">(Figure 6b</ref>), while maintaining similar performance with the others in relatively easy cases <ref type="figure">(Figure 6a</ref>). The result shows that our method succeeds in effective feature modulation, thereby improving the ability to adapt to various situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-art Methods</head><p>Quantitative and qualitative comparisons. We finally evaluate our proposed MAMNet by comparing with 11 stateof-the art SR methods: VDSR <ref type="bibr" target="#b1">[2]</ref>, LapSRN <ref type="bibr" target="#b35">[36]</ref>, DRRN <ref type="bibr" target="#b36">[37]</ref>, MemNet <ref type="bibr" target="#b37">[38]</ref>, SRDenseNet <ref type="bibr" target="#b4">[5]</ref>, DSRN <ref type="bibr" target="#b38">[39]</ref>, SRMDNF <ref type="bibr" target="#b39">[40]</ref>, IDN <ref type="bibr" target="#b40">[41]</ref>, CARN <ref type="bibr" target="#b34">[35]</ref>, MSRN <ref type="bibr" target="#b7">[8]</ref>, and D-DBPN <ref type="bibr" target="#b5">[6]</ref>. Here, EDSR <ref type="bibr" target="#b7">[8]</ref>, RDN <ref type="bibr" target="#b6">[7]</ref>, and RCAN <ref type="bibr" target="#b8">[9]</ref> are excluded from the comparison because they have extremely larger numbers of parameters. We select MAMNet with R = 64 and C = 64 (MAM-Net R64C64) as our final model. The ×2, ×3 and ×4 SR quantitative results are summarized in <ref type="table" target="#tab_7">Table 5</ref>. MAMNet shows the highest PSNR and SSIM values on all datasets for all scaling factors, and the performance gap with the other methods is particularly prominent in Urban100 and Manga109. These results demonstrate the effectiveness of our proposed MAMNet.</p><p>We also provide the visual results of ×2 super-resolved images in <ref type="figure" target="#fig_5">Figure 7</ref>, where only our model successfully restores complex patterns. For "img 092" from Urban100, our proposed method takes advantage of the peripheral information more actively, i.e., the information about the repeated pattern. Similarly, in "img 004", it can be seen that the learning of the repeated pattern is not performed well by merely using the local information from the LR image. On the other hand, our model recovers the pattern correctly. Furthermore, we show the ×4 super-resolved images from BSD100 and Urban100 in <ref type="figure" target="#fig_0">Figure 10</ref>. For the "253027" image, we can see that our network expresses the complicated stripes more finely. For "img 061", the outputs of the other models look blurry or have patterns in wrong directions, while only our MAMNet restores the correct pattern.</p><p>Visual results of challenging images for ×4 SR are shown in <ref type="figure">Figure 9</ref>. Here, we focus on comparison with the top two models, MSRN <ref type="bibr" target="#b7">[8]</ref> and D-DBPN <ref type="bibr" target="#b5">[6]</ref>. For image "Akuhamu", MAMNet restores better the densely written letters, "A" and "K". For images "YasasiiAkuma" and "YumeiroCooking", MAMNet reconstructs complicated straight lines and curves more clearly. For images "img 011", "img 012", "img 019", and "img 093", while the others fails to restore the patterns in terms of thickness, direction, and spacing, MAMNet successfully generates the repeated patterns. For image "img 082", MAMNet restores black sharp lines relatively well. These results demonstrate the strength of our proposed method in various difficult SR tasks.</p><p>Model efficiency. MAMNet enables highly effective and efficient SR through multi-path adaptive modulation. It is powerful, but relatively lightweight and fast compared to the other  state-of-the art models. We show the efficiency of our MAM-Net in <ref type="figure" target="#fig_0">Figure 10</ref>. MAMNet R64C64 show the best performance on both datasets with smaller numbers of parameters and shorter running time than D-DBPN and MSRN. MAM-Net R32C64 has similar or better performance on both datasets with only 43.69% and 43.52% of the number of parameters of MSRN and D-DBPN, respectively. In addition, its running time is only 22.59% (BSD100) and 26.76% (Urban100) of that of MSRN, and 19.26% (BSD100) and 17.56% (Urban100) of that of D-DBPN. For both datasets, MAMNet R16C64 outperforms MSRN with only 23.42% parameters of MSRN. It takes only 12.20% (BSD100) and 13.66% (Urban100) of the running time of MSRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a novel multi-path adaptive modulation network (MAMNet) for image SR. We proposed three feature modulation methods by exploiting the convolutional feature responses effectively. We demonstrated that the proposed MAMB is more effective and parameter-efficient than existing feature modulation methods for SR. The experimental results also demonstrated that the proposed MAMNet can achieve improved SR performance compared to the state-ofthe-art methods with a relatively small number of parameters.   <ref type="bibr" target="#b31">[32]</ref> and Urban100 <ref type="bibr" target="#b13">[14]</ref>. <ref type="figure">Figure 9</ref>: Visual comparison of ×4 SR results on the challenging images of Urban100 <ref type="bibr" target="#b13">[14]</ref> and Manga109 <ref type="bibr" target="#b32">[33]</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall architecture of our proposed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Multi-path adaptive modulation block (MAMB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) M CS I and M ICD .(b) M CS D .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of each path of our proposed multi-path adaptive modulation for "ppt3" from Set14<ref type="bibr" target="#b30">[31]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of differnet feature modulation methods for ×2 SR. The PSNR values are the average values for all images of the five datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visual comparison of ×2 SR results on Urban100<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Visual comparison of ×4 SR results on BSD100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>PSNR (dB) vs. running time (s) for ×2 SR. The PSNR values and running times are average values for each dataset. Two existing methods are shown with blue color, and our models with varying the number of parameters (R) are shown with red color. The area of each circle is proportional to the number of parameters in each model (also shown as numbers in parentheses).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of using different pooling methods for CSI. Average PSNR values (dB) for ×2 SR on the five datasets are shown. Red and blue colors indicate the best and second best performance for each dataset, respectively.</figDesc><table><row><cell>Methods</cell><cell cols="5">Set5 Set14 BSD100 Urban100 Manga109</cell></row><row><cell>Baseline</cell><cell>37.90 33.58</cell><cell cols="2">32.17</cell><cell>32.13</cell><cell>38.47</cell></row><row><cell>+ Max</cell><cell>37.94 33.53</cell><cell cols="2">32.16</cell><cell>32.09</cell><cell>38.44</cell></row><row><cell>+ Avg</cell><cell>37.96 33.59</cell><cell cols="2">32.07</cell><cell>32.24</cell><cell>38.65</cell></row><row><cell>+ Var</cell><cell>37.93 33.61</cell><cell cols="2">32.17</cell><cell>32.28</cell><cell>38.64</cell></row><row><cell>+ Power</cell><cell>37.92 33.59</cell><cell cols="2">32.16</cell><cell>32.20</cell><cell>38.51</cell></row><row><cell cols="2">+ Standardized var (Ours) 37.95 33.63</cell><cell cols="2">32.17</cell><cell>32.33</cell><cell>38.73</cell></row><row><cell>Methods</cell><cell cols="5">Set5 Set14 BSD100 Urban100 Manga109</cell></row><row><cell>Baseline</cell><cell cols="2">37.90 33.58</cell><cell>32.17</cell><cell>32.13</cell><cell>38.47</cell></row><row><cell cols="3">+ CAM of CBAM [19] (Max &amp; Avg) 37.91 33.51</cell><cell>32.14</cell><cell>32.14</cell><cell>38.19</cell></row><row><cell>+ RCAB [9] (Avg)</cell><cell cols="2">37.96 33.58</cell><cell>32.17</cell><cell>32.24</cell><cell>38.60</cell></row><row><cell>+ Var</cell><cell cols="2">37.93 33.55</cell><cell>32.17</cell><cell>32.26</cell><cell>38.67</cell></row><row><cell>+ Standardized var (Ours)</cell><cell cols="2">37.97 33.66</cell><cell>32.17</cell><cell>32.32</cell><cell>38.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Effect of using different CSI for modelling ICD. Average PSNR values (dB) for ×2 SR on the five datasets are shown. Red and blue colors indicate the best and second best performance for each dataset, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance and parameter efficiency of modelling CSD. Average PSNR values (dB) for ×2 SR on the five datasets are shown. Red and blue colors indicate the best and second best performance for each dataset, respectively.</figDesc><table><row><cell>Components</cell><cell></cell><cell cols="6">Different combinations of CSI, ICD, and CSD</cell><cell></cell></row><row><cell>CSI</cell><cell>×</cell><cell>✓</cell><cell>×</cell><cell>×</cell><cell>✓</cell><cell>✓</cell><cell>×</cell><cell>✓</cell></row><row><cell>ICD</cell><cell>×</cell><cell>×</cell><cell>✓</cell><cell>×</cell><cell>✓</cell><cell>×</cell><cell>✓</cell><cell>✓</cell></row><row><cell>CSD</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>✓</cell><cell>×</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Set5</cell><cell cols="8">37.90 37.95 37.97 37.95 37.97 37.97 37.98 37.99</cell></row><row><cell>Set14</cell><cell cols="8">33.58 33.63 33.66 33.59 33.71 33.63 33.64 33.64</cell></row><row><cell>BSD100</cell><cell cols="8">32.17 32.17 32.17 32.17 32.17 32.18 32.18 32.19</cell></row><row><cell>Urban100</cell><cell cols="8">32.13 32.33 32.32 32.13 32.34 32.35 32.34 32.38</cell></row><row><cell>Manga109</cell><cell cols="8">38.47 38.73 38.71 38.46 38.75 38.72 38.75 38.80</cell></row><row><cell>Average</cell><cell cols="8">34.40 34.55 34.54 34.40 34.56 34.56 34.56 34.59</cell></row><row><cell># params. (K)</cell><cell cols="8">1370 1370 1379 1380 1379 1380 1389 1389</cell></row><row><cell># params. ↑ (%)</cell><cell>-</cell><cell>0</cell><cell>0.68</cell><cell>0.75</cell><cell>0.68</cell><cell>0.75</cell><cell>1.43</cell><cell>1.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Ablation study on effects of CSI, ICD, and CSD. Average PSNR values (dB) for ×2 SR on the five datasets are shown. The row "Average" means the average PNSR values for all images in the five datasets. Red and blue colors indicate the best and second best performance for each dataset, respectively. The row "# params. ↑" shows the ratio of the increased number of parameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM ×2 VDSR [2] 37.53 / 0.9587 33.03 / 0.9124 31.90 / 0.8960 30.76 / 0.9140 37.22 / 0.9750 LapSRN [36] 37.52 / 0.9591 33.08 / 0.9130 31.80 / 0.8950 30.41 / 0.9101 37.27 / 0.9740 DRRN [37] 37.74 / 0.9591 33.23 / 0.9136 32.05 / 0.8973 31.23 / 0.9188 37.92 / 0.9760 MemNet [38] 37.78 / 0.9597 33.23 / 0.9142 32.08 / 0.8978 31.31 / 0.9195 37.72 / 0.9740 DSRN [39] 37.66 / 0.9590 33.15 / 0.9130 32.10 / 0.8970 30.97 / 0.9160 -/ -SRMDNF [40] 37.79 / 0.9601 33.32 / 0.9159 32.05 / 0.8985 31.33 / 0.9204 38.07 / 0.9761 IDN [41] 37.83 / 0.9600 33.30 / 0.9148 32.08 / 0.8985 31.27 / 0.9196 -/ -CARN [35] 37.76 / 0.9590 33.52 / 0.9166 32.09 / 0.8978 31.92 / 0.9256 -/ -MSRN [8] 37.90 / 0.9597 33.62 / 0.9177 32.16 / 0.8995 32.22 / 0.9295 38.40 / 0.9761 D-DBPN [6] 38.05 / 0.9599 33.79 / 0.9193 32.25 / 0.9001 32.51 / 0.9317 38.81 / 0.9766 MAMNet 38.10 / 0.9601 33.90 / 0.9199 32.30 / 0.9007 32.94 / 0.9352 39.15 / 0.9772 / 0.9265 30.37 / 0.8428 29.12 / 0.8056 28.31 / 0.8553 33.59 / 0.9442 MAMNet 34.61 / 0.9281 30.54 / 0.8459 29.25 / 0.8082 28.82 / 0.8648 34.14 / 0.9472 ×4 VDSR [2] 31.35 / 0.8838 28.01 / 0.7674 27.29 / 0.7251 25.18 / 0.7524 28.83 / 0.8870 LapSRN [36] 31.54 / 0.8850 28.19 / 0.7720 27.32 / 0.7270 25.21 / 0.7560 29.09 / 0.8900 DRRN [37] 31.68 / 0.8888 28.21 / 0.7720 27.38 / 0.7284 25.44 / 0.7638 29.46 / 0.8960 MemNet [38] 31.74 / 0.8893 28.26 / 0.7723 27.40 / 0.7281 25.50 / 0.7630 29.42 / 0.8942 SRDenseNet [5] 32.02 / 0.8934 28.50 / 0.7782 27.53 / 0.7337 26.05 / 0.7819 -/ -DSRN [39] 31.40 / 0.8830 28.07 / 0.7700 27.25 / 0.7240 25.08 / 0.7170 -/ -SRMDNF [40] 31.96 / 0.8925 28.35 / 0.7787 27.49 / 0.7337 25.68 / 0.7731 30.09 / 0.9024 IDN [41] 31.82 / 0.8903 28.25 / 0.7730 27.41 / 0.7297 25.41 / 0.7632 -/ -CARN [35] 32.13 / 0.8937 28.60 / 0.7806 27.58 / 0.7349 26.07 / 0.7837 -/ -MSRN [8] 32.21 / 0.8949 28.61 / 0.7827 27.60 / 0.7372 26.20 / 0.7903 30.53 / 0.9090 D-DBPN [6] 32.40 / 0.8966 28.75 / 0.7854 27.67 / 0.7385 26.38 / 0.7938 30.89 / 0.9127 MAMNet 32.42 / 0.8972 28.77 / 0.7854 27.70 / 0.7406 26.59 / 0.8013 30.94 / 0.9142</figDesc><table><row><cell cols="3">Set5 33.66 / 0.9213 29.77 / 0.8314 28.82 / 0.7976 27.14 / 0.8279 32.01 / 0.9340 Set14 BSD100 Urban100 Manga109 33.82 / 0.9227 29.87 / 0.8320 28.82 / 0.7980 27.07 / 0.8280 32.21 / 0.9350 34.02 / 0.9244 30.08 / 0.8361 28.95 / 0.8007 27.54 / 0.8378 32.72 / 0.9380 34.09 / 0.9248 30.00 / 0.8350 28.96 / 0.8001 27.56 / 0.8376 32.51 / 0.9369 33.88 / 0.9220 30.26 / 0.8370 28.81 / 0.7970 27.16 / 0.8280 -/ -PSNR / ×3 Scale Method VDSR [2] LapSRN [36] DRRN [37] MemNet [38] DSRN [39] SRMDNF [40] 34.12 / 0.9254 30.04 / 0.8382 28.97 / 0.8025 27.57 / 0.8398 33.00 / 0.9403</cell></row><row><cell>IDN [41]</cell><cell>34.11 / 0.9253 29.99 / 0.8354 28.95 / 0.8013 27.42 / 0.8359</cell><cell>-/ -</cell></row><row><cell>CARN [35]</cell><cell>34.29 / 0.9255 30.29 / 0.8407 29.06 / 0.8034 27.38 / 0.8404</cell><cell>-/ -</cell></row><row><cell>MSRN [8]</cell><cell>34.38</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Quantitative evaluation results of SR models for scaling factors of 2, 3 and 4. Red and blue colors indicate the best and second best performance, respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the "ICT Consilience Creative Program" (IITP-2018-2017-0-01015) supervised by the IITP (Institute for Information &amp; communications Technology Promotion). In addition, this work was also supported by the IITP grant funded by the Korea government (MSIT) (R7124-16-0004, Development of Intelligent Interaction Technology Based on Context Awareness and Human Intention Understanding).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-scale residual network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Channel-wise and spatial feature modulation network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11130</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Graphical models and image processing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Texture synthesis using convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ntire 2018 challenge on single image super-resolution: Methods and results</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparserepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Curves and Surfaces</title>
		<meeting>the International Conference on Curves and Surfaces</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<title level="m">Sketch-based manga retrieval using manga109 dataset, Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast, accurate, and, lightweight superresolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution via dual-state recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a single convolutional superresolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast and accurate single image super-resolution via information distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual comparison of ×2 SR results by our method and the other feature modulation methods on Set14</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
