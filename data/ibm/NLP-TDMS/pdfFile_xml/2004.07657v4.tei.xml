<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Old is G old : Redefining the Adversarially Learned One-Class Classifier Training Paradigm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Zaigham Zaheer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electronics and Telecommunications Research Institute</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ha</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electronics and Telecommunications Research Institute</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Astrid</surname></persName>
							<email>marcella.astrid@ust.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electronics and Telecommunications Research Institute</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Ik</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electronics and Telecommunications Research Institute</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Old is G old : Redefining the Adversarially Learned One-Class Classifier Training Paradigm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A popular method for anomaly detection is to use the generator of an adversarial network to formulate anomaly score over reconstruction loss of input. Due to the rare occurrence of anomalies, optimizing such networks can be a cumbersome task. Another possible approach is to use both generator and discriminator for anomaly detection. However, attributed to the involvement of adversarial training, this model is often unstable in a way that the performance fluctuates drastically with each training step. In this study, we propose a framework that effectively generates stable results across a wide range of training steps and allows us to use both the generator and the discriminator of an adversarial model for efficient and robust anomaly detection. Our approach transforms the fundamental role of a discriminator from identifying real and fake data to distinguishing between good and bad quality reconstructions. To this end, we prepare training examples for the good quality reconstruction by employing the current generator, whereas poor quality examples are obtained by utilizing an old state of the same generator. This way, the discriminator learns to detect subtle distortions that often appear in reconstructions of the anomaly inputs. Extensive experiments performed on Caltech-256 and MNIST image datasets for novelty detection show superior results. Furthermore, on UCSD Ped2 video dataset for anomaly detection, our model achieves a frame-level AUC of 98.1%, surpassing recent state-of-theart methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to rare occurrence of anomalous scenes, the anomaly detection problem is usually seen as one-class classification (OCC) in which only normal data is used to learn a novelty detection model <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref>. One of the recent trends to learn oneclass data is by using an encoder-decoder architecture such as denoising auto-encoder <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b49">49]</ref>. Generally, in this scheme, training is carried out until the model starts to produce good quality reconstructions <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b43">43]</ref>. During the test time, it is expected to show high reconstruction loss for abnormal data which corresponds to a high anomaly score. With the recent developments in Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">[8]</ref>, some researchers also explored the possibility of improving the generative results using adversarial training <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b32">32]</ref>. Such training fashion substantially enhances the data regeneration quality <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">43]</ref>. At the test time, the trained generator G is then decoupled from the discriminator D to be used as a reconstruction model. As reported in <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>, a wide difference in reconstruction loss between the normal and abnormal data can be achieved due to adversarial training, which results in a better anomaly detection system. However, relying only on the reconstruction capability of a generator does not oftentimes work well because the usual encoder-decoder style generators may unexpectedly well-reconstruct the unseen data which drastically degrades the anomaly detection performance.</p><p>A natural drift in this domain is towards the idea of using D along with the conventional utilization of G for anomaly detection. The intuition is to gain maximum benefits of the one-class adversarial training by utilizing both G and D instead of only G. However, this also brings along the problems commonly associated with such architectures. For example, defining a criteria to stop the training is still a challenging problem <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">30]</ref>. As discussed in Sabokrou et al. <ref type="bibr" target="#b41">[41]</ref>, the performance of such adversarially learnt one-class classification architecture is highly dependent on the criteria of when to halt the training. In the case of stopping prematurely, G will be undertrained and in the case of overtraining, D may get confused because of the real-looking fake data. Our experiments show that a G+D trained as a collective model for anomaly detection (referred to as a baseline) will not ensure higher convergence at any arbitrary training step over its predecessor. <ref type="figure" target="#fig_0">Figure 1</ref> shows frame-level area under the curve (AUC) performance of the baseline over several epochs of training on UCSD Ped2 dataset <ref type="bibr" target="#b3">[4]</ref>. Although we get high performance peaks at times, it can be seen that the performance fluctuates substantially even between two arbitrary consecutive epochs. Based on these findings, it can be argued that a D as we know it, may not be a suitable choice in a one-class classification problem, such as anomaly detection. Following this intuition, we devise an approach for training of an adversarial network towards anomaly detection by transforming the basic role of D from distinguishing between real and fake to identifying good and bad quality reconstructions. This property of D is highly desirable in anomaly detection because a trained G would not produce as good reconstruction for abnormal data as it would for the normal data conforming to the learned representations. To this end we propose a two-stage training process. Phase one is identical to the common practice of training an adversarial denoising auto-encoder <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b49">49]</ref>. Once G achieves a reasonably trained state (i.e. showing low reconstruction losses), we begin phase two in which D is optimized by training on various good quality and bad quality reconstruction examples. Good quality reconstruction examples come from real data as well as the data regenerated by G, whereas bad reconstruction examples are obtained by utilizing an old state of the generator (G old ) as well as by using our proposed pseudo-anomaly module. Shown in <ref type="figure">Figure 3</ref>, this pseudo-anomaly module makes use of the training data to create anomaly-like examples. With this two-phase training process, we expect D to be trained in such a way that it can robustly discriminate reconstructions coming from normal and abnormal data. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our model not only provides superior performance but also shows stability across several training epochs.</p><p>In summary, the contributions of our paper are as follows: 1) this work is among the first few to employ D along with G at test time for anomaly detection. Moreover, to the best of our knowledge, it is the first one to extensively report the impacts of using the conventional G+D formulation and the consequent instability. 2) Our approach of transforming the role of a discriminator towards anomaly detection problem by utilizing an old state G old of the generator along with the proposed pseudo-anomaly module, substantially improves stability of the system. Detailed analysis provided in this paper shows that our model is independent of a hard stopping criteria and achieves consistent results over a wide range of training epochs. 3) Our method outperforms state-of-the-art <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b59">59]</ref> in the experiments conducted on MNIST <ref type="bibr" target="#b17">[18]</ref> and Caltech-256 <ref type="bibr" target="#b8">[9]</ref> datasets for novelty detection as well as on UCSD Ped2 <ref type="bibr" target="#b3">[4]</ref> video dataset for anomaly detection. Moreover, on the latter dataset, our approach provides a substantial absolute gain of 5.2% over the baseline method achieving frame level AUC of 98.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Anomaly detection is often seen as a novelty detection problem <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref> in which a model is trained based on the known normal class to ultimately detect unknown outliers as abnormal. To simplify the task, some works proposed to use object tracking <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b57">57]</ref> or motion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>. Handpicking features in such a way can often deteriorate the performance significantly. With the increased popularity of deep learning, some researchers <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b35">35]</ref> also proposed to use pre-trained convolution network based features to train one-class classifiers. Success of such methods is highly dependent on the base model which is often trained on some unrelated datasets.</p><p>A relatively new addition to the field, image regeneration based works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b40">40]</ref> are the ones that make use of a generative network to learn features in an unsupervised way. Ionescu et al. in <ref type="bibr" target="#b12">[13]</ref> proposed to use convolutional auto-encoders on top of object detection to learn motion and appearance representations. Xu et al. <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b53">53]</ref> used a one-class SVM learned using features from stacked auto-encoders. Ravanbakhsh et al. <ref type="bibr" target="#b35">[35]</ref> used generator as a reconstructor to detect abnormal events assuming that a generator is unable to reconstruct the inputs that do not conform the normal training data. In <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>, the authors suggested to use a cascaded decoder to learn motion as well as appearance from normal videos. However, in all these schemes, only a generator is employed to perform detection. Pathak et al. <ref type="bibr" target="#b30">[30]</ref> proposed adversarial training to enhance the quality of regeneration. However, they also discard the discriminator once the training is finished. A unified generator and discriminator model for anomaly detection is proposed in Sabokrou et al. <ref type="bibr" target="#b41">[41]</ref>. The model shows promising results, however it is often not stable and the performance relies heavily on the criteria to stop training. Recently, Shama et al. <ref type="bibr" target="#b43">[43]</ref> proposed an idea of utilizing output  of an adversarial discriminator to increase the image quality of generated images. Although not related to anomaly detection, it provides an interesting intuition to make use of both adversarial components for an enhanced performance.</p><p>Our work, although built on top of an unsupervised generative network, is different from the approaches in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b40">40]</ref> as we explore to utilize the unified generator and discriminator model for anomaly detection. The most similar work to ours is by Sabokrou et al. <ref type="bibr" target="#b41">[41]</ref> and Lee et al. <ref type="bibr" target="#b19">[19]</ref> as they also explore the possibility of using discriminator, along with the conventional usage of generator, for anomaly detection. However, our approach is substantially different from these. In <ref type="bibr" target="#b41">[41]</ref>, a conventional adversarial network is trained based on a criteria to stop the training whereas, in <ref type="bibr" target="#b19">[19]</ref>, an LSTM based approach is utilized for training. In contrast, we utilize a pseudo-anomaly module along with an old state of the generator, to modify the ultimate role of a discriminator from distinguishing between real and fake to detecting between and bad quality reconstructions. This way, our overall framework, although trained adversarially in the beginning, finally aligns both the generator and the discriminator to complement each other towards anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present our OGNet framework. As described in Section 1, most of the existing GANs based anomaly detection approaches completely discard discriminator at test time and use generator only. Furthermore, even if both models are used, the unavailability of a criteria to stop the training coupled with the instability over training epochs caused by adversary makes the convergence uncertain. We aim to change that by redefining the role of a discriminator to make it more suitable for anomaly detection problems. Our solution is generic, hence it can be integrated with any existing one-class adversarial networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture Overview</head><p>In order to maintain consistency and to have a fair comparison, we kept our baseline architecture similar to the one proposed by Sabokrou et al. <ref type="bibr" target="#b41">[41]</ref>. The generator G, a typical denoising auto-encoder, is coupled with the discriminator D to learn one class data in an unsupervised adversarial fashion. The goal of this model is to play a min-max game to optimize the following objective function:</p><formula xml:id="formula_0">min G max D E X∼pt [log(1 − D(X))] + EX ∼pt+Nσ [log(D(G(X)))] ,<label>(1)</label></formula><p>whereX is the input image X with added noise N σ as in a typical denoising auto-encoder. Our model, built on top of the baseline, makes use of an old frozen generator (G old ) to create low quality reconstruction examples. We also propose a pseudo-anomaly module to assist D in learning the  <ref type="figure">Figure 3</ref>: Our proposed pseudo-anomaly module. A pseudo-anomalyX is created by regenerating two arbitrary training images through G old followed by a pixel-level mean. Finally,X pseudo is created as G(X) to mimic the regeneration behavior of G for anomalous inputs.</p><p>behavior of G in the case of unusual or anomalous input, which we found out to be very useful towards the robustness of our approach ( <ref type="table">Table 4</ref>). The overall purpose of our proposed framework is to alter the learning paradigm of D from distinguishing between real and fake to differentiating between good and bad reconstructions. This way, the discriminator gets aligned with the conventional philosophy of generative one-class learning models in which the reconstruction quality of the data from a known class is better than the data from unknown or anomaly classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>The training of our model is carried out in two phases (see <ref type="figure" target="#fig_1">Figure 2</ref>). Phase one is similar to the common practices in training an adversarial one-class classifier <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b36">36]</ref>. G tries to regenerate real-looking fake data which is then fed into D along with real data. The D learns to discriminate between real and fake data, success or failure of which then becomes a supervision signal for G. This training is carried out until G starts to create real looking images with a reasonably low reconstruction loss. Overall, phase one minimizes the following loss function:</p><formula xml:id="formula_1">L = L G+D + λL R ,<label>(2)</label></formula><p>where L G+D is the loss function of our joint training objective defined in Equation 1, L R = ||X − G(X)|| 2 is the reconstruction loss, and λ is a weighing hyperparameter. Additionally, as phase one progresses, we save a low-epoch generator model (G old ) for later use in phase two of the training. Deciding which low epoch to be used can be an intuitive selection based on the quality of regeneration. Obviously, we want G old to generate low quality images compared to a trained G. However, it is not necessary to select any specific epoch number for this generator. We will prove this empirically in Section 4 by showing that the final convergence of our model is not dependent on a strict selection of the epoch numbers and that various generic settings are possible to obtain a G old . Phase two of the training is where we make use of the frozen models G old and G to update D. This way D starts learning to discriminate between good and bad quality reconstructions, hence becoming suitable for one-class classification problems such as anomaly detection.</p><p>Details of the phase two training are discussed next: Goal. The essence of phase two training is to provide examples of good quality and bad quality reconstructions to D, with a purpose of making it learn about the kind of output that G would produce in the case of an unusual input. The training is performed for just a few iterations since the already trained D converges quickly. A detailed study on this is added in Section 4.</p><p>Good quality examples. D is provided with real data (X), which is the best possible case of reconstruction, and the actual high quality reconstructed data (X = G(X)) produced by the trained G as an example of good quality examples.</p><p>Bad quality examples. Examples of low quality reconstruction (X low ) are generated using G old . In addition, a pseudo-anomaly module, shown in <ref type="figure">Figure 3</ref>, is formulated with a combination of G old and the trained G, which simulates examples of reconstructed pseudoanomalies (X pseudo ). Pseudo anomaly creation. Given two arbitrary images X i and X j from the training dataset, a pseudo anomaly imagê X is generated as:</p><formula xml:id="formula_2">X = G old (X i ) + G old (X j ) 2 =X low i +X low j 2 , where i = j.</formula><p>(3) This way, the resultant image can contain diverse variations such as shadows and unusual shapes, which are completely unknown to both G and D models. Finally, as the last step in our pseudo-anomaly module, in order to mimic the behavior of G when it gets unusual data as input,X is then reconstructed using G to obtainX pseudo :</p><formula xml:id="formula_3">X pseudo = G(X).<label>(4)</label></formula><p>Example images at each intermediate step can be seen in <ref type="figure" target="#fig_4">Figures 3 and 4</ref>.</p><p>Tweaking the objective function. The model in phase two of the training takes the form:</p><formula xml:id="formula_4">max D αE X [log(1 − D(X))]+ (1 − α)EX [log(1 − D(X))] + βEX low [log(D(X low ))]+ (1 − β)EX pseudo [log(D(X pseudo ))] ,<label>(5)</label></formula><p>where α and β are the trade-off hyperparameters.</p><p>Quasi ground truth for the discriminator in phase one training is defined as:</p><formula xml:id="formula_5">GT phase one = 0 if input is X, 1 if input isX.<label>(6)</label></formula><p>However, for phase two training, it takes the form:</p><formula xml:id="formula_6">GT phase two = 0 if input is X orX, 1 if input isX low orX pseudo .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Testing</head><p>At test time, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, only G and D are utilized for one-class classification (OCC). Final classification decision for an input image X is given as:</p><formula xml:id="formula_7">OCC = normal class if D(G(X)) &lt; τ , anomaly class otherwise.<label>(8)</label></formula><p>where τ is a predefined threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The evaluation of our OGNet framework on three different datasets is reported in this section. Detailed analysis of the performance and its comparison with the state-of-the-art methodologies is also reported. In addition, we provide extensive discussion and ablation studies to show the stability as well as the significance of our proposed scheme. In order to keep the experimental setup consistent with the existing works <ref type="bibr">[22, 58, 25, 51, 11, 45, 40, 10, 44, 34, 35, 41, 13, 7,</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>28</head><p>, 27], we tested our method for the detection of outlier images as well as video anomalies. Evaluation criteria. Most of our results are formulated based on area under the curve (AUC) computed at frame level due to its popularity in related works <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b41">41]</ref>. Nevertheless, following the evaluation methods adopted in <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref> we also report F 1 score and Equal Error Rate (EER) of our approach.</p><p>Parameters and implementation details. The implementation is done in PyTorch <ref type="bibr" target="#b29">[29]</ref> and the source code is provided at https://github.com/xaggi/OGNet. Phase one of the training in our reports is performed from 20 to 30 epochs. These numbers are chosen because the baseline shows high performance peaks within this range <ref type="figure" target="#fig_0">(Figure 1</ref>). We train on Adam <ref type="bibr" target="#b14">[15]</ref> with the learning rate of generator and discriminator in all these epochs set to 10 −3 and 10 −4 , respectively. Phase two of the training is done for 75 iterations with the learning rate of the discriminator reduced to half. λ, α, and β are set to 0.2, 0.1, 0.001, respectively. Until stated otherwise, default settings of our experiments are set to the aforementioned values. However, for the detailed evaluation provided in a later part of this section, we also conducted experiments and reported results on a range of epochs and iterations for both phases of the training, respectively. Furthermore, until specified otherwise, we pick the generator after 1 st epoch and freeze it as G old . This selection is arbitrary and solely based on the intuition explained in Section 3. Additionally, in a later part of this section, we also present a robust and generic method to formulate G old without any need of handpicking an epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Caltech-256. This dataset <ref type="bibr" target="#b8">[9]</ref> contains a total of 30,607 images belong to 256 object classes and one 'clutter' class. Each category has different number of images, as low as 80 and as high as 827. In order to perform our experiments, we used the same setup as described in previous works <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b41">41]</ref>. In a series of three experiments, at most 150 images belong to 1, 3, and 5 randomly chosen classes are defined as training (inlier) data. Outlier images for test are taken from the clutter class in such a way that each experiment has exactly 50% ratio of outliers and inliers.</p><p>MNIST. This dataset <ref type="bibr" target="#b17">[18]</ref> consists of 60,000 handwritten digits from 0 to 9. The setup to evaluate our method on this dataset is also kept consistent with the previous works <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b41">41]</ref>.  <ref type="table">Table 1</ref>: AUC and F 1 score performance comparison of our framework on Caltech-256 <ref type="bibr" target="#b8">[9]</ref> with the other state of the art methods. Following the existing work <ref type="bibr" target="#b55">[55]</ref>, each subgroup of rows from top to bottom shows evaluation scores on inliers coming from 1, 3, and 5 different random classes respectively (best performance as bold and second best as underlined).</p><p>is of 240×360 pixels resolution. Pedestrians dominate most of the frames whereas anomalies include skateboards, vehicles, bicycles, etc. Similar to <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b41">41]</ref>, frame-level AUC and EER metrics are adopted to evaluate performance on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Outlier Detection in Images</head><p>One of the significant applications of a one-class learning algorithm is outlier detection. In this problem, objects belonging to known classes are treated as inliers based on which the model is trained. Other objects that do not belong to these classes are treated as outliers, which the model is supposed to detect based on its training. Results of the experiments conducted using Caltech-256 <ref type="bibr" target="#b8">[9]</ref> and MNIST <ref type="bibr" target="#b17">[18]</ref> datasets are reported and comparisons with state-ofthe-art outlier detection models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b21">21]</ref> are provided.</p><p>Results on Caltech-256. <ref type="figure" target="#fig_4">Figure 4b</ref> shows outlier examples reconstructed using G. It is interesting to observe that although the generated images are of reasonably good quality, our model still depicts superior results in terms of F 1 score and area under the curve (AUC), as listed in <ref type="table">Table 1</ref>, which demonstrates that our model is robust to the over-training of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MNIST.</head><p>As it is a well-studied dataset, various outlier detection related works use MNIST as a steppingstone to evaluate their approaches. Following <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b41">41]</ref>, we also report F 1 score as an evaluation metric of our method on this dataset. A comparison provided in <ref type="figure" target="#fig_5">Figure 5</ref> shows that our approach performs robustly to detect outliers even when the percentage of outliers is increased. An insight of the performance improvement by our approach is shown in <ref type="figure" target="#fig_6">Figure 6</ref>. It can be observed that as the phase two training continues, score distribution of inliers and outliers output by our network smoothly distributes to a wider range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Anomaly Detection in Videos</head><p>One-class classifiers are finding their best applications in the domain of anomaly detection for surveillance purposes <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b35">35</ref>]. However, this task is more complicated than the outlier detection because of the involvement of moving objects, which cause variations in appearance.    Experimental setup. Each frame I of the Ped2 dataset is divided into grayscale patches X I = {X 1 , X 2 , ..., X n } of size 45 × 45 pixels. Normal videos, which only contain scenes of walking pedestrians, are used to extract training patches. Test patches are extracted from abnormal videos which contain abnormal as well as normal scenes. In order to remove unnecessary inference of the patches, a motion detection criteria based on frame difference is set to discard patches without motion. A maximum of all patch-level anomaly scores is declared as the frame-level anomaly score of that particular frame as:</p><formula xml:id="formula_8">A I = max X D(G(X)), where X ∈ X I<label>(9)</label></formula><p>Performance evaluation. Frame-level AUC and EER are the two evaluation metrics used to compare our approach with a series of existing works <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b41">41]</ref> published within last 5 years. The corresponding results provided in <ref type="table" target="#tab_3">Table  2</ref> and <ref type="table" target="#tab_5">Table 3</ref> show that our method outperforms recent state-of-the-art methodologies in the task of anomaly detection. Comparing with the baseline, our approach achieves an absolute gain of 5.2% in terms of AUC. Examples of the reconstructed patches are provided in <ref type="figure" target="#fig_4">Figure 4</ref>. As shown in <ref type="figure" target="#fig_4">Figure 4b</ref>, although G generates noticeably good reconstructions of anomalous inputs, due to the presence of our proposed pseudo-anomaly module, D gets to learn the underlying patterns of reconstructed anomalous images. This is why, in contrast to the baseline, our framework provides consistent performance across a wide range of training epochs <ref type="figure" target="#fig_0">(Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>When to stop phase one training? The convergence of our framework is not strictly dependent on phase one training. <ref type="figure" target="#fig_7">Figure 7</ref> shows the AUC performance of phase two training applied after various epochs of phase one on Ped2 dataset <ref type="bibr" target="#b3">[4]</ref>. Values plotted at iterations = 0, representing the performance of the baseline, show a high variance. Interestingly, it can be seen that after few iterations into phase two training of our proposed approach, the model starts to converge better. Irrespective of the initial epoch in phase one training, models converged successfully showing consistent AUC performances. When to stop phase two training? As seen in <ref type="figure" target="#fig_7">Figure 7</ref> and <ref type="figure" target="#fig_8">Figure 8</ref>, it can be observed that once a specific model  is converged, further iterations do not deteriorate its performance. Hence, a model can be trained for any number of iterations as deemed necessary. Which low epoch generator is better? For the selection of G old , as mentioned earlier, the generator after the 1 st epoch of training was arbitrarily chosen in our experiments. This selection is intuitive and mostly based on the fact that the generator has seen all dataset once. In addition, we visually observed that after first epoch, although the generator was capable of reconstructing its input, the quality was not good enough, which is a suitable property for G old in our model. However, this way of selection is not a generalized solution across various datasets. Hence, to investigate the matter further, we evaluate a range of low epoch numbers as candidates for G old . The baseline epoch of G is kept fixed throughout this experiment. Results in <ref type="figure" target="#fig_8">Figure 8a</ref> show that irrespective of the low epoch number chosen as G old , the model converges and achieves state-of-the-art or comparable AUC. In pursuit of another more systematic way to obtain G old , we also explored the possibility of using average Method AUC Method AUC Unmasking <ref type="bibr" target="#b48">[48]</ref> 82.2% TSC <ref type="bibr" target="#b25">[25]</ref> 92.2% HybridDN <ref type="bibr" target="#b28">[28]</ref> 84.3% FRCN action <ref type="bibr" target="#b10">[11]</ref> 92.2% Liu et al <ref type="bibr" target="#b23">[23]</ref> 87.5% AbnormalGAN <ref type="bibr" target="#b35">[35]</ref> 93.5% ConvLSTMAE <ref type="bibr" target="#b24">[24]</ref> 88.1% MemAE <ref type="bibr" target="#b6">[7]</ref> 94.1% Ravanbakhsh et al <ref type="bibr" target="#b34">[34]</ref> 88.4% GrowingGas <ref type="bibr" target="#b46">[46]</ref> 94.1% ConvAE <ref type="bibr" target="#b9">[10]</ref> 90% FFP <ref type="bibr" target="#b22">[22]</ref> 95.4% AMDN <ref type="bibr" target="#b52">[52]</ref> 90.8% ConvAE+UNet <ref type="bibr" target="#b27">[27]</ref> 96.2% Hashing Filters <ref type="bibr" target="#b58">[58]</ref> 91% STAN <ref type="bibr" target="#b19">[19]</ref> 96.5% AE Conv3D <ref type="bibr" target="#b59">[59]</ref> 91.2% Object-centric <ref type="bibr" target="#b12">[13]</ref> 97.8% Baseline 92.9% Ours 98.1%  parameters of all previous G models. Hence, for each given epoch of the baseline that we pick as G, a G old is formulated by taking an average of all previous G models until that point. The results plotted in <ref type="figure" target="#fig_8">Figure 8b</ref> show that such G old also depicts comparable performances. Note that this formulation completely eradicates the need of handpicking a specific epoch number for G old , thus making our formulation generic towards the size of a training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation</head><p>Ablation results of our framework on UCSD Ped2 dataset <ref type="bibr" target="#b3">[4]</ref> are summarized in <ref type="table">Table 4</ref>. As shown, while each input component of our training model (i.e. real images X, high quality reconstructionsX, low quality re-constructionsX low , and pseudo anomaly reconstructionŝ X pseudo ) contributes towards a robust training, removing any of these at a time still shows better performance than the baseline. One interesting observation can be seen in the fourth column of the phase two training results. In this case, the performance is measured after we remove the last step of pseudo-anomaly module, which is responsible for providing regenerated pseudo-anomaly (X pseudo ) through</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase one</head><p>Phase two X -</p><formula xml:id="formula_9">X X low - - X pseudo - - - - X asX pseudo - - - - - AUC</formula><p>92.9% 94.4% 95.1% 95.9% 88.5% 98.1% <ref type="table">Table 4</ref>: Frame-level AUC performance ablation of our framework on UCSD Ped2 dataset.</p><p>G, as in Equation 4. Hence, by removing this part, the fake anomalies (X) obtained using Equation 3 are channeled directly to the discriminator as one of the two sets of bad reconstruction examples. With this configuration, the performance deteriorates significantly (i.e. 9.6% drop in the AUC). The model shows even worse performance than the baseline after phase one training. This shows the significance of our proposed pseudo-anomaly module. Once pseudo-anomalies are created within the module, it is necessary to obtain a regeneration result of these by inferring G. This helps D to learn the underlying patterns of reconstructed anomalous images, which results in a more robust anomaly detection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents an adversarially learned approach in which both the generator (G) and the discriminator (D) are utilized to perform a stable and robust anomaly detection. A unified G and D model employed towards such problems often produces unstable results due to the adversary. However, we attempted to tweak the basic role of the discriminator from distinguishing between real and fake to discriminating between good and bad quality reconstructions, a formulation that aligns well with the philosophy of conventional anomaly detection using generative networks. We also propose a pseudo-anomaly module which is employed to create fake anomaly examples from normal training data. These fake anomaly examples help D to learn about the behavior of G in the case of unusual input data.</p><p>Our extensive experimentation shows that the approach not only generates stable results across a wide range of training epochs but also outperforms a series of state-ofthe-art methods <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b41">41]</ref> for outliers and anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Dynamics of AUC performance over training epochs: The baseline shows high fluctuations while our approach not only shows stability across various epochs but also yields higher AUC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>igure 2 :</head><label>2</label><figDesc>Our proposed OGNet framework. Phase one is the baseline training, carried out to obtain a reasonably trained state of G and D. A frozen low epoch state (G old ) of the generator is stored during this training. In phase two, only D is updated to distinguish between good and bad quality reconstructions. Good quality examples correspond to real training images as well as the images reconstructed using G while bad quality examples are obtained using G old as well as the proposed pseudoanomaly module. This module assists D to learn the underlying patterns of anomalous input reconstructions. During test, inferences are carried out through G and D only and the output of D is considered as anomaly score. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Example images from different stages of our framework. (a) Left to right: Original image (X), high quality reconstructed (X), low quality reconstructed (X low ), pseudo anomaly (X), pseudo anomaly reconstructed (X pseudo ). (b) Left column shows outlier / anomaly examples whereas right column shows respective regenerated outputs G(X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>F 1 score results on MNIST dataset. Compared to state-of-the-art, our method retains superior performance even with an increased percentage of outliers at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Anomaly score distribution on MNIST dataset over various training iterations of our framework. Divisibility of inliers and outliers is improved significantly as phase two of the training proceeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The plot shows frame level AUC performance of our phase two training, starting after different epochs of phase one (baseline) training. The model after phase two training shows significantly less variance than baseline/phase one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Results from a series of experiments on UCSD Ped2 dataset show that our framework is not dependent on a strict choice of epoch number for G old . In (a), various G old selected at a varied range of epochs are experimented with a fixed G. In (b), an average of parameters from all past generators is taken as G old .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>EER results comparison with existing works on UCSD Ped2 dataset. Lower numbers mean better results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Frame-level AUC comparison on UCSD Ped2 dataset with state-of-the-art works published in last 5 years. Best and second best performances are highlighted as bold and underlined, respectively. Experiments with the G old taken across first 10 epochs, where G is kept fixed.</figDesc><table><row><cell>1.35</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the ICT R&amp;D program of MSIP/IITP. [2017-0-00306, Development of Multimodal Sensor-based Intelligent Systems for Outdoor Surveillance Robots]. Also, we thank HoChul Shin, Ki-In Na, Hamza Saleem, Ayesha Zaheer, Arif Mahmood, and Shah Nawaz for the discussions and support in improving our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning object motion patterns for anomaly detection and improved object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Gritai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keyed learning: An adversarial learning frameworkformalization, challenges, and anomaly detection applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Bergadano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETRI Journal</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="608" to="618" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lof: identifying density-based local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM sigmod record</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ucsd pedestrian dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="909" to="926" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abnormal detection using interaction energy potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3161" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online detection of abnormal events using incremental coding length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayanta</forename><surname>Kumar Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonny</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3619" to="3627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tube convolutional neural network (t-cnn) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5822" to="5831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object-centric auto-encoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a space-time mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1446" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding anomalies with generative adversarial networks for a patrolbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wallace</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esube</forename><surname>Bekele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>AT&amp;T Labs [On-</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<ptr target="http://yann.lecun.com/exdb/mnist" />
		<title level="m">Available</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stan: Spatiotemporal adversarial networks for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Hak Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1323" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust computation of linear models by convex relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="363" to="410" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classifier two sample test for video anomaly detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional lstm for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Somboon Hongeng, and Ramakant Nevatia. Event detection and analysis from video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Brémond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="873" to="889" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Trong-Nguyen Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hybrid deep network for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Trong Nguyen Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06347</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Trajectory-based anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><forename type="middle">Luca</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1544" to="1554" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coherence pursuit: Fast, simple, and robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George K Atia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="6260" to="6275" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Plug-and-play cnn for crowd motion analysis: An application in abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training adversarial discriminators for crosschannel abnormal event detection in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1896" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised behaviorspecific dictionary learning for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Huamin Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Søren Ingvor Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="28" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localisation based on the sparsity and reconstruction error of auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Hoseini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1122" to="1124" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Hoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep-cascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1992" to="2004" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Alon Shoshan, and Lihi Zelnik-Manor. Adversarial feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firas</forename><surname>Shama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep appearance features for abnormal behavior detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dual principal component pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manolis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Tsakiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1511" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust pca via outlier pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2496" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Provable self-representation based outlier detection in a union of subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ensemble grid formation to detect potential anomalous regions using context encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Muhammad Zaigham Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Ik</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Control, Automation and Systems (ICCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="661" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning semantic scene models by object classification and trajectory clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1940" to="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
