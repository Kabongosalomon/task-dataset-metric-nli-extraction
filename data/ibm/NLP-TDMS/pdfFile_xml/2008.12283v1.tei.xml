<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity and Evidence Guided Relation Extraction for DocRED</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
							<email>kevin.huang3@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research †</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
							<email>guangtao.wang@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research †</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
							<email>tengyuma@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research †</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
							<email>jing.huang@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research †</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Entity and Evidence Guided Relation Extraction for DocRED</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level relation extraction is a challenging task which requires reasoning over multiple sentences in order to predict relations in a document. In this paper, we propose a joint training framework E2GRE (Entity and Evidence Guided Relation Extraction) for this task. First, we introduce entity-guided sequences as inputs to a pretrained language model (e.g. BERT, RoBERTa). These entityguided sequences help a pretrained language model (LM) to focus on areas of the document related to the entity. Secondly, we guide the fine-tuning of the pretrained language model by using its internal attention probabilities as additional features for evidence prediction. Our new approach encourages the pretrained language model to focus on the entities and supporting/evidence sentences. We evaluate our E2GRE approach on DocRED, a recently released large-scale dataset for relation extraction. Our approach is able to achieve state-ofthe-art results on the public leaderboard across all metrics, showing that our E2GRE is both effective and synergistic on relation extraction and evidence prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation Extraction (RE), the problem of extracting relations between pairs of entities in plain text, has received increasing research attention in recent years <ref type="bibr" target="#b35">(Zhang et al., 2017;</ref><ref type="bibr" target="#b36">Zhao et al., 2019;</ref><ref type="bibr" target="#b9">Guo et al., 2019)</ref>. It has important downstream applications to many other Natural Language Processing (NLP) tasks, such as Knowledge Graph <ref type="bibr">Construction (Trisedya et al., 2019)</ref>, Information Retrieval, Question Answering <ref type="bibr" target="#b32">(Yu et al., 2017)</ref> and Dialogue Systems <ref type="bibr" target="#b31">(Young et al., 2018)</ref>.</p><p>The majority of existing RE datasets focus on predicting intra-sentence relations, i.e., extracting relations between entity pairs in the same sentence. For example, SemEval-2010 Task 8 <ref type="bibr" target="#b11">(Hendrickx et al., 2010)</ref>, and TACRED <ref type="bibr" target="#b35">(Zhang et al., 2017)</ref> are two popular RE datasets with intra-sentence relations. These datasets have facilitated much research progress in this area such as <ref type="bibr" target="#b0">Alt et al., 2019;</ref><ref type="bibr" target="#b36">Zhao et al., 2019)</ref> on SemEval-2010 Task 8 and <ref type="bibr" target="#b35">(Zhang et al., 2017;</ref><ref type="bibr" target="#b9">Guo et al., 2019;</ref><ref type="bibr" target="#b1">Baldini Soares et al., 2019;</ref><ref type="bibr" target="#b13">Joshi et al., 2019)</ref> on TACRED. However, in real world applications, the majority of relations are expressed across sentences. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example from the Do-cRED dataset <ref type="bibr" target="#b28">(Yao et al., 2019a)</ref>, which requires reasoning over three evidence sentences to predict the relational fact that "The Legend of Zelda", is the publisher of "Link".</p><p>In this paper, we focus on the document-level re-lation extraction problem and design a method to facilitate document-level reasoning. We work on the DocRED <ref type="bibr" target="#b28">(Yao et al., 2019a)</ref>, a recent large-scale document-level relation extraction dataset. This dataset is annotated with a set of named entities and relations, as well as a set of supporting/evidence sentences for each relation. Over 40% of the relations in DocRED require reasoning over multiple sentences. And supporting/evidence sentences can be used to provide an auxiliary task for explainable relation extraction.</p><p>A natural attempt to solve this problem is to fine-tune the large pretrained Language Models (LMs) (e.g., GPT <ref type="bibr" target="#b19">(Radford et al., 2019)</ref>, <ref type="bibr">BERT (Devlin et al., 2019a)</ref>, XLnet <ref type="bibr" target="#b27">(Yang et al., 2019)</ref>, RoBERTa (Yinhan Liu, 2020)), a paradigm that has proven to be extremely successful for many NLP tasks. For example, all recent papers on Do-cRED have used BERT as an encoder to obtain the state-of-the-art results <ref type="bibr">(Tang et al., 2020;</ref><ref type="bibr" target="#b15">Nan et al., 2020)</ref>. However, naively adapting pretrained LMs for document-level RE faces a key issue that limits its performance. Due to the length of a given document, there are more entities pairs with meaningful relations in document-level relation extraction than in the intra-sentence relation extraction. A pretrained LM has to simultaneously encode information regarding all pairs of entities for relation extraction. Therefore, attention values that the pretrained LM gives over all the tokens are more uniform for document-level RE compared to intrasentence RE. This problem of having more uniform attention values limits the model's ability to extract information from relevant tokens from the document, limiting the effectiveness of the pretrained LM.</p><p>In order to mitigate this problem, we propose our novel Entity and Evidence Guided Relation Extraction (E2GRE). For each entity in a document, we generate a new input sequence by appending the entity to the beginning of a document, and then feed it into the pretrained LM. Thus, for each document with N e entities, we generate N e entity-guided input sequences for training. By introducing these new training inputs, we encourage the pretrained LM to focus on the entity that is appended to the start of the document. We further exploit the pretrained LM by directly using internal attention probabilities as additional features for evidence prediction. The joint training of relation extraction and evidence prediction helps the model locate the correct semantics that are required for relation extraction. Both of these ideas take advantage of pretrained LMs in order to make full use of pretrained LMs for our task. Our main contribution is to propose the E2GRE approach, which consists of the two main ingredients below:</p><p>1. For every document, we generate multiple new inputs to feed into a pretrained language model: we concatenate every entity with the document and feed it as an input sequence to the language model. This allows the finetuning of the internal representations from the pretrained LM to be guided by the entity.</p><p>2. We further propose to use internal BERT attention probabilities as additional features for the evidence prediction. This allows the finetuning of the internal representations from the pretrained LM to be also guided by evidence/supporting sentences.</p><p>Each of these ideas give a significant boost in performance and by combining them, we are able to achieve the state-of-the-art results on DocRED leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relation Extraction</head><p>Relation Extraction is a long standing problem in NLP that has garnered significant research attention. Early work attempts to solve this problem used statistical methods with different types of feature engineering <ref type="bibr" target="#b33">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b2">Bunescu and Mooney, 2005)</ref>. Afterwards, neural models have shown better performance at capturing semantic relationship between entities. These methods include CNN-based approaches <ref type="bibr" target="#b34">(Zeng et al., 2014;</ref> and LSTM approaches <ref type="bibr" target="#b3">(Cai et al., 2016)</ref>.</p><p>On top of using CNNs/LSTM encoders, previous models add more layers to take advantage of these embeddings. For example, <ref type="bibr" target="#b10">Han et al. (2018)</ref> introduced using hierarchical attentions in order to generate relational information from coarse-tofine semantic ideas; <ref type="bibr" target="#b35">Zhang et al. (2017)</ref> applied GCN over the pruned dependency trees, and <ref type="bibr" target="#b9">Guo et al. (2019)</ref> introduced Attention Guided Graph Convolutional Networks (AG-GCNs) over dependency trees. These models have shown good performance on intra-sentence relation extraction, however, some of them are not easily adapted for intersentence document-level RE. <ref type="bibr" target="#b14">Li et al. (2016)</ref>; ; <ref type="bibr" target="#b17">Peng et al. (2017)</ref> were among the early work on cross sentences and document-level relation extraction. Most approaches for document-level RE are graph-based neural network methods.  first introduced a document graph being used for document-level RE; <ref type="bibr" target="#b17">Peng et al. (2017)</ref> proposed a graph-structured LSTM for cross-sentence n-ary relation extraction; and <ref type="bibr" target="#b20">(Song et al., 2018)</ref> further extended the approach to graph-state LSTM. In <ref type="bibr" target="#b12">(Jia et al., 2019)</ref>, an entity-centric, multi-scale representation learning on entity/sentence/document-level LSTM model was proposed for document-level n-ary RE task. <ref type="bibr" target="#b4">Christopoulou et al. (2019)</ref> recently proposed a novel edge-oriented graph model that deviates from existing graph models. <ref type="bibr" target="#b15">Nan et al. (2020)</ref> proposed an induced latent graph to perform document-level relation extraction on DocRED. These graph models generally focus on constructing unique nodes and edges, and have the advantage of connecting different granularity of information and aggregate them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pretrained Language Models</head><p>Pretrained Language Models (LMs) are powerful tools which emerged in recent years. Recent pretrained LMs <ref type="bibr" target="#b19">(Radford et al., 2019;</ref><ref type="bibr" target="#b6">Devlin et al., 2019a;</ref><ref type="bibr" target="#b27">Yang et al., 2019;</ref><ref type="bibr" target="#b30">Yinhan Liu, 2020)</ref> are Transformer-based <ref type="bibr">(Vaswani et al., 2017)</ref>, and trained with enormous amounts of data. <ref type="bibr" target="#b7">(Devlin et al., 2019b)</ref> was the first large pretrained transformer-based LM to be released, and immediately get the state-of-the-art performance on a number of NLP tasks. New pretrained LM models such as XLNet <ref type="bibr" target="#b27">(Yang et al., 2019)</ref> and RoBERTa (Yinhan Liu, 2020) further increase the performance on the most NLP tasks.</p><p>In order to take advantage of the large amounts of text that these models have seen, we finetune all of the weights inside the model. Finetuning on large pretrained LMs has been shown to be effective on relation extraction <ref type="bibr" target="#b24">(Wadden et al., 2019)</ref>. Generally, large pretrained LMs are used to encode a sequence and then generate the representation of a head/tail entity pair to learn a classification <ref type="bibr" target="#b8">(Eberts and Ulges, 2019;</ref><ref type="bibr" target="#b29">Yao et al., 2019b)</ref>. Baldini Soares et al. <ref type="formula" target="#formula_0">(2019)</ref> introduced a new concept similar to BERT called "matching-the-black" and pretrained a Transformer-like model for relation learning. The models were fine-tuned on SemEval-2010 Task 8 and TACRED achieved state-of-the-art results. Our method aims to improve the effectiveness of a pretrained LMs, and directly influence the finetuning of the pretrained LMs with our entity and evidence guided approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we introduce our E2GRE method. We first describe how to generate entity-guided inputs in Section 3.1. Then, we present the entityguided RE (Relation Extraction) in Section 3.2. Finally, we describe the entity and evidence-guided joint training for RE in Section 3.3. We use BERT as an embodiment of a pretrained LM, and use BERT when describing our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entity-Guided Input Sequences</head><p>The relation extraction task is to predict the relation between each pair of head entity and tail entity in a given document.</p><p>We design the entity-guided inputs to give BERT more guidance towards the entities when finetuning. Each training input is organized by concatenating the tokens of the first mention of a single entity, denoted by H (named Concatenated Head Entity), together with the document tokens D, to form: "[CLS]"+ H + "[SEP]" + D + "[SEP]", which is then fed into BERT. We generate such input sequences for each entity in the given document. Therefore, for a document with N e entities, N e new entity-guided input sequences are generated and fed into BERT separately.</p><p>Due to BERT's sequence length constraint of 512 tokens, if the length of the training input is longer than 512, we make use of a sliding window approach over the document: we separate the input into multiple sequences. The first sequence is the original input sequence up to 512 tokens. The second sequence is the same as the first sequence, with an offset to the document, such that it can reach the end. This is shown as "[CLS]"+ H + "[SEP]" + D[offset:end] + "[SEP]". We combine these two input sequences in our model by averaging the embeddings, and compute the BERT attention probabilities of the tokens twice in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity-Guided Relation Extraction</head><p>For a given training input, we have one head entity, which corresponds with the concatenated entity H in the input, and N e − 1 different tail entities, <ref type="figure">Figure 2</ref>: Diagram of our E2GRE framework. As shown in the diagram, we pass an input sequence consisting of an entity and document into BERT. We extract head and tails for relation extraction. We have learned relation vector weights shown in green. We also extract out sentence, relation vectors, and BERT attention probabilities for evidence predictions.</p><p>which are located within the document D. Our method predicts N e − 1 different relations for each training input, corresponding to N e − 1 head/tail entity pairs.</p><p>After passing a training input through BERT, we extract out the head entity embedding and a set of tail entity embeddings from the BERT output. We average the embeddings over the concatenated head entity tokens to obtain the head entity embedding h. This is shown as the Head Extraction in <ref type="figure">Fig. 2</ref>. In order to extract the k-th tail entity embedding t k , we locate the indices of the tokens of k-th tail entity, average the output embeddings of BERT at these indices to get t k (i.e., Tail Extraction in <ref type="figure">Fig.  2)</ref>.</p><p>After obtaining the head entity embedding h ∈ R d and all tail entity embeddings {t k |t k ∈ R d } in a entity-guided sequence, where 1 ≤ k ≤ N e − 1, we feed them into a bilinear layer with the sigmoid activation function to predict the probability of i-th relation between the head entity h and the k-th tail entity t k , denoted byŷ ik , as followŝ</p><formula xml:id="formula_0">y ik = δ(h T W i t k + b i )<label>(1)</label></formula><p>where δ is the sigmoid function, W i and b i are the learnable parameters corresponding to i-th relation, where 1 ≤ i ≤ N r , and N r is the number of relations.</p><p>Finally, we finetune BERT with a multi-label cross-entropy loss as follow:</p><formula xml:id="formula_1">L RE = − 1 N r 1 N e − 1 Ne−1 k=1 Nr i=1 (y ik log(ŷ ik ) + (1 − y ik )log(1 −ŷ ik ))<label>(2)</label></formula><p>During inference, the goal of relation extraction is to predict a relation for each pair of head/tail entity within a document. For a given entity-guided input sequence of "[CLS]"+ entity + "[SEP]" + document + "[SEP]", the output of our model is a set of N e − 1 relation predictions. We combine the predictions from every sequence generated from the same document and with different head entity, in order to obtain all relation predictions over the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evidence Guided Relation Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Evidence Prediction</head><p>Evidence/supporting sentences are the sentences containing important supporting facts for predicting the correct relationships between head and tail entities. Therefore, evidence prediction is a good auxiliary task to relation extraction and also provides explainability for the model.</p><p>The objective of evidence prediction is to predict whether a given sentence is evidence/supporting sentence for a given relation. Let N s be the number of sentences in the document. We first obtain the sentence embedding s ∈ R N S ×d by averaging all the embeddings of the words in s (i.e., Sentence Extraction in <ref type="figure">Fig. 2)</ref>. These word embeddings are derived from the BERT output embeddings.</p><p>Let r i ∈ R d be the relation embedding of ith relation (1 ≤ i ≤ N r }), which is initialized randomly and learnable in our model. We employ a bilinear layer with sigmoid activation function to predict the probability of the j-th sentence s j being a supporting sentence w.r.t. the given i-th relation r i as follows.</p><formula xml:id="formula_2">f i jk = s j W r i r i + b r î y i jk = δ(f i jk W r o + b r o )<label>(3)</label></formula><p>where s j represents the embedding of jth sentence, W r i /b r i and W r o /b r o are the learnable parameters w.r.t. i-th relation. We define the loss of evidence prediction under the given i-th relation as follows:</p><formula xml:id="formula_3">L Evi = − 1 N t 1 N s Nt k=1 Ns j=1 (y i jk log(ŷ i jk ) + (1 − y i jk )log(1 −ŷ i jk ))<label>(4)</label></formula><p>where y j ik ∈ {0, 1}, and y j ik = 1 means that sentence j is an evidence for inferring i-th relation. It should be noted that in the training stage, we use the embedding of true relation in Eq. 3. In testing/inference stage, we use the embedding of the relation predicted by the relation extraction model in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Evidence-guided Finetuning with BERT Attention Probabilities</head><p>Internal attention probabilities of BERT help locate the areas within a document where the BERT model focuses on. Therefore, these probabilities can guide the language model to focus on relevant areas of the document for relation extraction (See the attention visualization in Section 4.5). In fact, we find that the areas with higher attention values are usually come from the supporting sentences. Therefore, we believe these attention probabilities can be helpful for evidence prediction. For each pair of head h and tail t k , we make use of the attention probabilities extracted from the last l internal BERT layers for evidence prediction. Let Q ∈ R N h ×L×(d/N h ) be the query and K ∈ R N h ×L×(d/N h ) be the key of the multi-head self attention layer, N h be the number of attention heads as described in <ref type="bibr">(Vaswani et al., 2017)</ref>, L be the length of the input sequence (i.e., the length of entity-guided sequence defined in Section 3.2) and d being the embedding dimension. We first extract the output of multi-headed self attention (MHSA) A ∈ R N h ×L×L from a given layer in BERT as follows. These extraction outputs are shown as "Attention Extractor" in <ref type="figure">Fig. 2</ref>.</p><formula xml:id="formula_4">Attention = softmax( QK T d/N h ) (5) Att-head i = Attention(QW Q i , KW K i ) (6) A = Concat(Att-head i , · · · , Att-head n ) (7)</formula><p>For a given pair of head h and tail t k , we extract the attention probabilities corresponding to head and tail tokens to help relation extraction. Specifically, we concatenate the MHSAs for the last l BERT layers extracted by Eq. 7 to form an attention probability tensor as:Ã k ∈ R l×N h ×L×L .</p><p>Then, we calculate the attention probability representation of each sentence under a given head-tail entity pair as follows.</p><p>1. We first apply maximum pooling layer along the attention head dimension (i.e., second dimension) overÃ k . The max values are helpful to show where a specific attention head might be looking at. Afterwards we apply mean pooling over the last l layers. We obtaiñ A s = 1 l l i=1 maxpool(Ã ki ),Ã s ∈ R L×L from these two steps.</p><p>2. We then extract the attention probability tensor from the head and tail entity tokens according to the start and end positions of in the document. We average the attention probabilities over all the tokens for the head and tail embeddings to obtainÃ sk ∈ R L .</p><p>3. Finally, we generate sentence representations fromÃ sk by averaging over the attentions of each token in a given sentence from the document to obtain a sk ∈ R Ns</p><p>Once we get the attention probabilities a sk , we combine a sk with the evidence prediction result y s ik of sentence s from Eq. 3 to form the new sentence representation and feed it into a bilinear layer with sigmoid for evidence sentence prediction as follows:</p><formula xml:id="formula_5">ŷ ia k = δ(a sk W a i f i k + b a i )<label>(8)</label></formula><p>where f i k is the vector of fused representation of sentence embeddings and relation embeddings for a given head/tail entity pair.</p><p>Finally, we define the loss of evidence prediction under a given i-th relation based on attention probability representation as follows:</p><formula xml:id="formula_6">L a Evi = − 1 N t 1 N s Nt k=1 Ns j=1 (y ia jk log(ŷ ia jk ) + (1 − y ia jk )log(1 −ŷ ia jk ))<label>(9)</label></formula><p>whereŷ ia jk is the j-th value ofŷ ia k computed by Eq. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Joint Training with Evidence Prediction</head><p>We combine the relation extraction loss and attention probability guided evidence prediction loss as the final objective function for the joint training:</p><formula xml:id="formula_7">Loss = L RE + λ 1 * L a Evi<label>(10)</label></formula><p>where λ 1 &gt; 0 is the weight factor to make tradeoffs between two losses, which is data dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present the experimental results of our model E2GRE and compare with previously established baselines and published results, as well as the public leaderboard results on DocRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>DocRED <ref type="bibr" target="#b29">(Yao et al., 2019b</ref>) is a large documentlevel data set for the tasks of relation extraction and evidence sentence prediction. It consists of 5053 documents, 132375 entities, and 56354 relations mined from Wikipedia articles. For each (head, tail) entity pair, there are 97 different relation types as the candidates to predict. The first relation type is an "NA" relation between two entities, and the rest of them corresponds to a WikiData relation name. Each of the head/tail pair that contain valid relations also include a set of supporting/evidence sentences.</p><p>We follow the same setting in <ref type="bibr" target="#b29">(Yao et al., 2019b)</ref> to split the data into Train/Validation/Test for model evaluation to make a fair comparison. The number of documents in Train/Validation/Test is 3000/1000/1000, respectively.</p><p>The dataset is evaluated with the metrics of relation extraction RE F1, and evidence Evi F1. There are also instances where relational facts may occur in the validation and train set, and so we also evaluate on the Ign RE F1, which removes these relational facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>hyper-parameter Setting. The configuration for the BERT-base model follows the setting in <ref type="bibr" target="#b6">(Devlin et al., 2019a)</ref>. We set the learning rate as 1e-5, λ 1 as 1e-4, the hidden dimension of the relation vectors as 108, and extract internal attention probabilities from last three BERT layers.</p><p>We conduct most of our experiments by finetuning the BERT-base model. The implementation is based on the PyTorch <ref type="bibr" target="#b16">(Paszke et al., 2017)</ref> implementation of BERT 1 . We run our model on a single V100 GPU for 60 epochs, resulting in approximately one day of training. The DocRED baseline and our E2GRE model have 115M parameters 2 . Baseline Methods. We compare our model with the following published models. 1. Context Aware BiLSTM. <ref type="bibr" target="#b29">Yao et al. (2019b)</ref> introduced the original baseline to DocRED in their paper. They used a context-aware BiLSTM (+ additional features such as entity type, coreference and distance) to encode the document. Head and tail entities are then extracted for relation extraction. 2. BERT Two-Step. <ref type="bibr" target="#b25">Wang et al. (2019)</ref> introduced finetuning BERT in a two-step process, where the model first does predicts the NA relation, and then predicts the rest of the relations. 3 . 3. HIN. <ref type="bibr">Tang et al. (2020)</ref> introduced using a hierarchical inference network to help aggregate the information from entity to sentence and further to document-level in order to obtain semantic reasoning over an entire document. 4. BERT+LSR. <ref type="bibr" target="#b15">Nan et al. (2020)</ref> introduced using an induced latent graph structure to help learning how the information should flow between entities and sentences within a document. <ref type="table" target="#tab_1">Table 1</ref>, our method E2GRE is the current state-of-the-art model on the public leaderboard for DocRED. <ref type="table" target="#tab_3">Table 2</ref> compares our method with the baseline models. From <ref type="table" target="#tab_3">Table 2</ref>, we observe that our E2GRE method is not only competitive to the previous best methods on the development set, but also holds the following advantages over previous models.  Model Ign F1(%) RE F1(%) Evi F1 Context-Aware <ref type="bibr" target="#b29">(Yao et al., 2019b)</ref> 48.94 51.09 -BERT Two-Step <ref type="bibr" target="#b25">(Wang et al., 2019) -</ref>54.42 -HIN-BERT <ref type="bibr">(Tang et al., 2020)</ref> 54.29 56.31 -BERT + LSR <ref type="bibr" target="#b15">(Nan et al., 2020)</ref> 52  Our E2GRE method is not only competitive to the previous best methods on the development set, but also holds the following advantages over previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>• Our method is more intuitive and simpler in design compared to the HIN model and BERT+LSR model. In addition, our method provides interpretable relation extraction with supporting evidence prediction.</p><p>• Our method is also better than all other models on the Ign RE F1 metric. This shows that our model does not memorize relational facts between entities, but rather examine relevant areas in the document to generate the correct relation extraction.</p><p>Compared to the original BERT baseline, our training time is slightly longer, due to the multiple new entity-guided input sequences. We examined with the idea of generating new sequences based on each head and tail entity pair, but such a method would scale quadratically with the number of entities in the document. Using our entity-guided approach strikes a balance between performance and the training time. <ref type="table" target="#tab_5">Table 3</ref> shows the ablation study of our method on the effectiveness of entity-guided and evidence-guided training.  The baseline here is the joint training model of relation extraction and evidence prediction with BERT-base. We see that the entity-guided BERT improves the over this baseline by 2.5%, and evidenceguided training further improve the method by 1.7%. This shows that both parts of our method are important to the overall E2GRE method. Our E2GRE method not only obtains improvement on the relation extraction F1, but it also obtains significant improvement on evidence prediction compared to this baseline. This further shows that our evidence-guided finetuning method is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Method Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Rec <ref type="formula">(</ref>  Analysis of Number of BERT Layers. We also conduct experiments to analyze the impact of the number of BERT layers used for obtaining attention probability values, see the results in <ref type="table" target="#tab_7">Table 4</ref>. From this table, we observe that using more layers is not necessarily better for relation extraction. One possible reason may be that the BERT model encodes more syntactic information in the middle layers <ref type="bibr" target="#b5">(Clark et al., 2019)</ref>. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example from the validation set of our model. In this example, the relation between "The Legend of Zelda" and "Link" relies on information across multiple sentences in the given document. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the attention heatmap of naively applying BERT for relation extraction. This heatmap shows the attention of each word receives from 'The Legend of Zelda" and "Link". We observe that the model is able to locate the relevant areas of "Link" and "Legend of Zelda series", but the attention values over the rest of the document are very small. Therefore, the model has trouble in extracting out information within the document to generate a correct relation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Attention Visualizations</head><p>In contrast, <ref type="figure" target="#fig_2">Fig. 4</ref> shows that our E2GRE model highlights the evidence sentences, particularly in the areas where it finds relevant information. Phrases related to "Link" and "The Legend of Zelda series" are assigned with the higher weights. Words (such as"protagonist" or "involves") linking these phrases together are also highly weighted. Moreover, the scale of the attention probabilities for E2GRE is also much larger for E2GRE compared to the baseline. All of these phrases and bridging words are located within the evidence sentences, and make our model better at evidence prediction as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In order to more effectively exploit pretrained LMs for document-level RE, we propose a new approach called E2GRE (Entity and Evidence Guided Relation Extraction). We first generate new entityguided sequences to feed into a LM, focusing the model on the relevant areas in the document. Then we utilize the internal attentions extracted from the last l layers to help guide an LM to focus on relevant areas of the document. Our E2GRE method improves performance on both RE and evidence prediction on DocRED dataset, and achieves the state-of-the-art performance on the DocRED public leaderboard.</p><p>For future work, we plan to incorporate our ideas on using attention-guided multi-task learning to other NLP tasks with evidence sentences. Combining our approach with graph-based models for NLP tasks is another interesting direction to explore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Legend of Zelda : The Minish Cap ( ) is an action -adventure game and the twelfth entry in The Legend of Zelda series. [1] Developed by Capcom and Flagship , with Nintendo overseeing the development process , it was released for the Game Boy Advance handheld game console in Japan and Europe in 2004 and in North America and Australia the following year . [2] In June 2014 , it was made available on the Wii U Virtual Console . [3] The Minish Cap is the third Zelda game that involves the legend of the Four Sword , expanding on the story of and . [4] A magical talking cap named Ezlo can shrink series protagonist Link to the size of the Minish , a bug -sized race that live in Hyrule . [5] The game retains some common elements from previous Zelda installments , such as the presence of Gorons , while introducing Kinstones and other new gameplay features . [6] The Minish Cap was generally well received among critics . [7] It was named the 20th best Game Boy Advance game in an IGN feature , and was selected as the 2005 Game Boy Advance Game of the Year by GameSpot . Head Entity: The Legend of Zelda Tail Entity: Link Relation: "Publisher" Evidence Sentences: 0,3,4 An exemplar document in DocRED datasets where a head and tail entity pair span across multiple sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Baseline BERT attention heatmap over the tokenized document of a DocRED example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>E2GRE's attention heatmap over the tokenized document of a DocRED example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Top public leaderboard numbers on DocRED. Our E2GRE method uses RoBERTa-large.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Results of relation extraction on the super-</cell></row><row><cell>vised setting of DocRED. Shown above are compar-</cell></row><row><cell>isons between E2GRE, and other published models on</cell></row><row><cell>the validation set with BERT-base as the pretrained lan-</cell></row><row><cell>guage model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the entity-guided vs evidence-guided RE. BERT+Joint Training is the BERT baseline with joint training of RE and evidence prediction. Results are evaluated on the validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on different numbers of layers of attention probabilities from BERT that are used for evidence prediction. Results are evaluated on the validation set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/huggingface/pytorch-pretrained-BERT2  We will release the code after paper review. 3 BERT Two-Step is an arxiv preprint</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving relation extraction by pre-trained language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
		<idno>abs/1906.03088</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1072</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1906.04341</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1024</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical relation extraction with coarse-to-fine grained attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1247</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP, Brussels</title>
		<meeting><address><addrLine>Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1370</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL, Minneapolis</title>
		<meeting><address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno>abs/1907.10529</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Database</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Valencia, Spain. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">N-ary relation extraction using graphstate LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1246</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fang Fang, Shi Wang, and Pengfei Yin. 2020. Hin: Hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural relation extraction for knowledge base enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Bayu Distiawan Trisedya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1023</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Florence, Italy. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. NeurIPS, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1123</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ro{bert}a: A robustly optimized {bert} pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal Yinhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations. Under review</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Augmenting end-to-end dialog systems with commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Cambria</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subham</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved neural relation detection for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kazi Saidul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1053</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Vancouver, Canada. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="571" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118693.1118703</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving relation classification by entity pair graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaiyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
