<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Residual Networks with Exponential Linear Unit</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Shah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Veermata Jijabai Technological Institute Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Shinde</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Veermata Jijabai Technological Institute Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eashan</forename><surname>Kadam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Veermata Jijabai Technological Institute Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hena</forename><surname>Shah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Veermata Jijabai Technological Institute Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandip</forename><surname>Shingade</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Veermata Jijabai Technological Institute Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Residual Networks with Exponential Linear Unit</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Very deep convolutional neural networks introduced new problems like vanishing gradient and degradation. The recent successful contributions towards solving these problems are Residual and Highway Networks. These networks introduce skip connections that allow the information (from the input or those learned in earlier layers) to flow more into the deeper layers. These very deep models have lead to a considerable decrease in test errors, on benchmarks like ImageNet and COCO. In this paper, we propose the use of exponential linear unit instead of the combination of ReLU and Batch Normalization in Residual Networks. We show that this not only speeds up learning in Residual Networks but also improves the accuracy as the depth increases. It improves the test error on almost all data sets, like CIFAR-10 and CIFAR-100.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Vision Community has been mesmerized by the effectiveness of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref> that have led to a breakthrough in computer visionrelated problems. Hence, there has been a notable shift towards CNNs in many areas of computer vision <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Convolutional neural networks were popularized through AlexNet <ref type="bibr" target="#b9">[10]</ref> in 2009 and their much celebrated victory at the 2012 ImageNet competiton <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. After that, there have been several attempts at building deeper and deeper CNNs like the VGG network and GoogLeNet in 2014 which have 19 and 22 layers respectively <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>. But, very deep models introduce problems like vanishing and exploding gradients <ref type="bibr" target="#b2">[3]</ref>, which hamper their convergence.</p><p>The vanishing gradient problem is trivial in very deep networks. During the backpropagation phase, the gradients are computed by the chain rule. Multiplication of small numbers in the chain rule leads to an exponential decrease in the gradient. Due to this, very deep networks learn very slowly. Sometimes, the gradient in the earlier layer gets larger because derivatives of some activation functions can take larger values. This leads to the problem of exploding gradient. These problems have been reduced in practice through normalized initialization <ref type="bibr" target="#b2">[3]</ref> and most recently, Batch Normalization <ref type="bibr" target="#b3">[4]</ref>.</p><p>Exponential linear unit (ELU) <ref type="bibr" target="#b8">[9]</ref> also reduces the vanishing gradient problem. ELUs introduce negative values which push the mean activation towards zero. This reduces the bias shift and speeds up learning. ELUs give better accuracy and learning speed-up compared to the combination of ReLU <ref type="bibr" target="#b7">[8]</ref> and Batch Normalization <ref type="bibr" target="#b3">[4]</ref>.</p><p>After reducing the vanishing/exploding gradient problem, the networks start converging. However, the accuracy degrades in such very deep models <ref type="bibr" target="#b0">[1]</ref>. The most recent contributions towards solving this problem are Highway Networks <ref type="bibr" target="#b6">[7]</ref> and Residual Networks <ref type="bibr" target="#b0">[1]</ref>. These networks introduce skip connections, which allow information flow into the deeper layers and enable us to have deeper networks with better accuracy. The 152-layer ResNet outperforms all other models <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this paper, we propose to use exponential linear unit instead of the combination of ReLU and Batch Normalization. Since exponential linear units reduce the vanishing gradient problem and give better accuracy compared to the combination of ReLU and Batch Normalization, we use it in our model to further increase the accuracy of Residual Networks. We also notice that ELU speeds up learning in very deep networks as well. We show that our model increases the accuracy on datasets like CIFAR-10 and CIFAR-100, compared to the original model. It is seen that as the depth increases, the difference in accuracy between our model and the original model increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Deeper neural networks are very difficult to train. The vanishing/exploding gradients problem impedes the convergence of deeper networks <ref type="bibr" target="#b2">[3]</ref>. This problem has been solved by normalized initialization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. A notable recent contribution towards reducing the vanishing gradients problem is Batch Normalization <ref type="bibr" target="#b3">[4]</ref>. Instead of normalized initialization and keeping a lower learning rate, Batch Normalization makes normalization a part of the model and performs it for Once the deeper networks start converging, a degradation problem occurs. Due to this, the accuracy degrades rapidly after it is saturated. The training error increases as we add more layers to a deep model, as mentioned in <ref type="bibr" target="#b1">[2]</ref>. To solve this problem, several authors introduced skip connections to improve the information flow across several layers. Highway Networks <ref type="bibr" target="#b6">[7]</ref> have parameterized skip connections, known as information highways, which allow information to flow unimpeded into deeper layers. During the training phase, the skip connection parameters are adjusted to control the amount of information allowed on these highways.</p><p>Residual Networks (ResNets) [1] utilize shortcut connections with the help of identity transformation. Unlike Highway Networks, these neither introduce extra parameter nor computation complexity. This improves the accuracy of deeper networks. With increasing depth, ResNets give better function approximation capabilities as they gain more parameters. The authors' hypothesis is that the plain deeper networks give worse function approximation because the gradients vanish when they are propagated through many layers. To fix this problem, they introduce skip connections to the network. Formally, If the output of i th layer is H i and F represents multiple convolutional transformation from layer i − 1 to i, we obtain</p><formula xml:id="formula_0">H i = ReLU (F(H i−1 ) + id(H i−1 ))<label>(1)</label></formula><p>where id(·) represents the identity function and ReLU <ref type="bibr" target="#b7">[8]</ref> is the default activation function. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the basic building block of a Residual Network which consists of multiple convolutional and Batch Normalization layers. The identity transformation, id(·) is used to reduce the dimensions of H i−1 to match those of F(H i−1 ). In Residual Networks, the gradients and features learned in earlier layers are passed back and forth between the layers via the identity transformations id(·). Exponential Linear Unit (ELU) <ref type="bibr" target="#b8">[9]</ref> alleviates the vanishing gradient problem and also speeds up learning in deep neural networks which leads to higher classification accuracies. The exponential linear unit (ELU) is</p><formula xml:id="formula_1">f (x) = x if x &gt; 0 α(exp(x) − 1) if x ≤ 0</formula><p>The ReLUs are non-negative and thus have mean activations larger than zero, whereas ELUs have negative values, which push the mean activations towards zero. ELUs saturate to a negative value when the input gets smaller. This decreases the forward propagated variation and information, which draws the mean activations to zero. Units with nonzero mean activations act as a bias for the next layer. If these units do not cancel each other out, then the learning causes a bias shift for units in the next layer. Therefore, ELUs decrease the bias shift as the mean activations are closer to zero. Less bias shift also speeds up learning by bringing standard gradient closer towards the unit natural gradient. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the comparison of ReLU and ELU (α = 1.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Residual Networks with Exponential Linear Unit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ResNet Architecture</head><p>The Residual Network in <ref type="bibr" target="#b0">[1]</ref> is a functional composition of L residual blocks (ResBlocks), each encoding the update rule (1). <ref type="figure" target="#fig_0">Fig 1 shows</ref> the schematic illustration of the i th ResBlock. In this example, F consists of a sequence of layers: Conv-BN-ReLU-Conv-BN, where Conv and BN stands for Convolution and Batch Normalization respectively. This construction scheme is adopted in all our experiments while reproducing the results of <ref type="bibr" target="#b0">[1]</ref>. The function F is parameterized by some set of parameters W i , which we omit for notational simplicity. Normally, we use 64, 32 or 16 filters in the convolutional layers. The size of receptive field is 3 × 3. Although it does not seem attractive but, in practice it gives better accuracy without adding any overhead costs, as compared to plain networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ResNet with ELU</head><p>In comparison with the ResNet model <ref type="bibr" target="#b0">[1]</ref>, we use Exponential Linear Unit (ELU) in place of a combination of ReLU with Batch Normalization. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates our different experiments with ELUs in ResBlock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Conv-ELU-Conv-ELU</head><p>In this model, F consists of a sequence of layers: Conv-ELU-Conv-ELU. <ref type="figure" target="#fig_2">Fig. 3a</ref> represents the basic building block of this experiment. We trained our model using the specification mentioned in 4.1. But we found that after few iterations, the gradients blew up. When the learning rate is decreased, the 20-layer model starts converging but to very less accuracy. The deeper models like 56 and 110layer still do not converge after decreasing the learning rate. This model clearly fails as the trivial problem of exploding gradient can not be reduced in very deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">ELU-Conv-ELU-Conv</head><p>This is a full pre-activation unit ResBlock <ref type="bibr" target="#b27">[28]</ref> with ELU. The sequence of layers is ELU-Conv-ELU-Conv. <ref type="figure" target="#fig_2">Fig. 3b</ref> highlights the basic ResBlock of this experiment. During the training of this model too, the gradients exploded after few iterations. Due to the exponential function, the gradients get larger and lead to exploding gradient problem. Even decreasing the learning rate also does not reduce this problem. We decided to add a Batch Normalization layer before Addition to control this problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Conv-ELU-Conv-BN and ELU after Addition</head><p>To control the exploding gradient, we added a Batch Normalization before addition. So, the sequence of layers in this ResBlock is Conv-ELU-Conv-BN and ELU after addition. <ref type="figure" target="#fig_2">Fig. 3c</ref> represents the ResBlock used in this experiment. Thus in this ResBlock, the update rule (1) for the i th layer is</p><formula xml:id="formula_2">H i = ELU (F(H i−1 ) + id(H i−1 ))<label>(2)</label></formula><p>The Batch Normalization layer reduces the exploding gradient problem found in the previous two models. We found that this model gives better accuracy for 20-layer model. However, as we increased the depth of the network, the accuracy degrades for the deeper models. If the ELU activation function is placed after addtion, then the mean activation of the output pushes towards zero. This could be beneficial. However, this forces each skip connection to perturb the output. This has a harmful effect and we found that this leads to degradation of accuracy in very deep ResNets.  <ref type="figure" target="#fig_2">Fig. 3d</ref> gives an illustration of the basic building block of our model. Thus in our model, F represents the following sequence of layers: Conv-ELU-Conv-BN. The update rule (1) for the i th layer is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Conv-ELU-Conv-BN and No ELU after Addition</head><formula xml:id="formula_3">H i = F(H i−1 ) + id(H i−1 )<label>(3)</label></formula><p>This is the basic building block for all our experiments on CIFAR-10 and CIFAR-100 datasets. We show that not including ELU after addition does not degrade the accuracy, unlike the previous model. This ResBlock improves the learning behavior and the classification performance of the Residual Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We empirically demonstrate the effectiveness of our model on a series of benchmark data sets: CIFAR-10 and CIFAR-100. In our experiments, we compare the learning behavior and the classification performance of both the models on the CIFAR-10 and CIFAR-100 datasets. The experiments prove that our model outperforms the original ResNet model in terms of learning behavior and classification performance on both the datasets. Finally, we compare the classification performance of our model with other previously published state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CIFAR-10 Analysis</head><p>The first experiment was performed on the CIFAR-10 dataset <ref type="bibr" target="#b9">[10]</ref>, which consists of 50k training images and 10k test images in 10 classes. In our experiments, we performed training on the training set and evaluation on the test set.</p><p>The inputs to the network are 32 × 32 images which are color-normalized. We use a 3 × 3 receptive field in the convolution layer. We use a stack of 6n layers with 3×3 convolution on the feature maps of sizes {32, 16, 8} respectively, with 2n on each feature map. The number of filters are {16, 32, 64} respectively. The original ResNet model ends with a global average pooling, a 10-way fully-connected layer and a softmax layer. In our model, we add an ELU activation function just before the global average pooling layer.</p><p>These two models are trained on a AWS g2.2xlarge instance (which has a single GPU) with a mini batch-size of 128. We use a weight decay of 0.0001 and a momentum of 0.9, and adopt the weight initialization in <ref type="bibr" target="#b4">[5]</ref> and BN <ref type="bibr" target="#b3">[4]</ref> but with no dropout. We start with a learning rate of 0.1 and divide by 10 after 81 epochs, and again divide by 10 after 122 epochs. We use the data augmentation mentioned in <ref type="bibr" target="#b17">[18]</ref> during the training phase: Add 4 pixels on each side and do a random 32 × 32 crop from the padded image or its horizontal flip. During the testing phase, we only use a color-normalized 32 × 32 image. Our experiments are executed on 20, 32, 44, 56 and 110-layer networks.  <ref type="figure" target="#fig_4">5</ref> shows the comparison of learning behaviours between our model and the original ResNet model on CIFAR-10 dataset for 20, 32, 44, 56 and 110-layers. The graphs prove that for all the different number of layers, our model possesses a superior learning behavior and converges many epochs before the original model. As the depth of the model increases, our model also learns faster than the original model. The difference between the learning rate of these two models increases as the depth increases. Comparing <ref type="figure" target="#fig_4">Fig. 5a</ref> and <ref type="figure" target="#fig_4">Fig. 5e</ref>, we can easily notice the huge difference in learning rates for 20-layer and 110-layer models. After 125 epochs, both the models converge to almost the same value. But, our model has a slightly lower training loss compared to the original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Classification Performance</head><p>Fig <ref type="figure" target="#fig_6">. 6</ref> illustrates the comparison of classification performance between our model and the original one on CIFAR-10 dataset for 20, 32, 44, 56 and 110 layers. We observe that for the 20-layer model, the test error is nearly the same for both the models. But, as the depth increases, our model significantly outperforms the original model. <ref type="table" target="#tab_0">Table 1</ref> shows the test error for both the models from the epoch with the lowest validation error. <ref type="figure" target="#fig_6">Fig. 6</ref> shows that the gap between the test error of the two models increases as the depth is also increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CIFAR-100 Analysis</head><p>Similar to CIFAR-10, the CIFAR-100 dataset <ref type="bibr" target="#b9">[10]</ref> also contains 32 × 32 images with the same train-test split, but from 100 classes. For both the original model and our model, the experimental settings are exactly the same as those of CIFAR-10. We trained only for the 110-layer models as it gives us state-of-the-art results. <ref type="figure" target="#fig_5">Fig. 7a</ref> shows that for CIFAR-100 dataset as well, our model learns faster than the original ResNet model. The original model yields a test error of 27.23%, which is already state-of-the-art in <ref type="table">Table 2</ref>: Test error (%) of our model compared to other most competitive methods previously published. All these methods apply standard data augmentation on CIFAR-10 and CIFAR-100 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 CIFAR-100</head><p>Maxout <ref type="bibr" target="#b19">[20]</ref> 9.38 -DropConnect <ref type="bibr" target="#b18">[19]</ref> 9.32 -Net in Net <ref type="bibr" target="#b20">[21]</ref> 8.81 -Deeply Supervised <ref type="bibr" target="#b17">[18]</ref> 7.97 -Frac. Pool <ref type="bibr" target="#b21">[22]</ref> -27.62 All-CNN <ref type="bibr" target="#b15">[16]</ref> 7.25 -Learning-Activation <ref type="bibr" target="#b22">[23]</ref> 7.51 30.83 R-CNN <ref type="bibr" target="#b23">[24]</ref> 7.09 -Scalable BO <ref type="bibr" target="#b24">[25]</ref> 6.37 27.40 Highway Networks <ref type="bibr" target="#b25">[26]</ref> 7.60 32.24 Gen. Pool <ref type="bibr" target="#b26">[27]</ref> 6.05 -  CIFAR-100 with standard data augmentation. Our model reduces the test error to 26.55% and is again one of the best published single model performances. <ref type="figure" target="#fig_5">Fig. 7b</ref> shows that the test error of our model is much lower from the starting epoch itself. <ref type="table">Table 2</ref> shows the comparison of our result with other previously published results on the CIFAR-10 and CIFAR-100 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we introduce Residual Networks with exponential linear units which learn faster than the current Residual Networks. They also give better accuracy than the original ones when the depth is increased. On datasets like CIFAR-10 and CIFAR-100, we improve beyond the current state-of-the-art in terms of test error, while also learning faster than these models using ELUs. ELUs push the mean activations towards zero as they introduce small negative values. This reduces the bias shift and increases the learning speed. Our experiments show that not only does our model have superior learning behavior, but it also provides better accuracy as compared to the current model on CIFAR-10 and CIFAR-100 datasets. This enables the researchers to use very deep models and also increase their learning behavior and classification performance at the same time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An i th Residual Block in a Residual Networks each mini-batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The rectified linear unit (ReLU) and Exponential Linear Unit (ELU, α = 1.0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Conv-ELU-Conv-ELU (b) ELU-Conv-ELU-Conv (c) Conv-ELU-Conv-BN and ELU after Addition (d) Conv-ELU-Conv-BN and No ELU after Addition An i th Residual Block with Exponential Linear Unit (ELU) in Residual Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of test error on CIFAR-10 for the original ResNet model and when ELU is placed after addition in our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Fig. 4depicts the effects of including ELU after addition in this ResBlock. Comparison of the learning behavior of our model and the original ResNet model on CIFAR-10 dataset. We compare the results for 20, 32, 44, 56 and 110-layers and show that our model significantly outperforms the original ResNet model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of our model and the original ResNet model on CIFAR-100 dataset. We show that our model has superior learning behavior and classification performance compared to the original ResNet model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of the classification performance of our model and the original ResNet model for 20, 32, 44, 56, and 110-layers. We observe that our model outperforms the original one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test error (%) of our model compared to the original ResNet model. The test error of the original ResNet model refers to our reproduction of the experiments by He et al. [1]</figDesc><table><row><cell cols="3">Layers Original ResNet ResNet with ELU</cell></row><row><cell>20</cell><cell>8.38</cell><cell>8.32</cell></row><row><cell>32</cell><cell>7.51</cell><cell>7.30</cell></row><row><cell>44</cell><cell>7.17</cell><cell>6.93</cell></row><row><cell>56</cell><cell>6.97</cell><cell>6.31</cell></row><row><cell>110</cell><cell>6.42</cell><cell>5.62</cell></row><row><cell cols="2">4.1.1 Learning Behavior</cell><cell></cell></row><row><cell>Fig.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks at constrained time cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page">950</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by Exponential Linear Units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<title level="m">Deeplysupervised nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13</title>
		<editor>Dasgupta, S., Mcallester, D.</editor>
		<meeting>the 30th International Conference on Machine Learning (ICML-13</meeting>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<title level="m">Maxout networks</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<title level="m">Fractional max-pooling</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6830</idno>
		<title level="m">Learning activation functions to improve deep neural networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">33673375</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05700</idno>
		<title level="m">Scalable bayesian optimization using deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08985</idno>
		<title level="m">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity Mappings in Deep Residual Networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
