<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Amend Facial Expression Representation via De-albino and Affinity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications Nanjing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications Nanjing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Amend Facial Expression Representation via De-albino and Affinity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial Expression Recognition (FER) is a classification task that points to face variants. Hence, there are certain intimate relationships between facial expressions. We call them affinity features, which are barely taken into account by current FER algorithms. Besides, to capture the edge information of the image, Convolutional Neural Networks (CNNs) generally utilize a host of edge paddings. Although they are desirable, the feature map is deeply eroded after multi-layer convolution. We name what has formed in this process the albino features, which definitely weaken the representation of the expression. To tackle these challenges, we propose a novel architecture named Amend Representation Module (ARM). ARM is a substitute for the pooling layer. Theoretically, it could be embedded in any CNN with a pooling layer. ARM efficiently enhances facial expression representation from two different directions: 1) reducing the weight of eroded features to offset the side effect of padding, and 2) sharing affinity features over minibatch to strengthen the representation learning. In terms of data imbalance, we designed a minimal random resampling (MRR) scheme to suppress network overfitting. Experiments on public benchmarks prove that our ARM boosts the performance of FER remarkably. The validation accuracies are respectively 90.55% on RAF-DB, 64.49% on Affect-Net, and 71.38% on FER2013, exceeding current state-of-theart methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial expression is the external reflection of emotion, and in essence, the movement of subcutaneous muscles. In interpersonal communication, the facial expression is one of the most important media for people to convey feelings and attitudes to each other. Therefore, facial expression recognition is of great significance for human-computer interaction.</p><p>With GPU performance growing drastically, deep learning has come a long way in just over a decade. Gradu- ally, deep learning has become the mainstream method to promote FER. Relying on the famous neural networks in the classification era, some scholars put forward their algorithms to deal with the challenges of FER. Wang et al. <ref type="bibr" target="#b34">[35]</ref> propose Self-Cure Network based on ResNet-18 <ref type="bibr" target="#b13">[14]</ref> to suppress the uncertainties of data annotation in network training. The well-known neural networks applied to FER have achieved certain effects, whereas they cannot fully give play to the learning advantages of the deep network because of the limitation to the scale of the FER dataset. Creating larger datasets brings prohibitive annotation costs. How to improve the recognition accuracy from the limited datasets has become a major challenge for FER.</p><p>Not only that, but we find that certain intrinsic properties of the convolutional neural network limit and hinder its ability. To encode information accurately on the image, the edge information cannot be ignored. However, the convolution kernel perceives the edges of the image, especially the corners, less than the inside. The way the convolutional kernel works makes the weight of the information on the edges lower than that in the middle. We call the negligent property of convolutional kernel as perception bias. Therefore, it is accustomed to utilize padding, a simple process of adding zeros to the periphery of the images, to balance the perceptual frequency of each pixel. The same is true for the feature map. When the network is fairly shallow, it seems unnecessary to consider the influence of a few zeros on the formation of features, because the image pixels are relatively huge. Normally, as the network deepens, the channels increase exponentially. Conversely, the area of the feature map keeps shrinking. The pixel information becomes scarce and refined. That makes the deep convolutional layers show less and less receptive to the fade of the edges, as edge pixels account for a considerable part of the total. The only countermeasure is to keep padding to ensure the capture of key information. Since then, the network has fallen into the padding trap. The essence of convolution is to calculate the sum of the products of the pixel values in each region and those of the kernel. Excessive zeros involved will inevitably lead to serious information distortion. This is a serious issue that we have been neglecting in the past. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, just one convolution operation with padding, the periphery of the image becomes blurred visible to the naked eye. Layer by layer, padding gradually erodes the feature map from outside to inside. We call the information that gravely impaired in this way albino features. Actually, the unprocessed albino features directly affect the representations downward.</p><p>Hence, in order to avoid the problems mentioned above, we need to consider not only the internal correlation hide in the FER datasets but also how to eliminate the erosion of representation. In this work, we propose a module that can be embedded in any network with a pooling layer, called Amend Representation Module (ARM), which consists of an auxiliary block and two functional blocks.</p><p>To protect representation from erosion, we remove the pooling layer and replace it with De-albino (DA) block. The Feature Arrangement (FA) block, customized to assist the DA block to work better, arranges the raw feature map into a single channel under the premise of minimum position change of feature pixels. As shown in <ref type="figure">Figure 2</ref>, all the pixels that are severely eroded are concentrated in the outermost part of the feature map. Then we take advantage of the perception bias of the convolutional kernel to subtly reduce the weight of the albino features. The perception bias is positively correlated with the size of the convolutional layer kernel. It is worth noting that padding is resolutely denied throughout the module. Because we regard it as the culprit of the problem.</p><p>Secondly, inspired by the nature of facial expressions, which are variants of the human face, we consider that there are certain affinities among expression features. We explore the role of this relationship and obtain a benign response of FER performance. Sharing Affinity (SA) block combines a certain proportion of global average features for each expression, which improves the feature learning of the classic network on the FER dataset. <ref type="figure">Figure 2</ref>: A case to demonstrate our rearrangement method for feature map. This process only changes the absolute position of the feature points, and the relative position remains. The channel collapses in a ratio of 4. We name the gathered points which are originally at the same position as a feature cluster.</p><p>Most of the previous FER strategies are to extend the network structure or supplement the algorithm on the classic CNNs. While our ARM is an internal modification of the Convolutional Neural Network (CNN). By training the ARM variant of ResNet-18 <ref type="bibr" target="#b13">[14]</ref> on RAF-DB, we promote the performance of pre-trained ResNet-18 from 86.4 to 90.55, which is superior to the-state-of-art methods.</p><p>Our contribution is mainly divided into the following parts:</p><p>(1) We put forward for the first time the Padding Challenge, which is to eliminate the albino erosion existing in the padding convolution.</p><p>(2) We propose a de-albino strategy, which serves all the convolutional neural networks, to protect representations from albino features.</p><p>(3) We elaborately design an auxiliary block to rearrange the feature map, enhancing the de-albino effect.</p><p>(4) We make the first attempt to optimize the FER with affinity features among different facial expressions.</p><p>(5) We design a minimal random resampling scheme to deal with data imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional Neural Networks</head><p>In the late 1990s, Yann LeCun et al. <ref type="bibr" target="#b19">[20]</ref>, in order to solve the task of handwritten digit recognition, proposed LeNet, in which he first applied BP algorithm to the training of neural network structure. This feat established the modern structure of CNN: convolutional layer, pooling layer, and fully connected layer. However, ascribe to the lack of data and computing power, LeNet is difficult to deal with complex problems. Despite the effectiveness of identifying numbers, LeNet is not as good as machine learning algorithms in practical tasks.</p><p>Gradually, with the rapid progress of the GPU unit and the completion of large-scale datasets, CNN came back to the attention of researchers. In the 2012 ImageNet Image Recognition Contest, AlexNet, a brand new structure introduced by Hinton et al. <ref type="bibr" target="#b18">[19]</ref>, was the winner by an absolute advantage of 10.9 percentage points over the runner-up. It came to use padding to balance the weights of the pixels between the edges and the inside. Instead of Sigmoid, they applied the ReLU <ref type="bibr" target="#b12">[13]</ref> activation function to solve the gradient dispersion of the deep network. The proposed Dropout and Data Augmentation methods <ref type="bibr" target="#b18">[19]</ref> are effective in preventing overfitting. It was the first time that people realized that the structure of neural networks had a lot of room for improvement. In the years that followed, various CNNs sprang up.</p><p>In the 2014 ILSVRC Competition, GoogLeNet <ref type="bibr" target="#b31">[32]</ref>, the winner, was characterized by the application of the Network in Network structure, that is, the original node is also a Network. Karen Simonyan and Andrew Zisserman <ref type="bibr" target="#b30">[31]</ref> who proposed the VGG architecture won second place. They tended to use smaller kernels to deepen the network. This attempt effectively improved the performance of the model. It was also proved that VGG had a good ability to generalize on other datasets and outperformed GoogLeNet in multiple transfer learning tasks. In 2015, He Kaiming et al. <ref type="bibr" target="#b13">[14]</ref> from MSRA solved the degradation problem of the extremely deep network through the cross-layer transmission of Identity. They deepened the network to hundreds or even thousands of layers. The error rate also dropped to 3.6%, lower than the human rate of 5.1%. In the last ILSVRC competition, Hu J et al. <ref type="bibr" target="#b14">[15]</ref> proposed SENet, which recalibrates the channel dimension by squeezing features in the spatial dimension and extracting the correlation between channels, thus further reducing the error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Facial Expression Recognition</head><p>Facial Expression Recognition is a classification task of identifying the facial changes in response to human emotions from visual information. This technology enables machines to understand human emotions and make judgments in response to mental states. The research on facial expressions developed earlier in psychology and medicine. As early as the 19th century, Charles Darwin and Phillip Prodger <ref type="bibr" target="#b4">[5]</ref> explained the difference and connection of facial expressions between humans and animals. In 1971, Ekman and Friesen <ref type="bibr" target="#b7">[8]</ref> defined the six basic human expressions (that is happiness, sadness, surprise, fear, anger, and disgust), then systematically established a facial expression image library, which describes the details of each expression. In 1978, Suwa et al. <ref type="bibr" target="#b21">[22]</ref> proposed automatic expression analysis in facial video. However, due to the limitations of contemporary computer capabilities, facial expression research in the computer field started relatively late. In the 1990s, Kenji Mase <ref type="bibr" target="#b22">[23]</ref> proposed to leverage the optical flow method for automatic expression analysis, opening the computer era of facial expression recognition. In 2001, Tian et al. <ref type="bibr" target="#b32">[33]</ref> utilized action units (AU) instead of the traditional expression images to classify facial expressions, creating a precedent for automatic expression analysis.</p><p>Early feature learning methods (e.g., non-negative matrix factorization (NMF) <ref type="bibr" target="#b39">[40]</ref>, local binary patterns (LBP) <ref type="bibr" target="#b29">[30]</ref>, histograms of oriented gradients (HOG) <ref type="bibr" target="#b3">[4]</ref>, scaledinvariant feature transform (SIFT) <ref type="bibr" target="#b26">[27]</ref>, etc.) are costly and inefficient, remaining at the level of manual annotation and calculation. As datasets expand into the wild, such as FER2013 <ref type="bibr" target="#b1">[2]</ref>, AffectNet <ref type="bibr" target="#b25">[26]</ref>, EmotioNet <ref type="bibr" target="#b8">[9]</ref>, SFEW 2.0 <ref type="bibr" target="#b6">[7]</ref>, and RAF-DB <ref type="bibr" target="#b20">[21]</ref>, the engineered learning methods are difficult to cope with data extremely large-scale and complex. With the rapid development of GPU units, such algorithms have gradually faded and been replaced by deep learning. Not only learning deep features, but convolutional neural networks can also dynamically adjust neuron weights based on the deviation between the prediction and the target. Heechul Jung et al. <ref type="bibr" target="#b16">[17]</ref> design an integration network to process facial images and landmark points jointly. Huiyuan Yang et al. <ref type="bibr" target="#b36">[37]</ref> believe that expression is a superposition of neutral components and expression components. They exploit a network that generates neutral expressions to isolate residual expression components. Feifei Zhang et al. <ref type="bibr" target="#b38">[39]</ref> combine different poses and expressions to train GAN for pose-invariant expression recognition. Kai Wang et al. <ref type="bibr" target="#b34">[35]</ref> propose the Self-Cure algorithm to relabel uncertain expressions, which inhibited the network from overfitting incorrectly labeled samples. In order to recognize the occluded expressions, Bowen Pan et al. <ref type="bibr" target="#b27">[28]</ref> apply a network pretrained on an unoccluded data set to guide the fine-tuning of the occluded network. To solve the problem of label inconsistency when multiple datasets are merged, Jiabei Zeng et al. <ref type="bibr" target="#b37">[38]</ref> generate multiple pseudo-labels for images and then train the model in this environment to learn the underlying truth. Shikai Chen et al. <ref type="bibr" target="#b2">[3]</ref> generate multi-labels for label distribution learning by classifying action units and facial landmarks separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The padding is crucial to convolutional layers because of its functionality. Although boosting the performance of the network <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, it may be harmful to the representation. Sticking to the regular pooling layer does nothing about the padding's negative impacts. Conjunctively, considering the particularity of the Facial Expression Recognition (FER) task, that is, the affinity features between facial expressions, we propose the Amend Representation Module (ARM). In this section, we overview the ARM and then describe the function of each component in detail. It should be noted that the relationship between the two channels requires the de-albino kernel to be single-channel and unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given a dataset of facial expressions, firstly, we extract the feature of each image with the front part of the ResNet-18 <ref type="bibr" target="#b13">[14]</ref>, which is bounded by Global Average Pooling (GAP). Then, the learned feature map is fed into our ARM, which outputs high-quality representations.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our ARM consists of three crucial blocks, namely Feature Arrangement (FA) block, Dealbino (DA) block, and Sharing Affinity (SA) block. The FA block is an auxiliary block to amplify the function of the DA block. The latter realizes the weight distribution of features by means of convolution. Naturally, the weight with a higher degree of albino erosion is lower, and vice versa. The final SA block extracts global average representation (GAR) over mini-batch and then fuses it with individual representation according to their affinities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Regularization</head><p>Due to convolutional padding, the formation of albino features is inevitable. With reference to the position of the padding, it can be inferred that the edges and corners are where the de-albino features gather. The albino pixels are evenly dispersed in each channel, which is not conducive to "targeted therapy" because a slight expansion of the dealbino range may damage the key information. We rearrange the feature pixels into a single channel on the premise of the minimum relative position change. So that all pixels with the same degree of the albino are concentrated from the outside to the inside. The specific implementation is shown in <ref type="figure">Figure 2</ref>. Then a no-padding block is used to weaken the weight of the albino information at the edge and neutralize the baneful influence of padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Assigning Weights by Means of Convolution</head><p>To eliminate the impact of albino features, we elaborately designed a dedicated convolutional layer to dilute the weight of the peripheral pixels. It is characterized by no padding, large convolutional kernel, and big stride.</p><p>As shown in <ref type="figure">Figure 2</ref>, the most eroded points are concentrated at the edge of the rearranged feature map. We subtly exploit the perception bias illustrated in <ref type="figure" target="#fig_2">Figure 4</ref> to reduce the weights of albino points. The pixels at the edge are perceived less frequently by the convolutional kernel than the internal ones. Considering the absolutely large scale of the concentrated albino points, a regular-sized convention kernel is not suitable for our method. And it can be concluded that the larger the convolutional kernel, the more information is ignored, which happens to favor the network in this case. Because of abundant channel information flowing into space, the convolutional kernel that we take should be several times larger than the regular one. Obviously, the convolutional kernel has to be single-channel, consistent with the feature map. The stride of the kernel also has an effect on the perception of peripheral points. A shorter stride means less neglect, which limits the effectiveness of our de-albino block. In the trade-off between ignoring the albino features and retaining the key information, we set the convolution stride to about half of the kernel size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Sharing Affinity</head><p>Different from other classification tasks, facial expression recognition (FER) is a task based on face variants. Since the face is the main carrier, there are affinity features between the facial expressions. Convolutional neural networks tend to assign weights to features that favor evaluation indicators, which makes the learning short-sighted. We force the network to learn the global representation of the face to improve the recognition of unpopular categories.</p><p>We introduce the SA block to optimize the representation learning of the network. Firstly, given a batch of facial expressions, we extract the representations with FA and DA blocks, froming a set of representations R = {a i } N i=1 , where N and a i denote the batch size and the original representation of the i-th image, respectively. Affinity is extracted by averaging global representations, generating the global average representation a GAR as follows:</p><formula xml:id="formula_0">a GAR = N i=1 a i N<label>(1)</label></formula><p>And then we incorporate it into the representation of each expression as follows:</p><formula xml:id="formula_1">a i,SA = a i + 位a GAR<label>(2)</label></formula><p>where a i,SA denotes the synthetic representation output by SA block. The affinity is denoted as 位, which controls the contribution of the global average representation a GAR to a SA . 位 is a tunable parameter, which can be either a fixed value or a ratio that learns as the network iterates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, the ARM, proposed deep learning architecture, is evaluated on three FER datasets. We first explain why we choose these datasets and what they are. A description of metrics followed closely. We introduce the overall details of the experiment and indicate the fine-tuning on the AffectNet dataset. Then we conduct a verification experiment to demonstrate the adverse effect of albino features. To evaluate each block of the ARM, an ablation study is performed on RAF-DB dataset. Finally, our ARM is compared with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To evaluate our module, we select three public FER datasets collected in the wild that fully reflect real-world emotions. Compared with those produced in the laboratory, there is no deliberate pose before generating. They are more complex, variable, and challenging for feature learning. The details are as follows.</p><p>RAF-DB <ref type="bibr" target="#b20">[21]</ref> is a crowdsourced dataset. Each image was individually labeled by 40 taggers for the sake of rigor. There are two distinct subsets, the basic one being singletagged and the compound one being double-tagged. Only the former is used in our experiment. The basic dataset contains seven categories: surprise, fear, disgust, happiness, sadness, anger, and neutral. The number of the training set and testing set is 12,271 and 3,068 respectively, and the expressions of them have near-identical distribution.</p><p>AffectNet <ref type="bibr" target="#b25">[26]</ref> is the largest facial expression dataset so far, with a total of more than one million images. The main collection methods are searching through three search engines and downloading from the Internet. Images are annotated in two ways: manual and machine. Additionally, it contains ten categories, eight of which are basic expressions. In the manually labeled subset, excluding the extra contempt, the same seven basic expressions as in RAF-DB are used, namely 283,901 training images and 3,500 validation images. We also conduct eight types of basic expression experiments for better comparison. Because of the lack of a testing set, we fill this void with the validation set. Remarkably, while the categories are balanced on the validation set, they are extremely unbalanced on the training set, with a maximum gap of over 35 times.</p><p>FER2013 [2] contains 28,709 training samples, 3,589 validation samples and 3,589 test samples, all of which are 48x48 pixel grayscale faces. These facial images also fall into seven basic categories and have close distribution in three sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics and Results</head><p>The overall sample accuracy is the most common and straightforward metric, namely accuracy or weighted accuracy <ref type="bibr" target="#b33">[34]</ref>. In simple terms, it is calculated by dividing the number of correctly predicted samples by that of testing samples. Unlike the former, mean class accuracy, which is also called unweighted accuracy <ref type="bibr" target="#b33">[34]</ref>, is the average of the accuracy of each category. The sum of the accuracy of each class is first calculated and then divided by the number of classes.   Ascribe to the learning bias of CNN, categories that are easy to distinguish or have a large number of samples will gain more weight in neurons <ref type="bibr" target="#b0">[1]</ref>. Under the circumstances, the overall accuracy determined by a biased category may still be at a high level, whereas its performance is certainly not as good as the accuracy appears. To prevent overall sample accuracy from masking the illusion of good per-formance, we introduced unweighted accuracy as a backup metric. <ref type="figure" target="#fig_3">Figure 5</ref> shows the confusion matrixes of the ARM (ResNet-18) on RAF-DB, AffectNet (7cls &amp; 8cls), and FER2013. It's rarely seen in other strategies that neutral hits the highest accuracy of 98% on RAF-DB, slightly exceeding the happiness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Prior to training the network, all the images are resized to 224x224x3 pixels. Specifically, the images of FER2013 are single-channel. We duplicate its channel twice to meet the requirements. The backbone network is part of ResNet-18, where the layers before the GAP are retained and initialized with weights pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref>. We leverage this part of the network to extract the albino features, which are in the size of 512x7x7.</p><p>Based on the rearrangement principle described in Section 3.2, only the feature map of two channels can be obtained because the channel number is not an integer multiple of 4. As the channels of the feature map decrease, the spatial area of the feature map increases to 112x112 pixels. It is essential to emphasize that the feature map does not ideally lie flat in a single channel, whereas its channels should still be considered parallel. Considering the correlation of the two channels, the kernel of the De-albino block must be single-channel. The point set composed of features that are from the same position of each channel is called feature clusters. It's in the size of 16x16x1.</p><p>First, there is no doubt that padding will not be in our ARM. Then a proper convolutional kernel is definitely crucial to the experimental results. Obviously, if the size of the convolution kernel is slightly larger than that of the feature cluster, sometimes a feature cluster plays an absolute role within the sphere of convolution kernel, which weakens the diversity of the representation. So we set the convolutional kernel as 32x32x1 pixels, much larger than the feature cluster. Meanwhile, the stride of the convolutional kernel is set to half of the cluster size, which is 8. The two channels output from the DA block are averaged as the representation of each expression.</p><p>As for the SA block, the gain hyperparameter 位 varies depending on the affinity within the dataset. By default, it is initialized to 1.0 and updated with the gradient.</p><p>According to the outputs of these three blocks, we can efficiently obtain high-quality representations from the variant of ResNet-18. To optimize the parameters of the network, we adopt the Adam solver <ref type="bibr" target="#b17">[18]</ref> with mini-batch size 256 on RAF-DB. The learning rate is initialized as 0.001, and we apply a decay learning rate strategy with a coefficient of 0.9.</p><p>The Amend Representation Module (ARM) is implemented using the PyTorch toolbox <ref type="bibr" target="#b28">[29]</ref>. All of the reported results are obtained by running the Python code on an NVIDIA GeForce RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fine-tuning on the AffectNet Dataset</head><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, the sample numbers in each category in AffectNet <ref type="bibr" target="#b25">[26]</ref> is extremely unbalanced. Intuitively speaking, in the 8 basic categories, the ratio of the largest to smallest sample number is as high as <ref type="bibr" target="#b34">35</ref>.   For superior performance, we prudently designed a minimal random resampling (MRR) strategy to balance the numbers of categories. The implementation details are as follows: First, we assign a probability of selection to each expression, which is the reciprocal of the sample size. In this case, each image has the same probability of being picked after a random selection. During training, the sample size of each category is uniformly set to be consistent with the minimum category. Although the size of our training set has shrunk greatly from the original, we retain the scale advantage of the original dataset beacause the training set is different for each epoch. After multiple epochs of training, even the largest category can basically be traversed. It should be noted that we did not discard the massive amount of original data like Undersampling because the training set of each epoch is randomly re-sampled. The Oversampling strategy is not adopted because Oversampling will duplicate the minimum category dozens of times, which inevitably leads to serious overfitting of the model. The samples inside the superclasses may be completely different in different epochs of training. After all, these classes are only sampled at a minimum of one-35th, so we reduced the learning rate by adjusting its attenuation rate from 0.9 to 0.78. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>WA UA DLP-CNN <ref type="bibr" target="#b20">[21]</ref> 84.22 -IPA2LT <ref type="bibr" target="#b37">[38]</ref> 86.77 -RAN <ref type="bibr" target="#b35">[36]</ref> 86.90 -SCN <ref type="bibr" target="#b34">[35]</ref> 87.03 -DACL <ref type="bibr" target="#b9">[10]</ref> 87.78 80.44 PSR <ref type="bibr" target="#b33">[34]</ref> 88 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method WA</head><p>GoogLeNet <ref type="bibr" target="#b11">[12]</ref> 65.20 LLHF <ref type="bibr" target="#b10">[11]</ref> 66.31 Mollahosseini et al. <ref type="bibr" target="#b24">[25]</ref> 66. <ref type="bibr" target="#b39">40</ref> Bag of Words <ref type="bibr" target="#b15">[16]</ref> 67.40 Deep-emotion <ref type="bibr" target="#b23">[24]</ref> 70.02 ARM (ResNet-18) 71.38 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">The Adverse Effect of Albino Features</head><p>For the sake of scientific rigor, we conduct an independent experiment to demonstrate the negative effect of albino features. For the network part, ResNet-18 <ref type="bibr" target="#b13">[14]</ref> was modified to meet experimental requirements. Specific implementation details are as follows: De-albino block is inserted to replace the GAP to carry out convolution processing on the output 512x7x7 feature map. In the De-albino block, the kernel size is denoted as k, adjustable and the stride is fixed as 1. Finally, keep the fully connected layer and make it adaptive.</p><p>The experimental results are shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Viewpoints can be analyzed from the figure. With the increase of k, the gain effect presents a hump-like trend which fits with the fluctuation of the perception frequency ratio between the center and the edge. Experiments on RAF-DB show beyond doubt that, by reducing the weight of albino features through our De-albino block, the quality of representation can be effectively improved, thus slightly boosting the performance of the model. On the other hand, albino features do erode the representation of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>The effectiveness of three blocks in ARM. In order to better understand the respective function of each block, we conducted an ablation study to evaluate them on RAF-DB. We show the experiment results in <ref type="table" target="#tab_3">Table 2</ref>. According to the original intention of the design, the Feature Arrangement block is auxiliary, so we did not conduct a separate experimental evaluation for it. Analyzing the data, we can draw the following points. First of all, the main functional blocks, namely the DA and SA blocks, have obvious effects, and they can improve the network performance separately. Secondly, the DA block and the SA block react differently with the FA block. With the assistance of the FA block, the DA block can increase performance by 1% again, but the SA block does not seem to benefit from it. Observing the complete coordination performance of the three blocks, it can be seen that the SA block performs significantly on high-quality representation purified by the FA block and DA block. Therefore, it is better to use the SA block when the current representation is sufficiently accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Method Comparison</head><p>In <ref type="table" target="#tab_5">Table 3</ref>, we compare the proposed method with a series of state-of-the-art baselines, which are briefly described below. IPA2LT <ref type="bibr" target="#b37">[38]</ref> introduces a method of enriching dataset, which is creating a multi-label dataset by manual annotation and model prediction, to train an endto-end network that can discover the latent truth. RAN <ref type="bibr" target="#b35">[36]</ref> proposes a cascaded attention network based on the part and the whole of the face. SCN <ref type="bibr" target="#b34">[35]</ref> relabels low-quality images to suppress the influence of annotation errors. PSR <ref type="bibr" target="#b33">[34]</ref> develops a multi-branch pyramid network, which can split images of different sizes for processing, to solve the problem of different image sizes.</p><p>The additional network parameters and running time triggered by our method are negligible. Our ARM outperforms current state-of-the-art methods by a large margin, with 90.55%, 64.49%, and 71.38% on RAF-DB, Affect-Net, and FER2013, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>For feature extraction, properly we are not able to reject padding directly because of data loss. But we can eliminate its side effects before representation by replacing the pooling layer with the De-albino block. Not only that, we also designed an auxiliary Feature Arrangement block to assist it. The Sharing Affinity block shares the global representation with each expression, which strengthens the representation learning. Of course, this is limited to tasks similar to facial expression recognition, in which, all samples are variants of human faces. There is no special customization for the de-albino method, just the perception bias of convolution. More targeted methods will be explored in our later work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) An original facial expression image, and (b) its appearance after padding convolution. The edges of the expression are eroded by the padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of Amend Representation Module (ARM). The ARM composed of three blocks replaces the pooling layer of CNN. The solid arrows indicate the processing flow of one feature map, and the dotted arrows refer to the auxiliary flow of a batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the perception bias of the nopadding convolution on 7x7 pixels. The convolution kernel and stride are 3 and 1, respectively. The numbers on the pixels indicate the perceived frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The confusion matrixes on the test set for the RAF-DB, AffectNet (7cls &amp; 8cls), and FER2013.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Evaluation of the kernel size k of the De-albino block on RAF-DB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Comparison on RAF-DB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The statistics of manually annotated images in the AffectNet training set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of the three blocks in ARM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art methods on different datasets. Two metrics are used on RAF-DB, namely Weighted Accuracy (WA) and Unweighted Accuracy (UA).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">FER-2013 face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Universit de Montreal</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label distribution learning on auxiliary label space graphs for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuedong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongchao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13984" to="13993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The expression of the emotions in man and animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Darwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Prodger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2106" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace V Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprakash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial expression recognition in the wild via deep attentive center loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Amir Hossein Farzaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2402" to="2411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local learning with deep and handcrafted features for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="64827" to="64836" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning approaches for facial emotion recognition: A case study on fer-2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Giannopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in hybridization of intelligent methods</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>Isidoros Perikos, and Ioannis Hatzilygeroudis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local learning to improve bag of visual words model for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grozea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunjeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L茅on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A preliminary note on pattern recognition of human emotional expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujimora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 4th International Joint Conference on Pattern Recognition</title>
		<meeting>The 4th International Joint Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page" from="408" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognition of facial expression from optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Mase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TRANSACTIONS on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3474" to="3483" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep-emotion: Facial expression recognition using attentional convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Abdolrashidi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01019</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sift: Predicting amino acid changes that affect protein function. Nucleic acids research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pauline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henikoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3812" to="3814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Occluded facial expression recognition enhanced through privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangfei</forename><surname>Bowen Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="566" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter W Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognizing action units for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-I</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="115" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid with super resolution for in-thewild facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Hung</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guee-Sang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung-Jeong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Hyung</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="131988" to="132001" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Suppressing uncertainties for large-scale facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6897" to="6906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4057" to="4069" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facial expression recognition by de-expression residue learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umur</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2168" to="2177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facial expression recognition with inconsistently annotated datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="222" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint pose and expression modeling for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3359" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph-preserving sparse nonnegative matrix factorization with application to facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruicong</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Flierl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqi</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
