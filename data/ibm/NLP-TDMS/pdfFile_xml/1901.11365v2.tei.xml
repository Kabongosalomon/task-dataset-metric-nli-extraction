<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noise2Self: Blind Denoising by Self-Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Batson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Royer</surname></persName>
						</author>
						<title level="a" type="main">Noise2Self: Blind Denoising by Self-Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions ("J -invariant"), it is then possible to estimate the performance of a denoiser from noisy data alone. This allows us to calibrate J -invariant versions of any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We would often like to reconstruct a signal from highdimensional measurements that are corrupted, undersampled, or otherwise noisy. Devices like high-resolution cameras, electron microscopes, and DNA sequencers are capable of producing measurements in the thousands to millions of feature dimensions. But when these devices are pushed to their limits, taking videos with ultra-fast frame rates at very low-illumination, probing individual molecules with electron microscopes, or sequencing tens of thousands of cells simultaneously, each individual feature can become quite noisy. Nevertheless, the objects being studied are often very structured and the values of different features are Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). highly correlated. Speaking loosely, if the "latent dimension" of the space of objects under study is much lower than the dimension of the measurement, it may be possible to implicitly learn that structure, denoise the measurements, and recover the signal without any prior knowledge of the signal or the noise.</p><p>Traditional denoising methods each exploit a property of the noise, such as Gaussianity, or structure in the signal, such as spatiotemporal smoothness, self-similarity, or having low-rank. The performance of these methods is limited by the accuracy of their assumptions. For example, if the data are genuinely not low rank, then a low rank model will fit it poorly. This requires prior knowledge of the signal structure, which limits application to new domains and modalities. These methods also require calibration, as hyperparameters such as the degree of smoothness, the scale of self-similarity, or the rank of a matrix have dramatic impacts on performance.</p><p>In contrast, a data-driven prior, such as pairs (x i , y i ) of noisy and clean measurements of the same target, can be used to set up a supervised learning problem. A neural net trained to predict y i from x i may be used to denoise new noisy measurements <ref type="bibr" target="#b28">(Weigert et al., 2018)</ref>. As long as the new data are drawn from the same distribution, one can expect performance similar to that observed during training. <ref type="bibr">Lehtinen et al. demonstrated that clean targets are unnecessary (2018)</ref>. A neural net trained on pairs (x i , x i ) of independent noisy measurements of the same target will, under certain distributional assumptions, learn to predict the clean signal. These supervised approaches extend to image denoising the success of convolutional neural nets, which currently give state-of-the-art performance for a vast range of image-to-image tasks. Both of these methods require an experimental setup in which each target may be measured multiple times, which can be difficult in practice.</p><p>In this paper, we propose a framework for blind denoising based on self-supervision. We use groups of features whose noise is independent conditional on the true signal to predict one another. This allows us to learn denoising functions from single noisy measurements of each object, with performance close to that of supervised methods. The same approach can also be used to calibrate traditional image denoising methods such as median filters and non-local means, <ref type="figure" target="#fig_5">Figure 1. (a)</ref> The box represents the dimensions of the measurement x. J is a subset of the dimensions, and f is a J-invariant function: it has the property that the value of f (x) restricted to dimensions in J, f (x)J , does not depend on the value of x restricted to J, xJ . This enables self-supervision when the noise in the data is conditionally independent between sets of dimensions. Here are 3 examples of dimension partitioning: (b) two independent image acquisitions, (c) independent pixels of a single image, (d) independently detected RNA molecules from a single cell. and, using a different independence structure, denoise highly under-sampled single-cell gene expression data.</p><p>We model the signal y and its noisy measurement x as a pair of random variables in R m . If J ⊂ {1, . . . , m} is a subset of the dimensions, we write x J for x restricted to J.</p><p>Definition. Let J be a partition of the dimensions {1, . . . , m} and let J ∈ J . A function f :</p><formula xml:id="formula_0">R m → R m is J-invariant if f (x) J does not depend on the value of x J . It is J -invariant if it is J-invariant for each J ∈ J .</formula><p>We propose minimizing the self-supervised loss</p><formula xml:id="formula_1">L(f ) = E f (x) − x 2 ,<label>(1)</label></formula><p>over J -invariant functions f . Since f has to use information from outside of each subset of dimensions J to predict the values inside of J, it cannot merely be the identity.</p><p>Proposition 1. Suppose x is an unbiased estimator of y, i.e. E[x|y] = y, and the noise in each subset J ∈ J is independent from the noise in its complement J c , conditional on y.</p><formula xml:id="formula_2">Let f be J -invariant. Then E f (x) − x 2 = E f (x) − y 2 + E x − y 2 . (2)</formula><p>That is, the self-supervised loss is the sum of the ordinary supervised loss and the variance of the noise. By minimizing the self-supervised loss over a class of J -invariant functions, one may find the optimal denoiser for a given dataset.</p><p>For example, if the signal is an image with independent, mean-zero noise in each pixel, we may choose J = {{1}, . . . , {m}} to be the singletons of each coordinate. Then "donut" median filters, with a hole in the center, form a class of J -invariant functions, and by comparing the value of the self-supervised loss at different filter radii, we are able to select the optimal radius for denoising the image at hand (See §3).</p><p>The donut median filter has just one parameter and therefore limited ability to adapt to the data. At the other extreme, we may search over all J -invariant functions for the global optimum:</p><formula xml:id="formula_3">Proposition 2. The J -invariant function f * J minimizing (1) satisfies f * J (x) J = E[y J |x J c ] for each subset J ∈ J .</formula><p>That is, the optimal J -invariant predictor for the dimensions of y in some J ∈ J is their expected value conditional on observing the dimensions of x outside of J.</p><p>In §4, we use analytical examples to illustrate how the optimal J -invariant denoising function approaches the optimal general denoising function as the amount of correlation between features in the data increases.</p><p>In practice, we may attempt to approximate the optimal denoiser by searching over a very large class of functions, such as deep neural networks with millions of parameters. In §5, we show that a deep convolutional network, modified to become J -invariant using a masking procedure, can achieve state-of-the-art blind denoising performance on three diverse datasets.</p><p>Sample code is available on GitHub 1 and deferred proofs are contained in the Supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Each approach to blind denoising relies on assumptions about the structure of the signal and/or the noise. We review the major categories of assumption below, and the traditional and modern methods that utilize them. Most of the methods below are described in terms of application to image denoising, which has the richest literature, but some have natural extensions to other spatiotemporal signals and to generic measurements of vectors.</p><p>Smoothness: Natural images and other spatiotemporal signals are often assumed to vary smoothly <ref type="bibr" target="#b2">(Buades et al., 2005b)</ref>. Local averaging, using a Gaussian, median, or some other filter, is a simple way to smooth out a noisy input. The degree of smoothing to use, e.g., the width of a filter, is a hyperparameter often tuned by visual inspection.</p><p>Self-Similarity: Natural images are often self-similar, in that each patch in an image is similar to many other patches from the same image. The classic non-local means algorithm replaces the center pixel of each patch with a weighted average of central pixels from similar patches <ref type="bibr" target="#b1">(Buades et al., 2005a)</ref>. The more robust BM3D algorithm makes stacks of similar patches, and performs thresholding in frequency space <ref type="bibr" target="#b4">(Dabov et al., 2007)</ref>. The hyperparameters of these methods have a large effect on performance <ref type="bibr" target="#b9">(Lebrun, 2012)</ref>, and on a new dataset with an unknown noise distribution it is difficult to evaluate their effects in a principled way.</p><p>Convolutional neural nets can produce images with another form of self-similarity, as linear combinations of the same small filters are used to produce each output. The "deep image prior" of <ref type="bibr" target="#b24">(Ulyanov et al., 2017)</ref> exploits this by training a generative CNN to produce a single output image and stopping training before the net fits the noise.</p><p>Generative: Given a differentiable, generative model of the data, e.g. a neural net G trained using a generative adversarial loss, data can be denoised through projection onto the range of the net <ref type="bibr" target="#b23">(Tripathi et al., 2018)</ref>.</p><p>Gaussianity: Recent work <ref type="bibr" target="#b30">(Zhussip et al., 2018;</ref><ref type="bibr" target="#b13">Metzler et al., 2018</ref>) uses a loss based on Stein's unbiased risk estimator to train denoising neural nets in the special case that noise is i.i.d. Gaussian.</p><p>Sparsity: Natural images are often close to sparse in e.g. a wavelet or DCT basis <ref type="bibr" target="#b3">(Chang et al., 2000)</ref>. Compression algorithms such as JPEG exploit this feature by thresholding small transform coefficients <ref type="bibr" target="#b21">(Pennebaker &amp; Mitchell, 1992)</ref>. This is also a denoising strategy, but artifacts familiar from poor compression (like the ringing around sharp edges) may occur. Hyperparameters include the choice of basis and the degree of thresholding. Other methods learn an overcomplete dictionary from the data and seek sparsity in that basis <ref type="bibr" target="#b6">(Elad &amp; Aharon, 2006;</ref><ref type="bibr" target="#b18">Papyan et al., 2017)</ref>.</p><p>Compressibility: A generic approach to denoising is to lossily compress and then decompress the data. The accuracy of this approach depends on the applicability of the compression scheme used to the signal at hand and its robustness to the form of noise. It also depends on choosing the degree of compression correctly: too much will lose important features of the signal, too little will preserve all of the noise. For the sparsity methods, this "knob" is the degree of sparsity, while for low-rank matrix factorizations, it is the rank of the matrix.</p><p>Autoencoder architectures for neural nets provide a gen-eral framework for learnable compression. Each sample is mapped to a low-dimensional representation-the value of the neural net at the bottleneck layer-then back to the original space <ref type="bibr" target="#b7">(Gallinari et al., 1987;</ref><ref type="bibr" target="#b27">Vincent et al., 2010</ref>). An autoencoder trained on noisy data may produce cleaner data as its output. The degree of compression is determined by the width of the bottleneck layer.</p><p>UNet architectures, in which skip connections are added to a typical autoencoder architecture, can capture high-level spatially coarse representations and also reproduce fine detail; they can, in particular, learn the identity function . Trained directly on noisy data, they will do no denoising. Trained with clean targets, they can learn very accurate denoising functions <ref type="bibr" target="#b28">(Weigert et al., 2018)</ref>.</p><p>Statistical Independence: Lehtinen et al. observed that a UNet trained to predict one noisy measurement of a signal from an independent noisy measurement of the same signal will in fact learn to predict the true signal <ref type="bibr" target="#b11">(Lehtinen et al., 2018)</ref>. We may reformulate the Noise2Noise procedure in terms of J -invariant functions: if x 1 = y + n 1 and x 2 = y + n 2 are the two measurements, we consider the</p><formula xml:id="formula_4">composite measurement x = (x 1 , x 2 ) of a composite signal (y, y) in R 2m and set J = {J 1 , J 2 } = {{1, . . . , m}, {m + 1, . . . , 2m}}. Then f * J (x) J2 = E[y|x 1</formula><p>]. An extension to video, in which one frame is used to compute the pullback under optical flow of another, was explored in <ref type="bibr" target="#b5">(Ehret et al., 2018)</ref>.</p><p>In concurrent work, Krull et al. train a UNet to predict a collection of held-out pixels of an image from a version of that image with those pixels replaced (2018). A key difference between their approach and our neural net examples in §5 is in that their replacement strategy is not quite J -invariant. (With some probability a given pixel is replaced by itself.) While their method lacks a theoretical guarantee against fitting the noise, it performs well in practice, on natural and microscopy images with synthetic and real noise.</p><p>Finally, we note that the "fully emphasized denoising autoencoders" in <ref type="bibr" target="#b27">(Vincent et al., 2010)</ref> used the MSE between an autoencoder evaluated on masked input data and the true value of the masked pixels, but with the goal of learning robust representations, not denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Calibrating Traditional Models</head><p>Many denoising models have a hyperparameter controlling the degree of the denoising-the size of a filter, the threshold for sparsity, the number of principal components. If ground truth data were available, the optimal parameter θ for a family of denoisers f θ could be chosen by minimizing f θ (x) − y 2 . Without ground truth, we may nevertheless  <ref type="figure">Figure 2</ref>. Calibrating a median filter without ground truth. Different median filters may be obtained by varying the filter's radius. Which is optimal for a given image? The optimal parameter for J -invariant functions such as the donut median can be read off (red arrows) from the self-supervised loss. compute the self-supervised loss f θ (x) − x 2 . For general f θ , it is unrelated to the ground truth loss, but if f θ is Jinvariant, then it is equal to the ground truth loss plus the noise variance (Eqn. 2), and will have the same minimizer.</p><p>In <ref type="figure">Figure 2</ref>, we compare both losses for the median filter g r , which replaces each pixel with the median over a disk of radius r surrounding it, and the "donut" median filter f r , which replaces each pixel with the median over the same disk excluding the center, on an image with i.i.d. Gaussian noise. For J = {{1}, . . . , {m}} the partition into single pixels, the donut median is J -invariant. For the donut median, the minimum of the self-supervised loss f r (x) − x 2 (solid blue) sits directly above the minimum of the ground truth loss f r (x) − y 2 (dashed blue), and selects the optimal radius r = 3. The vertical displacement is equal to the variance of the noise. In contrast, the self-supervised loss g r (x) − x 2 (solid orange) is strictly increasing and tells us nothing about the ground truth loss g r (x) − y In Supp. <ref type="figure" target="#fig_5">Figure 1</ref>, we show the corresponding loss curves for J -invariant versions of a wavelet filter, where we tune the threshold σ, and NL-means, where we tune a cut-off distance h <ref type="bibr" target="#b1">(Buades et al., 2005a;</ref><ref type="bibr" target="#b3">Chang et al., 2000;</ref>. The partition J used is a 4x4 grid. Note that in all these examples, the function f θ is genuinely different than g θ , and, because the simple interpolation procedure may itself be helpful, it sometimes performs better.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we compare all three J -invariant denoisers on a single image. As expected, the denoiser with the best selfsupervised loss also has the best performance as measured by Peak Signal to Noise Ratio (PSNR). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Single-Cell</head><p>In single-cell transcriptomic experiments, thousands of individual cells are isolated, lysed, and their mRNA are extracted, barcoded, and sequenced. Each mRNA molecule is mapped to a gene, and that ∼20,000-dimensional vector of counts is an approximation to the gene expression of that cell. In modern, highly parallel experiments, only a few thousand of the hundreds of thousands of mRNA molecules present in a cell are successfully captured and sequenced <ref type="bibr" target="#b14">(Milo et al., 2010)</ref>. Thus the expression vectors are very undersampled, and genes expressed at low levels will appear as zeros. This makes simple relationships among genes, such as co-expression or transitions during development, difficult to see.</p><p>If we think of the measurement as a set of molecules captured from a given cell, then we may partition the molecules at random into two sets J 1 and J 2 . Summing (and normalizing) the gene counts in each set produces expression vectors x J1 and x J2 which are independent conditional on the true mRNA content y. We may now attempt to denoise x by training a model to predict x J2 from x J1 and vice versa.</p><p>We demonstrate this on a dataset of 2730 bone marrow cells from Paul et al. using principal component regression , where we use the self-supervised loss to find an optimal number of principal components. The data contain a population of stem cells which differentiate either into erythroid or myeloid lineages. The expression of genes preferentially expressed in each of these cell types is shown in <ref type="figure" target="#fig_0">Figure 3</ref> for both the (normalized) noisy data and data denoised with too many, too few, and an optimal number of principal components. In the raw data, it is difficult to discern any population structure. When the data is under-corrected, the stem cell marker Ifitm1 is still not visible. When it is over-corrected, the stem population appears to express substantial amounts of Klf1 and Mpo. In the optimally corrected version, Ifitm1 expression coincides with low expression of the other markers, identifying the stem population, and its transition to the two more mature states is easy to see.  <ref type="figure">(d)</ref> we show the the denoised data for the optimal number of principal components (17, red arrow). In (c) we show the result of using too few components and in (b) that of using too many. X-axes show square-root normalised counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">PCA</head><p>Cross-validation for choosing the rank of a PCA requires some care, since adding more principal components will always produce a better fit, even on held-out samples <ref type="bibr" target="#b0">(Bro et al., 2008)</ref>. Owen and Perry recommend splitting the feature dimensions into two sets J 1 and J 2 as well as splitting the samples into train and validation sets <ref type="bibr" target="#b16">(Owen &amp; Perry, 2009</ref>). For a given k, they fit a rank k principal component regression f k : X train,J1 → X train,J2 and evaluate its predictions on the validation set, computing f k (X valid,J1 ) − X valid,J2 2 . They repeat this, permuting train and validation sets and J 1 and J 2 . Simulations show that if X is actually a sum of a low-rank matrix plus Gaussian noise, then the k minimizing the total validation loss is often the optimal choice <ref type="bibr" target="#b16">(Owen &amp; Perry, 2009;</ref><ref type="bibr" target="#b17">Owen &amp; Wang, 2016)</ref>. This calculation corresponds to using the self-supervised loss to train and cross-validate a {J 1 , J 2 }invariant principal component regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theory</head><p>In an ideal situation for signal reconstruction, we have a prior p(y) for the signal and a probabilistic model of the noisy measurement process p(x|y). After observing some measurement x, the posterior distribution for y is given by Bayes' rule:</p><formula xml:id="formula_5">p(y|x) = p(x|y)p(y) p(x|y)p(y)dy .</formula><p>In practice, one seeks some function f (x) approximating a relevant statistic of y|x, such as its mean or median. The mean is provided by the function minimizing the loss:</p><formula xml:id="formula_6">E x f (x) − y 2 (</formula><p>The L 1 norm would produce the median) .</p><p>Fix a partition J of the dimensions {1, . . . , n} of x and suppose that for each J ∈ J , we have</p><formula xml:id="formula_7">p(x|y) = p(x J |y)p(x J c |y),</formula><p>i.e., x J and x J c are independent conditional on y. We consider the loss</p><formula xml:id="formula_8">E x f (x) − x 2 = E x,y f (x) − y 2 + x − y 2 − 2 f (x) − y, x − y .</formula><p>If f is J -invariant, then for each j the random variables f (x) j |y and x j |y are independent. The third term reduces to</p><formula xml:id="formula_9">j E y (E x|y [f (x) j − y j ])(E x|y [x j − y j ])</formula><p>, which vanishes when E[x|y] = y. This proves Prop. 1.</p><p>Any J -invariant function can be written as a collection of ordinary functions f J : R |J c | → R |J| , where we separate the output dimensions of f based on which input dimensions they depend on. Then</p><formula xml:id="formula_10">L(f ) = J∈J E f J (x J c ) − x J 2 .</formula><p>This is minimized at</p><formula xml:id="formula_11">f * J (x J c ) = E[x J |x J c ] = E[y J |x J c ].</formula><p>We bundle these functions into f * J , proving Prop. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">How good is the optimum?</head><p>How much information do we lose by giving up x J when trying to predict y J ? Roughly speaking, the more the features in J are correlated with those outside of it, the closer f * J (x) will be to E[y|x] and the better both will estimate y.  <ref type="figure">Figure 4</ref>. The optimal J -invariant predictor converges to the optimal predictor. Example images for Gaussian processes of different length scales. The gap in image quality between the two predictors tends to zero as the length scale increases. <ref type="figure">Figure 4</ref> illustrates this phenomenon for the example of Gaussian Processes, a computationally tractable model of signals with correlated features. We consider a process on a 33 × 33 toroidal grid. The value of y at each node is standard normal and the correlation between the values at p and q depends on the distance between them: K p,q = exp(− p − q 2 /2 2 ), where is the length scale. The noisy measurement x = y + n, where n is white Gaussian noise with standard deviation 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>While</head><p>E</p><formula xml:id="formula_12">y − f * J (x) 2 ≥ E y − E[y|x] 2</formula><p>for all , the gap decreases quickly as the length scale increases.</p><p>The Gaussian process is more than a convenient example; it actually represents a worst case for the recovery error as a function of correlation.  <ref type="figure">Figure 5</ref>. For any dataset, the error of the optimal predictor (blue) is lower than that for a Gaussian Process (red) with the same covariance matrix. We show this for a dataset of noisy digits: the quality of the denoising is visibly better for the Alphabet than the Gaussian Process (samples at σ = 0.8).</p><p>At the other extreme is data drawn from finite collection of templates, like symbols in an alphabet. If the alphabet consists of {a 1 , . . . , a r } ∈ R m and the noise is i.i.d. mean-zero Gaussian with variance σ 2 , then the optimal J-invariant prediction independent is a weighted sum of the letters from the alphabet. The weights w i = exp(− (a i − x) · 1 J c 2 /2σ 2 ) are proportional to the posterior probabilities of each letter. When the noise is low, the output concentrates on a copy of the closest letter; when the noise is high, the output averages many letters.</p><p>In <ref type="figure">Figure 5</ref>, we demonstrate this phenomenon for an alphabet consisting of 30 16x16 handwritten digits drawn from <ref type="bibr">MNIST (LeCun et al., 1998)</ref>. Note that almost exact recovery is possible at much higher levels of noise than the Gaussian process with covariance matrix given by the empirical covariance matrix of the alphabet. Any real-world dataset will exhibit more structure than a Gaussian process, so nonlinear functions can generate significantly better predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Doing better</head><p>If f is J -invariant, then by definition f (x) j contains no information from x j , and the right linear combination λf (x) j + (1 − λ)x j will produce an estimate of y j with lower variance than either. The optimal value of λ is given by the variance of the noise divided by the value of the self-supervised loss. The performance gain depends on the quality of f : for example, if f improves the PSNR by 10 dB, then mixing in the optimal amount of x will yield another 0.4 dB. (See <ref type="table" target="#tab_1">Table 1</ref> for an example and Supplement for proofs.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deep Learning Denoisers</head><p>The self-supervised loss can be used to train a deep convolutional neural net with just one noisy sample of each image in a dataset. We show this on three datasets from different domains (see <ref type="figure" target="#fig_3">Figure 6</ref>) with strong and varied heteroscedastic synthetic noise applied independently to each pixel. For the datasets Hànzì and ImageNet we use a mixture of Poisson, Gaussian, and Bernoulli noise. For the CellNet microscopy dataset we simulate realistic sCMOS camera noise. We use a random partition of 25 subsets for J , and we make the neural net J -invariant as in Eq. 3, except we replace the masked pixels with random values instead of local averages. We train two neural net architectures, a UNet and a purely convolutional net, DnCNN <ref type="bibr" target="#b29">(Zhang et al., 2017)</ref>. To accelerate training, we only compute the net outputs and loss for one partition J ∈ J per minibatch.</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, both neural nets trained with selfsupervision (Noise2Self) achieve superior performance to the classic unsupervised denoisers NLM and BM3D (at default parameter values), and comparable performance to the same neural net architectures trained with clean targets (Noise2Truth) and with independently noisy targets (Noise2Noise).</p><p>The result of training is a neural net g θ , which, when converted into a J -invariant function f θ , has low selfsupervised loss. We found that applying g θ directly to the noisy input gave slightly better (0.5 dB) performance than using f θ . The images in <ref type="figure" target="#fig_3">Figure 6</ref> use g θ .</p><p>Remarkably, it is also possible to train a deep CNN to denoise a single noisy image. The DnCNN architecture, with 560,000 parameters, trained with self-supervision on the noisy camera image from §3, with 260,000 pixels, achieves a PSNR of 31.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We have demonstrated a general framework for denoising high-dimensional measurements whose noise exhibits some conditional independence structure. We have shown how to use a self-supervised loss to calibrate or train any Jinvariant class of denoising functions.</p><p>There remain many open questions about the optimal choice of partition J for a given problem. The structure of J must reflect the patterns of dependence in the signal and independence in the noise. The relative sizes of each subset J ∈ J and its complement creates a bias-variance tradeoff in the loss, exchanging information used to make a prediction for information about the quality of that prediction.</p><p>For example, the measurements of single-cell gene expression could be partitioned by molecule, gene, or even pathway, reflecting different assumptions about the kind of stochasticity occurring in transcription.</p><p>We hope this framework will find application to other domains, such as sensor networks in agriculture or geology, time series of whole brain neuronal activity, or telescope observations of distant celestial bodies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Notation</head><p>For a variables x ∈ R m and J ⊂ {1, . . . , m}, we write x J for the restriction of x to the coordinates in J and x J c for the restriction of x to the coordinates in J c . If f : R m → R m is a function, we write f (x) J for the restriction of f (x) to the coordinates in J.</p><p>A partition J of a set X is a set of disjoint subsets of X whose union is all of X.</p><p>When J = {j} is a singleton, we write x −j for x J c , the restriction of x to the coordinates not equal to j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Gaussian Processes</head><p>Let x and y be random variables. Then the estimator of y from x minimizing the expected mean-square error (MSE) is x → E[y|x]. The expected MSE of that estimator is simply the variance of y|x:</p><formula xml:id="formula_13">E x y − E[y|x] 2 = E x Var(y|x)</formula><p>.</p><p>If x and y are jointly multivariate normal, then the righthand-side depends only on the covariance matrix Σ. If</p><formula xml:id="formula_14">Σ = Σ xx Σ yx Σ xy Σ yy ,</formula><p>then then right-hand-side is in fact a constant independent of x:</p><formula xml:id="formula_15">Var(y|x) = Σ yy − Σ yx Σ −1 xx Σ xy .</formula><p>(See Chapter 4 of .)</p><p>Lemma 1. Let Σ be a symmetric, positive semi-definite matrix with block structure</p><formula xml:id="formula_16">Σ = Σ 11 Σ 12 Σ 21 Σ 22 .</formula><p>Then Σ 11 Σ 12 Σ −1 22 Σ 21 .</p><p>Proof. Since Σ is PSD, we may factorize it as a product X T X for some matrix X. (For example, take the spectral decomposition Σ = V T ΛV , with Λ the diagonal matrix of eigenvalues, all of which are nonnegative since Σ is PSD and V the matrix of eigenvectors.</p><formula xml:id="formula_17">Set X = Λ 1/2 V .) Write X = X 1 X 2 , so that Σ ij = X T i X j .</formula><p>If π X2 is the projection operator onto the column-span of X 2 , then</p><p>In this section, we discuss approaches to modifying the input to a neural net or other function f to create a J -invariant function.</p><p>The basic idea is to choose some interpolation function s(x) and then define g by</p><formula xml:id="formula_18">g(x) J := f (1 J · s(x) + 1 J c · x) J ,</formula><p>where 1 J is the indicator function of the set J.</p><p>In Section 3 of the paper, on calibration, s is given by a local average, not containing the center. Explicitly, it is convolution with the kernel We also considered setting each entry of s(x) to a random variable uniform on [0, 1]. This produces a random Jinvariant function, ie, a distribution g(x) whose marginal g(x) J does not depend on x J .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Uniform Pixel Selection</head><p>In Krull et. al., the authors propose masking procedures that estimate a local distribution q(x) in the neighborhood of a pixel and then replace that pixel with a sample from the distribution. Because the value at that pixel is used to estimate the distribution, information about it leaks through and the resulting random functions are not genuinely Jinvariant.</p><p>For example, they propose a method called Uniform Pixel Selection (UPS) to train a neural net to predict x j from UPS j (x), where UPS j is the random function replacing the j th entry of x with the value of at a pixel k chosen uniformly at random from the r × r neighborhood centered at j .</p><p>Write ι jk (x) is the vector x with the value x j replaced by x k .</p><p>The function f * minimizing the self-supervised loss</p><formula xml:id="formula_19">E x f (UPS j (x)) j − x j 2 satisfies f * (x) j = E x [x j | UPS j (x)] = E x E k [x j |ι jk (x)] = 1 r 2 k E[x j |ι jk (x)] = 1 r 2 E[x j |ι jj (x)] + 1 r 2 k =j E[x j |ι jk (x)] = 1 r 2 x j + 1 r 2 k =j E[x j |x −j ] = 1 r 2 x j + 1 − 1 r 2 f * J (x) j , where f * J (x) j = E[x j |x −j ]</formula><p>is the optimum of the selfsupervised loss among J -invariant functions.</p><p>This means that training using UPS masking can, given sufficient data and a sufficiently expressive network, produce a linear combination of the noisy input and the Noise2Self optimum. The smaller the region used for selecting the pixel, the larger the contribution of the noise will be. In practice, however, a convolutional neural net may not be able to learn to recognize when it was handed an interesting pixel x j and when it had been replaced (say by comparing the value at a pixel in UPS j (x) to each of its neighbors).</p><p>One attractive feature of UPS is that it keeps the same perpixel data distribution as the input. If, for example, the input is binary, then local averaging and random uniform replacements will both be substantial deviations. This may regularize the behavior of the network, making it more sensible to pass in an entire copy of x to the trained network later, rather than iteratively masking it.</p><p>We suggest a simple modification: exclude the value of x j when estimating the local distribution. For example, replace it with a random neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Linear Combinations</head><p>In this section we note that if f is J -invariant, then f (x) j and x j give two uncorrelated estimators of y j for any coordinate j. Here we investigate the effect of taking a linear combination of them.</p><p>Given two uncorrelated and unbiased estimators u and v of some quantity y, we may form a linear combination:</p><formula xml:id="formula_20">w λ = λu + (1 − λ)v.</formula><p>The variance of this estimator is</p><formula xml:id="formula_21">λ 2 U + (1 − λ) 2 V,</formula><p>where U and V are the variances of u and v respectively. This expression is minimized at</p><formula xml:id="formula_22">λ = V /(U + V ).</formula><p>The variance of the mixed estimator w λ is U V /(U + V ) = V 1 1+V /U . When the variance of v is much lower than that of u, we just get V out, but when they are the same the variance is exactly halved. Note that this is monotonic in V , so if estimators v 1 , . . . , v n are being compared, their rank will not change after the original signal is mixed in. In terms of PSNR, the new value is PSNR(w λ , y) = 10 * log 10 1 + V /U V = PSNR(V ) + 10 * log 10 (1 + V /U ) ≈ PSNR(V ) + 10 log 10 (e)</p><formula xml:id="formula_23">V U − 1 2 V 2 U 2 ≈ PSNR(V ) + 4.3 · V U</formula><p>If we fix y, then x j and E[y j |x −j ] are both independent estimators of y j , so the above reasoning applies. Note that the loss itself is the variance of x j |x −j , whose two components are the variance of x j |y j and the variance of y j |x −j .</p><p>The optimal value of λ, then, is given by the variance of the noise divided by the value of the self-supervised loss.</p><p>For example the function f reduces the noise by a factor of 10 (ie, the variance of y j |x −j is a tenth of the variance of x j |y j ), then λ * = 1/11 and the linear combination has a PSNR 0.43 higher than that of f alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Calibrating Traditional Denoising Methods</head><p>The image denoising methods were all demonstrated on the full camera image included in the scikit-image library for python . An inset from that image was displayed in the figures.</p><p>We also used the scikit-image implementations of the median filter, wavelet denoiser, and NL-means. The noise standard deviation was 0.1 on a [0, 1] scale.</p><p>In addition to the calibration plots for the median filter in the text, we show the same for the wavelet and NL-means denoisers in Supp. <ref type="figure" target="#fig_5">Figure 1</ref>. , and applied to each one substantial Gaussian (µ = 0, σ = 0.7) and Bernoulli (half pixels blacked out) noise. Each Chinese character appears 6 times in the whole dataset of 78174 images. We then split this dataset in a training and test set (90% versus 10%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Neural Net Examples</head><p>CellNet We constructed a dataset of 34630 image tiles (128x128) obtained by random partitioning of a large collection of single channel fluorescence microscopy images of cultured cells. These images were downloaded from the Broad Bioimage Benchmark Collection . Before cropping, we first gently denoise the images using the non-local means algorithm. We do so in order to remove a very low and nearly imperceptible amount of noise already present in these images -indeed, the images have an excellent signal-to-noise ratio to start from.  dation Set consisting of 20121 RGB images -typically photographs. From these images we generated 60000 cropped images of dimension 128x128 with each RGB value within [0, 255]. These images were mistreated by the strong combination of Poisson (λ = 30), Gaussian (σ = 80), and Bernoulli noise (p = 0.2). In the case of Bernoulli noise, each pixel channel (R, G, or B) has probability p of being dark or hot, i.e. set to the value 0 or 255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Architecture</head><p>We use a UNet architecture modelled after . The network has an hourglass shape with skip connections between layers of the same scale. Each convolutional block consists of two convolutional layers with 3x3 filters followed by an InstanceNorm. The number of channels is <ref type="bibr">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256]</ref>. Downsampling uses strided convolutions and upsampling uses transposed convolutions. The network is implemented in PyTorch  and the code is also included in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Training</head><p>We convert a neural net f θ into a random J -invariant function:</p><formula xml:id="formula_24">J∈J 1 J · f θ (1 J c · x J + 1 J · u)<label>(1)</label></formula><p>where u is a vector of random numbers distributed uniformly on [0, 1]. To speed up training, we only compute the coordinates for one J per pass, and that J is chosen randomly for each batch with density 1/25. The loss is restricted to those coordinates.</p><p>We train with a batch size of 64 for Hànzì and CellNet and a batch size of 32 for ImageNet.</p><p>We train for 50 epochs for CellNet, 30 epochs for Hànzì and 1 epoch for ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Inference</head><p>We considered two approaches for inference. In the first, we consider a partition J containing 25 sets and apply Equation <ref type="formula" target="#formula_1">(1)</ref> to produce a genuinely J -invariant function. This requires |J | applications of the network.</p><p>In the second, we just apply the trained network to the full noisy data. This will include the information from x j in the prediction f θ (x) j . While the information in this pixel was entirely redundant during training, some regularization induced by the convolutional structure of the net and the training procedure may have caused it to learn a function which uses that information in a sensible way. Indeed, on our three datasets, the direct application was about 0.5 dB better than the J -independent version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Evaluation</head><p>We evaluated each reconstruction method using the Peak Signal-to-Noise Ratio (PSNR). For two images with range [0, 1], this is a log-transformation of the mean-squared error:</p><p>PSNR(x, y) = 10 * log 10 (1/ x − y 2 ).</p><p>Because of clipping, the noise on the image datasets is not conditionally mean-zero. (Any noise on a pixel with intensity 1, for example, must be negative.) This induces a bias: E[x|y] is shrunk slightly towards the mean intensity. For methods trained with clean targets, like Noise2Truth and DnCNN, this effect doesn't matter; the network can learn to produce the correct value. The outputs of the blind methods like Noise2Noise, Noise2Self, NL-means, and BM3D, will exhibit this shrinkage. To make up for this difference, we rescale the outputs of all methods to match the mean and variance of the ground truth.</p><p>We compute the PSNR for fully reconstructed images on hold-out test sets which were not part of the training or validation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Single-Cell Gene Expression</head><p>The lossy capture and sequencing process producing singlecell gene expression can be expressed as a Poisson distribution 1 . A given cell has a density λ = (λ 1 , . . . , λ m ) over genes i ∈ {1, . . . m}, with i λ i = 1. If we sample N molecules, we get a multinomial distribution which can be approximated as x i ∼ Poisson(N λ i ).</p><p>While one would like to model molecular counts directly, the large dynamic range of gene expression (about 5 orders of magnitude) makes linear models difficult to fit directly. Instead, one typically introduces a normalized variable z, for example</p><formula xml:id="formula_25">z i = ρ(N 0 * x i /N ),</formula><p>where N = i x i is the total number of molecules in a given cell, N 0 is a normalizing constant, and ρ is some nonlinearity. Common values for ρ include x → √ x and</p><p>x → log(1 + x).</p><p>Our analysis of the Paul et al. dataset  follows one from the tutorial for a diffusion-based denoiser called MAGIC, and we use the scprep package to perform normalization . In the language above, N 0 is the median of the total molecule count per cell and ρ is square root.</p><p>Because we work on the normalized variable z, the optimal denoiser would predict</p><formula xml:id="formula_26">E[z i |λ] ≈ E xi∼Poisson N λi √ x i N 0 /N .</formula><p>This function of λ i is positive, monotonic and maps 0 to 0, so it is directionally informative. Since expectations do not commute with nonlinear functions, inverting it would not produce an unbiased estimate of λ i . Nevertheless, it provides a quantitative estimate of gene expression which is well-adapted to the large dynamic range.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Self-supervised loss calibrates a linear denoiser for single cell data. (a) Raw expression of three genes: a myeloid cell marker (Mpo), an erythroid cell marker (Klf1), and a stem cell marker (Ifitm1). Each point corresponds to a cell. (e) Self-supervised loss for principal component regression. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Performance of classic, supervised, and self-supervised denoising methods on natural images, Chinese characters, and fluorescence microscopy images. Blind denoisers are NLM, BM3D, and neural nets (UNet and DnCNN) trained with self-supervision (N2S). We compare to neural nets supervised with a second noisy image (N2N) and with the ground truth (N2T).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Calibrating a wavelet filter and Non-local means without ground truth. The optimal parameter for J -invariant (masked) versions can be read off (red arrows) from the self-supervised loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of optimally tuned J -invariant versions of classical denoising models. Performance is better than original method at default parameter values, and can be further improved (+) by adding an optimal amount of the noisy input to the Jinvariant output ( §4.2).</figDesc><table><row><cell>METHOD</cell><cell cols="4">LOSS J-INVT J-INVT J-INVT+ DEFAULT PSNR</cell></row><row><cell cols="2">MEDIAN WAVELET NL-MEANS 0.0098 0.0107 0.0113</cell><cell>27.5 26.0 30.4</cell><cell>28.2 26.9 30.8</cell><cell>27.1 24.6 28.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of different denoising methods by Peak Signal to Noise Ratio (PSNR) on held-out test data. Error bars for CNNs from training five models.</figDesc><table><row><cell>METHOD</cell><cell>HÀNZÌ</cell><cell>IMAGENET</cell><cell>CELLNET</cell></row><row><cell cols="2">RAW NLM BM3D UNET DNCNN (N2S) (N2S) UNET (N2N) 13.3 ± 0.5 6.5 8.4 11.8 13.8 ± 0.3 13.4 ± 0.3 DNCNN (N2N) 13.6 ± 0.2 UNET (N2T) 13.1 ± 0.7 DNCNN (N2T) 13.9 ± 0.6</cell><cell>9.4 15.7 17.8 18.6 18.7 17.8 18.8 21.1 22.0</cell><cell>15.1 29.0 31.4 32.8 ± 0.2 33.7 ± 0.2 34.4 ± 0.1 34.4 ± 0.1 34.5 ± 0.1 34.4 ± 0.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/czbiohub/noise2self</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">(dashed orange). Note that the median and donut median are genuinely different functions with slightly different performance, but while the former can only be tuned by inspecting the output images, the latter can be tuned using a principled loss.More generally, let g θ be any classical denoiser, and let J be any partition of the pixels such that neighboring pixels are in different subsets. Let s(x) be the function replacing each pixel with the average of its neighbors. Then the function f θ defined byf θ (x) J := g θ (1 J · s(x) + 1 J c · x) J ,(3)for each J ∈ J , is a J -invariant version of g θ . Indeed, since the pixels of x in J are replaced before applying g θ , the output cannot depend on x J .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While the polymerase chain reaction (PCR) used to amplify the molecules for sequencing would introduce random multiplicative distortions, many modern datasets introduce unique molecular indentifiers (UMIs), barcodes attached to each molecule before amplification which can be used to deduplicate reads from the same original molecule.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thank you to James Webber, Jeremy Freeman, David Dynerman, Nicholas Sofroniew, Jaakko Lehtinen, Jenny Folkesson, Anitha Krishnan, and Vedran Hadziosmanovic for valuable conversations. Thank you to Jack Kamm for discussions on Gaussian Processes and shrinkage estimators. Thank you to Martin Weigert for his help running BM3D. Thank you to the referees for suggesting valuable clarifications. Thank you to the Chan Zuckerberg Biohub for financial support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-validation of component models: A critical look at current methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kjeldahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Smilde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A L</forename><surname>Kiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytical and Bioanalytical Chemistry</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1241" to="1251" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review of image denoising algorithms, with a new one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="490" to="530" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive wavelet thresholding for image denoising and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1532" to="1546" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Model-blind video denoising via frame-to-frame training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12766</idno>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memoires associatives distribuees: Une comparaison (Distributed associative memories: A comparison)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thiria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Soulie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COGNITIVA 87</title>
		<meeting>COGNITIVA 87<address><addrLine>Paris, La Villette</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-O</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10980</idno>
		<title level="m">Noise2Void -learning denoising from single noisy images</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis and implementation of the BM3D image denoising method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="175" to="213" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise2Noise: Learning image restoration without clean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2971" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Annotated high-throughput microscopy image sets for validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ljosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Sokolnicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="637" to="637" />
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised learning with Stein&apos;s unbiased risk estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10531</idno>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BioNumbers -the database of key numbers in molecular and cell biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Springer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="750" to="753" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Machine Learning: a Probabilistic Perspective. Adaptive computation and machine learning series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno>978-0-262- 01802-9</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bi-cross-validation of the SVD and the nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Perry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="564" to="594" />
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bi-cross-validation for factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convolutional dictionary learning via local processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03239</idno>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transcriptional heterogeneity and lineage commitment in myeloid progenitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Arkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jaitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kenigsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Keren-Shaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lara-Astiaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauridsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schlitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mildner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginhoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trumpp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Porse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1663" to="1677" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">JPEG still image data compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<idno>978-0-442-01272-4</idno>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Van Nostrand Reinhold</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>U-Net</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<title level="m">Convolutional networks for biomedical image segmentation</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
	<note>Noise2Self: Blind Denoising by Self-Supervision</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04477</idno>
		<title level="m">Correction by projection: Denoising images with generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10925</idno>
		<title level="m">Deep image prior</title>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">and contributors, t. s.-i. scikit-image: image processing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouillart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">453</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recovering gene interactions from single-cell data using data diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nainys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kathail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burdziak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pattabiraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bierie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mazutis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="716" to="729" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Content-Aware image restoration: Pushing the limits of fluorescence microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weigert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dibrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Broaddus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Culley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rocha-Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">;</forename><surname>Segovia-Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zerial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Solimena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomancak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
	<note>Norden</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training deep learning based image denoisers from undersampled measurements without ground truth and without image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhussip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soltanayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00961</idno>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-O</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10980</idno>
		<title level="m">Noise2Void -learning denoising from single noisy images</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Annotated high-throughput microscopy image sets for validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ljosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Sokolnicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="637" to="637" />
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Machine Learning: a Probabilistic Perspective. Adaptive computation and machine learning series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno>978-0-262- 01802-9</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transcriptional heterogeneity and lineage commitment in myeloid progenitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Arkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jaitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kenigsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Keren-Shaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lara-Astiaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauridsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schlitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mildner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginhoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trumpp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Porse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1663" to="1677" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>U-Net</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<title level="m">Convolutional networks for biomedical image segmentation</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">and contributors, t. s.-i. scikit-image: image processing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouillart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">453</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering gene interactions from single-cell data using data diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nainys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kathail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burdziak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pattabiraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bierie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mazutis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="716" to="729" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
