<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ENSEMBLE LEARNING IN CNN AUGMENTED WITH FULLY CONNECTED SUBNETWORKS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-24">24 Mar 2020 March 25, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Hirata</surname></persName>
							<email>daiki_hirata@pref.okayama.lg.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Industrial Technology Center of Okayama Prefecture</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norikazu</forename><surname>Takahashi</surname></persName>
							<email>takahashi@cs.okayama-u.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School Natural Science and Technology</orgName>
								<orgName type="institution">Okayama University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ENSEMBLE LEARNING IN CNN AUGMENTED WITH FULLY CONNECTED SUBNETWORKS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-24">24 Mar 2020 March 25, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>EnsNet · Convolutional Neural Networks · Ensemble Learning · Majority Voting · MNIST</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) have shown remarkable performance in general object recognition tasks. In this paper, we propose a new model called EnsNet which is composed of one base CNN and multiple Fully Connected SubNetworks (FCSNs). In this model, the set of feature-maps generated by the last convolutional layer in the base CNN is divided along channels into disjoint subsets, and these subsets are assigned to the FCSNs. Each of the FCSNs is trained independent of others so that it can predict the class label from the subset of the feature-maps assigned to it. The output of the overall model is determined by majority vote of the base CNN and the FCSNs. Experimental results using the MNIST, Fashion-MNIST and CIFAR-10 datasets show that the proposed approach further improves the performance of CNNs. In particular, an EnsNet achieves a state-of-the-art error rate of 0.16% on MNIST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b0">[1]</ref> are attracting a great deal of attention because they show remarkable performance in general object recognition tasks. Various methods have been proposed so far for improving the performance of CNNs: pre-processing <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, dropout <ref type="bibr" target="#b4">[5]</ref>, batch normalization <ref type="bibr" target="#b5">[6]</ref>, ensemble learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and so on.</p><p>In this paper, we propose a new model based on CNNs to further improve the performance in image recognitioin tasks. Our model consists of one base CNN and multiple Fully Connected SubNetworks (FCSNs). The base CNN generates a set of multi-channel feature-maps after each convolutional layer. The set of feature-maps generated by the last convolutional layer is divided along channels into disjoint subsets, and each subset is assigned to one of the FCSNs, which is trained independent of others so that it can predict the class label from the subset of the featuremaps assigned to it. The output of the overall model is determined by majority vote of the base CNN and the FCSNs. Namely, ensemble learning is performed in the proposed method. We thus call this model EnsNet in this paper. It is known that, in order for ensemble learning to be effective, the base learners must represent certain degree of diversity. In the proposed model, it is expected that FCSNs have this property because different subnetworks are trained using different training data.</p><p>In what follows, we first explain the architechture of the EnsNet and how to train it. We then provide results of some experiments using the MNIST <ref type="bibr" target="#b8">[9]</ref>, Fashion-MNIST <ref type="bibr" target="#b9">[10]</ref>, and CIFAR-10 <ref type="bibr" target="#b10">[11]</ref> datasets, which show that the proposed approach certainly improves the performance of CNNs. In particular, it is shown that an EnsNet achieves a state-of-the-art error rate of 0.16% on MNIST.  The proposed model called EnsNet consists of one base CNN and multiple subnetworks as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The structure of the base CNN varies depending on image recognition tasks. <ref type="table">Table 1</ref> shows two different structures used in the experiments shown in Section 3: one is for MNIST and Fashion-MNIST, and the other is for CIFAR-10. The ReLU activation function is used in the proposed model, though this is not explicitly shown in <ref type="table">Table 1</ref>. The set of feature-maps generated by the last convolutional layer of the base CNN is divided along channels into disjoint subsets, and each subset is fed into one of the subnetworks. Each subnetwork is a fully connected neural network consisting of multiple weight layers. <ref type="table" target="#tab_1">Table 2</ref> shows the details of the structure of the subnetworks used in the experiments: one is for MNIST and Fashion-MNIST, and the other is for CIFAR-10. The output of the overall model is determined by majority vote of the base CNN and the subnetworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>The EnsNet is trained by alternating two steps: one is the base CNN training step, and the other is the subnetworks training step. In the base CNN training step, the parameters of the convolutional layers and the fully connected layers of the base CNN are updated by some optimization algorithm, while the parameters of the subnetworks are fixed. In the experiments shown in Section 3, the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> is used. In the subnetworks training step, the parameters of the base CNN are fixed, and each subnetwork is trained independent of other subnetworks, using the corresponding subset of the feature-maps generated by the last convolutional layer of the base CNN and the target class labels as the training data. The parameters of the fully connected layers of each subnetwork are updated by the same optimization algorithm as the base CNN. <ref type="table">Table 1</ref>: Structures of the base CNN used in the experiments. The left one is for the MNIST and Fashion-MNIST datasets, and the right one is for the CIFAR-10 dataset. Both CNNs have nine weight layers. The size of each convolutional layer is denoted as "Conv&lt;receptive field size&gt;-&lt;number of channels&gt;", and the size of each fully connected layer is denoted as "FC-&lt;number of nodes&gt;". The ReLU activation function is used in both models but is not shown in this In order to evaluate the effectiveness of the EnsNet, we conducted classification experiments using the MNIST, Fashion-MNIST and CIFAR-10 datasets. The models used in the experiments were implemented in Chainer framework <ref type="bibr" target="#b14">[15]</ref>, and trained by the Adam optimizer <ref type="bibr" target="#b13">[14]</ref>. The parameters of the Adam optimizer were set as follows: α = 0.001, β 1 = 0.9, β 2 = 0.999, ǫ = 10 −8 , and a weight decay was set to 0.</p><p>The MNIST dataset is a collection of 28 × 28 gray scale images of handwritten digits from 0 to 9. The training and test sets consist of 60,000 and 10,000 images, respectively. Before training, we augmented data by rotating images by various angles between −10 • and 10 • , scaling images by various factors between 0.8 and 1.2, shifting images to the width direction or the height direction by a fraction between −0.08 and 0.08 of the total width or the total height,  stretching images by the shear transformation with various angles between −0.3 • and 0.3 • . We also set the batch size and the number of epochs to 100 and 1,300, respectively.</p><p>The Fashion-MNIST dataset is a collection of 28 × 28 gray scale images in 10 classes. The training and test sets consist of 60,000 and 10,000 images, respectively. Before training, we augmented data by rotating images by various angles between −5 • and 5 • . We also set the batch size and the number of epochs to 100 and 600, respectively.</p><p>The CIFAR-10 dataset is a collection of 32 × 32 colored images in 10 classes. The training and test sets consist of 50,000 and 10,000 images, respectively. Before training, we augmented data by rotating images by various angles between −10 • and 10 • , scaling images by various factors between 0.8 and 1.2, shifting images to the width direction or the height direction by a fraction between −0.08 and 0.08 of the total width or the total height, stretching images by the shear transformation with various angles between −0.3 • and 0.3 • . We also set the batch size and the number of epoch to 100 and 200. Furthermore, we decayed the learning rate α by 0.1 every 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effect of Fully Connected Subnetworks</head><p>In the first experiment, in order to evaluate the effectiveness of the fully connected subnetworks and the majority vote, we trained the EnsNet with the structure shown in the left columns of Tables 1 and 2, and the base CNN in the EnsNet using the MNIST dataset, and measured how the test set accuracies of these two models change as the number of epochs increases. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the results of this experiment. It is seen from <ref type="figure" target="#fig_2">Fig. 2</ref> that the test set accuracy of the EnsNet is higher than that of the base CNN. This means that the fully connected subnetworks and the majority vote can improve the performance of CNNs. <ref type="table">Table 3</ref>: Test set error rates of six models for MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Error rate RMDL <ref type="bibr" target="#b15">[16]</ref> 0.18% Dropconnect <ref type="bibr" target="#b11">[12]</ref> 0.21% MCDNN <ref type="bibr" target="#b16">[17]</ref> 0.23% APAC <ref type="bibr" target="#b17">[18]</ref> 0.23% EnsNet (Proposed) 0.16% Base CNN in EnsNet 0.21% <ref type="table">Table 4</ref>: Test set error rates of four models for Fashion-MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Error rate Random Erasing <ref type="bibr" target="#b18">[19]</ref> 3.65% VGG8B(2x)+LocalLearning+CO <ref type="bibr" target="#b19">[20]</ref> 4.14% EnsNet (Proposed) 4.70% Base CNN in EnsNet 5.00%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with Other Models</head><p>In the second experiment, we trained the EnsNet and some conventional models including the base CNN using the MNIST, Fashion-MNIST and CIFAR-10 datasets, and measured the error rates for the test sets. <ref type="table">Table 3</ref> shows the error rates of six models: Random Multimodel Deep Learning for Classification (RMDL) <ref type="bibr" target="#b15">[16]</ref>, Dropconnect <ref type="bibr" target="#b11">[12]</ref>, Multi-Column Deep Neural Network (MCDNN) <ref type="bibr" target="#b16">[17]</ref>, Augmented PAttern Classification (APAC) <ref type="bibr" target="#b17">[18]</ref>, EnsNet, and the base CNN in the EnsNet for the MNIST dataset. Here, by EnsNet, we mean the model with the structure shown in the left columns of <ref type="table" target="#tab_1">Tables 1 and 2</ref>. The EnsNet acheived the error rate of 0.16% which is lower than that of the RMDL, one of the state-of-the-art models for the MNIST dataset classification task. <ref type="table">Table 4</ref> shows the error rates of four models: Random Erasing <ref type="bibr" target="#b18">[19]</ref>, VGG8B(2x)+LocalLearning+CO <ref type="bibr" target="#b19">[20]</ref>, EnsNet, and the base CNN in EnsNet for the Fashion-MNIST dataset. Here, by EnsNet, we mean the model with the structure shown in the left columns of <ref type="table" target="#tab_1">Tables 1 and 2</ref>. The EnsNet is not the best, but outperforms the base CNN. <ref type="table" target="#tab_2">Table 5</ref> shows the error rates of the EnsNet with the structure shown in the right columns of <ref type="table" target="#tab_1">Tables 1 and 2</ref>, and the base CNN in the EnsNet for the CIFAR-10 dataset. The EnsNet acheived a lower error rate than the base CNN. These results mean that the fully connected sunetworks and the majority vote certainly improved the performance of CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclution</head><p>We proposed a new CNN model called EnsNet, which is composed of one base CNN and multiple fully connected subnetworks. In this model, the set of feature-maps generated by the last convolutional layer of the base CNN is divided into disjoint subsets, and each subset is fed into one of the subnetworks as its input. The training of the EnsNet is done by updating the parameters of the base CNN and those of the subnetworks alternately, and the prediction is done by the majority vote of the base CNN and the subnetworks. Experimental results using the MNIST, Fashion-MNIST and CIFAR-10 datasets show that the EnsNet outperforms the base CNN. In particular, the EnsNet achieves the lowest error rate among some of the state-of-the-art models. A future work is to evaluate the effectiveness of our approach on other CNN models such as ResNet <ref type="bibr" target="#b20">[21]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of the architecture of the EnsNet. Red boxes and blue ones represent the base CNN and the subnetworks, respectively. The integer on the top of each red box is the number of channels of the feature-maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The test set accuracy of the EnsNet and the base CNN during training of the MNIST dataset (left) and a zoomed view of it (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>table for simplicity.</figDesc><table><row><cell>Input: 28 × 28 MNIST or Fashion-MNIST image</cell><cell>Input: 32 × 32 CIFAR-10 image</cell></row><row><cell>Conv3-64 (zero padding)</cell><cell>Conv3-64 (zero padding)</cell></row><row><cell>BatchNormalization</cell><cell>BatchNormalization</cell></row><row><cell>Dropout(0.35)</cell><cell>Dropout(0.25)</cell></row><row><cell>Conv3-128</cell><cell>Conv3-128</cell></row><row><cell>BatchNormalization</cell><cell>BatchNormalization</cell></row><row><cell>Dropout(0.35)</cell><cell>Dropout(0.25)</cell></row><row><cell>Conv3-256 (zero padding)</cell><cell>Conv3-256 (zero padding)</cell></row><row><cell>BatchNormalization</cell><cell>BatchNormalization</cell></row><row><cell>maxpool(2 × 2)</cell><cell>maxpool(2 × 2)</cell></row><row><cell>Dropout(0.35)</cell><cell>Dropout(0.25)</cell></row><row><cell>Conv3-512 (zero padding)</cell><cell>Conv3-512 (zero padding)</cell></row><row><cell>BatchNormalization</cell><cell>BatchNormalization</cell></row><row><cell>Dropout(0.35)</cell><cell>Dropout(0.25)</cell></row><row><cell>Conv3-1024</cell><cell>Conv3-1024</cell></row><row><cell>BatchNormalization</cell><cell>BatchNormalization</cell></row><row><cell>Dropout(0.35)</cell><cell>Dropout(0.25)</cell></row><row><cell>Conv3-2000 (zero padding)</cell><cell>Conv3-2048 (zero padding)</cell></row><row><cell>BatchNormalization</cell><cell>BatchNormalization</cell></row><row><cell>maxpool(2 × 2)</cell><cell>maxpool(2 × 2)</cell></row><row><cell>Dropout(0.35)</cell><cell>Dropout(0.25)</cell></row><row><cell>Dividing feature-maps (10 divition) FC-512 BatchNormalization Dropout(0.5) Dropconnect(0.5) [12, 13] FC-512 FC-10 soft-max</cell><cell>Conv3-3000 (zero padding) BatchNormalization Dropout(0.25) Conv3-3500 (zero padding) BatchNormalization Dropout(0.25) Conv3-4000 (zero padding) BatchNormalization Dropout(0.25)</cell></row><row><cell></cell><cell>Dividing feature-maps (10 divition)</cell></row><row><cell></cell><cell>FC-512</cell></row><row><cell></cell><cell>BatchNormalization</cell></row><row><cell></cell><cell>Dropout(0.3)</cell></row><row><cell></cell><cell>Dropconnect(0.3)</cell></row><row><cell></cell><cell>FC-512</cell></row><row><cell></cell><cell>FC-10</cell></row><row><cell></cell><cell>soft-max</cell></row><row><cell>3 Classification Experiments</cell><cell></cell></row><row><cell>3.1 Setup</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The structures of the subnetworks used in the experiments. The left one is for MNIST and Fashion-MNIST datasets, and the right one is for CIFAR-10. All subnetworks have three weight layers.</figDesc><table><row><cell>Input: 200 × 6 × 6 feature-maps</cell><cell>Input: 400 × 7 × 7 feature-maps</cell></row><row><cell>(MNIST or Fashion-MNIST)</cell><cell>(CIFAR-10)</cell></row><row><cell>FC-512</cell><cell>FC-512</cell></row><row><cell>BatchNormalization</cell><cell>BatchNormalization</cell></row><row><cell>Dropout(0.5)</cell><cell>Dropout(0.3)</cell></row><row><cell>Dropconnect(0.5)</cell><cell>Dropconnect(0.3)</cell></row><row><cell>FC-512</cell><cell>FC-512</cell></row><row><cell>FC-10</cell><cell>FC-10</cell></row><row><cell>soft-max</cell><cell>soft-max</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Test set error rates of two models for CIFAR-10 dataset</figDesc><table><row><cell>Model</cell><cell>Error rate</cell></row><row><cell>EnsNet (Proposed)</cell><cell>23.75%</cell></row><row><cell>Base CNN in EnsNet</cell><cell>23.90%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">THE MNIST DATABASE of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Le</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Chainer -A flexible framework of neural networks</title>
		<ptr target="https://docs.chainer.org/en/stable/reference/generated/chainer.links.SimplifiedDropconnect.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chainer: A next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Workshop on Machine Learning Systems (LearningSys) in the Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RMDL: Random multimodel deep learning for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Information System and Data Mining</title>
		<meeting>the 2nd International Conference on Information System and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">APAC: Augmented pattern classification with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yokoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03229</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training neural networks with local error signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nøkland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Eidnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4839" to="4850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
