<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Complexity-Aware Cascades for Deep Pedestrian Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-07-19">19 Jul 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>UCSD</roleName><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
							<email>zwcai@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Yahoo Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saberian</surname></persName>
							<email>saberian@yahoo-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Yahoo Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Complexity-Aware Cascades for Deep Pedestrian Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-07-19">19 Jul 2015</date>
						</imprint>
					</monogr>
					<note>Nuno Vasconcelos UCSD</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network <ref type="figure">(CNN)</ref>. This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection is an important problem in computer vision. Many of its applications, e.g. smart vehicles or surveillance, require real-time detection. Since, under the popular sliding window paradigm, there are close to a million windows per 640×480 pixel image, detection complexity can easily become intractable. This is an impediment to the deployment of sophisticated classifiers, such as deep learning models, in the pedestrian detection arena. The most popular architecture for real-time object detection is the detector cascade of <ref type="bibr" target="#b31">[32]</ref>. It exploits the fact that most image patches can be assigned to the background class by evaluation of a few simple cascade stages. This guarantees computational efficiency without compromising accuracy, since the few resulting false positives can be rejected by more complex detectors, in the late cascade stages. Given that these are rarely used, their complexity is not an impediment to high detection speeds. In result, it is possible to have both efficient and accurate detection.</p><p>While the cascade detection principle is intuitive, its implementation is far from trivial. Early cascade designs required extensive heuristics to determine the cascade configuration <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3]</ref>, lacking the ability to explicitly optimize the trade-off between detection accuracy and complexity. A commonly used assumption is that all features have equivalent complexity. This significantly simplifies the design, which reduces to choosing the features that maximize detection accuracy. In fact, popular methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> simply use a boosting algorithm (typically AdaBoost <ref type="bibr" target="#b7">[8]</ref>) to design a non-cascaded classifier and then transform it into a cascade, by addition of thresholds. These approaches suffer from two main problems. First, they do not aim to select features that optimize the trade-off between detection accuracy and complexity. Second, the "equivalent feature complexity" hypothesis only produces sensible cascades when applied to features that indeed have similar complexity. This constraint is, however, frequently violated <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>In fact, it has been remarkably difficult to accommodate, in cascade learning, features significantly heavier than those in common use. This problem is particularly pressing given the recent success of deep learning in object recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>. The intractable computation of a deep learning model under the sliding window paradigm is usually addressed with recourse to object proposal mechanisms <ref type="bibr" target="#b30">[31]</ref>, giving rise to a two-stage cascade that is far from optimal, in terms of the trade-off between detection accuracy and speed. For pedestrian detection, object proposals are frequently implemented with weak pedestrian detectors, sometimes cascaded detectors themselves <ref type="bibr" target="#b14">[15]</ref>. Due to the adhoc nature of these solutions, deep learning models have not been competitive for pedestrian detection, contradicting their recognition and classification performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>In this work, we address these problems by seeking an algorithm for optimal cascade learning under a criterion that penalizes both detection errors and complexity. For the lat-ter, we introduce a measure of implementation complexity that allows the definition of a complexity risk akin to the empirical risk commonly used for classifier design. This makes it possible to define quantities such as complexity margins and complexity losses, and account for these in the learning process. We do this with recourse to a Lagrangian formulation, which optimizes for the usual classification risk under a constraint in the complexity risk. A boosting algorithm that minimizes this Lagrangian is then derived. This algorithm, denoted Complexity-Aware Cascade Training (CompACT), is shown to select inexpensive features in the early cascade stages, pushing the more expensive ones to the later stages. This enables the combination of features of vastly different complexities in a single detector. These properties are demonstrated by the successful application of CompACT to the problem of pedestrian detection, using a pool of features ranging from Haar wavelets to deep convolutional neural networks (CNNs).</p><p>Overall, this work makes three major contributions. First, it proposes a novel algorithm for learning a complexity aware cascade, so as to achieve an optimal trade-off between accuracy and speed. To the best of our knowledge, this is the first algorithm to explicitly account for variable feature complexity in cascade learning, supporting weak learners of widely different complexities. Second, Com-pACT seamlessly integrates handcrafted and CNN features in a unified detector. This generalizes the object proposal architecture, guaranteeing the seamlessly integration of CNN stages with stages of any other complexity. Finally, a Com-PACT cascade for pedestrian detection is shown to achieve state of the art results on both Caltech <ref type="bibr" target="#b5">[6]</ref> and KITTI <ref type="bibr" target="#b10">[11]</ref>, at faster speeds than the closest competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Detector cascades learned with boosting are commonly used for detecting template-like objects, e.g. faces <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref>, pedestrians <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>, or cars <ref type="bibr" target="#b25">[26]</ref>. Early approaches used heuristics to find a cascade configuration of good trade-off between classification accuracy and complexity <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref>. More recently, optimization of the accuracy-complexity trade-off has started to receive attention <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>. <ref type="bibr" target="#b37">[38]</ref> empirically added a complexity term to the objective function of RealBoost. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> introduced the Lagrangian formulation that we adopt, but use a single feature family throughout the cascade. Since early cascades stages must be very efficient, this implies adopting simple weak learners, e.g. decision stumps.</p><p>This has motivated extensive work on the design of efficient features. For pedestrian detection, the integral channel features of <ref type="bibr" target="#b4">[5]</ref> have recently become popular. They extend the Haar-like features of <ref type="bibr" target="#b31">[32]</ref> into a set of color and histogram-of-gradients (HOG) channels. More recently, a computationally efficient version of <ref type="bibr" target="#b31">[32]</ref>, denoted the ag-gregate channel features (ACF), has been introduced in <ref type="bibr" target="#b3">[4]</ref>. <ref type="bibr" target="#b22">[23]</ref> complemented ACF with local binary patterns (LBP) and covariance features, for better detection accuracy.</p><p>Several works proposed alternative feature channels, obtained by convolving different filters with the original HOG+LUV channels <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21]</ref>. The SquaresChn-Ftrs of <ref type="bibr" target="#b0">[1]</ref> reduce the large number of features of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref> to 16 box-like filters of various sizes. <ref type="bibr" target="#b20">[21]</ref> extended the locally decorrelated features of <ref type="bibr" target="#b12">[13]</ref> to ACF, learning four 5×5 PCA-like filters from each of the ACF channels. Instead of empirical filter design, Zhang et al <ref type="bibr" target="#b35">[36]</ref> exploited prior knowledge about pedestrian shape to design informed filters. They later found, however, that such filters are actually not needed <ref type="bibr" target="#b36">[37]</ref>. Instead, the number of filters appears to be the most important variable: features as simple as checkerboard-like patterns, or purely random filters, can achieve very good performance, as long as there are enough of them. Although reached state-of-the-art performance has been achieved <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>, they are relatively slow, due to the convolution computations with several hundred filters.</p><p>While deep convolutional learning classifiers have achieved impressive results for general object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, e.g. on VOC2007 or ImageNet, they have not excelled on pedestrian detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22]</ref>. Benchmarks like Caltech <ref type="bibr" target="#b5">[6]</ref> are still dominated by classical handcrafted features (see e.g. a recent comprehensive evaluation of pedestrian detectors by <ref type="bibr" target="#b1">[2]</ref>). Recently, <ref type="bibr" target="#b14">[15]</ref> transferred the R-CNN framework to the pedestrian detection task, showing some improvement over previous deep learning detectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22]</ref>. However, the gap to the state of the art is still significant. Deep models also tend to be too heavy for sliding window detection. This is usually addressed with object proposal mechanisms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b14">15]</ref> that pre-select the most promising image patches. This two-stage decomposition (proposal generation and classification) is a simple cascade mechanism. In this work, we consider the seamless combination of these two stages into a cascade explicitly designed to account for both accuracy and complexity, so as to achieve detectors that are both highly accurate and fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Complexity-Aware Cascade Training</head><p>In this section we introduce the CompACT algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">AdaBoost</head><formula xml:id="formula_0">F (x) = k f k (x),<label>(1)</label></formula><p>using functional gradient descent on a classification risk <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. AdaBoost <ref type="bibr" target="#b7">[8]</ref> is based on the exponential loss φ(yF (x)) = e −yF (x) , minimizing the empirical risk</p><formula xml:id="formula_1">R E [F ] ≃ 1 |S t | i e −yiF (xi) ,<label>(2)</label></formula><p>on training samples S t = {(x i , y i )}. Boosting iterations compute the functional derivative of (2) along the direction of weak learner g(x) at the location of the current predictor F (x),</p><formula xml:id="formula_2">&lt; δR E [F ], g &gt; = d dǫ R E [F + ǫg] ǫ=0 = 1 |S t | i d dǫ e −yi(F (xi)+ǫg(xi)) ǫ=0 = − 1 |S t | i y i w i g(x i ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">w i = w(x i ) = e −yiF (xi) .<label>(4)</label></formula><p>The predictor is updated by selecting the steepest descent direction within a weak learner pool G = {g 1 (x), · · · , g n (x)},</p><formula xml:id="formula_4">g * (x) = arg max g∈G &lt; −δR E [F ], g &gt; = arg max g∈G 1 |S t | i y i w i g(x i ).<label>(5)</label></formula><p>The optimal step size for the update is</p><formula xml:id="formula_5">α * = arg min α R E [F + αg * ].<label>(6)</label></formula><p>For binary g * (x), this has a closed form solution</p><formula xml:id="formula_6">α * = 1 2 log i|yi=g * (x) w k i i|yi =g * (x) w k i .<label>(7)</label></formula><p>Otherwise, the optimal step size is found by a line search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Complexity-Aware Learning</head><p>Complexity-aware learning aims for the best trade-off between classification accuracy and complexity. This can be formulated as a constrained optimization problem, where classification risk is minimized under a bound on a com- <ref type="bibr" target="#b7">(8)</ref> and is identical to the minimization of the Lagrangian</p><formula xml:id="formula_7">plexity risk R C [F ], F * (x) = arg min F R E [F ] s.t. R C [F ] &lt; γ,</formula><formula xml:id="formula_8">L[F ] = R E [F ] + ηR C [F ],<label>(9)</label></formula><p>where η is a Lagrange multiplier that only depends on γ. To define a complexity risk, we note that (2) can be written as</p><formula xml:id="formula_9">R E [F ] ≃ 1 |S t | i φ[ξ(y i , F (x i ))],<label>(10)</label></formula><p>with φ(v) = e −v and ξ(y, F (x)) = yF (x). The function ξ(.) is the margin of example x under predictor F (.) and measures the confidence of the classification. Large positive margins indicate that x is correctly classified with high confidence, large negative margins the same for incorrect classification, and a margin zero that the example is on the classification boundary. The loss φ(.) is usually monotonically decreasing, penalizing all examples with less than a small positive margin. This forces the learning algorithm to concentrate on these examples, so as to produce as few negative margins as possible. The exponential loss of Ad-aBoost makes the penalty exponential on the confidence of incorrectly classified examples.</p><p>In this work, we consider complexity risks of a similar form</p><formula xml:id="formula_10">R C [F ] ≃ 1 |S t | i τ [κ(y i , F (x i ))],<label>(11)</label></formula><p>where κ[y, F (x)] is a measure of complexity for the classification of example x under F (.) and τ (.) a non-negative loss function that penalizes complexity. Drawing inspiration from the classification risk, we measure complexity with the complexity margin</p><formula xml:id="formula_11">κ[y, F (x)] = yΩ(F (x)),<label>(12)</label></formula><p>where Ω(F (x)) is a function of the time required to evaluate F (x), e.g. a number of machine operations or some other empirical measure of complexity. The complexity margin of (12) assigns positive (negative) complexity to positive (negative) examples, reflecting the fact that the computation spent on negative examples is "wasted" or "negative" while that spent on positives is "justified" or "positive". While positives have to survive all cascade stages, negatives should be rejected with little computation. The complexity loss τ (v) then determines the complexity-aware behavior of learning algorithms. For example, a decreasing τ (v) for v &lt; 0, penalizes negative examples of large complexity. This encourages classifiers that reject negatives with as little computation as possible. On the other hand, an increasing τ (v) for v &gt; 0 penalizes positives of large complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Embedded Cascade</head><p>A cascaded classifier is implemented as a sequence of classification stages</p><formula xml:id="formula_12">h i (x) = sgn[F i (x) + T i ], where T i is a threshold.</formula><p>A popular architecture is the embedded cascade, whose predictor has the embedded structure,</p><formula xml:id="formula_13">F k (x) = F k−1 (x) + f k (x) = k j=1 f j (x).<label>(13)</label></formula><p>In this paper, the cascade complexity is measured by the average per stage complexity,</p><formula xml:id="formula_14">Ω(F (x)) = 1 m m k=1 r k (x)Ω(f k (x)),<label>(14)</label></formula><p>where, using u[·] to denote the Heaviside step function,</p><formula xml:id="formula_15">r k (x) = k−1 j=1 u F j (x) + T j ,<label>(15)</label></formula><p>is an indicator of examples that survive all stages prior to k,</p><formula xml:id="formula_16">i.e. r k (x) = 1 if F i (x) + T i &gt; 0, ∀i &lt; k, and r k (x) = 0 otherwise.</formula><p>Since the average complexity is bounded by the largest weak learner complexity, it leads to a more balanced Lagrangian in (9) than the total complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cascade Boosting</head><p>The minimization of (9) requires the functional derivative of the Lagrangian along the direction of weak learner g(x) at the location of the current predictor F (x), <ref type="formula" target="#formula_2">(3)</ref>. To compute the derivative of the complexity risk we define u(ǫ) as u(ǫ) = 1 for ǫ &gt; 0 and u(ǫ) = 0 otherwise, and write</p><formula xml:id="formula_17">&lt; δL[F ], g &gt;=&lt; δR E [F ], g &gt; +η &lt; δR C [F ], g &gt;,<label>(16)</label></formula><formula xml:id="formula_18">where &lt; δR E [F ], g &gt; is as in</formula><formula xml:id="formula_19">Ω(F (x) + ǫg(x)) = = Ω(F (x)) + u(ǫ) Ω(F (x) + g(x)) − Ω(F (x)) = Ω(F (x))[1 − u(ǫ)] + u(ǫ)Ω(F (x) + g(x)) = Ω(F (x))[1 − u(ǫ)] + u(ǫ) m + 1 m k=1 r k (x)Ω(f k (x)) + r m+1 (x)Ω(g(x)) = Ω(F (x)) 1 − u(ǫ) + m m + 1 u(ǫ) + u(ǫ) m + 1 r m+1 (x)Ω(g(x)) = Ω(F (x))[1 − u(ǫ)ζ m ] + u(ǫ) r m+1 (x) m + 1 Ω(g(x)),</formula><p>where ζ m = 1 − m m+1 and we have used <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_20">Since u(ǫ) is not differentiable, it is approximated by u(ǫ) ≈ σ(ǫ), where σ(ǫ) is a differentiable function with σ(0) = 0, leading to &lt; δR C [F ], g &gt; (17) = 1 |S t | i d dǫ τ y i Ω F (x i ) + ǫg(x i ) ǫ=0 = − 1 |S t | i y i ψ(y i , x i ) r m+1 (x i ) m + 1 Ω(g(x i )) − ζ m Ω(F (x i )) , where ψ(y i , x i ) = −τ ′ [y i Ω(F (x i ))] σ ′ (0).<label>(18)</label></formula><p>Each boosting iteration updates F (x) with a step along the steepest descent direction of (16) within the weak learner learner pool G,</p><formula xml:id="formula_21">g * (x) = arg max g∈G &lt; −δL[F ], g &gt; .<label>(19)</label></formula><p>Combining <ref type="formula" target="#formula_2">(3)</ref>, <ref type="bibr" target="#b15">(16)</ref>, and (17) and denoting r i = r m+1 (x i ),</p><formula xml:id="formula_22">ω i = ω(y i , x i ), g i = g(x i ), and ψ i = ψ(y i , x i ), this is the direction that maximizes D[g] = 1 |S t | i y i ω i g i + ηr i ψ i Ω(g i ) m + 1 .<label>(20)</label></formula><p>Note that the term ζ m Ω(F (x i )) of (17) does not depend on g and plays no role in the optimization. The optimal step size for the update is</p><formula xml:id="formula_23">α * = arg min α L[F + αg * ],<label>(21)</label></formula><p>and can be found by a line search. The cascade predictor is finally updated with</p><formula xml:id="formula_24">F new (x) = F (x) + α * g * (x).<label>(22)</label></formula><p>Note that, from <ref type="formula" target="#formula_0">(18)</ref>, σ ′ (0) is a constant that rescales all ψ i equally. Hence, in <ref type="bibr" target="#b19">(20)</ref>, it can be absorbed into η. Without loss of generality, we assume that σ ′ (0) = 1. This boosting algorithm is denoted the complexity aware cascade training (CompACT) boosting algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Properties</head><p>CompACT has a number of interesting properties. First, the contribution of each training example to the complexity term in <ref type="bibr" target="#b19">(20)</ref> is multiplied by r i . Hence, only examples that survive the current cascade F contribute to the complexity term. We refer to the x i such that r i = 1 as active examples. Note that, given the set of active examples</p><formula xml:id="formula_25">S a (F ) = {(x i , y i ) ∈ S t |r i = 1},<label>(23)</label></formula><p>associated with F , <ref type="bibr" target="#b19">(20)</ref> can be replaced by</p><formula xml:id="formula_26">D[g] = 1 |S t |   i y i ω i g i + i|ri=1 y i ηψ i Ω(g i ) m + 1   .<label>(24)</label></formula><p>This complies with the intuition that examples which do not reach stage m + 1 during the cascade operation should not affect the complexity term for that stage. Second, most implementations of cascaded classifiers use weak learners of example-independent complexity, i.e. Ω(g(x i )) = Ω g , ∀i. While this does not hold for the cascade in general (different examples can be rejected at different stages), it holds for the examples in S a , i.e. Ω(F (x i )) = Ω F , ∀x i ∈ S a . In this case, the complexity weights only depend on the label y i . Defining </p><formula xml:id="formula_27">ψ + = −τ ′ [Ω F ] (ψ − = −τ ′ [−Ω F ])</formula><formula xml:id="formula_28">D[g] = 1 |S t | i y i ω i g(x i ) − η m + 1 |S a | |S t | ξ F Ω g ,<label>(25)</label></formula><formula xml:id="formula_29">with ξ F = π − F ψ − F − π + F ψ + F .</formula><p>Since |S a | decreases with cascade length, the rescaling of η by |Sa| |St| gradually weakens the complexity constraint as the cascade grows. While in the early iterations there is pressure to select weak learners of reduced complexity, this pressure reduces as iterations progress. Gradually, complex weak learners are penalized less and the algorithm asymptotically reduces to a cascaded version of AdaBoost. This makes intuitive sense, since the latter cascade stages process a much smaller percentage of the examples than the earlier ones and have much less impact on the overall complexity. On the other hand, since the surviving examples are the most difficult to classify, accurate classification requires weak learner accuracy to increase with cascade length. This usually (but not always) implies that weak learner complexity increases as well because powerful features usually require heavy computation. By pushing the complexity to the later stages, the algorithm can learn cascades that are both accurate and computationally efficient. This effect is reinforced by the fact that 1/(m + 1) also decreases with cascade length.</p><p>The loss τ (v) enables fine-tuning of this general behavior, via ξ F . In this work, we adopt the hinge loss</p><formula xml:id="formula_30">τ (v) = max(0, −v), for which ψ − F = 1, ψ + F = 0 and ξ F = π −</formula><p>F . This assigns no penalty to the complexity of positive examples, encouraging CompACT to focus on the fast rejection of negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pedestrian Detection</head><p>This section discusses the proposed pedestrian detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Pools of Variable Complexity</head><p>CompACT seeks the optimal trade-off between accuracy and complexity, at each cascade stage. This is most effective when the feature pool is composed of features of various complexities. In the cascade literature, where most detectors use a single feature family, it is common practice to pre-compute a large number of feature responses at all image locations, before any detection takes place <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23]</ref>. This, however, has unfeasible complexity if the feature pool is very large (e.g. the 200,000∼500,000 features proposed per patch in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b22">23]</ref>) or some features are computationally intense (e.g. the CNN features of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>). In these cases, it is neither tractable nor necessary to pre-compute all features at all image locations. For example, a cascade of 2048 decision trees of depth 2, will evaluate at most 4096 features per patch. Since the cascade rejects most candidate patches after a few stages, the most intensive features (e.g. CNN) are unlikely to be needed at most image locations. Hence, while pre-computation is useful for low-complexity features, complex features should be evaluated as necessary. We refer to the former as pre-computed features and the latter as computed just-in-time (JIT). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Pre-computed Features</head><p>Our pre-computed feature set consists of ACF <ref type="bibr" target="#b3">[4]</ref>, mostly due to its computational efficiency. Following <ref type="bibr" target="#b3">[4]</ref>, we extract 10 LUV+HOG channels. Since these are precomputed, the complexity of using an ACF feature in any cascade stage is 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Just-in-time Features</head><p>The JIT pool contains several feature subsets. The ability to weigh accuracy vs. computation enables CompACT to seamlessly combine these feature sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SS:</head><p>The self-similarity (SS) features of <ref type="bibr" target="#b27">[28]</ref> capture the difference between local patches and have achieved good performance on edge detection tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7]</ref>. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7]</ref>, we compute SS features on a 12×6 grid of the 16×8 ACF patch. This results in 72 2 × 10 = 25, 560 SS features per patch. Since the computation of an SS feature involves 2 ACF values, its complexity is 2.</p><p>CB: Checkerboard features (CB) are the result of convolving the ACF channels with a set of checkerboard filters. <ref type="bibr" target="#b36">[37]</ref> has shown that a simple set of such features could achieve state-of-the-art performance for pedestrian detection. Based on their observation that the number of features determines performance (rather than feature type), we adopt the set of 8 simple 2×2 checkerboard filters of <ref type="figure" target="#fig_2">Figure 1</ref>. A CB has implementation complexity of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LDA:</head><p>Locally decorrelated HOG features, computed with linear discriminant analysis (LDA), have shown some superiority for object detection over HOG features <ref type="bibr" target="#b12">[13]</ref>. <ref type="bibr" target="#b20">[21]</ref> showed that the computation of these features on ACF channels leads to a big improvement over ACF. We adopt this feature family but, unlike <ref type="bibr" target="#b20">[21]</ref>, restrict the filter size to 3×3. LDA features have complexity 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN:</head><p>In addition to operators defined over the ACF channels, we consider a set of CNN features. The CNN is a smaller version of the popular model of <ref type="bibr" target="#b16">[17]</ref>, with five convolutional layers and one fully connected layer. The CNN is applied to 64×64 image patches, the first convolutional layer has 32 filters, the remaining four have 64, and the fully connected layer consists of 1024 hidden units. All convolutional filters have size 3×3, and stride 1. The CNN model was originally trained with the ILSVRC14-DET dataset <ref type="bibr" target="#b23">[24]</ref>, using the cropped object patches, and then fine tuned on the target pedestrian dataset. For feature extraction, we only use the output of the 5 th convolutional layer, which can be seen as CNN feature channels, similar to ACF. These features are denoted as CNN. Inspired by the good performance and simplicity of the checkerboard features on ACF, we also compute them on the conv5 feature channels. These are denoted CNNCB features. The complexity of CNN features is of a different nature than that of ACF features. First, the implementation on a different processor (GPU instead of CPU) makes the direct comparison of number of operations meaningless. Second, while the CNN features are computed on an "as needed" basis, the structure of the network makes it inefficient to compute each feature individually. If the CNN features are needed to classify a certain image window, it is significantly more efficient to compute the 5 th layer responses over the whole window than repeatedly applying the network to subwindow regions. We account for these difficulties by setting a trigger complexity Ω CN N for CNN features. That is, in <ref type="bibr" target="#b24">(25)</ref>, CNN features have Ω g = Ω CN N if no CNN feature has been used by the previous cascade stages to classify the current patch. Once the CNN features are computed, the complexity of using any CNN feature is 1, similar to ACF, while CNNCB features have complexity 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Embedding Large CNN Models</head><p>Large CNN models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref> are now popular in computer vision. However, the use of these models in CompACT is challenging, due to the computational cost of embedding them in the iterative boosting algorithm. Our attempts to do so revealed impractical. Instead, we limited the use of a large CNN to the final cascade stage. Upon learning the cascade, we simply used a large CNN classifier as the final weak learner g of <ref type="bibr" target="#b21">(22)</ref>. Note that this has no loss of optimality, since α was learned with <ref type="bibr" target="#b20">(21)</ref>. The CNN is simply a descent direction of (19) unavailable to prior stages. It differs from the standard proposal+CNN approach in that 1) not only the bounding boxes but also the confidence scores of the cascade are forwarded to the deep CNN stage, and 2) the combination of the proposal mechanism (cascade) and large CNN is optimal under the well defined risk of <ref type="bibr" target="#b8">(9)</ref>.</p><p>In our implementation, we considered both the Alex <ref type="bibr" target="#b16">[17]</ref> and VGG <ref type="bibr" target="#b28">[29]</ref> models. Previous implementations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> have warped cropped patches to size 227×227. However, such large patches are computationally expensive. We adopted the convolutional layers from the pre-trained models and two (randomly initialized) fully connected layers of 2048 units each. These networks were fine tuned to the pedestrian datasets using Caffe <ref type="bibr" target="#b15">[16]</ref>. This allowed us to use the canonical 128×64 size for the pedestrian template. For Alex-Net, we used a convolution stride of 2 on the first layer, instead of 4 in the original model. For VGG-Net, we used all aspects of the original configuration other than input size and fully connected layers. While the original VGG-Net is approximately 8 times slower than the Alex-Net, the modified VGG-Net is only twice as slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Various experiments were performed to evaluate the performance of CompACT cascades. All times reported are for implementation on a single CPU core (2.10GHz) of an Intel Xeon E5-2620 server with 64GB of RAM. An NVIDIA Tesla K40M GPU was used for CNN computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Cascade Configuration</head><p>We started by learning a CompACT cascade on the Caltech pedestrian dataset, using the set up of <ref type="bibr" target="#b3">[4]</ref>. The cascade used 2048 decision trees of depth 2, and was bootstrapped 6 times during training, after stages {32, 128, 256, 512, 1024, 1536}, using the procedure of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>. <ref type="figure" target="#fig_3">Figure 2</ref> presents the configuration of the learned cascade, showing how features of different complexities were chosen at different stages. ACF features, which are the cheapest, were the only selected for the first 200 stages, and rarely chosen after stage 500. This suggests that the these features are very efficient but not very discriminant. A better trade-off between these two goals is achieved by the SS features, which were selected throughout the training process. It is particularly interesting that these features are competitive even for the later cascade stages. This suggests that they can be very discriminant despite their simplicity. Similarly, CB features were selected across a large range of cascade stages. This is unlike LDA features, which were rarely selected. These features do not appear to achieve a good trade-off between discrimination and complexity. More surprisingly, the CNN features were also rarely selected, with CNNCB dominating the late cascade stages. This suggests that the CNNCB representation is more dis- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cascade Comparison</head><p>The CompACT cascade of the previous section was compared to cascades of other architectures. <ref type="table">Table 1</ref> presents a comparison to the predominant architecture in the literature: cascades of a single feature type. In this case, the complexity penalty of (25) is equal for all weak learners, and Com-pACT reduces to standard boosting. This was used to produce "standard" cascades of ACF, SS, CB, LDA, CNN and CNNCB features. We start by noting that the implemented ACF outperforms <ref type="bibr" target="#b3">[4]</ref>. This is due to the use of a different bootstrapping strategy. Clearly, SS outperforms the other ACF-based features (ACF, CB, and LDA), achieving higher accuracy and speed. This confirms <ref type="figure" target="#fig_3">Figure 2</ref>, where SS features were selected throughout the detector. CB and LDA are more discriminant than ACF, but have higher complexity. CNN features have higher accuracy than all ACF-based features at the cost of a ten-fold increase in complexity over ACF. Finally, CNNCB has the best detection results, but only a marginal gain over CNN and much higher computation. When compared to CompACT cascades, all single feature cascades perform poorly. CompACT-ACF, which is restricted to ACF-based features, has higher accuracy than all ACF-based single feature cascades and is faster than most. CompACT-CNN, which includes all features, has the best detection performance. Note that not only its detection performance is clearly superior to the best single-feature cascade (CNNCB) but it is also 10 times faster. <ref type="table">Table 2</ref> presents a comparison to cascades that combine multiple features. "Boosting" is a cascade learned without complexity constraints (η = 0 in <ref type="bibr" target="#b24">(25)</ref>). This is equivalent to applying existing cascade learning algorithms to the diverse feature set considered in this work. "Manual" is an attempt to "hand-code" the behavior of CompACT, by restricting the boosting algorithm without complexity constraint (η = 0) to use certain types of features in different cascade stages. This restriction is based on feature complexity, as illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>. The features were ranked by complexity and used sequentially, each feature type be- ing used in approximately 400 stages. The two sides of Table 2 differ in that only ACF-based features were used on the left, while both these and the small CNN model were used on the right. In both cases, the "manual" cascade has low complexity but poor accuracy. "Boosting," on the other hand, can produce a more accurate cascade. The price is, however, a significant increase in complexity. CompACT achieves the best trade-off between accuracy and complexity. Note also the introduction of the small CNN model enables substantially better cascades, as long as a complexity penalty is assigned to it during learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Large CNN models</head><p>While the previous experiments only use small models, a number of experiments were performed with large models. These experiments were performed on both Caltech and KITTI, in both cases using cascades of 4096 decision trees of depth 5. These were bootstrapped 9 times, after stages {32, 128, 256, 512, 1024, 1536, 2048, 2560, 3328}.</p><p>For Caltech, we used the training set size of <ref type="bibr" target="#b20">[21]</ref>, and the template size 64×32 as in <ref type="bibr" target="#b3">[4]</ref>. On KITTI, test images were upsampled by 2 to detect pedestrians of height 25. This enabled the use of a single template size. After upsampling, the detected bounding boxes (minimum height of 50) had twice the actual object size. They were rescaled down by a factor of 2. <ref type="table" target="#tab_1">Table 3</ref> compares the performance of the CompACT cascade with small CNNs (denoted CompACT) with several variants for the inclusion of large CNNs. In all these variants, the large CNN is computed only on windows selected by CompACT. The times noted as "+" reflect the added cost of running the image patches through it. The "Proposal" columns report to the use of the CompACT cascade as a proposal mechanism <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> for the CNN. The "Embedded" columns report to the use of the large CNN as the last stage of the cascade, as discussed in Section 4.2. Finally, the "Intermediate" columns report to an intermediate between these two architectures. As with proposals, the large CNN stage was only applied to the CompACT output, after nonmaximum suppression (NMS). However, the prediction was that of (22), i.e. the CNN and CompACT scores were combined, using the coefficient α learned by boosting.</p><p>A number of interesting conclusions are possible. First, under the proposal architecture, only VGG improved on the CompACT cascade. For Alex, there was no benefit. This shows that the CompACT cascade is already a very good classifier. Second, the embedding of the large CNN on the CompACT model achieved the best results in all cases.   This shows that the ComPACT cascade score contains information that complements that of the CNN scores. For both CNN models, it was better to combine scores with the CompACT cascade than to consider the latter simply as a proposal mechanism. Finally, the theoretically more sound embedding of the large CNN before NMS ("Embedding") always produced higher detection accuracy than the combination after NMS ("Intermediate"). This, however, had substantially less computation, since the number of bounding boxes is approximately 10 times smaller after NMS. CompACT achieves state-of-the-art performance, close to <ref type="bibr" target="#b36">[37]</ref>. Note that the competing detectors -Katamari <ref type="bibr" target="#b1">[2]</ref> and SpatialPooling+ <ref type="bibr" target="#b22">[23]</ref> -combine many features (HOG, LBP, spatial covariance, optical flow, multiple detectors, etc.) and are all quite slow. The same holds for the state-of-the-art implementation of Checkerboards, which requires a large number of filter channels <ref type="bibr" target="#b36">[37]</ref>. On the other hand, CompACT runs at 4 fps on a relatively slow processor. The CompACT-Deep cascade performs even better -7 points better than the state-of-the-art <ref type="bibr" target="#b36">[37]</ref> and 11 points better than the best deep pedestrian detector <ref type="bibr" target="#b14">[15]</ref>! CompACT-Deep runs at 2fps and is faster than the competing detectors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>. <ref type="figure" target="#fig_8">Figure 4</ref> and <ref type="table" target="#tab_3">Table 4</ref> summarize performance on KITTI. Since test images are larger than in Caltech, running times are higher on this dataset. Nevertheless, the CompACT cascade is the fastest of all the state-of-the-art detectors. Note that it uses approximately the same number of feature channels (including the CNN model) as pAUCEnsT <ref type="bibr" target="#b22">[23]</ref> and FilteredICF <ref type="bibr" target="#b36">[37]</ref>, which are both much less accurate and   slower. R-CNN <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12]</ref>, the only CNN detector on KITTI, is also substantially weaker than CompACT-Deep (difference larger than 8 points). Overall, the only approach competitive with the CompACT-Deep cascade is the Regionlets method of <ref type="bibr" target="#b32">[33]</ref>. However, this work only reports classification times, excluding the time needed to generate proposals, which can be on the order of several seconds. This is equivalent to only accounting for the processing time of the last stage of the CompACT-Deep model, which is 0.25 second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with the state-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed the CompACT boosting algorithm for learning complexity-aware detector cascades. By optimizing classification risk under a complexity constraint, CompACT produces cascades that push features of high complexity to the later cascade stages. This has been shown to enable the seamless integration of multiple feature families in a unified design. This integration extends to features, such as deep CNNs, that were previously beyond the realm of cascaded detectors. The proposed CompACT cascades also generalize the popular combination of object proposals+CNN, which they were shown to outperform. Finally, we have shown that a pedestrian detector learned by application of CompACT to a diverse feature pool achieves state-of-the-art detection rates on Caltech and KITTI, with much faster speeds than competing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>A</head><label></label><figDesc>decision rule h(x) = sign[F (x)] of predictor F (x) maps a feature vector x ∈ X to a class label y ∈ Y = {−1, 1}. Boosting learns a strong decision rule by combining a set of weaker learners f k (x),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>as the value of ψ i for positive (negative) examples, and π − F (π + F ) as the percentage of negative (positive) active examples, (20) reduces to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Eight 2×2 checkerboard-like filters used in this work. Red (Green) is used to represent value +1 (-1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Stage configuration of the proposed CompACT cascade (blue) and the manually set cascade (green). Only one in five (fifty) stages is shown for the CompACT (manual) cascade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Comparison to state-of-the-art on Caltech (reasonable).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3</head><label>3</label><figDesc>compares two CompACT pedestrian detectors to the state of the art on Caltech. CompACT refers to the model using "ACF + small CNN features", and CompACT-Deep to the model with the embedded VGG model in the last stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Comparison to state-of-the-art on KITTI Pedestrian (moderate).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison to single-feature cascades (MR: log-average miss-rate). 34.29 37.89 37.15 28.07 26.93 32.15 23.82 time (s) 0.07 0.08 0.23 0.16 0.87 2.05 0.11 0.28 Comparison to multiple-feature cascades.</figDesc><table><row><cell>Method</cell><cell cols="2">ACF SS</cell><cell cols="5">Single Type CB LDA CNN CNNCB ACF CNN CompACT</cell></row><row><cell cols="8">MR 42.6 Method Boosting Manual CompACT Boosting Manual CompACT ACF-based ACF-based+Small CNN</cell></row><row><cell>MR</cell><cell>33.06</cell><cell cols="2">36.08</cell><cell>32.15</cell><cell>22.37</cell><cell>25.46</cell><cell>23.82</cell></row><row><cell>time (s)</cell><cell>0.41</cell><cell>0.11</cell><cell></cell><cell>0.11</cell><cell>2.69</cell><cell>0.28</cell><cell>0.28</cell></row><row><cell cols="8">criminant. Recall that, while the CNN features are a little</cell></row><row><cell cols="8">more efficient, CompACT boosting weighs complexity less</cell></row><row><cell cols="8">heavily than discrimination in the late cascade stages.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Performance of CompACT cascades using large CNNs.</figDesc><table><row><cell cols="2">Method CompACT</cell><cell>Proposals Alex VGG Alex VGG Alex VGG Intermediate Embedded</cell></row><row><cell>MR</cell><cell>18.92</cell><cell>19.59 14.77 16.18 13.71 14.96 11.75</cell></row><row><cell>time (s)</cell><cell>0.25</cell><cell>+0.01 +0.03 +0.01 +0.03 +0.1 +0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison to state-of-the-art detectors on KITTI. Note: * ignores the time needed to compute object proposals.</figDesc><table><row><cell>Methods</cell><cell cols="4">Easy Moderate Hard Time (s)</cell></row><row><cell>DPM</cell><cell>45.50</cell><cell>38.35</cell><cell>34.78</cell><cell>10</cell></row><row><cell>DA-DPM</cell><cell>56.36</cell><cell>45.51</cell><cell>41.08</cell><cell>21</cell></row><row><cell>RCNN</cell><cell>61.61</cell><cell>50.13</cell><cell>44.79</cell><cell>4</cell></row><row><cell>FilteredICF</cell><cell>61.14</cell><cell>53.98</cell><cell>49.29</cell><cell>40</cell></row><row><cell>pAUCEnsT</cell><cell>65.26</cell><cell>54.49</cell><cell>48.60</cell><cell>60</cell></row><row><cell>regionlet</cell><cell>73.14</cell><cell>61.15</cell><cell>55.21</cell><cell>1  *</cell></row><row><cell>CompACT</cell><cell>65.35</cell><cell>54.92</cell><cell>49.23</cell><cell>0.75</cell></row><row><cell cols="2">CompACT-Deep 70.69</cell><cell>58.74</cell><cell>52.71</cell><cell>1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3666" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, CVRSUAD workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1841" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroCOLT</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Additive logistic regression: A statistical view of boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="407" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Class-specific hough forests for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1022" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative decorrelation for clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3158" to="3165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High detection-rate cascades for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Boosting algorithms as gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Frean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="512" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="546" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning optimal embedded cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boosting algorithms for detector cascade learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2569" to="2605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast pedestrian detection by cascaded random forest with dominant orientation templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic cascades for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boosting chain learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="709" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Informed haarlike features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast car detection using image strip features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2703" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
