<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning and Sentence Level Features for Named Entity Recognition on Tweets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-09-07">September 7, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pius</forename><surname>Von Däniken</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SpinningBytes AG Mark Cieliebak ZHAW</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Learning and Sentence Level Features for Named Entity Recognition on Tweets</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
						<meeting>the 3rd Workshop on Noisy User-generated Text <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="166" to="171"/>
							<date type="published" when="2017-09-07">September 7, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present our system for the WNUT 2017 Named Entity Recognition challenge on Twitter data. We describe two modifications of a basic neural network architecture for sequence tagging. First, we show how we exploit additional labeled data, where the Named Entity tags differ from the target task. Then, we propose a way to incorporate sentence level features. Our system uses both methods and ranked second for entity level annotations, achieving an F1-score of 40.78, and second for surface form annotations, achieving an F1-score of 39.33.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) is an important Natural Language Processing task. Its goal is to tag entities such as names of people and locations in text. State-of-the-art systems can achieve F1-scores of up to 92 points on English news texts ( <ref type="bibr" target="#b1">Chiu and Nichols, 2015)</ref>. Achieving good performance on more complex domains such as user generated texts on social media is still a hard problem. The best system submitted for the WNUT 2016 shared task achieved an F1-score of 52.41 on English Twitter data ( <ref type="bibr" target="#b13">Strauss et al., 2016)</ref>.</p><p>In this work, we present our submission for the WNUT 2017 shared task on "Novel and Emerging Entity Recognition" <ref type="bibr" target="#b3">(Derczynski et al., 2017</ref>). We extend a basic neural network architecture for sequence tagging ( <ref type="bibr" target="#b1">Chiu and Nichols, 2015;</ref><ref type="bibr">Col- lobert et al., 2011</ref>) by incorporating sentence level feature vectors and exploiting additional labeled data for transfer learning. We build on and take inspiration from recent work from <ref type="bibr" target="#b4">(Falkner et al., 2017;</ref><ref type="bibr" target="#b11">Sileo et al., 2017)</ref> on NER for French Twitter data ( <ref type="bibr" target="#b7">Lopez et al., 2017)</ref>.</p><p>Our submitted solution reached an F1-score of 41.76 for entity level annotations and 57.98 on surface form annotations. This places us second on entity level annotations, where the best system achieved an F1-score of 41.90, and fourth on surface form annotations, where the best system achieved an F1-score of 66.59.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Our solution is based on a sequence labeling system that uses a bidirectional LSTM <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) which extracts features for training a Conditional Random Field <ref type="bibr">(Sut- ton and McCallum, 2012)</ref>. We apply a transfer learning approach, since previous research has shown that this can improve sequence labeling systems ( <ref type="bibr" target="#b16">Yang et al., 2017</ref>). More precisely, we modify the base system to allow for joint training on the WNUT 2016 corpus ( <ref type="bibr" target="#b13">Strauss et al., 2016)</ref>, which uses a different tag set than our target task. In addition, we extend the system to incorporate sentence level feature vectors. All these methods are combined to build the system that we used for our submission to the WNUT 2017 shared task. <ref type="figure">Figure 1</ref> shows an overview of the different architectures, which are described in detail in the following sections. <ref type="figure">Figure 1a</ref> shows an overview of our base system. We use a bidirectional Long Short Term Memory network (LSTM) (Hochreiter and <ref type="bibr">Schmidhu- ber, 1997</ref>) to learn the potential function for a linear chain Conditional Random Field (CRF) <ref type="bibr">(Sut- ton and McCallum, 2012)</ref> to predict a sequence of Named Entity tags y 1:T from a sequence of feature vectors x 1:T . This is based on an architecture previously used in ( <ref type="bibr" target="#b1">Chiu and Nichols, 2015)</ref> Bidirectional LSTM: For every word in w t in a given input sentence w 1:T , we first compute a feature vector x t , which is the concatenation of all the word level features described in Section 2.5. The sequence of feature vectors x 1:T is then fed to a bidirectional LSTM. The output of both the forward and backward LSTM are concatenated to get o 1:T , which get passed through a Rectified Linear Unit, (ReLU ) <ref type="bibr">(Nair and Hin- ton, 2010</ref>). Every o t ∈ o 1:T then gets passed through a fully connected feed-forward network with one hidden layer and ReLU activation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Sequence Labeling System</head><formula xml:id="formula_0">s t = W 2 relu(W 1 o t + b 1 ) + b 2 .</formula><p>Let N tags be the number of possible NER-tags, d o the dimension of o t and d h the dimension of the hidden layer. The resulting vector s t ∈ R Ntags represents a score for every possible tag y at time step t. The values</p><formula xml:id="formula_1">W 1 ∈ R d h ×do , b 1 ∈ R d h , W 2 ∈ R Ntags×d h and b 2 ∈ R Ntags are weights of the feed-forward net- work.</formula><p>Conditional Random Field: A linear chain CRF models the conditional probability of an output sequence y 1:T given an input sequence x 1:T as:</p><formula xml:id="formula_2">p (y 1:T |x 1:T ) = 1 Z(x 1:T ) T t=1 e φ(y t−1 ,yt,x 1:T ,t,Θ) (1)</formula><p>where Z (x 1:T ) is a normalization constant:</p><formula xml:id="formula_3">Z (x 1:T ) = ∀y 1:T T t=1 e φ(y t−1 ,yt,x 1:T ,t,Θ) (2)</formula><p>φ is a potential function parametrized by a set of parameters Θ. In our case we use:</p><formula xml:id="formula_4">φ (y t−1 , y t , x 1:T , t, Θ = {θ, A}) = s θ,yt,t (x 1:T ) + A y t−1 ,yt<label>(3)</label></formula><p>Let θ be the parameters of the network described above. Then s θ,yt,t (x 1:T ) is the score that the network parametrized by θ outputs for tag y t at time step t given the input sequence x 1:T . A ∈ R Ntags×Ntags is a matrix such that A i,j is the score of transitioning from tag i to tag j.</p><p>Training: During training we try to maximize the likelihood of the true tag sequence y 1:T given the input feature vectors x 1:T . We use the Adam ( <ref type="bibr" target="#b6">Kingma and Ba, 2014</ref>) algorithm to optimize the parameters Θ = {θ, A}. Additionally we perform gradient clipping (Pascanu et al., 2012) and apply dropout ( <ref type="bibr" target="#b12">Srivastava et al., 2014</ref>) to the LSTM outputs o 1:T . The neural network parameters θ are randomly initialized from a normal distribution with mean zero and variance according to <ref type="bibr">(Glorot and Bengio, 2010</ref>) (normal Glorot initialization). The transition scores A are initialized from a uniform distribution with mean zero and variance according to <ref type="bibr">(Glorot and Bengio, 2010)</ref>, (uniform Glorot initialization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transfer Learning</head><p>In this setting we use the WNUT 2016 corpus ( <ref type="bibr" target="#b13">Strauss et al., 2016)</ref> as an additional source of labeled data. The idea is to train the upper layers of the neural network on both datasets to improve its generalization ability. It was shown in <ref type="bibr" target="#b16">(Yang et al., 2017</ref>) that this can improve the system performance. <ref type="figure">Figure 1b</ref> gives an overview of our transfer learning architecture. Modified Architecture: We share all network layers except for the last linear projection to get separate tag scores for each data set:</p><formula xml:id="formula_5">s 2016 t = W 2016 2 relu(W 1 o t + b 1 ) + b 2016 2 s 2017 t = W 2017 2 relu(W 1 o t + b 1 ) + b 2017 2<label>(4)</label></formula><p>The resulting tag scores get fed to separate CRFs, which have separate transition matrices A 2016 and A 2017 .</p><p>Training: During training we alternately use a batch from each dataset and backpropagate the loss of the corresponding CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Incorporating Sentence Level Features</head><p>Figure 1c shows how we include sentence level features into our architecture. In this setting we take an additional feature vector f sent = F (x 1:T ) ∈ R dsent for each input sentence x 1:T . Modified Architecture: We use an additional feed-forward network to extract tag scores s sent ∈ R Ntags from the sentence feature vector f sent :</p><formula xml:id="formula_6">s sent = W 2,sent relu (W 1,sent f sent + b 1,sent ) + b 2,sent</formula><p>The dimensions used are:</p><formula xml:id="formula_7">W 1,sent ∈ R d h,sent ×dsent , b 1,sent ∈ R d h,sent , W 2,sent ∈ R Ntags×d h,sent and b 2,sent ∈ R Ntags . The value d h,sent</formula><p>is the dimension of the hidden layer of the feed-forward network. Let s 1:T,word be the scores that the basic network described in Section 2.1 outputs for sequence x 1:T . To get the final scores s 1:T fed to the CRF we add s sent to every s t,word ∈ s 1:T,word : s t = s sent + s t,word .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combined System</head><p>The combined system adds the sentence level features to the transfer learning architecture. We share all layers except the linear projections to tag scores for both sentence features and word features in a manner analogous to Sections 2.2 and 2.3. The resulting architecture is shown in <ref type="figure">Figure 1d</ref>.   <ref type="figure">Figure 2</ref>. First, we add special padding tokens on both sides of the character sequence w, to extend it to a target length, l w,max . If there is an odd number of paddings, the additional padding is added on the right. For sequences longer than l w,max , only the first l w,max characters are used. An embedding matrix E char ∈ R Nc×dc maps characters to R dc vectors. N c is the number of unique characters in the dataset with the addition of the padding token.</p><note type="other">a Input Sequence Character Embeddings Filter Maps Convolution Max Pooling Feature Vector Figure 2:</note><p>Using E char , we embed the padded sequence w and get C w ∈ R lw,max×dc . A set of m convolution filters ∈ R dc×h is then applied to C w . This results in m feature maps M i ∈ R lw,max−h+1 , which are passed through a ReLU activation. The final feature vector F ∈ R m is attained by max pooling, such that F i = max M i .</p><p>The embedding matrix is initialized using uniform Glorot initialization. The m convolution filters are initialized using normal Glorot initialization.</p><p>Character Capitalization Convolution Features: Analogous to the word capitalization features, we use additional character capitalization features. The feature options are: upper, lower, punctuation, numeric and other. We apply a neural network with the same architecture as described above to extract the final character capitalization feature vector. Sentence Embeddings: In ( <ref type="bibr" target="#b9">Pagliardini et al., 2017</ref>) the authors introduce sent2vec, a new method for computing sentence embeddings. They show that these embeddings provide improved performance for several downstream tasks.</p><p>To train the sent2vec model, we use the same training set as the one used for word embeddings and we use default values for all the model parameters 2 . In particular, the resulting sentence feature vectors are in R 100 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We implemented the system described in Section 2.4 using the Tensorflow framework <ref type="bibr">3</ref> .</p><p>We monitored the systems performance during training and aborted experiments that had an F1-score of less than 40 after two epochs (evaluated on the development set). We let successful experiments run for the full 6 epochs (cf. Section 3.2). For the submission to WNUT 2017, we ran 6 successful experiments and submitted the one which 2 https://github.com/epfml/sent2vec 3 https://www.tensorflow.org/ <ref type="table">Table 1</ref>: Model Parameters had the highest entity level F1-score on the development set.</p><note type="other">Parameter Value l w,max 30 N tags WNUT 2016 21 N tags WNUT 2017 13 d wordCap 6 d c 15 LSTM hidden units 64 d h,word 128 d h,sent 128 m 10 h 3 Dropout rate 0.3 Learning Rate 0.003 Gradient Clip Norm 2 Batch size 100 Number of epochs 6</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>Tokenization: Since the WNUT 2016 and WNUT 2017 corpora are in the CoNLL format, they are already tokenized. To tokenize the additional tweets used for training word and sentence embeddings (cf. Section 2.5), we use the Twitter tokenizer provided by the Python NLTK library 4 .</p><p>Token Substitution: We perform some simple pattern-based token substitutions. To normalize Twitter user handles, we substitute every word starting with an @ character by a special user token. Similarly, all words starting with the prefix http are replaced by a url token. Finally, for words longer than one character, we remove up to one initial # character. <ref type="table">Table 1</ref> shows the parameters used for training the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments Performed After The Submission</head><p>Following the submission, we conducted additional experiments to investigate the influence of the transfer learning approach and sent2vec features on the system performance.   For each of the 4 systems described in Section 2, we ran 6 experiments. We use the same parameters as shown in Section 3.2. <ref type="table" target="#tab_3">Table 2</ref> shows precision, recall and F1-score of our system. We compute the mean and standard deviations over the 6 successful experiments we considered for submission (cf. Section 3). <ref type="table" target="#tab_4">Ta- ble 3</ref> shows the breakdown of the performance of the annotations we submitted for the WNUT 2017 shared task. <ref type="table" target="#tab_6">Table 4</ref> shows the performance of the different subsystems proposed in Section 2. We report the mean and standard deviation over the 6 experiments we performed after submission, for every system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>All reported scores were computed using the evaluation script provided by the task organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>From table 4 we can see that using sent2vec features increases precision and decreases recall slightly, leading to an overall lower performance compared to the basic system. The transfer learning system shows a more substantial decrease in precision and increase in recall and overall performs best out of the 4 systems. Combination of the two approaches is counterproductive and outperforms the basic system only slightly.</p><p>During training we observed that restarting experiments as described in Section 3 was only necessary when using sent2vec features.</p><p>One weakness of our transfer learning setting is that the two datasets we used have almost identical samples and only differ in their annotations. The WNUT 2016 corpus uses 10 entity classes: company, facility, Geo location, movie, music artist, other, person, product, sports team, and TV show. Further work is needed to study the effect of using an unrelated data set for transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We described a deep learning approach for Named Entity Recognition on Twitter data, which extends a basic neural network for sequence tagging by using sentence level features and transfer learning. Our approach achieved 2nd place at the WNUT 2017 shared task for Named Entity Recognition, obtaining an F1-score of 40.78.</p><p>For future work, we plan to explore the power of transfer learning for NER in more depth. For instance, it would be interesting to see how annotated NER data for other languages or other text types affects the system performance.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Entities</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>by considering character n-grams of the word. The embedding matrix E word is not updated during training. Word Capitalization Features: Following (Chiu and Nichols, 2015) we add explicit capitaliza- tion features, since capitalization information is lost during word embedding lookups. The 6 fea- ture options are: all capitalized, uppercase ini- tial, all lower cased, mixed capitalization, emoji and other. An embedding matrix E wordCap ∈ R 6×d wordCap is used to feed these features to the network and updated during training via back- propagation. E wordCap is initialized using normal Glorot initialization. Character Convolution Features: A convolu- tional neural network is used to extract additional character level features. Its architecture is shown in</figDesc><table>Neural Network used to extract character 
level features 

2.5 Features 

Word Embeddings: We use the FastText (Bo-
janowski et al., 2016) library to compute word 
embeddings. We train the model on a corpus of 
200 million tweets and all tweets from the WNUT 
2016 and WNUT 2017 corpora. The vocabulary 
contains all words occurring at least 10 times. 
Other parameters use the default values set by the 
library 1 . In particular, the size of the context win-
dow is set to 5 and the embedding dimension is 
100. 
This results in an embedding matrix E word ∈ 
R N vocab ×100 , where N vocab is the number of 
unique tokens in the WNUT 2016 and WNUT 
2017 corpora. FastText predicts embedding vec-
tors for words that were out-of-vocabulary during 
training </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Aggregated performance of all experi-
ments, run before the submission, evaluated on the 
test set 

Precision (%) Recall (%) 
F1 
Surface Forms 
45.47 
34.66 
39.33 
Entities Overall 
47.09 
35.96 
40.78 
Corporation 
8.24 
11.67 
9.66 
Creative Work 
21.92 
11.76 
15.31 
Group 
31.71 
9.22 
14.29 
Location 
58.95 
44.80 
50.91 
Person 
57.67 
61.97 
59.74 
Product 
20.00 
5.13 
8.16 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Performance of the submitted annotations evaluated on the test set</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of the different subsystems evaluated on the test set, after the submission 

transfer learning. Conférence sur l'Apprentissage 
Automatique. 

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the Thirteenth In-
ternational Conference on Artificial Intelligence and 
Statistics. PMLR, Chia Laguna Resort, Italy, vol-
ume 9 of Proceedings of Machine Learning Re-
search, pages 249-256. </table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/ fastText</note>

			<note place="foot" n="4"> http://www.nltk.org/api/nltk. tokenize.html#module-nltk.tokenize. casual</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.04606" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1511.08308" />
		<title level="m">Named Entity Recognition with Bidirectional LSTM-CNNs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy, User-generated Text (W-NUT) at EMNLP. ACL</title>
		<meeting>the 3rd Workshop on Noisy, User-generated Text (W-NUT) at EMNLP. ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Swiss Chocolate at CAp 2017 NER challenge: Partially annotated data and</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Dolce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Pius von Däniken, and Mark Cieliebak</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">French named entity recognition in twitter challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cdric</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Derbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlie</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coralie</forename><surname>Reutenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frdrique</forename><surname>Segond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.02507" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the difficulty of training Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1211.5063" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Sileo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<title level="m">Synapse at CAp 2017 NER challenge: Fasttext crf. Conférence sur l&apos;Apprentissage Automatique</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Results of the WNUT16 named entity recognition shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bethany</forename><forename type="middle">E</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2nd Workshop on Noisy Usergenerated Text</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="138" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="373" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003<address><addrLine>Stroudsburg, PA, USA, CONLL &apos;03</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.06345" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
