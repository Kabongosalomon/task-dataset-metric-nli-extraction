<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REGION ENSEMBLE NETWORK: IMPROVING CONVOLUTIONAL NETWORK FOR HAND POSE ESTIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-05-09">9 May 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">REGION ENSEMBLE NETWORK: IMPROVING CONVOLUTIONAL NETWORK FOR HAND POSE ESTIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-05-09">9 May 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional Network</term>
					<term>Hand Pose Esti- mation</term>
					<term>Ensemble Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hand pose estimation from monocular depth images is an important and challenging problem for human-computer interaction. Recently deep convolutional networks (ConvNet) with sophisticated design have been employed to address it, but the improvement over traditional methods is not so apparent.</p><p>To promote the performance of directly 3D coordinate regression, we propose a tree-structured Region Ensemble Network (REN), which partitions the convolution outputs into regions and integrates the results from multiple regressors on each regions. Compared with multi-model ensemble, our model is completely end-to-end training. The experimental results demonstrate that our approach achieves the best performance among state-of-the-arts on two public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Hand pose estimation from single depth image plays an important role in applications of human-computer interface (HCI) and augmented reality (AR). Though has been studied for several years [1], it is still challenging due to large view variance, high joint flexibility, poor depth quality, severe self occlusion and similar part confusion.</p><p>Recently, convolutional networks (ConvNets) have witnessed great growth in several computer vision tasks such as object classification <ref type="bibr" target="#b0">[2]</ref> and human pose estimation <ref type="bibr" target="#b1">[3]</ref> because of great modeling capacity and end-to-end feature learning. ConvNets have also been introduced to solve the problem of hand pose estimation, often with complicated structure design such as multi-branch inputs <ref type="bibr" target="#b2">[4]</ref> <ref type="bibr" target="#b3">[5]</ref> and multimodel regression <ref type="bibr" target="#b3">[5]</ref>  <ref type="bibr" target="#b4">[6]</ref> [7] <ref type="bibr" target="#b6">[8]</ref>. However, ConvNets remain unable to obtain significant advantage over traditional random forest based methods <ref type="bibr" target="#b7">[9]</ref>  <ref type="bibr" target="#b8">[10]</ref>.</p><p>Inspired by model ensemble and multi-view voting <ref type="bibr" target="#b0">[2]</ref>, we present a single ConvNet architecture named Region Ensemble Net (REN) 1 <ref type="figure" target="#fig_0">(Fig.1)</ref> to directly regress the 3D joint coordinates in monocular depth images with end-to-end optimization and inference. We implement it by training individual fully-connected (FC) layers on multiple feature regions and combining them as ensembles. As shown in our experiments, REN significantly promotes the performance of our ConvNet, which outperforms all state-of-the-art methods on two challenging hand pose benchmarks <ref type="bibr" target="#b2">[4]</ref>  <ref type="bibr" target="#b9">[11]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Hand pose estimation with ConvNets Recently deep Con-vNets have been applied on hand pose estimation for depth imaging <ref type="bibr" target="#b10">[12]</ref> <ref type="bibr" target="#b11">[13]</ref>. Tompson et al. <ref type="bibr" target="#b2">[4]</ref> use ConvNets to produce 2D heat maps and infer the 3D hand pose with inverse kinematics. Oberweger et al. <ref type="bibr" target="#b3">[5]</ref> directly regress the 3D positions with multi-stage ConvNets using a linear layer as pose prior. In <ref type="bibr" target="#b4">[6]</ref>, a feedback loop is employed to iteratively correct the mistake, in which 3 ConvNets are used for initialization, synthesis and pose updating. Ge et al. <ref type="bibr" target="#b5">[7]</ref> employ 3 Con-vNets to separately regress 2D heat maps for each view with depth projections and fuse them to produce 3D hand pose. In <ref type="bibr" target="#b12">[14]</ref>, physical joint constraints are incorporated into a forward kinematics based layer in ConvNet. Similarly, Zhang et al. <ref type="bibr" target="#b6">[8]</ref> embeds skeletal manifold into ConvNets and trains the model end-to-end to render sequential prediction. Multi-model ensemble methods for ConvNets Traditional ensemble learning means that training multiple individual models and combining their outputs via averaging or weighted fusions, which is widely adopted in recognition competitions <ref type="bibr" target="#b0">[2]</ref>. In addition to bagging <ref type="bibr" target="#b0">[2]</ref>  <ref type="bibr" target="#b13">[15]</ref>, boosting is also introduced for people counting <ref type="bibr" target="#b14">[16]</ref>. However, using multiple ConvNets requires large memory and time, which is not practical for applications. Multi-branch ensemble methods for ConvNets We view single ConvNet with the fusion of multiple branches as a generalized type of ensemble. One popular strategy is to fuse different scaling inputs <ref type="bibr" target="#b2">[4]</ref>  <ref type="bibr" target="#b3">[5]</ref> or different image cues <ref type="bibr" target="#b15">[17]</ref> [18] <ref type="bibr" target="#b17">[19]</ref> with multi-input branches. Another approach is to employ multi-output branches with shared convolutional feature extractor, either training with different samples <ref type="bibr" target="#b18">[20]</ref> or learning to predict different categories <ref type="bibr" target="#b19">[21]</ref>. Compared with multi-input, multi-output methods cost less time because inference of FC layers is much faster than that of convolutional layers. Our method also falls into such category. Multi-view testing for ConvNets Multi-view testing is widely used to improve accuracy for object classification <ref type="bibr" target="#b0">[2]</ref>. In <ref type="bibr" target="#b0">[2]</ref>, predictions from 10-crop (4 corner and 1 center with horizontal flip) are averaged on single ConvNet. In <ref type="bibr" target="#b20">[22]</ref>, fully-convolutional networks are employed in testing with inputs of multi-scale and multi-view and average pooling is applied on the score map to obtain the final scores. Such strategy has not been applied on hand pose estimation yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">REGION ENSEMBLE NETWORK (REN)</head><p>As in <ref type="figure" target="#fig_0">Fig.1</ref>, REN starts with a ConvNet for feature extraction. Then the features are divided into multiple grid regions. Each region is fed into FC layers and learnt to fuse for pose prediction. In this section we introduce the basic network architecture, region ensemble structure and implementation details. Network architecture The architecture of our ConvNet for feature extraction consists of six 3 × 3 convolution layers ( <ref type="figure" target="#fig_1">Fig.2</ref>) that accepts a 96×96 depth image as inputs. Each convolution layer is followed by a Rectified Linear Unit (ReLU) activation. Two residual connections <ref type="bibr" target="#b21">[23]</ref> are adopted between pooling layers with 1 × 1 convolution filters for dimension increase. The dimension of output feature maps is 12 × 12 × 64. For regression, we use two 2048 dimension FC layers with dropout rate of 0.5 for each regressor. The output of regressor is a 3 × J vector representing the 3D world coordinates for hand joints, where J is the number of joints. Region Ensemble Multi-view testing averages predictions from different crops of original image, which reduces the varaince for image classification <ref type="bibr" target="#b0">[2]</ref>. Directly using multiple inputs is time-consuming. Because each activation in the convolutional feature maps is contributed by a receptive field in the image domain, we can project the multi-view inputs onto the regions of the feature maps. So multi-view voting is equal to utilizing each regions to separately predict the whole hand pose and combining the results. Based on it, we define a tree-structured network consisting of a single ConvNet trunk and several regression branches <ref type="figure" target="#fig_0">(Fig.1)</ref>. We uniformly divide the feature maps of ConvNet into an n × n grid. For each grid region, we feed it into the FC layers respectively as branches. A simple strategy for combination of different branches is bagging, which averages all outputs of branches. To better boost the predictions from all the regions, we employ region ensemble strategy instead of bagging: features from the last FC layers of all regions are concatenated and used to infer the hand pose with an extra regression layer. The whole network can be trained end-to-end by minimizing the regression loss. We set n = 2 to balance the trade-off between performance and efficiency, so the receptive field of single region within the 96 × 96 image bounding is 62 × 62, which can be seen as the corner crop in <ref type="bibr" target="#b0">[2]</ref>. Including the center crop does not provide any further increase in accuracy. Note that we do not adopt multi-scale regions because it will lead to imbalanced parameter number in FC layers.</p><p>There are three main differences between proposed methods and multi-view voting: 1) To our knowledge, all multiview testing methods before are designed for image classification while region ensemble can be applied on both classification and regression. 2) We adopt end-to-end training for region ensemble instead of testing only, making the ConvNet adjust the contributions from each views. 3) We replace the average pooling with FC on concatenated features to learn to fuse, which increases the learning ability of the network. Implementation We follow the practice in <ref type="bibr" target="#b2">[4]</ref> [5] using Caffe <ref type="bibr" target="#b22">[24]</ref>. We first segment the foreground and extract a cube of size 150mm from the depth image centered in the centroid of hand region. Then the cube is resized into 96 × 96 patch of depth values normalized to [−1, 1] as input for ConvNet, with data augmentation of random translation, scaling and rotation. We use stochastic gradient descent (SGD) with a mini-batch size of 128. The learning rate starts from 0.005 and is divided by 10 after every 50000 iterations, and the model is trained for up to 200000 iterations. In the meanwhile, we use a weight decay of 0.0005 and a momentum of 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We apply our method on two publicly datasets: ICVL <ref type="bibr" target="#b9">[11]</ref> and NYU <ref type="bibr" target="#b2">[4]</ref>. The former dataset has 300K images for train- <ref type="figure">Fig. 3</ref>. Example results on ICVL <ref type="bibr" target="#b9">[11]</ref> and NYU <ref type="bibr" target="#b2">[4]</ref>: basic network (first row) and region ensemble network (second row).</p><p>ing and 1.6K for testing with 16 joints. The latter dataset has 72K images for training and 8K for testing with 14 joints. The performance is evaluated by two metrics: per-joint average Euclidean distance (in millimeters) and percentage of frames in which all errors of joints are below a threshold <ref type="bibr" target="#b3">[5]</ref>. First we compare our REN with baseline and different ensemble settings on ICVL dataset. Then we compare it with several state-of-the-art methods on both datasets. Self-comparison For comparison, we implement four baseline: 1) Basic network has the same convolution structure in <ref type="figure" target="#fig_1">Fig.2</ref> and single regressor on the full feature map. 2) Basic Large network is the same as basic network except for using 8192 dimensions in the second FC layer, which contains the similar number of parameters to REN. 3) Basic Bagging network has four basic networks trained independently on the same data with different random order and augmentation. The average predictions of all the networks form the final prediction. 4) Region Bagging network shares the same region division with REN but predicts independent hand pose for each region and averages them as prediction.</p><p>Results in <ref type="figure">Fig.4</ref> shows that: 1) bagging or ensemble based methods are more effective since all the ensemble versions significantly outperform the single one. 2) region ensemble is much better than basic bagging and slightly better than region bagging. Qualitative results of region ensemble (second row) and basic network (first row) are shown in <ref type="figure">Fig.3</ref>. <ref type="table">Table.</ref>1 further compares the running time (Nvidia Titan X GPU) for different approaches. Our REN obtains the most accurate results with nearly the same number of parameters as basic large network and region bagging, while the basic bagging takes significantly more parameters and time. And it runs up to over 3000fps on a single GPU, which is fast enough for practical use.</p><p>Comparison with state-of-the-arts We compare our methods against several state-of-the-art approaches on ICVL dataset <ref type="bibr" target="#b9">[11]</ref>   <ref type="bibr" target="#b6">[8]</ref>. <ref type="figure" target="#fig_4">Fig.5 and 6</ref> show that proposed REN obtains the best accuracy among all the algorithms.  <ref type="figure">Fig. 4</ref>. Self-comparison for percentage of success frames on ICVL dataset <ref type="bibr" target="#b9">[11]</ref>.</p><p>In details, on ICVL our method outperforms LSN <ref type="bibr" target="#b8">[10]</ref> on the threshold of (5mm, 15mm) and (20mm, 60mm), and surpasses other methods with a large margin. And the mean errors obtains 0.63mm decrease compared with LSN, which is a 7.77% relative improvement. Similarly on NYU, our results are better than multi-view ConvNets <ref type="bibr" target="#b5">[7]</ref> on the threshold of (5mm, 15mm) and (40mm, 80mm) and significantly more accurate than other approaches. Note that either LSN or multi-view ConvNets employ multiple models with complicated inputs, while our REN only uses single model without multi-stage regression, which indicates the power for proposed region ensemble strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>To boost the performance of single ConvNet for hand pose estimation, we present a simple but powerful region ensem-      <ref type="figure">Fig. 6</ref>. Comparison with state-of-the-arts on NYU <ref type="bibr" target="#b2">[4]</ref> datasets: distance error (left) and percentage of success frames (right). ble structure by dividing the feature maps into different regions and jointly training multiple regressors on all regions with fusion. Such strategy significantly improves the Con-vNet without extra large computation overhead. The experimental results demonstrate that our method outperforms all the state-of-the-arts on two datasets. In the future we will investigate and analysis more ensemble methods for ConvNets. Since proposed region ensemble is general, we will also try to apply them on more tasks such as human pose estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of region ensemble network (REN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Structure of ConvNet for feature extraction. The dotted arrows represent residual connections<ref type="bibr" target="#b21">[23]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison with state-of-the-arts on ICVL<ref type="bibr" target="#b9">[11]</ref> dataset: distance error (left) and percentage of success frames (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Average 3D distance error (mm) and GPU forward time (ms) of different methods on ICVL dataset<ref type="bibr" target="#b9">[11]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Error(mm) Time(ms)</cell></row><row><cell>Basic</cell><cell>8.36</cell><cell>0.21</cell></row><row><cell>Basic Large</cell><cell>8.18</cell><cell>0.22</cell></row><row><cell>Basic Bagging</cell><cell>7.94</cell><cell>0.88</cell></row><row><cell>Region Bagging</cell><cell>7.63</cell><cell>0.31</cell></row><row><cell>Region Ensemble</cell><cell>7.47</cell><cell>0.31</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Codes are available at https://github.com/guohengkai/ region-ensemble-network</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work is supported by NSFC (No. 61271390), and 863 Plan (No. 2015AA016304). Thanks Shulan Pan for paper edition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">REFERENCES</head><p>[1] James Steven Supancic III, Gregory Rogez, Yi Yang, Jamie Shotton, and Deva Ramanan, "Depth-based hand pose estimation: methods, data, and challenges," in IEEE International Conference on Computer Vision, 2015.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hands deep in deep learning for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Computer Vision Winter Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to search on manifolds for 3d pose estimation of articulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00596</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hand pose estimation from local surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth estimation for speckle projection system using progressive reliable points growing matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanwu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied optics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="516" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-accuracy stereo matching based on adaptive ground control points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanwu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1412" to="1423" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model-based deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to count with cnn boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Twostream convolutional neural network for accurate rgb-d fingertip detection using depth and edge information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07978</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeptrack: Learning discriminative feature representations online for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate fingertip detection from binocular mask images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convolutional neural net bagging for online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
	<note>Computer Vision and Image Understanding</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Network of experts for large-scale image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Haris</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06119</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deephand: robust hand pose estimation by completing a matrix imputed with deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
