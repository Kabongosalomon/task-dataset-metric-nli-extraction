<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BomJi at SemEval-2018 Task 10: Combining Vector-, Pattern-and Graph-based Information to Identify Discriminative Attributes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
							<email>esantus@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachussetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universität Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuele</forename><surname>Chersoni</surname></persName>
							<email>emmanuelechersoni@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Aix-Marseille University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BomJi at SemEval-2018 Task 10: Combining Vector-, Pattern-and Graph-based Information to Identify Discriminative Attributes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes BomJi, a supervised system for capturing discriminative attributes in word pairs (e.g. yellow as discriminative for banana over watermelon). The system relies on an XGB classifier trained on carefully engineered graph-, pattern-and word embeddingbased features. It participated in the SemEval-2018 Task 10 on Capturing Discriminative Attributes, achieving an F1 score of 0.73 and ranking 2nd out of 26 participant systems. 2  The pre-trained vectors are available, respectively, at https://code.google.com/archive/p/ word2vec/ (Google News, 300 dimensions) and at https://nlp.stanford.edu/projects/glove/ (Common Crawl, 840B tokens, 300 dimensions).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent introduction of popular software packages for training neural word embeddings <ref type="bibr">(Mikolov et al., 2013a,b;</ref><ref type="bibr" target="#b9">Levy and Goldberg, 2014)</ref> has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks <ref type="bibr" target="#b0">(Baroni et al., 2014)</ref>.</p><p>However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement <ref type="bibr" target="#b1">(Batchkarov et al., 2016;</ref><ref type="bibr" target="#b7">Faruqui et al., 2016;</ref><ref type="bibr" target="#b15">Santus et al., 2016a)</ref>.</p><p>Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings <ref type="bibr" target="#b8">(Krebs and Paperno, 2016)</ref>: given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter).</p><p>Since even related words may differ for some non-shared attributes (e.g. hypernyms and hyponyms), the ability of automatically recognize discriminative features would be an extremely useful addition for the creation of ontologies and other types of lexical resources and would make machine decisions interpretable, enabling human validation . Moreover, one can think to applications to many other NLP domains, such as machine translation and dialogue systems <ref type="bibr" target="#b8">(Krebs and Paperno, 2016)</ref>.</p><p>In the present contribution, we describe the BomJi classification system, which we used for the identification of discriminative features between concept pairs. According to the official evaluation results provided by the organizers 1 , our system ranked second out of 26 participants. Our score, F 1 = 0.73 lags slightly behind the best score of 0.75. After the evaluation period, we run further experiments including all investigated features and found that the system can achieve up to 0.75 F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Capturing Discriminative Attributes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task and Dataset Description</head><p>The task of capturing discriminative attributes between words can be described as a binary classification task, in which the system has to assign a positive label if the feature is discriminative for the first concept over the second one, and a negative label otherwise.</p><p>In the test data, the first two words correspond to the concepts being compared (they are called, respectively, the pivot and the comparison term) and the third word is the feature, which could describe a discriminative attribute or not (some examples are shown in <ref type="table" target="#tab_1">Table 1</ref>). In the paper, we will refer   to the elements of the triples as w1, w2 and f eat.</p><p>A training and validation set have been provided for system development (figures in <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embeddings and Graphs</head><p>For the Discriminative Attributes task, we combined word embeddings, patterns and information extracted from a graph-based distributional model.</p><p>Concerning the word embeddings, we used the vectors produced by two popular frameworks for word embeddings: Word2Vec <ref type="bibr">(Mikolov et al., 2013a,b)</ref> and GloVe <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref>. <ref type="bibr">2</ref> The Word2Vec Skip-Gram architecture is a singlelayer neural network, based on the dot-product between word vectors, in which the vector representation is optimized to predict the context of a target word given the word itself. The context generally consists of a word window of a fixed width around the target. The other framework, GloVe, is similar to traditional count models based on matrix factorization (Turney and Pantel, 2010; <ref type="bibr" target="#b0">Baroni et al., 2014)</ref>, in the sense that vectors are trained on global word-word co-occurrence counts. In the case of GloVe, the training objective is to learn word vectors such that their dot product equals the logarithm of the probability of the word to cooccur <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref>.</p><p>As for graph-based information, we used the Jo-BimText architecture introduced by <ref type="bibr" target="#b3">Biemann and Riedl (2013)</ref>. In JoBimText, lexical items are represented as the set of their p most salient contexts, where the contexts are words connected to the target by a given syntactic link or by a lexical pattern, and saliency is defined as an association measure between target and context, such as Positive Local Mutual Information <ref type="bibr" target="#b6">(Evert, 2004)</ref>. Differently from vector models, similarity between words in JoBimText is simply based on the overlap count of their common contexts.</p><p>Regarding patterns, first we extracted sentences where words and their features co-occur from a web-scale sentence-based index of English web  and then we extracted the patterns connecting our target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Methodology</head><p>The predictions submitted for the evaluation of Task 10 were obtained with a system that consists of a classifier, the Extreme Gradient Boosting (or XGBoost, Chen and Guestrin <ref type="formula">(2016)</ref>), trained on vectors aggregating carefully engineered graph-, pattern-and word embedding features.</p><p>In this section, we provide an overview of each feature type, leaving the discussion of their contribution to Section 3. The total of 55,026 features we used can be divided into five major groups.</p><p>CO-OCCURRENCE. Thirteen features related to word and word-feature frequency were calculated on the basis of the information extracted from a corpus of 3.2B words, corresponding to about 20% of the Common Crawl. For each word-feature combination (i.e. w1 − f eat and w2 − f eat), we calculated: i) the co-occurrence count; ii) word count; iii) feature count; iv) Positive Pointwise Mutual Information (PPMI <ref type="bibr" target="#b5">(Church and Hanks, 1990)</ref>) between each word and the feature; v) Positive Local Mutual Information (PLMI <ref type="bibr" target="#b6">(Evert, 2004)</ref>) between each word and the feature. Further, we added three features representing the subtractions between the values of i), iv) and v) for the two word-feature combinations.</p><p>JOBIMTEXT. Another set of twenty-four features comes directly from JoBimText. They were calculated after extracting information through the public accessible JoBimText API 3 , which returns a JSON file containing -for every target -a sorted list of N features and their association scores (up to N = 1, 000). As Jo-BimText distinguishes features according to their POS and dependency roles (i.e. features are in form WORD#POS#DEPENDENCY), a given f eat may appear multiple times in different POSdependency combinations. However, we found that f eat rarely appears in the top N features of w1 and w2, so we calculated our features not only for the given targets (i.e. w x ), but also for the first among their top 10 neighbors for which f eat was found (i.e. top(neighbor(w x ) f eat, max = 10)) and the first among the top 10 f eat neighbors for which the target was found (i.e. top(neighbor(f eat) w x , max = 10)). This allowed us to check also whether the neighbors of the given words were associated with the candidate discriminative attributes or vice versa. The features are defined as follows (here they are described only with reference to the query on w x , but this should be generalized to the other cases):</p><p>• prediction by rank: it is 1 if f eat is ranked higher for w1 than for w2, 0 otherwise;</p><p>• prediction by score: it is 1 if the total score between w1 − f eat is higher than for w2 − f eat, 0 otherwise;</p><p>• total score: sum of the scores of w1−f eat if prediction by score is 1, of w2 − f eat otherwise;</p><p>• top rank: top rank of f eat for w1 if prediction by score is 1, for w2 otherwise;</p><p>• bottom rank: last rank of f eat for w1 if prediction by score is 1, for w2 otherwise;</p><p>• number of occurrences: count of how many times a feature appears among the features of w1 if prediction by score is 1, otherwise the occurrences among the features w2 are counted;</p><p>• which neighbor?: integer showing whether the query was performed on w1/w2 (in this case it would be initialized to 0), or on its neighbors (in this case it would be initialized with the rank of the first neighbor where f eat was found);</p><p>• which f eat neighbor?: integer showing whether the query was performed on w1/w2 (in this case it would be initialized to 0) or on the f eat neighbors (in this case it would be initialized with the rank of the first f eat neighbor where w1 or w2 was found).</p><p>WORD EMBEDDING FEATURES. <ref type="bibr" target="#b11">Mikolov et al. (2013a)</ref> showed how vector offsets encode semantic information. We decided to include five features computed from either the Word2Vec or the Glove vectors, in order to take advantage of the offset information.</p><p>They are computed, respectively, as: cos((w1 − w2), f eat), cos((w1 − f eat), (w2 − f eat)), cos((w1 − f eat), w2), cos((w2 − f eat), w1). Finally, also the cosine between the word vectors (i. <ref type="figure">e. cos((w1, w2)</ref>) has been included.</p><p>WORD EMBEDDING VECTORS. These features are the simple concatenation of the three vectors of w1, w2 and f eat. Again, we have two versions of these features, one for Word2Vec and one for Glove.</p><p>PATTERNS. In order to characterize the relation between words and features, we used an index to extract patterns occurring between them, independently of the order in which they appeared, and limited the maximum number of results to 10,000 sentences.</p><p>The patterns consist of sequences of either lemmatized tokens, POS or dependency tags, which are used to abstract from the surface form, thereby increasing the recall. Since the number of extracted patterns was far too high, we decided to use only patterns with a frequency higher than 100, obtaining a set of 53,136 items, using the observed pattern frequency per word pair as a predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Choosing the Training Set</head><p>During the practice phase, we noticed that the training set and the validation set show very different distributions. Running 5-fold cross validation experiments on either dataset, we obtained very high scores (sometimes close to 0.95). However, such scores did not generalize to the other dataset, where they dropped to about 0.60.</p><p>This was only partially due to lexical memorization (some lexemes were present in multiple triples of the same dataset, cf. <ref type="bibr" target="#b10">Levy et al. (2015)</ref>; <ref type="bibr" target="#b16">Santus et al. (2016b)</ref>). In fact, investigating the frequency of the words in the triples, we found that, on average, in our index, the first and the second words, w1 and w2, were about four times more frequent in the validation than in the training set (respectively 4.7M and 5.4M versus 0.9M and 1M ); similarly, the third word (i.e. f eat) was almost twice more frequent in the validation than in the training set (i.e. 3.9M versus 2.9M ). When the test set was made available, we could verify that its frequency distribution resembled the one in the validation set, with the first and second words respectively at 3.3M and 2M , and the third at about 4.5M occurrences.</p><p>Given these differences, we have chosen to train our system only on the validation set, tuning the hyper-parameters by means of 5-fold cross validation. Because of its small size, we decided to train our second submission on a derived training set (henceforth New Validation), consisting of the 2,722 triples from the validation set plus 2,278 triples randomly extracted from the training, for a  <ref type="table">Table 3</ref>: Results both in absolute (F1) and in incremental terms (F1++: in brackets the features used to obtain the score) on the test set, organized by training set. In bold, we report the best results. In bold-italics, we report the submitted systems. total of 5,000 samples. The use of different training data was the only difference between the two submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Selection</head><p>During the practice phase, we performed experiments with several classifiers, including K-Neighbors (with K = 3), Decision Tree (with max depth = 5), Random Forest (with max depth = 5, n estimators = 10 and max f eatures = 1), Multilayer Perceptron (with alpha = 1), AdaBoost and XGB (the latter two with default settings).</p><p>Before running the classifiers, we also used Linear Support Vector Classification (SVC) with penalty = l1 and we tested several values of C (i.e. 0.05, 0.1, 0.25, 0.5, 1) for feature selection.</p><p>In almost all settings we found that the best performing classifiers were the Random Forest, the Multilayer Perceptron and, above all others, XGB. With respect to the value of C for the feature selection, instead, we noticed that it varied in relation to the feature types, with minor influence on the performance of XGB (+/-2%). In the final submission, therefore, we opted for removing this step from the pipeline and for keeping the full vector.</p><p>Concerning feature selection, we found that the pattern features had a neutral effect on the performance during cross validation. Similarly we noticed that Glove and Word2Vec performed comparably. Thus, we opted for submitting the output of the systems without using the pattern features and only Glove features (Word2Vec had lower coverage on the dataset). As it can be noticed in <ref type="table">Table  3</ref>, however, this decision has slightly lowered the performance of our system in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Contribution</head><p>In order to measure the contribution of the features, we re-ran the experiments over the test set, after training our model on the three available sets: training, validation and new validation sets.</p><p>Results are reported in <ref type="table">Table 3</ref>, in which it is easy to notice a few things: the performance is strongly related to the choice of the training set, with Validation being better that New Validation, which is in turn better than the original Training set; the thirteen co-occurrence features are those that provide the major contribution to the performance, reaching a F1 score of 0.72. Further useful features are the word embedding vectors (900 features), the word embedding features (5 features) and, to some extent, the information from JoBim-Text. Pattern-based features perform the worst, almost on par with random guessing.</p><p>The submitted systems do not correspond to the systems obtaining the best performance in postevaluation experiments (see the bold and bolditalics scores in <ref type="table">Table 3</ref>); this was due to the use of Glove instead of Word2Vec in our submitted systems, because none of the embedding models had an edge over the other in the validation process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Examples of triples from the training dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Examples Features</cell><cell>Split P-N</cell></row><row><cell>Training</cell><cell>17,782</cell><cell>1,292</cell><cell>37.06%-62.94%</cell></row><row><cell>Validation</cell><cell>2,722</cell><cell>576</cell><cell>50.1%-49.9%</cell></row><row><cell>Test</cell><cell>2,340</cell><cell>577</cell><cell>44.74%-55.26%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Number of examples, distinctive features and Positive-Negative split for each dataset.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/ competitions/17326#results</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See www.jobimtext.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">ConclusionsIn this paper we have presented BomJi, a supervised system for capturing discriminative attributes in word pairs (e.g. yellow as discriminative for banana over watermelon). BomJi relies on an XGB classifier trained on carefully engineered</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>graph-, pattern-and word embedding-based features. In the paper we have reported the contribution for each features, discussing the model selection and showing that a major factor affecting the performance was the choice of the training data.</p><p>In the official Task 10 evaluation, our submitted systems achieved an F1 score of 0.73, ranking 2nd out of 26 participant systems.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t Count, Predict! A Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Critique of Word Similarity as a Method for Evaluating Distributional Semantic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Batchkarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP</title>
		<meeting>the 1st Workshop on Evaluating Vector-Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Framework for Enriching Lexical Semantic Resources with Distributional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="312" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Text: Now in 2D! A Framework for Lexical Expansion with Contextual Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language Modeling</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="95" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM Sigkdd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM Sigkdd International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word Association Norms, Mutual Information, and Lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Evert</forename></persName>
		</author>
		<title level="m">The Statistics of Word Cooccurrences: Word Pairs and Collocations</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Problems With Evaluation of Word Embeddings Using Word Similarity Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP</title>
		<meeting>the 1st Workshop on Evaluating Vector-Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Capturing Discriminative Attributes in a Distributional Space: Task Proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP</title>
		<meeting>the 1st Workshop on Evaluating Vector-Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="51" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Word Embedding as Implicit Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do Supervised Distributional Methods Really Learn Lexical Inference Relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient Estimation of Word Representations in Vector Space</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a Web-Scale Dependency-Parsed Corpus from Common Crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Testing APSyn Against Vector Cosine on Similarity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuele</forename><surname>Chersoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Blache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACLIC</title>
		<meeting>PACLIC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nine Features in a Random Forest to Learn Taxonomical Semantic Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin-Shing</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning: Vector Space Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
