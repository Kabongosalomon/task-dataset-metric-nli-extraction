<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analyzing and Improving the Image Quality of StyleGAN Tero Karras NVIDIA Samuli Laine NVIDIA Miika Aittala NVIDIA Janne Hellsten NVIDIA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA and Aalto University Timo Aila NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Analyzing and Improving the Image Quality of StyleGAN Tero Karras NVIDIA Samuli Laine NVIDIA Miika Aittala NVIDIA Janne Hellsten NVIDIA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The resolution and quality of images produced by generative methods, especially generative adversarial networks (GAN) <ref type="bibr" target="#b15">[16]</ref>, are improving rapidly <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b4">5]</ref>. The current state-of-the-art method for high-resolution image synthesis is StyleGAN <ref type="bibr" target="#b23">[24]</ref>, which has been shown to work reliably on a variety of datasets. Our work focuses on fixing its characteristic artifacts and improving the result quality further.</p><p>The distinguishing feature of StyleGAN <ref type="bibr" target="#b23">[24]</ref> is its unconventional generator architecture. Instead of feeding the input latent code z ∈ Z only to the beginning of a the network, the mapping network f first transforms it to an intermediate latent code w ∈ W. Affine transforms then produce styles that control the layers of the synthesis network g via adaptive instance normalization (AdaIN) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8]</ref>. Additionally, stochastic variation is facilitated by providing additional random noise maps to the synthesis network. It has been demonstrated <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">38]</ref> that this design allows the intermediate latent space W to be much less entangled than the input latent space Z. In this paper, we focus all analysis solely on W, as it is the relevant latent space from the synthesis network's point of view.</p><p>Many observers have noticed characteristic artifacts in images generated by StyleGAN <ref type="bibr" target="#b2">[3]</ref>. We identify two causes for these artifacts, and describe changes in architecture and training methods that eliminate them. First, we investigate the origin of common blob-like artifacts, and find that the generator creates them to circumvent a design flaw in its architecture. In Section 2, we redesign the normalization used in the generator, which removes the artifacts. Second, we analyze artifacts related to progressive growing <ref type="bibr" target="#b22">[23]</ref> that has been highly successful in stabilizing high-resolution GAN training. We propose an alternative design that achieves the same goal -training starts by focusing on low-resolution images and then progressively shifts focus to higher and higher resolutions -without changing the network topology during training. This new design also allows us to reason about the effective resolution of the generated images, which turns out to be lower than expected, motivating a capacity increase (Section 4).</p><p>Quantitative analysis of the quality of images produced using generative methods continues to be a challenging topic. Fréchet inception distance (FID) <ref type="bibr" target="#b19">[20]</ref> measures differences in the density of two distributions in the highdimensional feature space of an InceptionV3 classifier <ref type="bibr" target="#b39">[39]</ref>. Precision and Recall (P&amp;R) <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b26">27]</ref> provide additional visibility by explicitly quantifying the percentage of generated images that are similar to training data and the percentage of training data that can be generated, respectively. We use these metrics to quantify the improvements.</p><p>Both FID and P&amp;R are based on classifier networks that have recently been shown to focus on textures rather than shapes <ref type="bibr" target="#b11">[12]</ref>, and consequently, the metrics do not accurately capture all aspects of image quality. We observe that the perceptual path length (PPL) metric <ref type="bibr" target="#b23">[24]</ref>, originally introduced as a method for estimating the quality of latent space <ref type="bibr">Figure 1</ref>. Instance normalization causes water droplet -like artifacts in StyleGAN images. These are not always obvious in the generated images, but if we look at the activations inside the generator network, the problem is always there, in all feature maps starting from the 64x64 resolution. It is a systemic problem that plagues all StyleGAN images.</p><p>interpolations, correlates with consistency and stability of shapes. Based on this, we regularize the synthesis network to favor smooth mappings (Section 3) and achieve a clear improvement in quality. To counter its computational expense, we also propose executing all regularizations less frequently, observing that this can be done without compromising effectiveness.</p><p>Finally, we find that projection of images to the latent space W works significantly better with the new, pathlength regularized StyleGAN2 generator than with the original StyleGAN. This makes it easier to attribute a generated image to its source (Section 5).</p><p>Our implementation and trained models are available at https://github.com/NVlabs/stylegan2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Removing normalization artifacts</head><p>We begin by observing that most images generated by StyleGAN exhibit characteristic blob-shaped artifacts that resemble water droplets. As shown in <ref type="figure">Figure 1</ref>, even when the droplet may not be obvious in the final image, it is present in the intermediate feature maps of the generator. <ref type="bibr" target="#b0">1</ref> The anomaly starts to appear around 64×64 resolution, is present in all feature maps, and becomes progressively stronger at higher resolutions. The existence of such a consistent artifact is puzzling, as the discriminator should be able to detect it.</p><p>We pinpoint the problem to the AdaIN operation that normalizes the mean and variance of each feature map separately, thereby potentially destroying any information found in the magnitudes of the features relative to each other. We hypothesize that the droplet artifact is a result of the generator intentionally sneaking signal strength information past instance normalization: by creating a strong, localized spike that dominates the statistics, the generator can effectively scale the signal as it likes elsewhere. Our hypothesis is supported by the finding that when the normalization step is removed from the generator, as detailed below, the droplet artifacts disappear completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generator architecture revisited</head><p>We will first revise several details of the StyleGAN generator to better facilitate our redesigned normalization. These changes have either a neutral or small positive effect on their own in terms of quality metrics. <ref type="figure" target="#fig_1">Figure 2a</ref> shows the original StyleGAN synthesis network g <ref type="bibr" target="#b23">[24]</ref>, and in <ref type="figure" target="#fig_1">Figure 2b</ref> we expand the diagram to full detail by showing the weights and biases and breaking the AdaIN operation to its two constituent parts: normalization and modulation. This allows us to re-draw the conceptual gray boxes so that each box indicates the part of the network where one style is active (i.e., "style block"). Interestingly, the original StyleGAN applies bias and noise within the style block, causing their relative impact to be inversely proportional to the current style's magnitudes. We observe that more predictable results are obtained by moving these operations outside the style block, where they operate on normalized data. Furthermore, we notice that after this change it is sufficient for the normalization and modulation to operate on the standard deviation alone (i.e., the mean is not needed). The application of bias, noise, and normalization to the constant input can also be safely removed without observable drawbacks. This variant is shown in <ref type="figure" target="#fig_1">Figure 2c</ref>, and serves as a starting point for our redesigned normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Instance normalization revisited</head><p>One of the main strengths of StyleGAN is the ability to control the generated images via style mixing, i.e., by feeding a different latent w to different layers at inference time. In practice, style modulation may amplify certain feature maps by an order of magnitude or more. For style mixing to work, we must explicitly counteract this amplification on a per-sample basis -otherwise the subsequent layers would not be able to operate on the data in a meaningful way.</p><p>If we were willing to sacrifice scale-specific controls (see video), we could simply remove the normalization, thus removing the artifacts and also improving FID slightly <ref type="bibr" target="#b26">[27]</ref>. We will now propose a better alternative that removes the artifacts while retaining full controllability. The main idea is to base normalization on the expected statistics of the incoming feature maps, but without explicit forcing.   Here we have broken the AdaIN to explicit normalization followed by modulation, both operating on the mean and standard deviation per feature map. We have also annotated the learned weights (w), biases (b), and constant input (c), and redrawn the gray boxes so that one style is active per box. The activation function (leaky ReLU) is always applied right after adding the bias. (c) We make several changes to the original architecture that are justified in the main text. We remove some redundant operations at the beginning, move the addition of b and B to be outside active area of a style, and adjust only the standard deviation per feature map. (d) The revised architecture enables us to replace instance normalization with a "demodulation" operation, which we apply to the weights associated with each convolution layer.</p><formula xml:id="formula_0">c 1 A A A w 2 w 3 w 4 b 2 b 3 + + b 4 + B B B … c 1 Upsample Conv 3×3 A w 3 Mod Demod Conv 3×3 w 2 A Mod Demod Conv 3×3 w 4 A Demod Mod b 2 + B b 3 + B b 4 + B … (a) StyleGAN (b) StyleGAN (detailed) (c) Revised architecture (d) Weight demodulation</formula><p>Recall that a style block in <ref type="figure" target="#fig_1">Figure 2c</ref> consists of modulation, convolution, and normalization. Let us start by considering the effect of a modulation followed by a convolution. The modulation scales each input feature map of the convolution based on the incoming style, which can alternatively be implemented by scaling the convolution weights:</p><formula xml:id="formula_1">w ijk = s i · w ijk ,<label>(1)</label></formula><p>where w and w are the original and modulated weights, respectively, s i is the scale corresponding to the ith input feature map, and j and k enumerate the output feature maps and spatial footprint of the convolution, respectively. Now, the purpose of instance normalization is to essentially remove the effect of s from the statistics of the convolution's output feature maps. We observe that this goal can be achieved more directly. Let us assume that the input activations are i.i.d. random variables with unit standard deviation. After modulation and convolution, the output activations have standard deviation of</p><formula xml:id="formula_2">σ j = i,k w ijk 2 ,<label>(2)</label></formula><p>i.e., the outputs are scaled by the L 2 norm of the corresponding weights. The subsequent normalization aims to restore the outputs back to unit standard deviation. Based on Equation 2, this is achieved if we scale ("demodulate") each output feature map j by 1/σ j . Alternatively, we can again bake this into the convolution weights:</p><formula xml:id="formula_3">w ijk = w ijk i,k w ijk 2 + ,<label>(3)</label></formula><p>where is a small constant to avoid numerical issues.</p><p>We have now baked the entire style block to a single convolution layer whose weights are adjusted based on s using Equations 1 and 3 ( <ref type="figure" target="#fig_1">Figure 2d</ref>). Compared to instance normalization, our demodulation technique is weaker because it is based on statistical assumptions about the signal instead of actual contents of the feature maps. Similar statistical analysis has been extensively used in modern network initializers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>, but we are not aware of it being previously used as a replacement for data-dependent normalization. Our demodulation is also related to weight normalization <ref type="bibr" target="#b37">[37]</ref> that performs the same calculation as a part of reparameterizing the weight tensor. Prior work has identified weight normalization as beneficial in the context of GAN training <ref type="bibr" target="#b43">[43]</ref>.</p><p>Our new design removes the characteristic artifacts <ref type="figure">(</ref>  <ref type="table">Table 1</ref>. Main results. For each training run, we selected the training snapshot with the lowest FID. We computed each metric 10 times with different random seeds and report their average. Path length corresponds to the PPL metric, computed based on path endpoints in W <ref type="bibr" target="#b23">[24]</ref>, without the central crop used by Karras et al. <ref type="bibr" target="#b23">[24]</ref>. The FFHQ dataset contains 70k images, and the discriminator saw 25M images during training. For LSUN CAR the numbers were 893k and 57M. ↑ indicates that higher is better, and ↓ that lower is better. the opposite is not true <ref type="bibr" target="#b26">[27]</ref>. In practice our design can be implemented efficiently using grouped convolutions, as detailed in Appendix B. To avoid having to account for the activation function in Equation 3, we scale our activation functions so that they retain the expected signal variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Image quality and generator smoothness</head><p>While GAN metrics such as FID or Precision and Recall (P&amp;R) successfully capture many aspects of the generator, they continue to have somewhat of a blind spot for image quality. For an example, refer to  We observe a correlation between perceived image quality and perceptual path length (PPL) <ref type="bibr" target="#b23">[24]</ref>, a metric that was originally introduced for quantifying the smoothness of the mapping from a latent space to the output image by measuring average LPIPS distances <ref type="bibr" target="#b50">[50]</ref> between generated images under small perturbations in latent space. Again consulting <ref type="figure" target="#fig_2">Figures 13 and 14</ref>, a smaller PPL (smoother generator mapping) appears to correlate with higher overall image qual-FID and P&amp;R use high-level features from InceptionV3 <ref type="bibr" target="#b39">[39]</ref> and VGG-16 <ref type="bibr" target="#b39">[39]</ref>, respectively, which were trained in this way and are thus expected to be biased towards texture detection. As such, images with, e.g., strong cat textures may appear more similar to each other than a human observer would agree, thus partially compromising density-based metrics (FID) and manifold coverage metrics (P&amp;R). ity, whereas other metrics are blind to the change. <ref type="figure">Figure 4</ref> examines this correlation more closely through per-image PPL scores on LSUN CAT, computed by sampling the latent space around w ∼ f (z). Low scores are indeed indicative of high-quality images, and vice versa. <ref type="figure">Figure 5a</ref> shows the corresponding histogram and reveals the long tail of the distribution. The overall PPL for the model is simply the expected value of these per-image PPL scores. We always compute PPL for the entire image, as opposed to Karras et al. <ref type="bibr" target="#b23">[24]</ref> who use a smaller central crop.</p><p>It is not immediately obvious why a low PPL should correlate with image quality. We hypothesize that during training, as the discriminator penalizes broken images, the most direct way for the generator to improve is to effectively stretch the region of latent space that yields good images. This would lead to the low-quality images being squeezed into small latent space regions of rapid change. While this improves the average output quality in the short term, the accumulating distortions impair the training dynamics and consequently the final image quality.</p><p>Clearly, we cannot simply encourage minimal PPL since that would guide the generator toward a degenerate solution with zero recall. Instead, we will describe a new regularizer that aims for a smoother generator mapping without this drawback. As the resulting regularization term is somewhat expensive to compute, we first describe a general optimization that applies to any regularization technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lazy regularization</head><p>Typically the main loss function (e.g., logistic loss <ref type="bibr" target="#b15">[16]</ref>) and regularization terms (e.g., R 1 <ref type="bibr" target="#b30">[30]</ref>) are written as a single expression and are thus optimized simultaneously. We observe that the regularization terms can be computed less frequently than the main loss function, thus greatly diminishing their computational cost and the overall memory usage. <ref type="table">Table 1</ref>, row C shows that no harm is caused when R 1 regularization is performed only once every 16 minibatches, and we adopt the same strategy for our new regularizer as well. Appendix B gives implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Path length regularization</head><p>We would like to encourage that a fixed-size step in W results in a non-zero, fixed-magnitude change in the image. We can measure the deviation from this ideal empirically by stepping into random directions in the image space and observing the corresponding w gradients. These gradients should have close to an equal length regardless of w or the image-space direction, indicating that the mapping from the latent space to image space is well-conditioned <ref type="bibr" target="#b33">[33]</ref>.</p><p>At a single w ∈ W, the local metric scaling properties of the generator mapping g(w) : W → Y are captured by the Jacobian matrix J w = ∂g(w)/∂w. Motivated by the desire to preserve the expected lengths of vectors regardless of the direction, we formulate our regularizer as</p><formula xml:id="formula_4">E w,y∼N (0,I) J T w y 2 − a 2 ,<label>(4)</label></formula><p>where y are random images with normally distributed pixel intensities, and w ∼ f (z), where z are normally distributed. We show in Appendix C that, in high dimensions, this prior is minimized when J w is orthogonal (up to a global scale) at any w. An orthogonal matrix preserves lengths and introduces no squeezing along any dimension.</p><p>To avoid explicit computation of the Jacobian matrix, we use the identity J T w y = ∇ w (g(w) · y), which is efficiently computable using standard backpropagation <ref type="bibr" target="#b5">[6]</ref>. The constant a is set dynamically during optimization as the long-running exponential moving average of the lengths J T w y 2 , allowing the optimization to find a suitable global scale by itself.</p><p>Our regularizer is closely related to the Jacobian clamping regularizer presented by Odena et al. <ref type="bibr" target="#b33">[33]</ref>. Practical differences include that we compute the products J T w y analytically whereas they use finite differences for estimating J w δ with Z δ ∼ N (0, I). It should be noted that spectral normalization <ref type="bibr" target="#b31">[31]</ref> of the generator <ref type="bibr" target="#b46">[46]</ref> only constrains the largest singular value, posing no constraints on the others and hence not necessarily leading to better conditioning. We find that enabling spectral normalization in addition to our contributions -or instead of them -invariably compromises FID, as detailed in Appendix E.</p><p>In practice, we notice that path length regularization leads to more reliable and consistently behaving models, making architecture exploration easier. We also observe that the smoother generator is significantly easier to invert (Section 5). <ref type="figure">Figure 5b</ref> shows that path length regularization clearly tightens the distribution of per-image PPL scores, without pushing the mode to zero. However, <ref type="table">Table 1</ref>, row D points toward a tradeoff between FID and PPL in datasets that are less structured than FFHQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Progressive growing revisited</head><p>Progressive growing <ref type="bibr" target="#b22">[23]</ref> has been very successful in stabilizing high-resolution image synthesis, but it causes its own characteristic artifacts. The key issue is that the progressively grown generator appears to have a strong location preference for details; the accompanying video shows that when features like teeth or eyes should move smoothly over the image, they may instead remain stuck in place before jumping to the next preferred location. <ref type="figure">Figure 6</ref> shows a related artifact. We believe the problem is that in progressive growing each resolution serves momentarily as the output resolution, forcing it to generate maximal frequency details, which then leads to the trained network to have excessively high frequencies in the intermediate layers, compromising shift invariance <ref type="bibr" target="#b49">[49]</ref>. Appendix A shows an example. These <ref type="figure">Figure 6</ref>. Progressive growing leads to "phase" artifacts. In this example the teeth do not follow the pose but stay aligned to the camera, as indicated by the blue line. issues prompt us to search for an alternative formulation that would retain the benefits of progressive growing without the drawbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Alternative network architectures</head><p>While StyleGAN uses simple feedforward designs in the generator (synthesis network) and discriminator, there is a vast body of work dedicated to the study of better network architectures. Skip connections <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b21">22]</ref>, residual networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">31]</ref>, and hierarchical methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref> have proven highly successful also in the context of generative methods. As such, we decided to re-evaluate the network design of StyleGAN and search for an architecture that produces high-quality images without progressive growing. <ref type="figure" target="#fig_5">Figure 7a</ref> shows MSG-GAN <ref type="bibr" target="#b21">[22]</ref>, which connects the matching resolutions of the generator and discriminator using multiple skip connections. The MSG-GAN generator is modified to output a mipmap <ref type="bibr" target="#b42">[42]</ref> instead of an image, and a similar representation is computed for each real im- age as well. In <ref type="figure" target="#fig_5">Figure 7b</ref> we simplify this design by upsampling and summing the contributions of RGB outputs corresponding to different resolutions. In the discriminator, we similarly provide the downsampled image to each resolution block of the discriminator. We use bilinear filtering in all up and downsampling operations. In <ref type="figure" target="#fig_5">Figure 7c</ref> we further modify the design to use residual connections. <ref type="bibr" target="#b2">3</ref> This design is similar to LAPGAN <ref type="bibr" target="#b6">[7]</ref> without the per-resolution discriminators employed by Denton et al. <ref type="table" target="#tab_2">Table 2</ref> compares three generator and three discriminator architectures: original feedforward networks as used in StyleGAN, skip connections, and residual networks, all trained without progressive growing. FID and PPL are provided for each of the 9 combinations. We can see two broad trends: skip connections in the generator drastically improve PPL in all configurations, and a residual discriminator network is clearly beneficial for FID. The latter is perhaps not surprising since the structure of discriminator resembles classifiers where residual architectures are known to be helpful. However, a residual architecture was harmful in the generator -the lone exception was FID in LSUN CAR when both networks were residual.</p><p>For the rest of the paper we use a skip generator and a residual discriminator, without progressive growing. This corresponds to configuration E in <ref type="table">Table 1</ref>, and it significantly improves FID and PPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Resolution usage</head><p>The key aspect of progressive growing, which we would like to preserve, is that the generator will initially focus on low-resolution features and then slowly shift its attention to finer details. The architectures in <ref type="figure" target="#fig_5">Figure 7</ref> make it possible for the generator to first output low resolution images that are not affected by the higher-resolution layers in a significant way, and later shift the focus to the higher-resolution We can see that in the beginning the network focuses on lowresolution images and progressively shifts its focus on larger resolutions as training progresses. In (a) the generator basically outputs a 512 2 image with some minor sharpening for 1024 2 , while in (b) the larger network focuses more on the high-resolution details.</p><p>layers as the training proceeds. Since this is not enforced in any way, the generator will do it only if it is beneficial. To analyze the behavior in practice, we need to quantify how strongly the generator relies on particular resolutions over the course of training.</p><p>Since the skip generator <ref type="figure" target="#fig_5">(Figure 7b</ref>) forms the image by explicitly summing RGB values from multiple resolutions, we can estimate the relative importance of the corresponding layers by measuring how much they contribute to the final image. In <ref type="figure" target="#fig_6">Figure 8a</ref>, we plot the standard deviation of the pixel values produced by each tRGB layer as a function of training time. We calculate the standard deviations over 1024 random samples of w and normalize the values so that they sum to 100%.</p><p>At the start of training, we can see that the new skip generator behaves similar to progressive growing -now achieved without changing the network topology. It would thus be reasonable to expect the highest resolution to dominate towards the end of the training. The plot, however, shows that this fails to happen in practice, which indicates that the generator may not be able to "fully utilize" the target resolution. To verify this, we inspected the generated images manually and noticed that they generally lack some of the pixel-level detail that is present in the training datathe images could be described as being sharpened versions of 512 2 images instead of true 1024 2 images.</p><p>This leads us to hypothesize that there is a capacity problem in our networks, which we test by doubling the number of feature maps in the highest-resolution layers of both networks. <ref type="bibr" target="#b3">4</ref> This brings the behavior more in line with expecta-  <ref type="table">Table 3</ref>. Improvement in LSUN datasets measured using FID and PPL. We trained CAR for 57M images, CAT for 88M, CHURCH for 48M, and HORSE for 100M images.</p><p>tions: <ref type="figure" target="#fig_6">Figure 8b</ref> shows a significant increase in the contribution of the highest-resolution layers, and <ref type="table">Table 1</ref>, row F shows that FID and Recall improve markedly. The last row shows that baseline StyleGAN also benefits from additional capacity, but its quality remains far below StyleGAN2. <ref type="table">Table 3</ref> compares StyleGAN and StyleGAN2 in four LSUN categories, again showing clear improvements in FID and significant advances in PPL. It is possible that further increases in the size could provide additional benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Projection of images to latent space</head><p>Inverting the synthesis network g is an interesting problem that has many applications. Manipulating a given image in the latent feature space requires finding a matching latent code w for it first. Previous research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> suggests that instead of finding a common latent code w, the results improve if a separate w is chosen for each layer of the generator. The same approach was used in an early encoder implementation <ref type="bibr" target="#b32">[32]</ref>. While extending the latent space in this fashion finds a closer match to a given image, it also enables projecting arbitrary images that should have no latent representation. Instead, we concentrate on finding latent codes in the original, unextended latent space, as these correspond to images that the generator could have produced.</p><p>Our projection method differs from previous methods in two ways. First, we add ramped-down noise to the latent code during optimization in order to explore the latent space more comprehensively. Second, we also optimize the stochastic noise inputs of the StyleGAN generator, regularizing them to ensure they do not end up carrying coherent signal. The regularization is based on enforcing the autocorrelation coefficients of the noise maps to match those of unit Gaussian noise over multiple scales. Details of our projection method can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Attribution of generated images</head><p>Detection of manipulated or generated images is a very important task. At present, classifier-based methods can quite reliably detect generated images, regardless of their exact origin <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b41">41]</ref>. However, given the rapid pace of progress in generative methods, this may not be a lasting situation. Besides general detection of fake images, we may also consider a more limited form of the problem:  <ref type="figure">Figure 10</ref>. LPIPS distance histograms between original and projected images for generated (blue) and real images (orange). Despite the higher image quality of our improved generator, it is much easier to project the generated images into its latent space W. The same projection method was used in all cases.</p><p>being able to attribute a fake image to its specific source <ref type="bibr" target="#b1">[2]</ref>. With StyleGAN, this amounts to checking if there exists a w ∈ W that re-synthesis the image in question. We measure how well the projection succeeds by computing the LPIPS <ref type="bibr" target="#b50">[50]</ref> distance between original and resynthesized image as D LPIPS [x, g(g −1 (x))], where x is the image being analyzed andg −1 denotes the approximate projection operation. <ref type="figure">Figure 10</ref> shows histograms of these distances for LSUN CAR and FFHQ datasets using the original StyleGAN and StyleGAN2, and <ref type="figure" target="#fig_7">Figure 9</ref> shows example projections. The images generated using StyleGAN2 can be projected into W so well that they can be almost unambiguously attributed to the generating network. However, with the original StyleGAN, even though it should technically be possible to find a matching latent code, it appears that the mapping from W to images is too complex for this to succeed reliably in practice. We find it encouraging that StyleGAN2 makes source attribution easier even though the image quality has improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>We have identified and fixed several image quality issues in StyleGAN, improving the quality further and considerably advancing the state of the art in several datasets. In some cases the improvements are more clearly seen in motion, as demonstrated in the accompanying video. Appendix A includes further examples of results obtainable using our method. Despite the improved quality, StyleGAN2 makes it easier to attribute a generated image to its source.</p><p>Training performance has also improved. At 1024 2 resolution, the original StyleGAN (config A in <ref type="table">Table 1</ref>) trains at 37 images per second on NVIDIA DGX-1 with 8 Tesla V100 GPUs, while our config E trains 40% faster at 61 img/s. Most of the speedup comes from simplified dataflow due to weight demodulation, lazy regularization, and code optimizations. StyleGAN2 (config F, larger networks) trains at 31 img/s, and is thus only slightly more expensive to train than original StyleGAN. Its total training time was 9 days for FFHQ and 13 days for LSUN CAR.</p><p>The entire project, including all exploration, consumed 132 MWh of electricity, of which 0.68 MWh went into training the final FFHQ model. In total, we used about 51 single-GPU years of computation (Volta class GPU). A more detailed discussion is available in Appendix F.</p><p>In the future, it could be fruitful to study further improvements to the path length regularization, e.g., by replacing the pixel-space L 2 distance with a data-driven feature-space metric. Considering the practical deployment of GANs, we feel that it will be important to find new ways to reduce the training data requirements. This is especially crucial in applications where it is infeasible to acquire tens of thousands of training samples, and with datasets that include a lot of intrinsic variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image quality</head><p>We include several large images that illustrate various aspects related to image quality. <ref type="figure">Figure 11</ref> shows hand-picked examples illustrating the quality and diversity achievable using our method in FFHQ, while <ref type="figure" target="#fig_1">Figure 12</ref> shows uncurated results for all datasets mentioned in the paper. <ref type="figure" target="#fig_2">Figures 13 and 14</ref> demonstrate cases where FID and P&amp;R give non-intuitive results, but PPL seems to be more in line with human judgement.</p><p>We also include images relating to StyleGAN artifacts. <ref type="figure">Figure 15</ref> shows a rare case where the blob artifact fails to appear in StyleGAN activations, leading to a seriously broken image. <ref type="figure">Figure 16</ref> visualizes the activations inside <ref type="table">Table 1</ref> configurations A and F. It is evident that progressive growing leads to higher-frequency content in the intermediate layers, compromising shift invariance of the network. We hypothesize that this causes the observed uneven location preference for details when progressive growing is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>We implemented our techniques on top of the official TensorFlow implementation of StyleGAN 5 corresponding to configuration A in <ref type="table">Table 1</ref>. We kept most of the details unchanged, including the dimensionality of Z and W (512), mapping network architecture (8 fully connected layers, 100× lower learning rate), equalized learning rate for all trainable parameters <ref type="bibr" target="#b22">[23]</ref>, leaky ReLU activation with α = 0.2, bilinear filtering <ref type="bibr" target="#b49">[49]</ref> in all up/downsampling layers <ref type="bibr" target="#b23">[24]</ref>, minibatch standard deviation layer at the end of the discriminator <ref type="bibr" target="#b22">[23]</ref>, exponential moving average of generator weights <ref type="bibr" target="#b22">[23]</ref>, style mixing regularization <ref type="bibr" target="#b23">[24]</ref>, nonsaturating logistic loss <ref type="bibr" target="#b15">[16]</ref> with R 1 regularization <ref type="bibr" target="#b30">[30]</ref>, Adam optimizer <ref type="bibr" target="#b24">[25]</ref> with the same hyperparameters (β 1 = 0, β 2 = 0.99, = 10 −8 , minibatch = 32), and training datasets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">44]</ref>. We performed all training runs on NVIDIA DGX-1 with 8 Tesla V100 GPUs using Tensor-Flow 1.14.0 and cuDNN 7.4.2.</p><p>Generator redesign In configurations B-F we replace the original StyleGAN generator with our revised architecture. In addition to the changes highlighted in Section 2, we initialize components of the constant input c 1 using N (0, 1) and simplify the noise broadcast operations to use a single shared scaling factor for all feature maps. Similar to Karras et al. <ref type="bibr" target="#b23">[24]</ref>, we initialize all weights using N (0, 1) and all biases and noise scaling factors to zero, except for the biases of the affine transformation layers, which we initialize to one. We employ weight modulation and demodulation in all convolution layers, except for the output layers (tRGB in <ref type="figure" target="#fig_5">Figure 7</ref>) where we leave out the demodulation. With 1024 2 output resolution, the generator contains a total of 18 affine transformation layers where the first one corresponds to 4 2 resolution, the next two correspond to 8 2 , and so forth.</p><p>Weight demodulation Considering the practical implementation of Equations 1 and 3, it is important to note that the resulting set of weights will be different for each sample in a minibatch, which rules out direct implementation using standard convolution primitives. Instead, we choose to employ grouped convolutions <ref type="bibr" target="#b25">[26]</ref> that were originally proposed as a way to reduce computational costs by dividing the input feature maps into multiple independent groups, each with their own dedicated set of weights. We implement Equations 1 and 3 by temporarily reshaping the weights and activations so that each convolution sees one sample with N groups -instead of N samples with one group. This approach is highly efficient because the reshaping operations do not actually modify the contents of the weight and activation tensors.</p><p>Lazy regularization In configurations C-F we employ lazy regularization (Section 3.1) by evaluating the regularization terms (R 1 and path length) in a separate regularization pass that we execute once every k training iterations. We share the internal state of the Adam optimizer between the main loss and the regularization terms, so that the optimizer first sees gradients from the main loss for k iterations, followed by gradients from the regularization terms for one iteration. To compensate for the fact that we now perform k+1 training iterations instead of k, we adjust the optimizer hyperparameters λ = c · λ, β 1 = (β 1 ) c , and β 2 = (β 2 ) c , where c = k/(k + 1). We also multiply the regularization term by k to balance the overall magnitude of its gradients. We use k = 16 for the discriminator and k = 8 for the generator.</p><p>Path length regularization Configurations D-F include our new path length regularizer (Section 3.2). We initialize the target scale a to zero and track it on a per-GPU basis as the exponential moving average of J T w y 2 using decay coefficient β pl = 0.99. We weight our regularization term by</p><formula xml:id="formula_5">γ pl = ln 2 r 2 (ln r − ln 2) ,<label>(5)</label></formula><p>where r specifies the output resolution (e.g. r = 1024). We have found these parameter choices to work reliably across all configurations and datasets. To ensure that our regularizer interacts correctly with style mixing regularization, we compute it as an average of all individual layers of the synthesis network. Appendix C provides detailed analysis of the effects of our regularizer on the mapping between W and image space.  <ref type="table">Tables 1 and 3</ref>. The images correspond to random outputs produced by our generator (config F), with truncation applied at all resolutions using ψ = 0.5 <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_2">Figure 13</ref>. Uncurated examples from two generative models trained on LSUN CAT without truncation. FID, precision, and recall are similar for models 1 and 2, even though the latter produces cat-shaped objects more often. Perceptual path length (PPL) indicates a clear preference for model 2. Model 1 corresponds to configuration A in <ref type="table">Table 3</ref>, and model 2 is an early training snapshot of configuration F. <ref type="figure">Figure 14</ref>. Uncurated examples from two generative models trained on LSUN CAR without truncation. FID, precision, and recall are similar for models 1 and 2, even though the latter produces car-shaped objects more often. Perceptual path length (PPL) indicates a clear preference for model 2. Model 1 corresponds to configuration A in <ref type="table">Table 3</ref>, and model 2 is an early training snapshot of configuration F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature map 64 2</head><p>Feature map 128 2 Feature map 256 2 Feature map 512 2 Generated image <ref type="figure">Figure 15</ref>. An example of the importance of the droplet artifact in StyleGAN generator. We compare two generated images, one successful and one severely corrupted. The corresponding feature maps were normalized to the viewable dynamic range using instance normalization. For the top image, the droplet artifact starts forming in 64 2 resolution, is clearly visible in 128 2 , and increasingly dominates the feature maps in higher resolutions. For the bottom image, 64 2 is qualitatively similar to the top row, but the droplet does not materialize in 128 2 . Consequently, the facial features are stronger in the normalized feature map. This leads to an overshoot in 256 2 , followed by multiple spurious droplets forming in subsequent resolutions. Based on our experience, it is rare that the droplet is missing from StyleGAN images, and indeed the generator fully relies on its existence. Progressive growing In configurations A-D we use progressive growing with the same parameters as Karras et al. <ref type="bibr" target="#b23">[24]</ref> (start at 8 2 resolution and learning rate λ = 10 −3 , train for 600k images per resolution, fade in next resolution for 600k images, increase learning rate gradually by 3×). In configurations E-F we disable progressive growing and set the learning rate to a fixed value λ = 2 · 10 −3 , which we found to provide the best results. In addition, we use output skips in the generator and residual connections in the discriminator as detailed in Section 4.1.</p><p>Dataset-specific tuning Similar to Karras et al. <ref type="bibr" target="#b23">[24]</ref>, we augment the FFHQ dataset with horizontal flips to effectively increase the number of training images from 70k to 140k, and we do not perform any augmentation for the LSUN datasets. We have found that the optimal choices for the training length and R 1 regularization weight γ tend to vary considerably between datasets and configurations. We use γ = 10 for all training runs except for configuration E in <ref type="table">Table 1</ref>, as well as LSUN CHURCH and LSUN HORSE in <ref type="table">Table 3</ref>, where we use γ = 100. It is possible that further tuning of γ could provide additional benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance optimizations</head><p>We profiled our training runs extensively and found that -in our case -the default primitives for image filtering, up/downsampling, bias addition, and leaky ReLU had surprisingly high overheads in terms of training time and GPU memory footprint. This motivated us to optimize these operations using hand-written CUDA kernels. We implemented filtered up/downsampling as a single fused operation, and bias and activation as another one. In configuration E at 1024 2 resolution, our optimizations improved the overall training time by about 30% and memory footprint by about 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effects of path length regularization</head><p>The path length regularizer described in Section 3.2 is of the form:</p><formula xml:id="formula_6">L pl = E w E y J T w y 2 − a 2 ,<label>(6)</label></formula><p>where y ∈ R M is a unit normal distributed random variable in the space of generated images (of dimension M = 3wh, namely the RGB image dimensions), J w ∈ R M ×L is the Jacobian matrix of the generator function g : R L → R M at a latent space point w ∈ R L , and a ∈ R is a global value that expresses the desired scale of the gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Effect on pointwise Jacobians</head><p>The value of this prior is minimized when the inner expectation over y is minimized at every latent space point w separately. In this subsection, we show that the inner expectation is (approximately) minimized when the Jacobian matrix J w is orthogonal, up to a global scaling factor. The general strategy is to use the well-known fact that, in high dimensions L, the density of a unit normal distribution is concentrated on a spherical shell of radius √ L. The inner expectation is then minimized when the matrix J T w scales the function under expectation to have its minima at this radius. This is achieved by any orthogonal matrix (with suitable global scale that is the same at every w).</p><p>We begin by considering the inner expectation</p><formula xml:id="formula_7">L w := E y J T w y 2 − a 2 .</formula><p>We first note that the radial symmetry of the distribution of y, as well as of the l 2 norm, allows us to focus on diagonal matrices only. This is seen using the Singular Value Decomposition J T w = UΣV T , where U ∈ R L×L and V ∈ R M ×M are orthogonal matrices, andΣ = [Σ 0] is a horizontal concatenation of a diagonal matrix Σ ∈ R L×L and a zero matrix 0 ∈ R L×(M −L) <ref type="bibr" target="#b14">[15]</ref>. Because rotating a unit normal random variable by an orthogonal matrix leaves the distribution unchanged, and rotating a vector leaves its norm unchanged, the expression simplifies to</p><formula xml:id="formula_8">L w = E y UΣV T y 2 − a 2 = E y Σ y 2 − a 2 .</formula><p>Furthermore, the zero matrix inΣ drops the dimensions of y beyond L, effectively marginalizing its distribution over those dimensions. The marginalized distribution is again a unit normal distribution over the remaining L dimensions. We are then left to consider the minimization of the expression</p><formula xml:id="formula_9">L w = Eỹ ( Σỹ 2 − a) 2 ,</formula><p>over diagonal square matrices Σ ∈ R L×L , whereỹ is unit normal distributed in dimension L. To summarize, all matrices J T w that share the same singular values with Σ produce the same value for the original loss.</p><p>Next, we show that this expression is minimized when the diagonal matrix Σ has a specific identical value at every diagonal entry, i.e., it is a constant multiple of an identity matrix. We first write the expectation as an integral over the probability density ofỹ:</p><formula xml:id="formula_10">L w = ( Σỹ 2 − a) 2 pỹ(ỹ) dỹ = (2π) − L 2 ( Σỹ 2 − a) 2 exp −ỹ Tỹ 2 dỹ</formula><p>Observing the radially symmetric form of the density, we change into a polar coordinatesỹ = rφ, where r ∈ R + is the distance from origin, and φ ∈ S L−1 is a unit vector, i.e., a point on the L − 1-dimensional unit sphere. This change of variables introduces a Jacobian factor r L−1 :</p><formula xml:id="formula_11">L w = (2π) − L 2 S ∞ 0 (r Σφ 2 − a) 2 r L−1 exp − r 2 2 dr dφ</formula><p>The probability density (2π) −L/2 r L−1 exp − r 2 2 is then an L-dimensional unit normal density expressed in polar coordinates, dependent only on the radius and not on the angle. A standard argument by Taylor approximation shows that when L is high, for any φ the density is well approximated by density (2πe/L) −L/2 exp − 1 2 (r − µ) 2 /σ 2 , which is a (unnormalized) one-dimensional normal density in r, centered at µ = √ L of standard deviation σ = 1/ √ 2 <ref type="bibr" target="#b3">[4]</ref>. In other words, the density of the L-dimensional unit normal distribution is concentrated on a shell of radius √ L. Substituting this density into the integral, the loss becomes approximately</p><formula xml:id="formula_12">L w ≈ (2πe/L) −L/2 S ∞ 0 (r Σφ 2 − a) 2 exp   − r − √ L 2 2σ 2    dr dφ,<label>(7)</label></formula><p>where the approximation becomes exact in the limit of infinite dimension L.</p><p>To minimize this loss, we set Σ such that the function (r Σφ 2 − a) 2 obtains minimal values on the spherical shell of radius √ L. This is achieved by Σ = a √ L I, whereby the function becomes constant in φ and the expression reduces to</p><formula xml:id="formula_13">L w ≈ (2πe/L) −L/2 A(S)a 2 L −1 ∞ 0 r − √ L 2 exp   − r − √ L 2 2σ 2    dr,</formula><p>where A(S) is the surface area of the unit sphere (and like the other constant factors, irrelevant for minimization).</p><p>Note that the zero of the parabola (r − √ L) 2 coincides with the maximum of the probability density, and therefore this choice of Σ minimizes the inner integral in Eq. 7 separately for every φ.</p><p>In summary, we have shown that -assuming a high dimensionality L of the latent space -the value of the path length prior (Eq. 6) is minimized when all singular values of the Jacobian matrix of the generator are equal to a global constant, at every latent space point w, i.e., they are orthogonal up to a globally constant scale.</p><p>While in theory a merely scales the values of the mapping without changing its properties and could be set to a fixed value (e.g., 1), in practice it does affect the dynamics of the training. If the imposed scale does not match the scale induced by the random initialization of the network, the training spends its critical early steps in pushing the weights towards the required overall magnitudes, rather than enforcing the actual objective of interest. This may degrade the internal state of the network weights and lead to sub-optimal performance in later training. Empirically we find that setting a fixed scale reduces the consistency of the training results across training runs and datasets. Instead, we set a dynamically based on a running average of the existing scale of the Jacobians, namely a ≈ E w,y J T w y 2 . With this choice the prior targets the scale of the local Jacobians towards whatever global average already exists, rather than forcing a specific global average. This also eliminates the need to measure the appropriate scale of the Jacobians  <ref type="figure" target="#fig_5">Figure 17</ref>. The mean and standard deviation of the magnitudes of sorted singular values of the Jacobian matrix evaluated at random latent space points w, with largest eigenvalue normalized to 1. In both datasets, path length regularization (Config D) and novel architecture (Config F) exhibit better conditioning; notably, the effect is more pronounced in the Cars dataset that contains much more variability, and where path length regularization has a relatively stronger effect on the PPL metric <ref type="table">(Table 1)</ref>. explicitly, as is done by Odena et al. <ref type="bibr" target="#b33">[33]</ref> who consider a related conditioning prior. <ref type="figure" target="#fig_5">Figure 17</ref> shows empirically measured magnitudes of singular values of the Jacobian matrix for networks trained with and without path length regularization. While orthogonality is not reached, the eigenvalues of the regularized network are closer to one another, implying better conditioning, with the strength of the effect correlated with the PPL metric <ref type="table">(Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Effect on global properties of generator mapping</head><p>In the previous subsection, we found that the prior encourages the Jacobians of the generator mapping to be everywhere orthogonal. While <ref type="figure" target="#fig_5">Figure 17</ref> shows that the mapping does not satisfy this constraint exactly in practice, it is instructive to consider what global properties the constraint implies for mappings that do. Without loss of generality, we assume unit global scale for the matrices to simplify the presentation.</p><p>The key property is that that a mapping g : R L → R M with everywhere orthogonal Jacobians preserves the lengths of curves. To see this, let u : [t 0 , t 1 ] → R L parametrize a curve in the latent space. Mapping the curve through the generator g, we obtain a curveũ = g • u in the space of images. Its arc length is</p><formula xml:id="formula_14">L = t1 t0 |ũ (t)| dt,<label>(8)</label></formula><p>where prime denotes derivative with respect to t. By chain rule, this equals</p><formula xml:id="formula_15">L = t1 t0 |J g (u(t))u (t)| dt,<label>(9)</label></formula><p>where J g ∈ R L×M is the Jacobian matrix of g evaluated at u(t). By our assumption, the Jacobian is orthogonal, and consequently it leaves the 2-norm of the vector u (t) unaffected:</p><formula xml:id="formula_16">L = t1 t0 |u (t)| dt.<label>(10)</label></formula><p>This is the length of the curve u in the latent space, prior to mapping with g. Hence, the lengths of u andũ are equal, and so g preserves the length of any curve.</p><p>In the language of differential geometry, g isometrically embeds the Euclidean latent space R L into a submanifold M in R M -e.g., the manifold of images representing faces, embedded within the space of all possible RGB images. A consequence of isometry is that straight line segments in the latent space are mapped to geodesics, or shortest paths, on the image manifold: a straight line v that connects two latent space points cannot be made any shorter, so neither can there be a shorter on-manifold image-space path between the corresponding images than g • v. For example, a geodesic on the manifold of face images is a continuous morph between two faces that incurs the minimum total amount of change (as measured by l 2 difference in RGB space) when one sums up the image difference in each step of the morph.</p><p>Isometry is not achieved in practice, as demonstrated in empirical experiments in the previous subsection. The full loss function of the training is a combination of potentially conflicting criteria, and it is not clear if a genuinely isometric mapping would be capable of expressing the image manifold of interest. Nevertheless, a pressure to make the mapping as isometric as possible has desirable consequences. In particular, it discourages unnecessary "detours": in a nonconstrained generator mapping, a latent space interpolation between two similar images may pass through any number of distant images in RGB space. With regularization, the mapping is encouraged to place distant images in different regions of the latent space, so as to obtain short image paths between any two endpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Projection method details</head><p>Given a target image x, we seek to find the corresponding w ∈ W and per-layer noise maps denoted n i ∈ R ri×ri where i is the layer index and r i denotes the resolution of the ith noise map. The baseline StyleGAN generator in 1024×1024 resolution has 18 noise inputs, i.e., two for each resolution from 4×4 to 1024×1024 pixels. Our improved architecture has one fewer noise input because we do not add noise to the learned 4×4 constant <ref type="figure" target="#fig_1">(Figure 2)</ref>.</p><p>Before optimization, we compute µ w = E z f (z) by running 10 000 random latent codes z through the mapping network f . We also approximate the scale of W by computing σ 2 w = E z f (z) − µ w 2 2 , i.e., the average square Euclidean distance to the center.</p><p>At the beginning of optimization, we initialize w = µ w and n i <ref type="figure">= N (0, I)</ref>   <ref type="figure" target="#fig_6">Figure 18</ref>. Effect of noise regularization in latent-space projection where we also optimize the contents of the noise inputs of the synthesis network. Top to bottom: target image, re-synthesized image, contents of two noise maps at different resolutions. When regularization is turned off in this test, we only normalize the noise maps to zero mean and unit variance, which leads the optimization to sneak signal into the noise maps. Enabling the noise regularization prevents this. The model used here corresponds to configuration F in <ref type="table">Table 1</ref>.</p><p>the components of w as well as all components in all noise maps n i . The optimization is run for 1000 iterations using Adam optimizer <ref type="bibr" target="#b24">[25]</ref> with default parameters. Maximum learning rate is λ max = 0.1, and it is ramped up from zero linearly during the first 50 iterations and ramped down to zero using a cosine schedule during the last 250 iterations. In the first three quarters of the optimization we add Gaussian noise to w when evaluating the loss function as w = w + N (0, 0.05 σ w t 2 ), where t goes from one to zero during the first 750 iterations. This adds stochasticity to the optimization and stabilizes finding of the global optimum.</p><p>Given that we are explicitly optimizing the noise maps, we must be careful to avoid the optimization from sneaking actual signal into them. Thus we include several noise map regularization terms in our loss function, in addition to an image quality term. The image quality term is the LPIPS <ref type="bibr" target="#b50">[50]</ref> distance between target image x and the synthesized image: L image = D LPIPS [x, g(w, n 0 , n 1 , . . .)]. For increased performance and stability, we downsample both images to 256×256 resolution before computing the LPIPS distance. Regularization of the noise maps is performed on  <ref type="table">Table 4</ref>. Effect of spectral normalization with FFHQ at 1024 2 . The first row corresponds to StyleGAN2, i.e., config F in <ref type="table">Table 1</ref>.</p><p>In the subsequent rows, we enable spectral normalization in the generator (SN-G) and in the discriminator (SN-D). We also test the training without weight demodulation (Demod) and path length regularization (P.reg). All of these configurations are highly detrimental to FID, as well as to Recall. ↑ indicates that higher is better, and ↓ that lower is better. multiple resolution scales. For this purpose, we form for each noise map greater than 8×8 in size a pyramid down to 8×8 resolution by averaging 2×2 pixel neighborhoods and multiplying by 2 at each step to retain the expected unit variance. These downsampled noise maps are used for regularization only and have no part in synthesis. Let us denote the original noise maps by n i,0 = n i and the downsampled versions by n i,j&gt;0 . Similarly, let r i,j be the resolution of an original (j = 0) or downsampled (j &gt; 0) noise map so that r i,j+1 = r i,j /2. The regularization term for noise map n i,j is then where the noise map is considered to wrap at the edges. The regularization term is thus sum of squares of the resolutionnormalized autocorrelation coefficients at one pixel shifts horizontally and vertically, which should be zero for a normally distributed signal. The overall loss term is then L total = L image + α i,j L i,j . In all our tests, we have used noise regularization weight α = 10 5 . In addition, we renormalize all noise maps to zero mean and unit variance after each optimization step. <ref type="figure" target="#fig_6">Figure 18</ref> illustrates the effect of noise regularization on the resulting noise maps.</p><formula xml:id="formula_17">L i,j = 1 r 2 i,j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results with spectral normalization</head><p>Since spectral normalization (SN) is widely used in GANs <ref type="bibr" target="#b31">[31]</ref>, we investigated its effect on StyleGAN2. <ref type="table">Table 4</ref> gives the results for a variety of configurations where spectral normalization is enabled in addition to our techniques (weight demodulation, path length regularization) or instead of them.  <ref type="table">Table 5</ref>. Computational effort expenditure and electricity consumption data for this project. The unit for computation is GPUyears on a single NVIDIA V100 GPU -it would have taken approximately 51 years to execute this project using a single GPU. See the text for additional details about the computation and energy consumption estimates. Initial exploration includes all training runs after the release of StyleGAN <ref type="bibr" target="#b23">[24]</ref> that affected our decision to start this project. Paper exploration includes all training runs that were done specifically for this project, but were not intended to be used in the paper as-is. FFHQ config F refers to the training of the final network. This is approximately the cost of training the network for another dataset without hyperparameter tuning. Other runs in paper covers the training of all other networks shown in the paper. Backup runs left out includes the training of various networks that could potentially have been shown in the paper, but were ultimately left out to keep the exposition more focused. Video, figures, etc. includes computation that was spent on producing the images and graphs in the paper, as well as on the result video. Public release covers testing, benchmarking, and large-scale image dumps related to the public release.</p><p>Interestingly, adding spectral normalization to our generator is almost a no-op. On an implementation level, SN scales the weight tensor of each layer with a scalar value 1/σ(w). The effect of such scaling, however, is overridden by Equation 3 for the main convolutional layers as well as the affine transformation layers. Thus, the only thing that SN adds on top of weight demodulation is through its effect on the tRGB layers.</p><p>When we enable spectral normalization in the discriminator, FID is slightly compromised. Enabling it in the generator as well leads to significantly worse results, even though its effect is isolated to the tRGB layers. Leaving SN enabled, but disabling a subset of our contributions does not improve the situation. Thus we conclude that StyleGAN2 gives better results without spectral normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Energy consumption</head><p>Computation is a core resource in any machine learning project: its availability and cost, as well as the associated energy consumption, are key factors in both choosing research directions and practical adoption. We provide a detailed breakdown for our entire project in <ref type="table">Table 5</ref> in terms of both GPU time and electricity consumption.</p><p>We report expended computational effort as single-GPU years (Volta class GPU). We used a varying number of NVIDIA DGX-1s for different stages of the project, and converted each run to single-GPU equivalents by simply scaling by the number of GPUs used.</p><p>The entire project consumed approximately 131.61 megawatt hours (MWh) of electricity. We followed the Green500 power measurements guidelines <ref type="bibr" target="#b10">[11]</ref> as follows. For each job, we logged the exact duration, number of GPUs used, and which of our two separate compute clusters the job was executed on. We then measured the actual power draw of an 8-GPU DGX-1 when it was training FFHQ config F. A separate estimate was obtained for the two clusters because they use different DGX-1 SKUs. The vast majority of our training runs used 8 GPUs, and for the rest we approximated the power draw by scaling linearly with n/8, where n is the number of GPUs.</p><p>Approximately half of the total energy was spent on early exploration and forming ideas. Then subsequently a quarter was spent on refining those ideas in more targeted experiments, and finally a quarter on producing this paper and preparing the public release of code, trained models, and large sets of images. Training a single FFHQ network (config F) took approximately 0.68 MWh (0.5% of the total project expenditure). This is the cost that one would pay when training the network from scratch, possibly using a different dataset. In short, vast majority of the electricity used went into shaping the ideas, testing hypotheses, and hyperparameter tuning. We did not use automated tools for finding hyperparameters or optimizing network architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>We redesign the architecture of the StyleGAN synthesis network. (a) The original StyleGAN, where A denotes a learned affine transform from W that produces a style and B is a noise broadcast operation. (b) The same diagram with full detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Replacing normalization with demodulation removes the characteristic artifacts from images and activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figures 13 and 14that contrast generators with identical FID and P&amp;R scores but markedly different overall quality.<ref type="bibr" target="#b1">2</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Connection between perceptual path length and image quality using baseline StyleGAN (config A) with LSUN CAT. (a) Random examples with low PPL (≤ 10 th percentile). (b) Examples with high PPL (≥ 90 th percentile). There is a clear correlation between PPL scores and semantic consistency of the images. (a) Distribution of PPL scores of individual images generated using baseline StyleGAN (config A) with LSUN CAT (FID = 8.53, PPL = 924). The percentile ranges corresponding to Figure 4 are highlighted in orange. (b) StyleGAN2 (config F) improves the PPL distribution considerably (showing a snapshot with the same FID = 8.53, PPL = 387).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Three generator (above the dashed line) and discriminator architectures. Up and Down denote bilinear up and downsampling, respectively. In residual networks these also include 1×1 convolutions to adjust the number of feature maps. tRGB and fRGB convert between RGB and high-dimensional per-pixel data. Architectures used in configs E and F are shown in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>StyleGAN-sized (config E) (b) Large networks (config F) Contribution of each resolution to the output of the generator as a function of training time. The vertical axis shows a breakdown of the relative standard deviations of different resolutions, and the horizontal axis corresponds to training progress, measured in millions of training images shown to the discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Example images and their projected and re-synthesized counterparts. For each configuration, top row shows the target images and bottom row shows the synthesis of the corresponding projected latent vector and noise inputs. With the baseline StyleGAN, projection often finds a reasonably close match for generated images, but especially the backgrounds differ from the originals. The images generated using StyleGAN2 can be projected almost perfectly back into generator inputs, while projected real images (from the training set) show clear differences to the originals, as expected. All tests were done using the same projection method and hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Four hand-picked examples illustrating the image quality and diversity achievable using StylegGAN2 (config F). Uncurated results for each dataset used in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Generated image Feature map 128 2 Figure 16 .</head><label>216</label><figDesc>Generated image Feature map 128 2 (a) Progressive growing (config A) (b) Without progressive growing (config F) Progressive growing leads to significantly higher frequency content in the intermediate layers. This compromises shiftinvariance of the network and makes it harder to localize features precisely in the higher-resolution layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>j (x, y) · n i,j (x − 1, y) j (x, y) · n i,j (x, y − 1) 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of generator and discriminator architectures without progressive growing. The combination of generator with output skips and residual discriminator corresponds to configuration E in the main result table.</figDesc><table><row><cell>FFHQ</cell><cell cols="2">D original FID PPL</cell><cell cols="2">D input skips FID PPL</cell><cell cols="2">D residual FID PPL</cell></row><row><cell>G original G output skips G residual</cell><cell>4.32 4.33 4.35</cell><cell>265 169 203</cell><cell>4.18 3.77 3.96</cell><cell>235 127 229</cell><cell>3.58 3.31 3.79</cell><cell>269 125 243</cell></row><row><cell>LSUN Car</cell><cell cols="2">D original FID PPL</cell><cell cols="2">D input skips FID PPL</cell><cell cols="2">D residual FID PPL</cell></row><row><cell>G original G output skips G residual</cell><cell>3.75 3.77 3.93</cell><cell>905 544 981</cell><cell>3.23 3.86 3.40</cell><cell>758 316 667</cell><cell>3.25 3.19 2.66</cell><cell>802 471 645</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>for all i. The trainable parameters are</figDesc><table><row><cell cols="2">Generated target image</cell><cell cols="2">Real target image</cell></row><row><cell>No noise regularization</cell><cell>With noise regularization</cell><cell>No noise regularization</cell><cell>With noise regularization</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>SN-G SN-D Demod P.reg FID ↓ PPL ↓ Pre. ↑ Rec. ↑</figDesc><table><row><cell>1</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>2.83</cell><cell>145.0</cell><cell>0.689</cell><cell>0.492</cell></row><row><cell>2 3 4 5 6 7</cell><cell>-</cell><cell>--</cell><cell>----</cell><cell>--</cell><cell>2.98 3.40 3.38 3.33 3.36 3.22</cell><cell>131.4 130.9 162.6 394.9 217.1 394.4</cell><cell>0.700 0.720 0.705 0.705 0.695 0.692</cell><cell>0.469 0.435 0.468 0.463 0.464 0.489</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In rare cases (perhaps 0.1% of images) the droplet is missing, leading to severely corrupted images. See Appendix A for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We believe that the key to the apparent inconsistency lies in the particular choice of feature space rather than the foundations of FID or P&amp;R. It was recently discovered that classifiers trained using ImageNet<ref type="bibr" target="#b35">[35]</ref> tend to base their decisions much more on texture than shape<ref type="bibr" target="#b11">[12]</ref>, while humans strongly focus on shape<ref type="bibr" target="#b27">[28]</ref>. This is relevant in our context because (a) Low PPL scores (b) High PPL scores</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In residual network architectures, the addition of two paths leads to a doubling of signal variance, which we cancel by multiplying with 1/ √ 2. This is crucial for our networks, whereas in classification resnets<ref type="bibr" target="#b17">[18]</ref> the issue is typically hidden by batch normalization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We double the number of feature maps in resolutions 64 2 -1024 2 while keeping other parts of the networks unchanged. This increases the total number of trainable parameters in the generator by 22% (25M → 30M) and in the discriminator by 21% (24M → 29M).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/NVlabs/stylegan</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Ming-Yu Liu for an early review, Timo Viitanen for help with the public release, David Luebke for in-depth discussions and helpful comments, and Tero Kuosmanen for technical support with the compute infrastructure.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2StyleGAN: How to embed images into the StyleGAN latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Source generator attribution via inversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Albright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Mccloskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Which face is real?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jevin</forename><surname>West</surname></persName>
		</author>
		<ptr target="http://www.whichfaceisreal.com/learn.html" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Equilibrated adaptive learning rates for non-convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dauphin</surname></persName>
		</author>
		<idno>abs/1502.04390</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Harm de Vries, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1506.05751</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Feature-wise transformations. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://distill.pub/2018/feature-wise-transformations.1" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<idno>abs/1610.07629</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Style generator inversion for image enhancement and animation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Power measurement tutorial for the Green500 list</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pyla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://www.top500.org/green500/resources/tutorials/" />
		<imprint>
			<date type="published" when="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno>abs/1811.12231</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring the structure of a real-time, arbitrary neural artistic stylization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>abs/1705.06830</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations. Johns Hopkins Studies in the Mathematical Sciences</title>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1704.00028</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1703.06868</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MSG-GAN: multiscale gradients for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1710" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The importance of shape in early lexical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<idno>1988. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Development</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can forensic detectors identify GAN generated images?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunquan</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asia-Pacific</forename></persName>
		</author>
		<title level="m">Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Which training methods for GANs do actually converge? CoRR, abs/1801.04406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno>abs/1802.05957</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">StyleGAN -Encoder for official Ten-sorFlow implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Nikitko</surname></persName>
		</author>
		<ptr target="https://github.com/Puzer/stylegan-encoder/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Is generator conditioning causally related to GAN performance?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>abs/1802.08768</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno>abs/1806.00035</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<idno>abs/1602.07868</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Interpreting the latent space of GANs for semantic face editing. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">FakeSpotter: A simple baseline for spotting AI-synthesized fake faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Run</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1909.06122</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">CNN-generated images are surprisingly easy to spot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1912.11035</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramidal parametrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On the effects of batch and weight normalization in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1704.03971</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/1506.03365</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attributing fake images to GANs: Analyzing fingerprints in generated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno>abs/1811.08180</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>abs/1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Stack-GAN: text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Stack-GAN++: realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>abs/1710.10916</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Detecting and simulating artifacts in GAN fake images. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
