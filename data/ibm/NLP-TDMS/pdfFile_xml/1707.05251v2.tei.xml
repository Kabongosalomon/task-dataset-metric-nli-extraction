<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aesthetic-Driven Image Enhancement by Adversarial Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-Sensetime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CUHK-Sensetime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">CUHK-Sensetime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Aesthetic-Driven Image Enhancement by Adversarial Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS • Computing methodologies → Image manipulation;</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce EnhanceGAN, an adversarial learning based model that performs automatic image enhancement. Traditional image enhancement frameworks typically involve training models in a fully-supervised manner, which require expensive annotations in the form of aligned image pairs. In contrast to these approaches, our proposed EnhanceGAN only requires weak supervision (binary labels on image aesthetic quality) and is able to learn enhancement operators for the task of aesthetic-based image enhancement. In particular, we show the effectiveness of a piecewise color enhancement module trained with weak supervision, and extend the proposed En-hanceGAN framework to learning a deep filtering-based aesthetic enhancer. The full differentiability of our image enhancement operators enables the training of EnhanceGAN in an end-to-end manner. We further demonstrate the capability of EnhanceGAN in learning aesthetic-based image cropping without any groundtruth cropping pairs. Our weakly-supervised EnhanceGAN reports competitive quantitative results on aesthetic-based color enhancement as well as automatic image cropping, and a user study confirms that our image enhancement results are on par with or even preferred over professional enhancement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Image enhancement is considered a skillful artwork that involves transforming or altering a photograph using various methods and techniques to improve the aesthetics of a photo. Examples of enhancement include adjustments of color, contrast and white balance, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. This task is conventionally conducted manually through some professional tools. Manual editing is time-consuming even for a professionally trained artist. While there are an increasing number of applications that allow casual users to choose a fixed set of filters and/or to alter the composition of a photo in more convenient ways, human involvement is still inevitable. Given the increasing amount of digital photographs captured daily with mobile devices, it is desirable to perform image enhancement with minor human involvement or in a smart and fully automatic manner. Previous research efforts have shown some success in automat- * {dy015, ccloy, x t anд }@ie.cuhk.edu.hk Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ACMMM'18, July 2018, Seoul, Korea  ing color enhancement <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref>, style transfer <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> and image cropping <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">43]</ref>. However, most of these models require full supervision. In particular, we need to provide input and manually-enhanced image pairs to learn the capability of color enhancement <ref type="bibr" target="#b44">[45]</ref> or image re-composition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43]</ref>. Unfortunately, such data is scarce due to the expensive cost of obtaining professional annotations. Ignatov et al. <ref type="bibr" target="#b20">[21]</ref> has recently demonstrated the possibility of enhancing low-quality photos towards DSLR-quality. The training images, however, have to be captured by specialized time-synchronized hardware to form pairs, and further registered to remove misalignment between image pairs. In this study, we present a novel approach that trains a weakly-supervised image enhancement model from unpaired images without strong human supervision. In particular, we attempt to learn image enhancement from images with only binary labels on aesthetic quality, i.e.,, good or poor quality. As the edited image should be closer (in the sense of aesthetic quality) to the photo collections by professional photographers compared to the original image with poor aesthetic quality, this notion can be well formulated in an adversarial learning framework <ref type="bibr" target="#b12">[13]</ref>. Specifically, we have a discriminator D that attempts to distinguish images of poor and good aesthetic quality. Such a network can be trained by an abundant amount of images from existing aesthetic datasets <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>. A generator G, on the other hand, generates a set of manipulation parameters given an image with poor aesthetic quality. The task of G is to fool D so that D confuses the G's outputs (i.e.,, enhanced images) as images with high quality.</p><p>The main contribution of this study is three-fold:</p><p>(1) EnhanceGAN leverages abundant images that are annotated only with good and poor quality labels. No knowledge of the groundtruth enhancement action is given to the system. Image enhancement operators are learned in a weakly-supervised manner through adversarial learning driven by aesthetic judgement. (2) The framework permits multiple forms of color enhancement. We carefully design a piecewise color enhancement operator and a deep filtering-based enhancer to be fullydifferentiable for end-to-end learning in an adversarial learning framework. (3) EnhanceGAN is extensible to include further image enhancement schemes (provided that the enhancement operations are fully-differentiable). We explore such capability of En-hanceGAN for aesthetic-based automatic image cropping and present competitive results.</p><p>Owing to the subjective nature of image enhancement, we show the effectiveness of EnhanceGAN for image enhancement in two sets of evaluations. We quantitatively evaluate the performance of color enhancement by multiple state-of-the-art aesthetic evaluators on reserved unseen images and we show quantitative results for automatic image cropping on a standard benchmark dataset. We also perform a blind user study to compare our method with human enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Aesthetic Quality Assessment. The task of aesthetic quality assessment is to distinguish high-quality photos from low-quality ones based on human perceived aesthetics. Previous successful attempts train convolutional neural networks for binary classification of image quality <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> or aesthetic score regression <ref type="bibr" target="#b25">[26]</ref>. We refer readers to a comprehensive study <ref type="bibr" target="#b8">[9]</ref> on the state-of-the-art models on image aesthetic assessment. Although the focus of image enhancement is not on assessing the quality of a given image, our work is closely related to this research domain in the sense that image enhancement aims at improving the aesthetic quality of the given input. Automatic Image Enhancement: The majority of techniques for image manipulation with the goal to enhance the aesthetic quality of an image can be divided into two genres, namely (1) color enhancement and (2) image re-composition. Pixel-level manipulation and image restoration (e.g., super resolution <ref type="bibr" target="#b9">[10]</ref>, de-haze <ref type="bibr" target="#b14">[15]</ref> and de-artifacts <ref type="bibr" target="#b40">[41]</ref>) are also closely related to image enhancement but is beyond the focus in this work. Color Enhancement. The visual quality of an image can be enhanced by color adjustment, where regression models and ranking models have been trained to map the input image to a corresponding enhanced groundtruth <ref type="bibr" target="#b11">[12]</ref>. Such color mappings are learned <ref type="bibr" target="#b43">[44]</ref> from a small set of labeled data by professional editors. To alleviate the lack of sufficient labeled data, recent research efforts formulate the color enhancement problem as the color transfer problem <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>. In particular, the popular exemplar-based color transfer approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref> seek to retrieve the most suitable matching exemplar based on image content and perform color transfer onto the given input. However, they suffer from potential visual artifacts due to erroneous exemplars. Ignatov et al. <ref type="bibr" target="#b20">[21]</ref> present a fully-supervised approach aided by adversarial learning, where they seek to improve low-quality images towards DSLR quality. However, their method still requires carefully aligned input and groundtruth pairs and the effectiveness of their model is limited to images captured by particular mobile devices. Stylistic image enhancement <ref type="bibr" target="#b44">[45]</ref> and creative style transfer <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref> are also closely related to color enhancement, but their focus is to transform an input image into an output that matches the artistic style of a given exemplar, instead of focusing on improving the aesthetic quality of the image. Aesthetic-oriented evaluation typically does not apply on those methods. Unlike style transfer studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45]</ref>, our weakly supervised model is able to enhance an image through chrominance and even spatial manipulations driven by content and aesthetic quality of the image. Thus each individual image would experience different manipulations. Unlike exemplar matching <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>, our framework does not require finding the correct exemplars for color/style transfer, and hence our model is not limited by the subset of exemplars.</p><p>Cropping and Re-targeting. Image cropping and re-targeting aim at finding the most visually significant region based on aesthetic value or human attention focus. Aesthetic-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref> evaluate the crop window candidates based on handcrafted low-level features or learned aesthetic features, while attention/composition-based approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref> rely on image saliency and produce the cropping window encapsulating the most salient region. These systems for cropping and re-targeting are mostly based upon a limited amount of labeled cropping data (∼1000 training image pairs in total), where the cropping problem is modeled as window regression or window candidate classification in a fully-supervised learning manner <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Network fine-tuning with pre-trained convolutional neural network has obtained some success with extensive data augmentation <ref type="bibr" target="#b8">[9]</ref>. The non-parametric images/windows retrievalbased method by Chen et al. <ref type="bibr" target="#b5">[6]</ref> also features a weakly-supervised model for image cropping. In this work, we present an alternative learning-based weakly-supervised attempt and demonstrate the capability of the proposed EnhanceGAN framework in extending the image enhancement tasks to automatic image cropping, and show competitive results on a standard benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AESTHETIC-DRIVEN IMAGE ENHANCEMENT</head><p>We formulate the problem of image enhancement in an adversarial learning framework <ref type="bibr" target="#b12">[13]</ref>. Specifically, our proposed approach builds upon Wasserstein GAN (W-GAN) by Arjovsky et al. <ref type="bibr" target="#b0">[1]</ref>. The full architecture of our proposed framework is shown in <ref type="figure" target="#fig_3">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>Generative Adversarial Network (GAN) <ref type="bibr" target="#b12">[13]</ref> has shown a powerful capability of generating realistic natural images. Typical GANs contain a generator G and a discriminator D, and it was proven <ref type="bibr" target="#b12">[13]</ref>  </p><formula xml:id="formula_0">+E z∼p z [log(1 − D(G(z)))]<label>(1)</label></formula><p>would reach a global optimum when p д converges to the real data distribution p dat a , where p д is the distribution of the samples G(z) obtained when z ∼ p z , and z is a random or encoded vector. In this work we follow the practice in <ref type="bibr" target="#b0">[1]</ref> and adopt the loss function based on Wasserstein distance,</p><formula xml:id="formula_1">(2) L = E I∼p d at a [f W (I)] − E I∼p дen [f W (I)],</formula><p>where f W (·) is a K-Lipschitz function parameterized by W , which is approximated by our discriminator network D as detailed in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generator Network (Net-G)</head><p>Different from most existing GAN frameworks, our generator does not generate images by itself. Instead, the generator G in our En-hanceGAN is responsible for learning the image enhancement operator Φ(·), according to which the input image will be transformed to the enhanced output:</p><formula xml:id="formula_2">I output = Φ(I; θ ) = T θ (I),<label>(3)</label></formula><p>where T denotes the fully-differentiable transformation applied to the input image parameterized by θ = G(I). The base architecture of our generator network is a ResNet-101 <ref type="bibr" target="#b15">[16]</ref> without the last fully-connected layer, and we further remove the last pooling layer to preserve spatial information in the feature maps. As such, this ResNet module acts as a fully-convolutional feature extractor given an input image (see <ref type="figure" target="#fig_3">Fig. 2a</ref>). The 2048 output feature maps produced by the ResNet module has a spatial size f × f and is subsequently utilized in our enhancement parameter generation modules. In this work, we explore multiple forms of image aesthetic enhancement, including two fully-differentiable color enhancement operators and an image editing operator for automatic image cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Piecewise Color Enhancer.</head><p>Image brightness and lighting contrast can be adjusted based on the luminance channel of the image in the CIELab color space, whereas image chrominance resides in the other two channels. Hence, the color enhancement operator Φ(I) can be applied to an image in a piecewise manner. To this end, the piecewise color enhancement module is designed to learn a set of parameters θ L , θ AB ∈ {θ color |θ color = G(I)}, where θ L and θ AB denotes respectively the adjustments for better lighting and chrominance. Specifically, we follow the idea behind gamma correction <ref type="bibr" target="#b32">[33]</ref> to adjust the brightness and contrast of an image in pixel level (i.e.,, the L channel of image I in the CIELab color space: I L ) by designing a piecewise transformation function T θ L (see <ref type="figure" target="#fig_3">Fig. 2c</ref>) defined on each pixel m ∈ I L :</p><formula xml:id="formula_3">T θ L (m) =                    0 if m ≤ b k 1 (m − b) 1 p if b &lt; m ≤ a m if a &lt; m ≤ 1 − a k 2 (m − k 3 ) 1 q + k 3 if 1 − a &lt; m ≤ 1 − b 1 if m ≥ 1 − b , where k 1 = a(a − b) − 1 p , k 2 = a(a − b) − 1</formula><p>q and k 3 = 1 − a to ensure that T θ L is continuous. We further constrain p ≥ 1 in order to lighten the dark regions and 0 &lt; q &lt; 1 to darken the over-exposed region. Similarly, we follow "The LAB Color Move" 2 and the curve adjustment instructed in <ref type="bibr" target="#b16">[17]</ref> and design a similar process to enhance the image color. In particular, the adjustments T A and T B defined respectively on pixels m ∈ I A and m ∈ I B (i.e.,, the A and B channels in image I, see <ref type="figure" target="#fig_3">Fig. 2c</ref>) can be formulated as follows:</p><formula xml:id="formula_4">T θ A (m) =          0 if m ≤ α 1 1−2α (m − α) if α &lt; m &lt; 1 − α 1 if m ≥ 1 − α , T θ B (m) =          0 if m ≤ β 1 1−2β (m − β) if β &lt; m &lt; 1 − β 1 if m ≥ 1 − β .</formula><p>Under this formulation, the parameter sets θ L = [a, b, p, q] and</p><formula xml:id="formula_5">θ AB = [θ A , θ B ] = [α, β]</formula><p>can be learned end-to-end. Our piecewise color enhancement operator can therefore be written as</p><formula xml:id="formula_6">Φ(I; θ ) = T θ col or (I) = T θ L (I L ) ⊕ T θ A (I A ) ⊕ T θ B (I B ),<label>(4)</label></formula><p>where ⊕ denotes channel-wise concatenation in the CIELab color space. Directly learning one single set of such parameters may not be optimal as color enhancement prediction is multi-modal to some extents -an image can have several plausible color enhancement solutions (e.g.,, an enhancement solution can simply adjust the color saturation of an image, or further tuning lighting contrast from low to high or vice versa). With inspirations drawn from attention models <ref type="bibr" target="#b41">[42]</ref>, our piecewise enhancement operator is built by appending a convolution layer (2048 → 7) with kernel size 1 × 1 to the ResNet module. In particular, the first 6 feature maps correspond to the candidate sets of the color adjustment parameters [a, b, p, q, α, β]. The 7 th feature map is an f × f softmax probability map corresponding to f 2 possible predictions θ color <ref type="bibr" target="#b39">[40]</ref> is adopted to aggregate the parameter candidates with the highest probabilities (see <ref type="figure" target="#fig_3">Fig. 2b</ref>). We show in the experiment section that our piecewise color enhancement operator is able to improve both the color and lighting contrast of the image as compared with the input (see <ref type="figure" target="#fig_1">Fig. 1, 2</ref>).</p><formula xml:id="formula_7">i = [a i , b i , p i , q i , α i , β i ], i ∈ {1, 2, ..., f 2 }, where prob(θ color = θ color i |I) = p i . Top-K average pooling</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Deep</head><p>Filtering-based Enhancer. The piecewise color enhancement operator is limited to learning enhancement parameter θ color = [θ L , θ A , θ B ] for a pre-defined set of transformations ϕ 1 (·) = T θ L , ϕ 2 (·) = T θ A and ϕ 3 (·) = T θ B . We can extend equation (4) to a general form by writing the enhancement operator Φ(·) to be a linear combination of individual transforming operations ϕ 1 (·), ϕ 2 (·), ..., ϕ n (·) as follows,</p><formula xml:id="formula_8">Φ(I; θ ) = Φ(I; θ ω ) = n i=1 ω i ϕ i (I)<label>(5)</label></formula><p>θ ω = [ω 1 , ω 2 , ..., ω n ] = G(I) (6) In particular, ϕ i can be any form of image enhancement transformation provided that it is fully-differentiable. Under this formulation, the weight parameters θ ω can be learned end-to-end by a convolution layer (2048 → n + 1) where the (n + 1) t h feature map is also a f × f softmax probability map for Top-K average pooling similar <ref type="bibr" target="#b1">2</ref> The LAB Color Move: https://goo.gl/i2ppcw to that of the piecewise color enhancement operator (see Sec. 3.2.1). In this work, we choose three default R,G,B color enhancement filters from Adobe Photoshop 3 and approximate each of the filtering operations with a 3-layer convolutional neural network (see <ref type="figure" target="#fig_3">Fig. 2c</ref>). More color filtering operators ϕ(·) with learnable parameter θ can be easily extended in this manner provided that the transformations are differentiable. We show in the experiment section that the deep filtering-based aesthetic enhancer produce smooth and harmonious color improvement as compared to the piecewise color enhancer (see <ref type="figure" target="#fig_4">Fig. 3, 4, 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Image Cropping</head><p>Operator. We further extend the image enhancement operation to explore the possibility of learning the task of aesthetic-based image cropping without any cropping labels. The goal of image cropping is to produce a set of cropping coordinates θ cr op = [x, y, w, h] given an input image. This can be achieved by a convolution layer (2048 → (4 + 1)) where the 5 th feature map is the softmax probability map for Top-K average pooling. It is worth mentioning that directly extracting a sub-image from the input given the learned cropping coordinates θ cr op is not differentiable. To ensure that the gradients can back-propagate to the cropping parameters via the transformation Φ(I) = T θ c r op (I), bilinear sampling from a sampling grid <ref type="bibr" target="#b22">[23]</ref> is used to sub-sample the input image based on coordinates output θ cr op of the cropping module (see <ref type="figure" target="#fig_3">Fig. 2c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.4</head><p>Generator Loss Function L G . We formulate the loss function for the generator network as a weighted sum of an adversarial loss component L дan as well as the regularization component L r eд . These terms are weighted to ensure that the loss terms are balanced in their scales. This formulation makes the training process more stable and has better performance (see Sec. 4.2). Adversarial Loss: Following Arjovsky et al. <ref type="bibr" target="#b0">[1]</ref>, the adversarial gradient to the generator network G is computed from the following loss function:</p><formula xml:id="formula_9">L дan = 1 n n i=1 f W (I output i</formula><p>).</p><p>Regularization Loss: As regularization, we use the feature reconstruction loss <ref type="bibr" target="#b24">[25]</ref> to account for the semantic difference between the enhanced/cropped image and the input as measured by feature similarity:</p><formula xml:id="formula_11">L r eд 1 = 1 n n i=1 || f vдд (I output i ) − f vдд (I i )|| 2 2 ,<label>(8)</label></formula><p>where f vдд denotes the f c7 feature output from the VGG-16 network <ref type="bibr" target="#b34">[35]</ref> trained for ImageNet. Also, the notion that an edited image should have better aesthetic quality (lower f W (·) values) than the original further gives us an intuitive loss for further regularizing the end-to-end training:</p><formula xml:id="formula_12">L r eд 2 = 1 n n i=1 ϕ(I output i , I i )<label>(9)</label></formula><p>where ϕ(I output i</p><formula xml:id="formula_13">, I i ) = 0 if f W (I output i ) &lt; f W (I i ) || f W (I output i ) − f W (I i )|| 2 2</formula><p>otherwise. <ref type="bibr" target="#b2">3</ref> The three filters are: the cooling80 filter, the waming85 filter and the underwater filter. The three filters represent respectively blue-ish (#00b5f f ), red-ish (#ec8a00) and green-ish (#00c2b1) effect under default settings. We show in the experiment section that the regularization facilitates the aesthetic-driven adversarial learning of the generator network (see <ref type="table" target="#tab_0">Table 1</ref>, 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discriminator Network (Net-D)</head><p>The proposed framework consists of a discriminator network that is able to assess image aesthetic quality. The discriminator network D is designed to share the ResNet-101 <ref type="bibr" target="#b15">[16]</ref> base architecture of G during pre-training. As shown in <ref type="figure" target="#fig_3">Fig. 2</ref>, the last layer for 1000-class classification in the original ResNet-101 is replaced by a 2-neuron fully-connected layer. We pre-train discriminator D for binary aesthetic classification with the cross-entropy loss as in <ref type="bibr" target="#b8">[9]</ref>. After pre-training, the discriminator network D is appended with another 1-neuron fully-connected layer to perform output aggregation as an approximator to f W in Eq. <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9)</ref>. Deriving from Eq. 2, the loss function L D in subsequent adversarial training can be written as:</p><formula xml:id="formula_14">L D = E I∼p good [f W (I good )] − E θ ∼I bad [f W (I output )],<label>(10)</label></formula><p>where I output = T {θ } (I bad ), I bad ∼ p bad .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Our weakly-supervised EnhanceGAN is tasked to learn image enhancement operators based only on binary labels on image aesthetic quality. Specifically, we train the EnhanceGAN with the benchmark datasets used in aesthetic quality assessment and perform quantitative evaluations on reserved unseen data. A user study is also performed to confirm the validity of our quantitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>CUHK-PhotoQuality Dataset (CUHK-PQ) <ref type="bibr" target="#b36">[37]</ref>: The dataset contains 4,072 high-quality images and 11,812 low-quality images. This dataset is used to pre-train the feature extractor (ResNet module) of our EnhanceGAN, with 10% of the images reserved for validation. We follow the training protocol as in <ref type="bibr" target="#b8">[9]</ref> and pre-train the ResNet-module for binary image aesthetic assessment (see Sec. 3.3), obtaining a balanced accuracy <ref type="bibr" target="#b8">[9]</ref> of 94.3% on the validation set. This pre-trained network (denoted as CUHK-Net) is also used as one of the quantitative aesthetic evaluators (see <ref type="table" target="#tab_0">Table 1</ref>,2). AVA Dataset <ref type="bibr" target="#b30">[31]</ref>: The Aesthetic Visual Analysis (AVA) dataset is by far the largest benchmark for image aesthetic assessment. Each of the 255,530 images is labeled with aesthetic scores ranging from 1 to 10. We follow <ref type="bibr" target="#b30">[31]</ref> and partition the images into high-quality set and low-quality set based on the average scoring. In this study we select a subset of low-quality images based on the semantic tags provided in the AVA data for analysis <ref type="bibr" target="#b3">4</ref> , covering a diverse set of images that require different enhancement operations for quality enhancement. In particular, the "Real" input to the discriminator in our EnhanceGAN is chosen from the top 30% of the high-quality images. The "Fake" inputs are the low-quality images that have an average score &lt; 5. A total of 20, 000 low-quality images are used for training the EnhanceGAN, each paired with k=5 highquality images sampled from the k-nearest neighbor (k-NN) in the feature space of f vдд (All training images are from the AVA standard training partition  in CIELab color space. We set the image input size to be 224 × 224, resulting in 2048 feature maps with spatial size of 7 × 7 (see <ref type="figure" target="#fig_3">Fig. 2a</ref>). We found K = 3 in Top-K averaging pooling <ref type="figure" target="#fig_3">(Fig. 2b)</ref> to be robust in producing parameter candidates. Using a larger K value or global average pooling (i.e., selecting from a large number of parameter candidates) could potentially suffer from noisy parameter predictions, while max pooling only concerns one single prediction and is prone to error. We use RMSprop <ref type="bibr" target="#b37">[38]</ref> with a learning rate lr G = 5e −5 for the generator network and lr D = 5e −7 for the discriminator network after pre-training. A batch size of 64 is used to train each of the image enhancement operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluations</head><p>Image Aesthetic Assessment: Evaluating aesthetic quality of enhanced images quantitatively is non-trivial due to the subjective nature of this task. Inspired by <ref type="bibr" target="#b7">[8]</ref> that uses multiple evaluators that are trained discriminatively for assessing the quality of generated image captions, we also prepare multiple aesthetic evaluators for evaluation. These evaluators are either elaboratively trained, i.e.,, AVA-Net and CUHK-Net (which have balanced accuracy of 89.1% and 94.3% on the CUHK-PQ dataset, respectively); or publicly available, i.e.,, the RANK <ref type="bibr" target="#b25">[26]</ref> and DAN <ref type="bibr" target="#b8">[9]</ref>. The results on test images are summarized respectively in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>. A higher aesthetic score suggests better aesthetic quality. Compared with the fully-supervised DSLR model <ref type="bibr" target="#b20">[21]</ref>, our <ref type="table">Table 2</ref>: Quantitative evaluation for color enhancement on the MIT-Adobe FiveK Dataset. Top1-Expert (Top2-Expert) is the best (second best) scores among five groundtruth images produced by the experts as in the dataset. Our weaklysupervised EnhanceGAN is reasonably competitive as we have not used any expert labels in this dataset.  <ref type="table" target="#tab_0">Table 1</ref>). This result is also consistent with the user study described next. Note that the aesthetic-based evaluation may not be fair to DSLR model <ref type="bibr" target="#b20">[21]</ref> since their objective function is not to optimize aesthetic quality but focuses more on improving image sharpness, texture details and small color variations. By contrast, our EnhanceGAN renders perceptually better aesthetic-driven color quality (see <ref type="figure" target="#fig_4">Fig. 3</ref>). We further evaluate our EnhanceGAN on MIT-Adobe FiveK Dataset. We observe that our EnhanceGAN produces competitive results compared to the commercial baseline of Photoshop-Auto 5 and Top2-Expert (see <ref type="table">Table 2</ref>). We also evaluate our EnhanceGAN on the 5-image evaluation set in <ref type="bibr" target="#b26">[27]</ref>. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, our EnhanceGAN produces consistently more natural color enhancement results while the exemplar-based method <ref type="bibr" target="#b26">[27]</ref> features a more aggressive change in image styles. We also show an ablation test on different losses in <ref type="table" target="#tab_0">Table 1</ref>. We use the piecewise color enhancer in this test. In general, all losses contribute to the performance of EnhanceGAN, with L дan playing the dominant role. It is noteworthy that the deep filtering-based operator of Enhance-GAN performs much better than the random weights baseline, as shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_4">Fig. 3</ref>. The results suggest that EnhanceGAN learns meaningful parameters driven by image aesthetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AVA</head><p>Automatic Image Cropping: We conduct an additional experiment to demonstrate the extensibility of the proposed method. We quantitatively evaluate the performance of our EnhanceGAN for image cropping on the popular CUHK Image Cropping Dataset <ref type="bibr" target="#b42">[43]</ref>, which contains 950 images and 3 sets of cropping groundtruth by 3 different annotators. We perform a 5-fold cross-validation test for all the supervised baselines. Note that these baselines are fine-tuned using the CUHK Image Cropping Dataset, while EnhanceGAN is NOT finetuned with any groundtruth cropping labels. Despite the weakly-supervised nature of our approach, EnhanceGAN achieves competitive performance and even surpasses some methods with full supervision 6 , as shown in <ref type="table">Table 3</ref>. Still, it is unfair to directly compare EnhanceGAN to the fully-supervised cropping methods. Following the learning scheme in Deng et al. <ref type="bibr" target="#b8">[9]</ref>, we show that our EnhanceGAN can be further finetuned (FT) towards state-ofthe-art performance when groundtruth labels are available (see <ref type="table">Table 3</ref>). We also observe that the cropping operator of Enhance-GAN has learned to be attentive to specific regions of the input that are relevant to the image content, hence producing reasonable crop-coordinate candidates resided on the corresponding neurons of the feature maps (see <ref type="figure">Fig. 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Color Harmony</head><p>We spotted interesting patterns regarding the enhanced outputs by our proposed model. As shown in <ref type="figure">Figure 5</ref>, EnhanceGAN learns to produce color palettes that approximate certain color harmony schemes <ref type="bibr">[? ]</ref>, such as Complementary, Split-complementary and Triadic schemes. For example, for input images from human and nature categories, the color palettes from the enhanced outputs show a sign of approximately Split-complementary or Triadic schemes. For images from sky-and-seascape categories, the color palettes are enhanced towards the Complementary scheme. This further <ref type="table">Table 3</ref>: Quantitative evaluation on CUHK Image Cropping Dataset <ref type="bibr" target="#b42">[43]</ref>. The first number is average overlap ratio, higher is better. The second number (shown in parenthesis) is average boundary displacement error, lower is better <ref type="bibr" target="#b42">[43]</ref>. Our EnhanceGAN is by itself competitive compared to weakly-supervised methods, and can be further finetuned (FT) towards state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full supervision Photographer1 Photographer2 Photographer3</head><p>Park et al. <ref type="bibr" target="#b31">[32]</ref> 0.603 (0.106) 0.582 (0.113) 0.609 (0.110) Yan et al. <ref type="bibr" target="#b42">[43]</ref> 0.749 (0.067) 0.729 (0.072) 0.732 (0.072) A2-RL <ref type="bibr" target="#b27">[28]</ref> 0.793 (0.054) 0.791 (0.055) 0.783 (0.055) Deng et al. <ref type="bibr" target="#b8">[9]</ref> 0 shows that the aesthetic enhancement by our weakly-supervised EnhanceGAN is consistent with harmonic color patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">User Study</head><p>The subjective nature of image enhancement evaluation also calls for a validation through human survey. For the purpose of the user study, we asked a professional editor to enhance each of the 100 images in V al 100 in Adobe Photoshop. Image editing options including the tools "Levels", "Curves", "Auto Tone", "Auto Contrast" and "Auto Color" in Adobe Photoshop were available to the professional editor. All enhanced images were stored using sRGB JPEG-format with the highest quality (Quality = 12 in Adobe Photoshop). We wrote a ranking software and distributed to a total of 47 participants. All participants were shown a sequence of 100 image sets, where each image set contained a random ordering of the input image, the image enhanced by the piecewise color enhancer, the image enhanced by the deep filtering-based enhancer, the Photoshop-Auto output, and the human edited image. Participants were instructed to rank each set by clicking the best-quality image, the excellent-quality image, the good-quality image, the average-quality image, and the poor-quality image on the screen in order. No time constraints were placed.</p><p>The results of our user study are shown in <ref type="figure" target="#fig_8">Fig. 7</ref>. Each image in V al 100 received 47 ratings, where we assign "best quality", "excellent quality", "good quality", "average quality" and "poor quality" to aesthetic scores of 10, 7.50, 5.00, 2.50 and 0.00, respectively. We observe that among the images ranked as the "best quality" or "excellent quality", the majority of them are from the EnhanceGAN outputs and the human editing. Our piecewise color enhancer and deep filtering-based enhancer obtain mean aesthetic scores of 5.189 and 5.289, matching 5.232 for human edited images and surpassing scores for Photoshop-Auto and the original inputs. Some of the images enhanced by EnhanceGAN receive even higher voting than the ones produced by the professional editor, as shown in <ref type="figure" target="#fig_9">Fig. 8</ref>. The results demonstrate the effectiveness of the proposed EnhanceGAN for automatic image enhancement and confirm our quantitative evaluation results as in Sec. 4.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have introduced EnhanceGAN for automatic image enhancement. Unlike most existing approaches that require well-aligned and paired images for training, EnhanceGAN only requires weak supervision in the form of binary label on aesthetic quality. We have demonstrated its capability in learning different enhancement operators in an aesthetic-driven manner. EnhanceGAN is fully-differentiable and can be trained end-to-end. We have quantitatively evaluated the performance of EnhanceGAN, and through a user study, we have shown that the high-quality results produced by EnhanceGAN are on par with professional editing.  The box plot below shows the ranking for each image, and the amount of dots denotes the number of users who gives a particular rank as in {Poor, Average, Good, Excellent, Best}. We show the complete results in the supplementary material.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>© 2018 Copyright held by the owner/author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Examples of image enhancement given original input (a). Can you distinguish which figure is enhanced by human and which by our adversarial learning model? (Answer in footnote 1 . Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label></label><figDesc>Row 3: (b) piecewise color enhancer (c ) deep filtering Row 2: (b) deep filtering (c ) piecewise color enhancer Row 1: (b) piecewise color enhancer (c ) deep filtering Answer: None of them are the results from human editing arXiv:1707.05251v2 [cs.CV] 2 Jul 2018</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our proposed EnhanceGAN framework. ResNet module is the feature extractor; in this work, we use the ResNet-101 [16] and removed the last average pooling layer and the final fc layer. (Best viewed in color.) that the minimax game min G max D V (D, G) = E I∼p d at a [logD(I)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>From left to right, top to bottom: (1) input image (2) random combination of pre-trained filters (3) DSLR iphone [21] (4) EnhanceGAN: image cropping (5) En-hanceGAN: piecewise color enhancer (5) EnhanceGAN: deep filtering-based enhancer. (Best viewed in color. More results in the supplementary material.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visual results on the 5-image evaluation set in<ref type="bibr" target="#b26">[27]</ref>. Column 1: input image; Column 2: output by the state-ofthe-art exemplar-based method in Lee et al.<ref type="bibr" target="#b26">[27]</ref>; Column 3: output by the piecewise color enhancer. Column 4: output by the deep filtering-based enhancer. The average aesthetic scores as evaluated by<ref type="bibr" target="#b8">[9]</ref> are shown below. (Best viewed in color. More results in the supplementary material.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Examples of image color palettes. (a) first row: Human, enhanced outputs show Split-complimentary harmony patterns; (b) second row: Nature, enhanced outputs show Split-complimentary or Triadic harmony patterns (c) third row: Skyscape, enhanced outputs show Complimentary harmony patterns. (Best viewed in color) SoftMax The focus of attention as revealed by overlaying the softmax feature map (as in Fig. 2b) onto the input image. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>.806 (0.031) 0.775 (0.038) 0.773 (0.038) EnhanceGAN (FT) 0.828 (0.043) 0.805 (0.050) 0.798 (0.051) Weak supervision Chen et al.[5] 0.664 (0.092) 0.656 (0.095) 0.644 (0.099) EnhanceGAN 0.715 (0.077) 0.701 (0.081) 0.702 (0.080)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>User study on V al 100 . Our EnhanceGAN shows competitive performance as compared to human editing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Visual results from our user study. From left to right: (a) Original image; (b) Photoshop-Auto (c) Enhance-GAN -piecewise color enhancer; (d) EnhanceGAN -deep filtering-based enhancer; (e) Human editing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation for color enhancement on the unseen V al 100 images by multiple aesthetic evaluators. AVA-net and CUHK-net denotes the ResNet-based evaluators finetuned for binary image aesthetic assessment with AVA dataset and CUHK-PQ dataset, respectively (see Sec. 4.1). The averaged softmax scores for all images in V al 100 are shown as final results. Our weakly supervised EnhanceGAN is reasonably competitive for the task of aesthetic-based color enhancement, as is also validated by additional stateof-the-arts image aesthetic assessment models<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="4">AVA-net CUHK-net RANK [26] DAN [9]</cell></row><row><cell>Original Input</cell><cell>0.705</cell><cell>0.542</cell><cell>0.487</cell><cell>0.479</cell></row><row><cell>DSLR iphone [21]</cell><cell>0.624</cell><cell>0.449</cell><cell>0.476</cell><cell>0.359</cell></row><row><cell>DSLR sony [21]</cell><cell>0.524</cell><cell>0.385</cell><cell>0.475</cell><cell>0.295</cell></row><row><cell cols="2">DSLR bl ackber r y [21] 0.616</cell><cell>0.462</cell><cell>0.477</cell><cell>0.395</cell></row><row><cell>Photoshop-Auto</cell><cell>0.687</cell><cell>0.530</cell><cell>0.481</cell><cell>0.447</cell></row><row><cell>Piecewise Enhancer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o L дan</cell><cell>0.708</cell><cell>0.705</cell><cell>0.484</cell><cell>0.491</cell></row><row><cell>w/o L r eд 1 , L r eд 2</cell><cell>0.751</cell><cell>0.755</cell><cell>0.492</cell><cell>0.527</cell></row><row><cell>EnhanceGAN</cell><cell>0.764</cell><cell>0.805</cell><cell>0.494</cell><cell>0.573</cell></row><row><cell>Deep Filtering</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>random weights</cell><cell>0.687</cell><cell>0.432</cell><cell>0.479</cell><cell>0.438</cell></row><row><cell>EnhanceGAN</cell><cell>0.769</cell><cell>0.752</cell><cell>0.498</cell><cell>0.624</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). The binary good/bad quality is the only form of supervision in our training framework. No groundtruth enhancement operation is provided. Test Data 1: We keep 100 random images from the standard test partition of AVA (denoted as V al 100 ) for evaluation.</figDesc><table><row><cell>Input</cell><cell>Exemplar-based</cell><cell>Piecewise Color Enhancer</cell><cell>Deep Filtering-based Enhancer</cell></row><row><cell>Aesthetic Score 0.374</cell><cell>Aesthetic Score = 0.455</cell><cell>Aesthetic Score = 0.508</cell><cell>Aesthetic Score = 0.521</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Test Data 2: The MIT-Adobe FiveK Dataset [3] contains 5,000</cell></row><row><cell></cell><cell></cell><cell></cell><cell>images, each is enhanced by 5 experts towards personal quality</cell></row><row><cell></cell><cell></cell><cell></cell><cell>improvement. The standard test partition of the FiveK dataset as</cell></row><row><cell></cell><cell></cell><cell></cell><cell>in [3] is used in for evaluation.</cell></row></table><note>Implementation Details: The generator network G in our En- hanceGAN is fully convolutional, allowing for arbitrary-sized input</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This corresponds to nine classes in the AVA dataset, i.e.,, Landscape, Seascape, Cityscape, Rural, Sky, Water, Nature, Animals and Portraiture.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Photoshop "autoTone+autoContrast+autoColor"<ref type="bibr" target="#b5">6</ref> The non-parametric retrieval-based method by Chen et al.<ref type="bibr" target="#b5">[6]</ref> included groundtruth crops in their model candidates for crop selection/evaluation, which is not comparable to the reported benchmarks as in<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b42">43]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for photo-quality assessment and enhancement based on visual aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Photographic Global Tonal Adjustment with a Database of Input / Output Image Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic image cropping: A computational complexity study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaocheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoheng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset and Comparative Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Han</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chen</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Yu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to Compose with Professional Photographs on the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Klopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Liu</forename><surname>Shao-Yi Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object-aware image thumbnailing using image classification and enhanced detection of ROI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards Diverse and Natural Image Descriptions via a Conditional GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image Aesthetic Assessment: An Experimental Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="80" to="106" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic image cropping using visual composition, boundary simplicity and content preservation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Survey of Color Mapping and its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Hasan Sheikh Faridul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christel</forename><surname>Pouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Chamaret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Trémeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics (State of the Art Reports</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Characterizing and Improving Stability in Neural Style Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">New Perspectives on Adobe Photoshop CS5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Hosie-Bounar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Geller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comprehensive. Cengage Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic thumbnail generation based on visual representativeness and foreground recognizability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huarong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Color transfer using probabilistic moving least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngbae</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>So Kweon, and Seon Joo Kim</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of aesthetics-driven image recomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wong</forename><surname>Md Baharul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wong</forename><surname>Lai-Kuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chee-Onn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency based automatic image cropping using support vector machine classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nehal</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yogesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meghrajani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Innovations in Information, Embedded and Communication Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic content-aware color and tone stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaohui Shen, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04595</idno>
		<title level="m">A2-RL: Aesthetics Aware Reinforcement Learning for Automatic Image Cropping</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rapid: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural Network for Photo Aesthetic Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AVA: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naila</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling photo composition and its application to photo re-arrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate control of contrast on microcomputer displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Pelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Tse</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hsuan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Hsi</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05339</idno>
		<title level="m">Photo Filter Recommendation by Category-Aware Aesthetic Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Instance Normalization: The Missing Ingredient for Fast Stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">UntrimmedNets for Weakly Supervised Action Recognition and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">D3: Deep dual-domain based fast restoration of JPEGcompressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning the change for automatic image cropping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhou</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Sing Bing Kang, and Xiaoou Tang</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A learningto-rank approach for image color enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhou</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Sing Bing Kang, and Xiaoou Tang</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic photo adjustment using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
