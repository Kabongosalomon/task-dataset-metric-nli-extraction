<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training strategies and datasets for facial representation learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian@adrianbulat.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
							<email>shiyang.c@samsung.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
							<email>jing.yang2@nottingham.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Nottingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Garbett</surname></persName>
							<email>atgarbett@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Samsung AI</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Sanchez</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Samsung AI</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>g.tzimiropoulos@qmul.ac.uk</email>
							<affiliation key="aff5">
								<orgName type="department">Samsung AI Cambridge Queen Mary</orgName>
								<orgName type="institution">University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-training strategies and datasets for facial representation learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>What is the best way to learn a universal face representation? Recent work on Deep Learning in the area of face analysis has focused on supervised learning for specific tasks of interest (e.g. face recognition, facial landmark localization etc.) but has overlooked the overarching question of how to find a facial representation that can be readily adapted to several facial analysis tasks and datasets. To this end, we make the following 4 contributions: (a) we introduce, for the first time, a comprehensive evaluation benchmark for facial representation learning consisting of 5 important face analysis tasks. (b) We systematically investigate two ways of large-scale representation learning applied to faces: supervised and unsupervised pre-training. Importantly, we focus our evaluations on the case of fewshot facial learning. (c) We investigate important properties of the training datasets including their size and quality (labelled, unlabelled or even uncurated). (d) To draw our conclusions, we conducted a very large number of experiments. Our main two findings are: (1) Unsupervised pre-training on completely in-the-wild, uncurated data provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered. (2) Many existing facial video datasets seem to have a large amount of redundancy. We will release code here for pre-trained models and data to facilitate future research. <ref type="figure">Figure 1</ref>: We advocate for a new paradigm to solving face analysis based on the following pipeline: (1) collection of largescale unlabelled facial dataset, (2) (task agnostic) network pre-training for universal facial representation learning, and (3) facial task-specific fine-tuning. Our main result is that even when training on a completely in-the-wild, uncurated dataset downloaded from Flickr, this generic pipeline provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered. nition, landmark localization and 3D reconstruction. 2. Within this benchmark, and for the first time, we systematically evaluate 2 ways of large-scale representation learning applied to faces: supervised and unsupervised pre-training. Importantly, we focus our evaluations on the case of few-shot facial learning where only a limited amount of data is available for the downstream tasks. 3. We systematically evaluate the role of datasets in learning the facial feature presentations by constructing training datasets of varying size and quality. To this end, we considered ImageNet, several existing curated face datasets but also a new in-the-wild, uncurated face dataset downloaded from Flick. 4. We conducted extensive experiments to answer the aforementioned research questions and from them we were able to draw several interesting observations and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning</head><p>Our main findings are: (a) Even when training on a completely in-the-wild, uncurated dataset downloaded from Flickr, unsupervised pre-training pipeline provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered. (b) We found that many existing facial video datasets seem to have a large amount of redundancy. Given that unsupervised pre-training is cheap and that the cost of annotating facial datasets is often significant, some of our findings could be particularly important for researchers when collecting new facial datasets is under consideration. Finally, we will release code, pre-trained models and data to facilitate future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised learning with Deep Neural Networks has been the standard approach to solving several Computer Vision problems over the recent past years <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref>. Among others, this approach has been very successfully applied to several face analysis tasks including face detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>, recognition <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b15">16]</ref> and landmark localization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b69">70]</ref>. For example, face recognition was one of the domains where even very early attempts in the area of deep learning demonstrated performance of super-human accuracy <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b61">62]</ref>. Beyond deep learning, this success can be largely attributed to the fact that for most face-related application domains, large scale datasets could be readily collected and annotated, see for example <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>There are several concerns related to the above approach. Firstly, from a practical perspective, collecting and annotating new large scale face datasets is still necessary; examples of this are context-dependent domains like emotion recognition <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref> and surveillance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>, or new considerations of existing problems like fair face recognition <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b60">61]</ref>. Secondly, from a methodological point of view, it is unsatisfactory for each and every application to require its own large-scale dataset, although there is only one object of interest, that is the human face.</p><p>To this end, we investigate, for the first time to our knowledge, the task of large-scale learning universal facial representation in a principled and systematic manner. In particular, we shed light to the following research questions:</p><p>• "What is the best way to learn a universal facial representation that can be readily adapted to new tasks and datasets? Which facial representation is more amenable to few-shot facial learning?" • "What is the importance of different training dataset properties (including size and quality) in learning this representation? Can we learn powerful facial feature representations from uncurated facial data as well?" To address the aforementioned questions, we make the following 4 contributions: 1. We introduce, for the first time, a comprehensive and principled evaluation benchmark for facial representation learning consisting of 5 important face analysis tasks, namely face recognition, AU recognition, emotion recog-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Facial transfer learning: Transfer learning in Computer Vision typically consists of ImageNet pre-training followed by fine-tuning on the downstream task <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10]</ref>. Because most recent face-related works are based on the collection of larger and larger facial datasets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44]</ref>, the importance of transfer learning has been overlooked in face analysis and, especially, the face recognition literature. ImageNet pre-training has been applied to face analysis when training on small datasets is required, for example for emotion recognition <ref type="bibr" target="#b44">[45]</ref>, face anti-spoofing <ref type="bibr" target="#b47">[48]</ref> and facial landmark localization <ref type="bibr" target="#b69">[70]</ref>. Furthermore, the VGG-Face <ref type="bibr" target="#b46">[47]</ref> or other large face datasets (e.g. <ref type="bibr" target="#b43">[44]</ref>) have been identified as better alternatives by several works, see for example <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref>. To our knowledge, we are the first to systematically evaluate supervised network pre-training using both ImageNet and VGG-Face datasets on several face analysis tasks. Facial datasets: The general trend is to collect larger and larger facial datasets for the face-related task in hand <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44]</ref>. Also it is known that label noise can severely impact accuracy (e.g. see <ref type="table" target="#tab_5">Table 6</ref> of <ref type="bibr" target="#b15">[16]</ref>). Beyond faces, the work of <ref type="bibr" target="#b39">[40]</ref> presents a study which shows the benefit of weakly supervised pre-training on much larger datasets for general image classification and object detection. Similarly, we also investigate the impact of the size of facial datasets on unsupervised pre-training for facial representation learning. Furthermore, one of our main results is to show that a high-quality facial representation can be learned even when a completely uncurated face dataset is used. Few-shot face analysis: Few-shot refers to both low data and label regime. There is very little work in this area. To our knowledge, there is no prior work on few-shot face recognition where the trend is to collect large-scale datasets with millions of samples (e.g. <ref type="bibr" target="#b23">[24]</ref>). There is no systematic study for the task of emotion recognition, too. There is only one work on few-shot learning for facial landmark localization, namely that of <ref type="bibr" target="#b0">[1]</ref> which, different to our approach, proposes an auto-encoder approach for network pre-training. To our knowledge, our evaluation framework provides the very first comprehensive attempt to evaluate the transferability of facial representations for few-shot learning for several face analysis tasks. Semi-supervised face analysis: Semi-supervised learning has been applied to the domain of Action Unit recognition where data labelling is extremely laborious <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b83">84]</ref>. Al-though these methods work with few labels, they are domain specific (as opposed to our work), assuming also that extra annotations are available in terms of "peak" and "valley" frames which is also an expensive operation. Unsupervised learning: There is a very large number of recently proposed unsupervised/self-supervised learning methods, see for example <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22]</ref>. To our knowledge, only very few attempts from this line of research have been applied to faces so far. The authors of <ref type="bibr" target="#b71">[72]</ref> learn face embeddings in a self-supervised manner by predicting the motion field between two facial images. The authors of <ref type="bibr" target="#b65">[66]</ref> propose to combine several facial representations learned using an autoencoding framework.</p><p>In this work, we explore learning facial representations in an unsupervised manner using the state-of-the-art method of <ref type="bibr" target="#b8">[9]</ref> and show how to effectively fine-tune the learned representations to the various face analysis tasks of our benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Supervised deep learning directly applied to large labelled datasets is the de facto approach to solving the most important face analysis tasks. In this section, we propose to take a different path to solving face analysis based on the following 2-stage pipeline: (task agnostic) network pretraining followed by task adaptation. Importantly, we argue that network pre-training should be actually considered as part of the method and not just a simple initialization step. We explore two important aspects of network pre-training: (1) the method used, and (2) the dataset used. Likewise, we highlight hyper-parameter optimization for task adaptation as an absolutely crucial component of the proposed pipeline. Finally, we emphasize the importance of evaluating face analysis on low data regimes, too. We describe important aspects of the pipeline in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Pre-training</head><p>Supervised pre-training of face networks on ImageNet or VGG datasets is not new. We use these networks as strong baselines. For the first time, we comprehensively evaluate their impact on the most important face analysis tasks. Unsupervised pre-training: Inspired by <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9]</ref>, we explore, for the first time in literature, large-scale unsupervised learning on facial images to learn a universal, taskagnostic facial representation. To this end, we adopt the recently proposed SwAV <ref type="bibr" target="#b8">[9]</ref> which simultaneously clusters the data while enforcing consistency between the cluster assignments produced for different augmentations of the same image. The pretext task is defined as a "swapped" prediction problem where the code of one view is predicted from the representation of another:</p><formula xml:id="formula_0">L(z 0 , z 1 ) = (z 0 , q 1 ) + (z 1 , q 0 ),<label>(1)</label></formula><p>where z 0 , z 1 are the features produced by the network for two different views of the same image and q 0 , q 1 their corresponding codes computed by matching these feature using a set of prototypes. is a cross-entropy (with temperature) loss. The full training details are available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-training Datasets</head><p>With pre-training being now an important part of the face analysis pipeline, it is important to investigate what datasets can be used to this end. We argue that supervised pre-training is sub-optimal due to two main reasons: (a) the resulting models may be overly specialized to the source domain and task (e.g. face recognition pre-training) or be too generic (e.g. ImageNet pre-training), and (b) the amount of labeled data may be limited and/or certain parts of the natural data distribution may not be covered. To alleviate this, for the first time, we propose to explore 4 facial datasets of interest within unsupervised pre-training: (a) Large-Scale-</p><formula xml:id="formula_1">Face (&gt; 5.0M ), (b) Full VGG-Face (∼ 3.4M ), (c) Small VGG-Face (∼ 1M ), (d) Flickr-Face (∼ 1.5M</formula><p>). We used existing datasets to create Large-Scale-Face. Flickr-Face is an uncurated dataset we created from Flickr images. See also Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Facial Task Adaptation</head><p>End facial tasks: To draw as safe conclusions as possible, we used a large variety of face tasks (5 in total) including face recognition (classification), facial Action Unit intensity estimation (regression), emotion recognition in terms of valence and arousal (regression), 2D facial landmark localization (pixel-wise regression), and 3D face reconstruction (GCN regression). For these tasks, we used, in total, 10 datasets for evaluation purposes. Adaptation methods: We are given a pre-trained model on task m, composed of a backbone g(.) and a network head h m (.). The model follows the ResNet-50 <ref type="bibr" target="#b26">[27]</ref> architecture. We considered two widely-used methods for task adaptation: (a) Network fine-tuning adapts the weights of g(.) to the new task m i . The previous head is replaced with a taskspecific head h mi (.) that is trained from scratch. (b) Linear layer adaptation keeps the weights of g(.) fixed and trains only the new head h mi (.). Depending on the task, the structure of the head varies. This will be defined for each task in the corresponding section. See also Section 5. Hyper-parameter optimization: We find that, without a proper hyper-parameters selection for each task and setting, the produced results are often misleading. In order to alleviate this and ensure a fair comparison, we search for the following optimal hyper-parameters: (a) learning rate, (b) scheduler duration and (c) backbone learning rate for the pre-trained ResNet-50. This search is repeated for each data point defined by the tuple (task, dataset, pre-training method and % of training data). In total, this yields in an extraordinary number of experiments for discovering the optimal hyperparameters. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the importance of hyperparameters on accuracy for the task of facial landmark localization. In particular, for 1 specific value of learning rate, about 40 different combinations of scheduler duration and backbone relative's learning rate are evaluated. 24 of those combinations are placed on the perimeter of the figure. The 3 closed curves represent the Normalized Mean Error (NME) for each hyperparameter combination for each pre-training method. We observe that accuracy greatly varies for different hyperparameters.</p><formula xml:id="formula_2">1_0.1 1_0.5 1_1.0 1_10.0 1_5.0 2_0.1 2_0.5 2_1.0 2_10.0 2_5.0 4_0.1 4_0.5 4_1.0 4_10.0 4_5.0 6_0.1 6_0.5 6_1.0 6_10.0 6_5.0 8_0.1 8_0.5 8_1.0 8_5.0 4 8 12 Ours (VGG-F) Sup. (Imagenet) Sup. (VGG-F)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Few-shot Learning-based Evaluation</head><p>We explore, for the first time, evaluating the face models using a varying percentage of training data for each face analysis task. Specifically, beyond the standard evaluation using 100% of training data, we emphasize the importance of the low data regime, in particular 10% and 2%, which has a clear impact when new datasets are to be collected and annotated. The purpose of the proposed evaluation is not only to show which method works the best for this setting but also to draw interesting conclusions about the redundancy of existing facial datasets. See also Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Self-distillation for Semi-supervised Learning</head><p>The low data regime of the previous section refers to having both few data and few labels. We further propose to investigate the case of semi-supervised learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b11">12]</ref> where a full facial dataset has been collected but only few labels are provided. To this end, we propose a simple self-distillation technique which fully utilizes network pre-training: we use the fine-tuned network to generate in an online manner new labels for training an identically sized student model on unlabeled data. The student is initialized from a pre-trained model trained in a fully unsupervised manner. The self-distillation process is repeated iteratively for T steps, where, at each step, the previously trained model becomes the teacher. Formally, the knowledge transfer is defined as</p><formula xml:id="formula_3">argmin θt L((f (x, θ t−1 ), f (x, θ t ))), where</formula><p>x is the input sample, θ t−1 and θ t are the parameters of the teacher and the student, respectively, and L is the task loss (e.g. pixel-wise 2 loss for facial landmark localization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ablation Studies</head><p>In this section, we study and answer key questions related to our approach. Fine-tuning vs. linear adaptation: Our results, provided in the supplementary material, show that linear adaptation results in significant performance degradation. As our ultimate goal is high accuracy for the end facial task, linear adaptation is not considered for the rest of our experiments. How much facial data are required? Unlike supervised, unsupervised pre-training does not require labels and hence it can be applied easily to all types of combinations of facial datasets. Then, a natural question arising is how much data is needed to learn a high-quality representation. To this end, we used 3 datasets of varying size. The first one, comprising ∼ 3.3M images, is the original VGG-Face dataset (VGG-Face). The second comprises ∼ 1M images randomly selected from VGGFace2 (VGG-Face-small). The last one, coined as Large-Scale-Face, comprises over 5M images, and is obtained by combining VGG-Face, 300W-LP <ref type="bibr" target="#b85">[86]</ref>, IMDb-face <ref type="bibr" target="#b66">[67]</ref>, AffectNet <ref type="bibr" target="#b43">[44]</ref> and Wider-Face <ref type="bibr" target="#b78">[79]</ref>. We trained 3 models on these datasets and evaluated them for the tasks of facial landmark localization, AU intensity estimation and face recognition. As the results from <ref type="table" target="#tab_1">Table 2</ref> show, VGG-Face vs. VGG-Face-small yields small yet noticeable improvements especially for the case of 2% of labelled data. We did not observe further gains by training on Large-Scale-Face. Curated vs. uncurated datasets: While the previous section investigated the quantity of data required, it did not explore the question of data quality. While we did not use any labels during the unsupervised pre-training phase, one may argue that all datasets considered are sanitized as they were collected by human annotators with a specific task in mind.</p><p>In this section, we go beyond sanitized datasets, by creating a completely uncurated, in-the-wild, dataset, coined Flick-Face, of ∼ 1.5M facial images by simply downloading images from Flickr (using standard search keywords like "faces", "humans", etc.) and filtering them with a face detector (the dataset will be made available). We then trained a model on it and evaluated it on the same tasks/datasets of the previous section. <ref type="table" target="#tab_1">Table 2</ref> shows some remarkable results: the resulting model is on par with the one trained on the full VGG-Face dataset (Section 5 shows that it outperforms all other pre-training methods, too). We believe that this result can pave a whole new way to how practitioners, both in industry and academia, collect and label facial datasets for new tasks and applications. Pre-training task or data? In order to fully understand whether the aforementioned gains are coming from the unsupervised task alone, the data, or both, we pre-trained a model on ImageNet dataset using both supervised and unsupervised pre-training. Our experiments showed that both models performed similarly (e.g. 4.97% vs 5.1% on 300W@2% of data) and significantly more poorly than models trained on face datasets. We conclude that both unsupervised pre-training and data are required for high ac-curacy.</p><p>Effect of unsupervised method: Herein, we compare the results obtained by changing the unsupervised pre-training method from SwAV to Moco-v2 <ref type="bibr" target="#b25">[26]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows that SwAV largely outperforms Moco-v2, emphasizing the importance of utilizing the most powerful available unsupervised method. Note, that better representation learning as measured on imagenet, doesn't equate with better representation in general <ref type="bibr" target="#b12">[13]</ref>, hence way it's important to validate the performance of different methods for faces too. Furthermore, we evaluated SwAV models using different batchsizes which is shown to be an important hyper-parameter. We found both models to perform similarly. See SwAV (256) in <ref type="table" target="#tab_0">Table 1</ref> for the model trained with batch-size 256.</p><p>With small batch-size training requires less resources, yet we found that it was prolonged by 2×. Self-distillation for semi-supervised learning: Herein, we evaluate the effectiveness of network pre-training on selfdistillation (see Section 3.5) for the task of semi-supervised facial landmark localization (300W). See supplementary for results on AU intensity estimation (DISFA).</p><p>We compare unsupervised vs. supervised pre-training on VGG-Face as well as training from scratch. These networks are fine-tuned on 300W using 100% and, the most interesting, 10% and 2% of the data. Then, they are used as students for self-distillation. <ref type="figure">Fig. 3</ref> clearly shows the effectiveness of unsupervised student pre-training. Furthermore, a large pool of unlabelled data was formed by 300W, AFLW <ref type="bibr" target="#b31">[32]</ref>, WFLW <ref type="bibr" target="#b72">[73]</ref> and COFW <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>), and then used for self-distillation. <ref type="figure">Fig. 4 (left)</ref> shows the impact on the accuracy of the final model by adding more and more unlabelled data to the self-distillation process. Clearly, selfdistillation based on network pre-training is capable of effectively utilizing a large amount of unlabelled data. Finally, <ref type="figure">Fig. 4 (right)</ref> shows the impact of the number of selfdistillation steps on accuracy. Other supervised pre-training: Our best supervised pretrained network is that based on training CosFace [69] on VGG-Face. Herein, for completeness, we compare this to supervised pre-training on another task/dataset, namely facial landmark localization. As <ref type="table" target="#tab_2">Table 3</ref> shows, the supervised pre-trained model on VGG-Face outperforms it by large margin. This is expected due to the massive size of VGG-Face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Main Results</head><p>In this section, we thoroughly test the generalizability of the universal facial representations by adapting the resulting models to the most important facial analysis tasks. The full training and implementation details for each of this tasks can be found in the supplementary material. Training code will also be made available.</p><p>Data &amp; label regime: For all datasets and tasks, we used fine-tuning for network adaptation using 3 data and label regimes: full (100%), low (10%) and very low (2% or less). Models compared: For unsupervised network pre-training, we report the results of two models, one trained on the full VGG-Face and one on Flickr-Face. These models are denoted as Ours (VGG-F) and Ours (Flickr-F). These models are compared with supervised pre-training on ImageNet and VGG-Face (denoted as VGG-F), as well as the model trained from scratch. Comparison with SOTA: Where possible, we also present the results reported by state-of-the-art methods for each task on the few-shot setting. Finally, for each task, and, to put our results into perspective, we report the accuracy of a state-of-the-art method for the given task. We note however, that the results are not directly compa-rable, due to different networks, losses, training procedure, and even training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Face Recognition</head><p>For face recognition, we fine-tuned the models on the VGGFace <ref type="bibr" target="#b6">[7]</ref> and tested them on the IJB-B <ref type="bibr" target="#b70">[71]</ref> and IJB-C <ref type="bibr" target="#b41">[42]</ref> datasets. The task specific head h(.) consists of a linear layer. The whole network was optimized using the CosFace loss <ref type="bibr" target="#b68">[69]</ref>. Note that, for this experiment, since training was done on VGGFace <ref type="bibr" target="#b6">[7]</ref>, the results of supervised pre-training on VGG-Face are omitted (as meaningless). Results are shown in <ref type="table" target="#tab_3">Table 4</ref>. Both Ours (VGG-F) and Ours (Flickr-F) perform similarly and both they outperform the other baselines by large margin for the low (10%) and very low (2%) data regimes. For the latter case, the accuracy drops significantly for all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Facial Landmark Localization</head><p>. We fine-tuned the pre-trained models for facial landmark localization on 300W <ref type="bibr" target="#b55">[56]</ref>, AFLW-19 <ref type="bibr" target="#b31">[32]</ref>, WFLW <ref type="bibr" target="#b72">[73]</ref> and COFW-68 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref> reporting results in terms of NME inter-ocular <ref type="bibr" target="#b55">[56]</ref> or NME diag <ref type="bibr" target="#b31">[32]</ref>. We followed the current best practices based on heatmap regression <ref type="bibr" target="#b2">[3]</ref>. In order to accommodate for the pixel-wise nature of the task, the task specific head h(.) is defined as a set of 3 1 × 1 conv. layers with 256 channels, each interleaved with bilinear upsampling operations for recovering part of the lost resolution. Additional high resolution information is brought up via skip connections and summation from the lower part of the network. Despite the simple and un-optimized architecture we found that the network performs very well, thanks to the strong facial representation learned. All models were trained using a pixel-wise MSE loss. Results are shown in <ref type="table" target="#tab_4">Table 5</ref>: unsupervised pre-training (both models) outperform the other baselines for all data regimes, especially for the low and very low cases. For the latter case, Ours (VGG-F) outperforms Ours (Flickr-F) probably because Ours (VGG-F) contains a more balanced distribution of facial poses. The best supervised pre-training method is VGG-F showing the importance of pre-training on facial datasets. Furthermore, <ref type="table" target="#tab_5">Table 6</ref> shows comparison with few very recent works on few-shot face alignment. Our method scores significantly higher across all data regimes and datasets tested setting a new state-of-the-art despite the straightforward network architecture and the generic nature of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Action Unit (AU) Intensity Estimation</head><p>We fine-tuned and evaluated the pre-trained models for AU intensity estimation on the corresponding partitions of BP4D <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b81">82]</ref> and DISFA <ref type="bibr" target="#b40">[41]</ref> datasets. The network head  h(.) is implemented using a linear layer. The whole network is trained to regress the intensity value of each AU using an 2 loss. We report results in terms of intra-class correlation (ICC) <ref type="bibr" target="#b58">[59]</ref>.</p><p>Results are shown in <ref type="table" target="#tab_6">Table 7</ref>: unsupervised pre-training (both models) outperform the other baselines for all data regimes. Notably, our models achieve very high accuracy even for the case when 2% of data was used. Supervised pre-training on VGG-F also works well. Furthermore, <ref type="table" target="#tab_5">Table 6</ref> shows comparison with very recent works on semi-supervised AU intensity estimation. We note that these methods had access to all training data; only the amount of labels was varied. Our methods, although trained under both very low data and label regimes, outperformed them by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Emotion Recognition</head><p>We fine-tuned the models for valence and arousal estimation on the well-established AffectNet <ref type="bibr" target="#b43">[44]</ref>. We report results in terms of RMSE and CCC <ref type="bibr" target="#b53">[54]</ref> (for other metric see supplementary). The task specific head h(.) is a linear layer that regresses the valence and arousal values and also predicts the basic emotion classes. The network was trained SOTA (from paper) <ref type="bibr" target="#b45">[46]</ref> 0.57 -0.72 - to jointly minimise the RMSE and CCC losses for valence and arousal, and the cross-entropy loss for classification.</p><p>Results are shown in <ref type="table" target="#tab_8">Table 9</ref>: again, for all data regimes, our unsupervised models outperform the supervised baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">3D Face Reconstruction</head><p>We fine-tuned all models on the 300W-LP <ref type="bibr" target="#b85">[86]</ref> dataset and tested them on AFLW2000-3D <ref type="bibr" target="#b85">[86]</ref>. Our task specific head is implemented with a GCN based on spiral convolutions <ref type="bibr" target="#b37">[38]</ref>. The network was trained to minimise the 1 distance between the predicted and the ground truth vertices. Results are shown in <ref type="table" target="#tab_0">Table 10</ref>: it can be seen that, for all data regimes, our unsupervised models outperform the supervised baselines. Supervised pre-training on VGG-F also works well.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusions</head><p>Several conclusions can be drawn from our results: Unsupervised pre-training followed by task-specific finetuning provides very strong baselines for face analysis. For example, we showed that such generically built baselines outperformed recently proposed methods for fewshot/semi-supervised learning (e.g. for facial landmark localization and AU intensity estimation) some of which are based on quite sophisticated techniques. Moreover, we showed that unsupervised pre-training largely boosts selfdistillation. Hence, it might be useful for newly-proposed task-specific methods to consider such a pipeline for both development and evaluation especially when newlyachieved accuracy improvements are to be reported.</p><p>Furthermore, these results can be achieved even by simply training on uncurated facial datasets that can be readily downloaded from image repositories. The excellent results obtained by pre-training on Flickr-Face are particularly encouraging. Note that we could have probably created a bet-ter and more balanced dataset in terms of facial pose by running a method for facial pose estimation.</p><p>When new datasets are to be collected, such powerful pre-trained networks can be potentially used for minimizing data collection and label annotation labour. Our results show that many existing datasets (e.g. AFLW, DISFA, BP4D, even AffectNet) seem to have a large amount of redundancy. This is more evident for video datasets (e.g. DISFA, BP4D).</p><p>Note that by no means our results imply or suggest that all face analysis can be solved with small labelled datasets. For example, for face recognition, it was absolutely necessary to fine-tune on the whole VGG-Face in order to get high accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Facial landmark localization accuracy in terms of NME (%) of 3 different pre-training methods for selected combinations of hyperparameters. The labels on the figure's perimeter show the scheduler length (first value) and backbone relative's learning rate (second value) separated by an underscore. Each circle on the radar plot denotes a constant error level. Points located closer to the center correspond to lower error levels. Accuracy greatly varies for different hyperparameters. See also text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Effectiveness of network pre-training on selfdistillation for the tasks of facial landmark localization. Self-distillation accuracy (for facial landmark localization) vs. (left) amount of unlabeled data (100% corresponds to 300W), and (right) number of distillation steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ImageNet) 0.360 0.705 0.327 0.620 Sup. (VGG-F) 0.369 0.706 0.330 0.624 Ours (VGG-F) 0.356 0.710 0.326 0.629 10% Scratch 0.402 0.625 0.366 0.536 Sup. (ImageNet) 0.383 0.654 0.351 0.566 Sup. (VGG-F) 0.401 0.636 0.372 0.532 Ours (VGG-F) 0.382 0.678 0.344 0.599 2% Scratch 0.453 0.515 0.400 0.422 Sup. (ImageNet) 0.411 0.557 0.362 0.456 Sup. (VGG-F) 0.416 0.607 0.384 0.485 Ours (VGG-F) 0.370 0.593 0.338 0.471 SOTA (from paper) [33] 0.35 0.71 0.32 0.63</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between the facial representations learned by MoCov2 and SwAV, by fine-tuning the models on 2% of 300W and DISFA.</figDesc><table><row><cell>Method</cell><cell cols="2">300W DISFA</cell></row><row><cell></cell><cell cols="2">NME ICC</cell></row><row><cell>Scratch</cell><cell>13.5</cell><cell>.237</cell></row><row><cell>MoCov2</cell><cell>11.9</cell><cell>.280</cell></row><row><cell>SwAV</cell><cell>4.97</cell><cell>.560</cell></row><row><cell cols="2">SwAV (256) 5.00</cell><cell>.549</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Impact of different datasets on the facial representations learned in an unsupervised manner for the tasks of facial landmark localization (300W), AU intensity estimation (DISFA) and face recognition (IJB-B).</figDesc><table><row><cell>Data</cell><cell>Unsup.</cell><cell cols="3">300W DISFA IJBB</cell></row><row><cell>amount</cell><cell>Data</cell><cell>NME</cell><cell>ICC</cell><cell>10 −4</cell></row><row><cell></cell><cell>VGG-Face-small</cell><cell>3.91</cell><cell>.583</cell><cell>0.910</cell></row><row><cell>100%</cell><cell>VGG-Face Large-Scale-Face</cell><cell>3.85 3.83</cell><cell>.598 .593</cell><cell>0.912 0.912</cell></row><row><cell></cell><cell>Flickr-Face</cell><cell>3.86</cell><cell>.590</cell><cell>0.911</cell></row><row><cell></cell><cell>VGG-Face-small</cell><cell>4.37</cell><cell>.572</cell><cell>0.887</cell></row><row><cell>10%</cell><cell>VGG-Face Large-Scale-Face</cell><cell>4.25 4.30</cell><cell>.592 .597</cell><cell>0.889 0.892</cell></row><row><cell></cell><cell>Flickr-Face</cell><cell>4.31</cell><cell>.581</cell><cell>0.887</cell></row><row><cell></cell><cell>VGG-Face-small</cell><cell>5.46</cell><cell>.550</cell><cell>0.729</cell></row><row><cell>2%</cell><cell>VGG-Face Large-Scale-Face</cell><cell>4.97 4.98</cell><cell>.560 .551</cell><cell>0.744 0.743</cell></row><row><cell></cell><cell>Flickr-Face</cell><cell>5.05</cell><cell>.571</cell><cell>0.740</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Supervised pre-training applied to different datasets. The models are evaluated for AU intensity estimation on DISFA.</figDesc><table><row><cell>Data</cell><cell></cell><cell>Pretrain. method</cell><cell></cell></row><row><cell>amount</cell><cell cols="3">Sup. (ImageNet) Sup. (VGG-F) Sup. (300W)</cell></row><row><cell>100%</cell><cell>.560</cell><cell>.575</cell><cell>.463</cell></row><row><cell>10%</cell><cell>.556</cell><cell>.560</cell><cell>.460</cell></row><row><cell>1%</cell><cell>.453</cell><cell>.542</cell><cell>.414</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Face recognition results in terms of TAR for various FARs on IJB-B and IJB-C.</figDesc><table><row><cell>Data</cell><cell>Pretrain.</cell><cell>IJB-B</cell><cell></cell><cell></cell><cell>IJB-C</cell><cell></cell></row><row><cell>amount</cell><cell>method</cell><cell>10 −6 10 −5 10 −4 10 −3 10 −2</cell><cell>10 −6</cell><cell>10 −5</cell><cell>10 −4</cell><cell>10 −3 10 −2</cell></row><row><cell></cell><cell>Scratch</cell><cell>0.389 0.835 0.912 0.950 0.975</cell><cell>0.778</cell><cell>0.883</cell><cell cols="2">0.931 0.961 0.981</cell></row><row><cell>100%</cell><cell cols="2">Sup. (ImageNet) 0.390 0.843 0.912 0.950 0.975</cell><cell>0.831</cell><cell>0.891</cell><cell cols="2">0.931 0.961 0.981</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>0.406 0.834 0.911 0.951 0.975</cell><cell>0.807</cell><cell>0.880</cell><cell cols="2">0.932 0.962 0.982</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>0.432 0.835 0.912 0.950 0.976</cell><cell>0.882</cell><cell>0.882</cell><cell cols="2">0.932 0.961 0.981</cell></row><row><cell></cell><cell>Scratch</cell><cell>0.326 0.645 0.848 0.926 0.965</cell><cell>0.506</cell><cell cols="3">0.7671 0.8840 0.940 0.721</cell></row><row><cell>10%</cell><cell cols="2">Sup. (ImageNet) 0.320 0.653 0.858 0.926 0.966</cell><cell>0.503</cell><cell>0.779</cell><cell cols="2">0.891 0.941 0.973</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>0.334 0.758 0.887 0.940 0.970</cell><cell>0.715</cell><cell>0.834</cell><cell cols="2">0.909 0.952 0.978</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>0.392 0.784 0.889 0.941 0.972</cell><cell>0.733</cell><cell>0.847</cell><cell cols="2">0.911 0.953 0.977</cell></row><row><cell></cell><cell>Scratch</cell><cell>0.086 0.479 0.672 0.800 0.909</cell><cell>0.400</cell><cell>0.570</cell><cell cols="2">0.706 0.829 0.922</cell></row><row><cell>2%</cell><cell cols="2">Sup. (ImageNet) 0.264 0.553 0.694 0.820 0.915</cell><cell>0.493</cell><cell>0.599</cell><cell cols="2">0.723 0.841 0.928</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>0.282 0.558 0.740 0.870 0.944</cell><cell>0.486</cell><cell>0.649</cell><cell cols="2">0.786 0.891 0.954</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>0.333 0.547 0.744 0.873 0.948</cell><cell>0.455</cell><cell>0.637</cell><cell cols="2">0.786 0.893 0.956</cell></row><row><cell cols="2">SOTA (from paper) [16]</cell><cell cols="3">0.401 0.821 0.907 0.950 0.978 0.0.767 0.879</cell><cell cols="2">0.929 0.964 0.984</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Facial landmark localization results on 300W (test set), COFW, WFLW and AFLW in terms of NME inter-ocular , except for AFLW where NME diag is used.</figDesc><table><row><cell>Data amount</cell><cell>Pretrain. method</cell><cell cols="4">300W COFW WFLW AFLW</cell></row><row><cell></cell><cell>Scratch</cell><cell>4.50</cell><cell>4.10</cell><cell>5.10</cell><cell>1.59</cell></row><row><cell></cell><cell cols="2">Sup. (ImageNet) 4.16</cell><cell>3.63</cell><cell>4.80</cell><cell>1.59</cell></row><row><cell>100%</cell><cell>Sup. (VGG-F)</cell><cell>3.97</cell><cell>3.51</cell><cell>4.70</cell><cell>1.58</cell></row><row><cell></cell><cell cols="2">Ours (Flickr-F) 3.86</cell><cell>3.45</cell><cell>4.65</cell><cell>1.57</cell></row><row><cell></cell><cell cols="2">Ours (VGG-F) 3.85</cell><cell>3.32</cell><cell>4.57</cell><cell>1.55</cell></row><row><cell></cell><cell>Scratch</cell><cell>6.61</cell><cell>5.63</cell><cell>6.82</cell><cell>1.84</cell></row><row><cell></cell><cell cols="2">Sup. (ImageNet) 5.15</cell><cell>5.32</cell><cell>6.56</cell><cell>1.81</cell></row><row><cell>10%</cell><cell>Sup. (VGG-F)</cell><cell>4.55</cell><cell>4.46</cell><cell>5.87</cell><cell>1.77</cell></row><row><cell></cell><cell cols="2">Ours (Flickr-F) 4.31</cell><cell>4.27</cell><cell>5.45</cell><cell>1.73</cell></row><row><cell></cell><cell cols="2">Ours (VGG-F) 4.25</cell><cell>3.95</cell><cell>5.44</cell><cell>1.74</cell></row><row><cell></cell><cell>Scratch</cell><cell cols="2">13.52 14.7</cell><cell>10.43</cell><cell>2.23</cell></row><row><cell></cell><cell cols="2">Sup. (ImageNet) 8.04</cell><cell>8.05</cell><cell>8.99</cell><cell>2.09</cell></row><row><cell>2%</cell><cell>Sup. (VGG-F)</cell><cell>5.45</cell><cell>5.55</cell><cell>6.94</cell><cell>2.00</cell></row><row><cell></cell><cell cols="2">Ours (Flickr-F) 5.05</cell><cell>5.18</cell><cell>6.53</cell><cell>1.86</cell></row><row><cell></cell><cell cols="2">Ours (VGG-F) 4.97</cell><cell>4.70</cell><cell>6.29</cell><cell>1.88</cell></row><row><cell cols="2">SOTA (from paper) [70]</cell><cell>3.85</cell><cell>3.45</cell><cell>4.60</cell><cell>1.57</cell></row><row><cell cols="2">SOTA (from paper) [34]</cell><cell>-</cell><cell>-</cell><cell>4.37</cell><cell>1.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">: Comparison against state-of-the-art in few-shot fa-</cell></row><row><cell>cial landmark localization.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>300W</cell><cell cols="3">100% 10% 1.5%</cell></row><row><cell>RCN+ [28]</cell><cell>3.46</cell><cell>4.47</cell><cell>-</cell></row><row><cell>TS 3 [18]</cell><cell>3.49</cell><cell>5.03</cell><cell>-</cell></row><row><cell>3FabRec [1]</cell><cell>3.82</cell><cell cols="2">4.47 5.10</cell></row><row><cell cols="2">Ours (VGG-F) 3.20</cell><cell cols="2">3.48 4.13</cell></row><row><cell>AFLW</cell><cell cols="2">100% 10%</cell><cell>1%</cell></row><row><cell>RCN+ [28]</cell><cell>1.61</cell><cell>-</cell><cell>2.88</cell></row><row><cell>TS 3 [18]</cell><cell>-</cell><cell>2.14</cell><cell>-</cell></row><row><cell>3FabRec [1]</cell><cell>1.87</cell><cell cols="2">2.03 2.38</cell></row><row><cell cols="2">Ours (VGG-F) 1.54</cell><cell cols="2">1.70 1.91</cell></row><row><cell>WFLW</cell><cell cols="3">100% 10% 0.7%</cell></row><row><cell>SA [50]</cell><cell>4.39</cell><cell>7.20</cell><cell>-</cell></row><row><cell>3FabRec [1]</cell><cell>5.62</cell><cell cols="2">6.73 8.39</cell></row><row><cell cols="2">Ours (VGG-F) 4.57</cell><cell cols="2">5.44 7.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>AU intensity estimation results in terms of ICC on BP4D and DISFA.</figDesc><table><row><cell>Data</cell><cell>Pretrain.</cell><cell cols="2">DISFA</cell><cell cols="2">BP4D</cell></row><row><cell>amount</cell><cell>method</cell><cell cols="4">finetune linear finetune linear</cell></row><row><cell></cell><cell>Scratch</cell><cell>.318</cell><cell>-</cell><cell>.617</cell><cell>-</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>.560</cell><cell>.316</cell><cell>.708</cell><cell>.587</cell></row><row><cell>100%</cell><cell>Sup. (VGG-F)</cell><cell>.575</cell><cell>.235</cell><cell>.700</cell><cell>.564</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>.590</cell><cell>.373</cell><cell>.715</cell><cell>.599</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>.598</cell><cell>.342</cell><cell>.719</cell><cell>.610</cell></row><row><cell></cell><cell>Scratch</cell><cell>.313</cell><cell>-</cell><cell>.622</cell><cell>-</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>.556</cell><cell>.300</cell><cell>.698</cell><cell>.573</cell></row><row><cell>10%</cell><cell>Sup. (VGG-F)</cell><cell>.560</cell><cell>.232</cell><cell>.692</cell><cell>.564</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>.581</cell><cell>.352</cell><cell>.699</cell><cell>.603</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>.592</cell><cell>.340</cell><cell>.706</cell><cell>.604</cell></row><row><cell></cell><cell>Scratch</cell><cell>.237</cell><cell>-</cell><cell>.586</cell><cell>-</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>.453</cell><cell>.301</cell><cell>.689</cell><cell>.564</cell></row><row><cell>1%</cell><cell>Sup. (VGG-F)</cell><cell>.542</cell><cell>.187</cell><cell>.690</cell><cell>.562</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>.571</cell><cell>.321</cell><cell>.695</cell><cell>.596</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>.560</cell><cell>.326</cell><cell>.694</cell><cell>.592</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison against the state-of-the-art in semisupervised AU intensity estimation. Results for each individual AU can be found in the supplementary material.</figDesc><table><row><cell>Method</cell><cell>Data amount</cell><cell cols="2">Dataset DISFA BP4D</cell></row><row><cell>KBSS [83]</cell><cell>1%</cell><cell>.350</cell><cell>.645</cell></row><row><cell>KJRE [85]</cell><cell>6%</cell><cell>.350</cell><cell>.600</cell></row><row><cell>CLFL [84]</cell><cell>1%</cell><cell>.408</cell><cell>.680</cell></row><row><cell>SSCFL [57]</cell><cell>2%</cell><cell>.413</cell><cell>.680</cell></row><row><cell>Ours (VGG-F)</cell><cell>1%</cell><cell>.574</cell><cell>.707</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Valence and arousal estimation results in terms of RMSE and CCC on AffectNet.</figDesc><table><row><cell>Data</cell><cell>Pretrain.</cell><cell>Valence</cell><cell>Arousal</cell></row><row><cell>amount</cell><cell>method</cell><cell cols="2">RMSE CCC RMSE CCC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>3D face reconstruction reconstruction in terms of NME (68 points) on AFLW2000-3D.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Pretrain. method</cell><cell></cell></row><row><cell>Data</cell><cell>Scratch</cell><cell>Sup.</cell><cell>Sup.</cell><cell>Ours</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell>(Imagenet)</cell><cell>(VGG-F)</cell><cell>(Flickr-F)</cell><cell>(VGG-F)</cell></row><row><cell cols="2">100% 3.70</cell><cell>3.58</cell><cell>3.51</cell><cell>3.53</cell><cell>3.42</cell></row><row><cell>10%</cell><cell>4.72</cell><cell>4.06</cell><cell>3.82</cell><cell>3.81</cell><cell>3.72</cell></row><row><cell>2%</cell><cell>7.11</cell><cell>6.15</cell><cell>4.42</cell><cell>4.50</cell><cell>4.31</cell></row><row><cell></cell><cell cols="4">SOTA (from paper) [14]: 3.39</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3fabrec: Fast fewshot face alignment by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Browatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-stage convolutional part heatmap regression for the 1st 3d face alignment in the wild (3dfaw) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face recognition in poor-quality video: Evidence from security surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A Mike</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="248" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rogerio S Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>arXiv, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566,2020.5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faster, better and more detailed: 3d face reconstruction with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Retinaface</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00641</idno>
		<title level="m">Singlestage dense face localisation in the wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Teacher supervises students how to learn from partially labeled images for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using cnn-rnn and c3d hybrid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangju</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanliu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Occlusion coherence: Detecting and localizing occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08347</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scfacesurveillance cameras face database. Multimedia tools and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mislav</forename><surname>Grgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kresimir</forename><surname>Delac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonja</forename><surname>Grgic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic, dimensional and continuous emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hatice</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Synthetic Emotions (IJSE)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1546" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Video-based emotion recognition in the wild using deep transfer learning and score fusion. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heysem</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furkan</forename><surname>Gürpınar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">Ali</forename><surname>Salah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Shvetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Kuharenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04598</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on computer vision workshops (ICCV workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Factorized higher-order cnns with an application to spatio-temporal emotion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Luvli face alignment: Estimating landmarks&apos; location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8236" to="8246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dsfd: dual shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5060" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep facial expression recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A simple approach to intrinsic correspondence learning on unstructured 3d meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaak</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Dielen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Campen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S Mohammad Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep learning for emotion recognition on small datasets using transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Wei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><forename type="middle">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on international conference on multimodal interaction</title>
		<meeting>the 2015 ACM on international conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="443" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Michel Valstar, and Georgios Tzimiropoulos. A transfer learning approach to heatmap regression for action unit intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioanna</forename><surname>Ntinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recognizing multimodal face spoofing with face recognition networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Parkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Grinchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR-W</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P Jonathon</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">N</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><forename type="middle">A</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eilidh</forename><surname>Noyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelsey</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><forename type="middle">G</forename><surname>Cavazos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Jeckeln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="6171" to="6176" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Shengju Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10153" to="10163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE FG 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Avec 2015: The 5th international audio/visual emotion challenge and workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1335" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Face recognition: too bias, or not too bias?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gennady</forename><surname>Joseph P Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Livitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Henon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samson</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timoner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="0" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge: Database and results. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01864</idno>
		<title level="m">Anestis Zaganidis, and Georgios Tzimiropoulos. Semi-supervised au intensity estimation with contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Shrout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intraclass correlations: uses in assessing rater reliability</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">420</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Sixta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Buch-Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Escalera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07838</idno>
		<title level="m">Fairface challenge at eccv 2020: Analyzing bias in face recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">End-toend multimodal emotion recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Björn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Avec 2016: Depression, mood, and emotion recognition workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><forename type="middle">Torres</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giota</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international workshop on audio/visual emotion challenge</title>
		<meeting>the 6th international workshop on audio/visual emotion challenge</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fera 2015-second facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Towards a general model of knowledge for facial analysis by multi-source transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The devil of face recognition is in the noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liren</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Fanface: a simple orthogonal improvement to deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12621" to="12628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaun</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="692" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Weakly-supervised deep convolutional neural network learning for facial action unit intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Gang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2314" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Context-aware feature and label fusion for facial action unit intensity estimation with partially labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Joint representation and estimator learning for facial action unit intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Gang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3457" to="3466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Face alignment in full pose range: A 3d total solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="78" to="92" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
