<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Saliency Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern Polytechincal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyuan</forename><surname>Wan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern Polytechincal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<email>junweihan2010@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern Polytechincal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Saliency Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, massive saliency detection methods have achieved promising results by relying on CNN-based architectures.</p><p>Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Apart from the traditional transformer architecture used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-taskattention mechanism. Experimental results show that our model outperforms existing state-of-the-art results on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models.</p><p>Recently, Transformer [61] was proposed to model global long-range dependencies among word sequences for machine translation. The core of the Transformer is the selfattention mechanism, which leverages the query-key correlation to relate different positions in a sequence <ref type="bibr" target="#b60">[61]</ref>. Transformer stacks the self-attention layers multiple times in both encoder and decoder, thus can model long-range dependencies in every layer. Thus, it is natural to introduce the Transformer to SOD, leveraging the global cues in the model all the way.</p><p>In this paper, for the first time, we rethink SOD from a new sequence-to-sequence perspective and develop a novel unified model for both RGB and RGB-D SOD based on a pure transformer, which is named Visual Saliency Transformer. We follow the recently proposed ViT models</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>SOD aims to detect and segment salient objects or regions when viewing an image. Apart from performing SOD on RGB images, RGB-D SOD has also gained growing interest in recent years, since depth data can provide useful spatial structure information for saliency detection.</p><p>Current state-of-the-art SOD methods are dominated by convolutional architectures <ref type="bibr" target="#b27">[28]</ref>, on both RGB and RGB-D data. They often adopt an encoder-decoder CNN architecture <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">57]</ref>, where the encoder encodes the input image * Equal contribution. † Corresponding author. to multi-level features and the decoder integrates the extracted features to predict the final saliency map. Based on this simple architecture, most efforts have been made to build a powerful decoder for predicting better saliency results. To this end, they introduced various attention models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b6">7]</ref>, multi-scale feature integration methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43]</ref>, and multi-task learning frameworks <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b24">25</ref>]. An additional demand for RGB-D SOD is to effectively fuse cross modality information, i.e., the appearance information and the depth cues. Previous works propose various modality fusion methods, such as feature fusion methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>, knowledge distillation <ref type="bibr" target="#b52">[53]</ref>, dynamic convolution <ref type="bibr" target="#b47">[48]</ref>, attention models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b76">77]</ref>, and graph neural networks <ref type="bibr" target="#b42">[43]</ref>. As a result, CNN-based SOD methods have achieved impressive results <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b86">87]</ref>.</p><p>However, all previous methods are limited in learning global long-range dependencies. Global contexts <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37]</ref> and global contrast <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref> have been proved crucial for saliency detection for a long time. Nevertheless, due to the intrinsic limitation of CNNs that they extract features in local sliding windows, previous methods can hardly exploit the crucial global cues. Although some methods utilized fully connected layers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22]</ref>, global pooling layers <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b64">65]</ref>, and non-local modules <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b6">7]</ref> to incorporate the global context, they only did such in certain layers and the standard CNN-based architecture remains unchanged. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b72">73]</ref> to divide each image into patches and adopt the Transformer model on the patch sequence. Then, the Transformer propagates long-range dependencies between image patches, without any need of using convolution. However, it is not straightforward to apply ViT for SOD. On the one hand, how to perform dense prediction tasks based on pure transformer still remains an open question. On the other hand, ViT usually tokenizes the image to a very coarse scale. How to adapt ViT to the high-resolution prediction demand of SOD is also unclear.</p><p>To solve the first problem, we design a token-based transformer decoder by introducing task-related tokens to learn decision embeddings. Then, we propose a novel patch-task-attention mechanism to generate denseprediction results, which provides a new paradigm for using transformer in dense prediction tasks. Motivated by previous SOD models <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b24">25]</ref> that leveraged boundary detection to boost the SOD performance, we build a multi-task decoder to simultaneously conduct saliency and boundary detection by introducing a saliency token and a boundary token. This strategy simplifies the multitask prediction workflow by simply learning task-related tokens, thus largely reduces the computational costs while obtaining better results. To solve the second problem, inspired by the Tokens-to-Token (T2T) transformation <ref type="bibr" target="#b72">[73]</ref>, which reduces the length of tokens, we propose a new reverse T2T transformation to upsample tokens by expanding each token into multiple sub-tokens. Then, we upsample patch tokens progressively and fuse them with low-level tokens to obtain the final full-resolution saliency map. In addition, we also use a cross modality transformer to deeply explore the interaction between multi-modal information for RGB-D SOD. Finally, our VST outperforms existing state-of-the-art SOD methods with a comparable number of parameters and computational costs, on both RGB and RGB-D data.</p><p>Our main contributions can be summarized as follows:</p><p>• For the first time, we design a novel unified model based on the pure transformer architecture for both RGB and RGB-D SOD, from a new perspective of sequence-to-sequence modeling. • We design a multi-task transformer decoder to jointly conduct saliency and boundary detection by introducing task-related tokens and patch-task-attention. • We propose a new token upsampling method for transformer-based framework. • Our proposed VST model achieves state-of-the-art results on both RGB and RGB-D SOD benchmark datasets, which demonstrates its effectiveness and the potential of transformer-based models for SOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Learning Based SOD</head><p>CNN-based approaches have become a mainstream trend in both RGB and RGB-D SOD and achieved promising performance. Most methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b16">17]</ref> leveraged a multi-level feature fusion strategy by using UNet <ref type="bibr" target="#b56">[57]</ref> or HED-style <ref type="bibr" target="#b69">[70]</ref> network structures. Some works introduced the attention mechanism to learn more discriminative features, including spatial and channel attention <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref> or pixel-wise contextual attention <ref type="bibr" target="#b36">[37]</ref>. Other works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6]</ref> tried to design recurrent networks to refine the saliency map step-by-step. In addition, some works introduced multi-task learning, e.g., fixation prediction <ref type="bibr" target="#b66">[67]</ref>, image caption <ref type="bibr" target="#b75">[76]</ref>, and edge detection <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b24">25]</ref> to boost the SOD performance.</p><p>As for RGB-D SOD, many methods have designed various models to fuse RGB and depth features and obtained significant results. Some models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref> adopted simple feature fusion methods, i.e., concatenation, summation, or multiplication. Others <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b30">31]</ref> leveraged the depth cues to generate spatial attention or channel attention to enhance the RGB features. Pang et al. <ref type="bibr" target="#b47">[48]</ref> introduced dynamic convolution and Li et al. <ref type="bibr" target="#b28">[29]</ref> yielded affine transformation parameters to enhance the RGB features. Piao et al. <ref type="bibr" target="#b52">[53]</ref> proposed a depth distiller to transfer the depth knowledge to the RGB stream. Luo et al. <ref type="bibr" target="#b42">[43]</ref> introduced graph neural networks to model relations between multimodal data. In addition, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7]</ref> leveraged the crossattention mechanism to propagate long-range cross-modal interactions between RGB and depth cues.</p><p>Different from previous CNN-based methods, we are the first to rethink SOD from a sequence-to-sequence perspective and propose a unified model based on pure transformer for both RGB and RGB-D SOD. In our model, we follow <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b24">25]</ref> to leverage boundary detection to boost the SOD performance. However, different from these CNNbased models, we design a novel token-based multitask decoder to achieve this goal under the transformer framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformers in Computer Vision</head><p>Vaswani et al. <ref type="bibr" target="#b60">[61]</ref> first proposed a transformer encoderdecoder architecture for machine translation, where multihead self-attention and point-wise feed-forward layers are stacked multiple times. Recently, more and more works have introduced the Transformer model to various computer vision tasks and achieved excellent results. A bunch of works combined CNNs and transformers into hybrid architectures for object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b88">89]</ref>, panoptic segmentation <ref type="bibr" target="#b61">[62]</ref>, lane shape prediction <ref type="bibr" target="#b39">[40]</ref>, multiple object tracking <ref type="bibr" target="#b58">[59]</ref>, and so on. Typically, they first use CNNs to extract image features and then leverage the Transformer to incorporate long-range dependencies. Other works design pure transformer models to process images from the sequence-to-sequence perspective. ViT <ref type="bibr" target="#b12">[13]</ref> divided each image into a sequence of flattened 2D patches and then adopted the Transformer for image classification. Touvron et al. <ref type="bibr" target="#b59">[60]</ref> introduced a teacher-student strategy to improve the data-efficiency of ViT. In <ref type="bibr" target="#b72">[73]</ref>, Yuan et al. proposed the T2T module to model local structures and presented an efficient deep-narrow ViT structure. In this work, we adopt T2T-ViT as the backbone and propose a novel multitask decoder and a reverse T2T token upsampling method. It is noteworthy that our usage of task-related tokens is different from previous models. In <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b10">11]</ref>, the class token is directly used for image classification via adopting a multilayer perceptron on the token embedding. However, we can not obtain dense prediction results directly from a single task token. Thus, we propose to perform patch-task-attention between patch tokens and the task tokens to predict saliency and boundary maps. We believe our strategy will also inspire future transformer models for other dense prediction tasks.</p><p>Another related work to ours is <ref type="bibr" target="#b84">[85]</ref>, which introduces transformer into the semantic segmentation task. The authors adopted a vision transformer as a backbone and then reshaped the token sequences to 2D image features. Then, they predicted full-resolution segmentation maps using convolution and bilinear upsampling. Their model still falls into the hybrid architecture category. In contrast, our model is a pure transformer architecture and does not rely on any convolution operation and bilinear upsampling. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overall architecture of our proposed VST model. The main components include a transformer encoder based on T2T-ViT, a transformer converter to convert patch tokens from the encoder space to the decoder space, and a multi-task transformer decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Visual Saliency Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformer Encoder</head><p>Similar to other CNN-based SOD methods, which often utilize pretrained image classification models such as VGG <ref type="bibr" target="#b57">[58]</ref> and ResNet <ref type="bibr" target="#b22">[23]</ref> as the backbone of their encoders to extract image features, we adopt the pretrained T2T-ViT <ref type="bibr" target="#b72">[73]</ref> model as our backbone, which is detailed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Tokens to Token</head><p>Given a sequence of patch tokens T with length l from the previous layer, T2T-ViT iteratively applies the T2T module, which is composed of a re-structurization step and a soft split step, to model the local structure information in T and obtain a new sequence of tokens.</p><p>Re-structurization. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), the tokens T is first transformed using a transformer layer to obtain new tokens T ∈ R l×c :</p><formula xml:id="formula_0">T = MLP(MSA(T )),<label>(1)</label></formula><p>where MSA and MLP denote the multi-head self-attention and multilayer perceptron in the original Transformer <ref type="bibr" target="#b60">[61]</ref>, respectively. Note that layer normalization <ref type="bibr" target="#b0">[1]</ref> is applied before each block. Then, T is reshaped to a 2D image I ∈ R h×w×c , where l = h × w, to recover spatial structures, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a).</p><p>Soft split. After the re-structurization step, I is first split into k×k patches with s overlapping. p zero-padding is also utilized to pad image boundaries. Then, the image patches are unfolded to a sequence of tokens T o ∈ R lo×ck 2 , where the sequence length l o is computed as: Different from the original ViT <ref type="bibr" target="#b12">[13]</ref>, the overlapped patch splitting adopted in T2T-ViT introduces local correspondence within neighbouring patches, thus bringing spatial prior knowledge. The T2T transformation can be conducted iteratively for multiple times. In each time, the re-structurization step first transforms previous token embeddings to new embeddings and also integrates long-range dependencies within all tokens. Then, the soft split operation aggregates the tokens in each k × k neighbour into a new token, which is ready to use for the next layer. Furthermore, when setting s &lt; k − 1, the length of tokens can be reduced progressively.</p><formula xml:id="formula_1">l o = h o × w o = h + 2p − k k − s + 1 × w + 2p − k k − s + 1 .</formula><p>We follow <ref type="bibr" target="#b72">[73]</ref> to first soft split the input image into patches and then adopt the T2T module twice. Among the three soft split steps, the patch sizes are set to k = <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3]</ref>, the overlappings are set to s = [3, 1, 1], and the padding sizes are set to p = [2, 1, 1]. As such, we can obtain multilevel tokens T 1 ∈ R l1×c , T 2 ∈ R l2×c , and T 3 ∈ R l3×c . Given the width and height of the input image as H and W , respectively, then</p><formula xml:id="formula_2">l 1 = H 4 × W 4 , l 2 = H 8 × W 8 , and l 3 = H 16 × W 16 .</formula><p>We follow <ref type="bibr" target="#b72">[73]</ref> to set c = 64 and use a linear projection layer on T 3 to transform its embedding dimension from c to d = 384.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Encoder with T2T-ViT Backbone</head><p>The final token sequence T 3 is added with the sinusoidal position embedding <ref type="bibr" target="#b60">[61]</ref> to encode 2D position information. Then, L E transformer layers are used to model longrange dependencies among T 3 to extract powerful patch to-</p><formula xml:id="formula_3">ken embeddings T E ∈ R l3×d .</formula><p>For RGB SOD, we adopt a single transformer encoder to obtain RGB encoder patch tokens T E r ∈ R l3×d from each input RGB image. For RGB-D SOD, we follow two-stream architectures to further use another transformer encoder to extract the depth encoder patch tokens T E d from the input depth map in a similar way, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer Convertor</head><p>We insert a convertor module between the transformer encoder and decoder to convert the encoder patch tokens T E * from the encoder space to the decoder space, thus obtaining the converted patch tokens T C ∈ R l3×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">RGB-D Convertor</head><p>We fuse T E r and T E d in the RGB-D converter to integrate the complementary information between the RGB and depth data. To this end, we design a Cross Modality Transformer (CMT), which consists of L C alternating cross-modalityattention layers and self-attention layers.</p><p>Cross-modality-attention. Under the pure transformer architecture, we modify the standard self-attention layer to propagate long-range cross-modal dependencies between the image and depth data, thus obtaining the crossmodality-attention, which is detailed as follows.</p><p>First, similar with the self-attention in <ref type="bibr" target="#b60">[61]</ref>, T E r is embedded to queries Q r ∈ R l3×d , keys K r ∈ R l3×d , and values V r ∈ R l3×d through three linear projections. Similarly, we can obtain the depth queries Q d , keys K d , and values V d from T E d . Next, we compute the "Scaled Dot-Product Attention" <ref type="bibr" target="#b60">[61]</ref> between the queries from one modality with the keys from the other modality. Then, the output is computed as a weighted sum of the values. The processes are formulated as:</p><formula xml:id="formula_4">Attention(Q r , K d , V d ) = softmax(Q r K d / √ d)V d , Attention(Q d , K r , V r ) = softmax(Q d K r / √ d)V r .<label>(3)</label></formula><p>We follow the standard Transformer architecture in <ref type="bibr" target="#b60">[61]</ref> and adopt the multi-head attention mechanism in the crossmodality-attention. The same positionwise feed-forward network, residual connections, and layer normalization <ref type="bibr" target="#b0">[1]</ref> are also used, forming our CMT layer.</p><p>After each adoption of the proposed CMT layer, we use one standard transformer layer on each RGB and depth patch token sequence, further enhancing their token embeddings. After alternately using CMT and transformer for L C times, we fuse the obtained RGB tokens and depth tokens by concatenation and then project them to the final converted tokens T C , as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">RGB Convertor</head><p>To align with our RGB-D SOD model, for RGB SOD, we simply use L C standard transformer layers on T E r to obtain the converted patch token sequence T C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-task Transformer Decoder</head><p>Our decoder aims to decode the patch tokens T C to saliency maps. Hence, we propose a novel token upsampling method with multi-level token fusion and a tokenbased multi-task decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Token Upsampling and Multi-level Token Fusion</head><p>We argue that directly predicting saliency maps from T C can not obtain high-quality results since the length of T C is relatively small, i.e., l 3 = H 16 × W 16 , which is limited for dense prediction. Thus, we propose to upsample patch tokens first and then conduct dense prediction. Most CNNbased methods <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b18">19]</ref> adopt bilinear upsampling to recover large scale feature maps. Alternatively, we propose a new token upsampling method under the transformer framework. Inspired by the T2T module <ref type="bibr" target="#b72">[73]</ref> that aggregates neighbour tokens to reduce the length of tokens progressively, we propose a reverse T2T (RT2T) transformation to upsample tokens by expanding each token into multiple sub-tokens, as shown in <ref type="figure" target="#fig_1">Figure 2</ref></p><formula xml:id="formula_5">(b).</formula><p>Specifically, we first project the input patch tokens to reduce their embedding dimension from d = 384 to c = 64. Then, we use another linear projection to expand the embedding dimension from c to ck 2 . Next, similar to the soft split step in T2T, each token is seen as a k × k image patch and neighbouring patches have s overlapping. Then, we can fold the tokens as an image using p zero-padding. The output image size can be computed using (2) reversely, i.e., given the length of the input patch tokens as h o × w o , the spatial size of the out image is h × w. Finally, we reshape the image back to the upsampled tokens with size l o × c, where l o = h × w. By setting s &lt; k − 1, the RT2T transformation can increase the length of the tokens. Motivated by T2T-ViT, we use RT2T three times and set k = [3, 3, 7], s = [1, 1, 3], and p = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>. Thus, the length of the patch tokens can be gradually upsampled to H × W , equaling to the original size of the input image.</p><p>Furthermore, motivated by the widely proved successes of multi-level feature fusion in existing SOD methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43]</ref>, we leverage low-level tokens with larger lengths from the T2T-ViT encoder, i.e., T 1 and T 2 , to provide accurate local structural information. For both RGB and RGB-D SOD, we only use the low-level tokens from the RGB transformer encoder. Concretely, we progressively fuse T 2 and T 1 with the upsampled patch tokens via concatenation and linear projection. Then, we adopt one transformer layer to obtain the decoder tokens T D i at each level i, where i = 2, 1. The whole process is formulated as:</p><formula xml:id="formula_6">T D i = MLP(MSA(Linear([RT2T(T D i+1 ), T i ])), (4) where [, ]</formula><p>means concatenation along the token embedding dimension. "Linear" means linear projection to reduce the embedding dimension after the concatenation to c. Finally, we use another linear projection to recover the embedding dimension of T D i back to d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Token Based Multi-task Prediction</head><p>Inspired by existing pure transformer methods <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b12">13]</ref>, which add a class token on the patch token sequence for image classification, we also leverage task-related tokens to predict results. However, we can not obtain dense prediction results by directly using MLP on the task token embedding, as done in <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b12">13]</ref>. Hence, we propose to perform patch-task-attention between the patch tokens and the taskrelated token to perform SOD. In addition, motivated by the widely used boundary detection in SOD models <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b24">25]</ref>, we also adopt the multi-task learning strategy to jointly perform saliency and boundary detection, thus using the latter to help boost the performance of the former.</p><p>To this end, we design two task-related tokens, i.e., a saliency token t s ∈ R 1×d and a boundary token t b ∈ R 1×d . At each decoder level i, we add the saliency and boundary tokens t s and t b on the patch token sequence T D i , and then process them using L D transformer layers. As such, the two task tokens can learn image-dependent task-related embeddings from the interaction with the patch tokens. After this, we take the updated patch tokens as input and perform the token upsampling and multi-level fusion process in (4) to obtain upsampled patch tokens T D i−1 . Next, we reuse the updated t s and t b in the next level i − 1 to further update them and T D i−1 . We repeat this process until we reach the last decoder level with the 1 4 scale. For saliency and boundary prediction, we perform patchtask-attention between the final decoder patch tokens T D 1 and the saliency and boundary tokens t s and t b . For saliency prediction, we first embed T D 1 to queries Q D s ∈ R l1×d and embed t s to a key K s ∈ R 1×d and a value V s ∈ R 1×d . Similarly, for boundary prediction, we embed T D 1 to Q D b and embed t b to K b and V b . Then, we adopt the patch-task-attention to obtain the task-related patch tokens:</p><formula xml:id="formula_7">T D s = sigmoid(Q D s K s / √ d)V s + T D 1 , T D b = sigmoid(Q D b K b / √ d)V b + T D 1 .<label>(5)</label></formula><p>Here we use the sigmoid activation for the attention computation since in each equation we only have one key.</p><p>Since T D s and T D b are at the 1 4 scale, we adopt the third RT2T transformation to upsample them to the full resolution. Finally, we apply two linear transformations with the sigmoid activation to project them to scalars in [0, 1], and then reshape them to a 2D saliency map and a 2D boundary map, respectively. The whole process is given in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>For RGB SOD, we evaluate our VST model on six widely used benchmark datasets, including ECSSD <ref type="bibr" target="#b70">[71]</ref> (1,000 images), HKU-IS <ref type="bibr" target="#b31">[32]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For fair comparisons, we follow most previous methods to use the training set of DUTS to train our VST for RGB SOD and use 1,485 images from NJUD, 700 images from NLPR, and 800 images from DUTLF-Depth to train our VST for RGB-D SOD. We follow <ref type="bibr" target="#b80">[81]</ref> to use a sober operator to generate the boundary ground truth from GT saliency maps. For depth data preprocessing, we normalize the depth maps to [0,1] and duplicate them to three channels. Finally, we resize each image or depth map to 256 × 256 pixels and then randomly crop 224 × 224 image regions as the model input and use random flipping as data augmentation.</p><p>We use the pre-trained T2T-ViT t -14 <ref type="bibr" target="#b72">[73]</ref> model as our backbone since it has similar computational complexity as ResNet50 <ref type="bibr" target="#b22">[23]</ref> does. This model uses the efficient Performer <ref type="bibr" target="#b9">[10]</ref> and c = 64 in T2T modules, and sets L E = 14.</p><p>In our convertor and decoder, we set L C = 4 and L D = 2 according to experimental results. We set the batchsizes as 11 and 8, and the total training steps as 40,000 and 60,000, for RGB and RGB-D SOD, respectively. For both of them, Adam <ref type="bibr" target="#b26">[27]</ref> is adopted as the optimizer and the binary cross entropy loss is used for both saliency and boundary predic-tion. The initial learning rate is set to 0.0001 and decreased by a factor of 10 at the half and the three quarters of total step, respectively. Deep supervision is also used to facilitate the model training, where we use the patch-task attention to predict saliency and boundary at each decoder level. We implemented our model using Pytorch <ref type="bibr" target="#b49">[50]</ref> and trained it on a GTX 1080 Ti GPU. Our code will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Since our RGB-D VST is built by adding one more transformer encoder and additional CMT based on our RGB VST, while the other parts of the two models are the same, we conduct ablation studies based on our RGB-D VST to verify all of our proposed model components. The experimental results on four RGB-D SOD datasets, i.e., NJUD, DUTLF-Depth, STERE, and LFSD, are given in <ref type="table" target="#tab_1">Table 1</ref>. We remove the transformer convertor and the decoder from our RGB-D VST as the baseline model. To be specific, it uses the two-stream transformer encoder to extract RGB encoder patch tokens T E r and the depth encoder patch tokens T E d , and then directly concatenate them and predict the saliency map with 1/16 scale by using MLP on each patch token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of CMT.</head><p>For cross-modal information fusion, we deploy our proposed CMT right after the transformer encoder to substitute the concatenation fusion method in the baseline model, shown as "+CMT" in <ref type="table" target="#tab_1">Table 1</ref>. Compared to the baseline, CMT brings performance gain especially on the NJUD and LFSD datasets, hence demonstrating its effectiveness.</p><p>Effectiveness of RT2T. Based on "+CMT" model, we further simply use bilinear upsampling ("+CMT+Bili") to progressively upsample tokens to the full resolution and then predict the saliency map. The results show that using bilinear upsamping to increase the resolution of the saliency map can largely improve the model performance. Then, we replace bilinear upsampling with our proposed RT2T token upsampling method ("+CMT+RT2T"). We find that RT2T leads to obvious performance improvement compared with using bilinear upsampling, which verifies its effectiveness. <ref type="table" target="#tab_1">Table 1</ref>. Ablation studies of our proposed model. "Bili" denotes bilinear upsampling. "F" means multi-level token fusion. "TMD" denotes our proposed token-based multi-task decoder, while "C2D" means using conventional two-stream decoder to perform saliency and boundary detection without using task-related tokens. The best results are labeled in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>NJUD <ref type="bibr" target="#b25">[26]</ref> DUTLF-Depth <ref type="bibr" target="#b51">[52]</ref> STERE <ref type="bibr" target="#b45">[46]</ref> LFSD <ref type="bibr" target="#b32">[33]</ref>   shows that using boundary detection can bring further performance gain for SOD on three out of four datasets.</p><formula xml:id="formula_8">Sm ↑ maxF ↑ E max ξ ↑ MAE ↓ Sm ↑ maxF ↑ E max ξ ↑ MAE ↓ Sm ↑ maxF ↑ E max ξ ↑ MAE ↓ Sm ↑ maxF ↑ E max ξ ↑ MAE ↓</formula><p>To very the effectiveness of our token-based prediction scheme, we try to directly use a conventional two-stream decoder (C2D) by using the "+RT2T+F" architecture twice to predict the saliency map and boundary map via MLP, without using task-related tokens. This model is denoted as "+CMT+RT2T+F+C2D" in The results show that using our TMD can achieve better results than using C2D on three out of four datasets, and also with much less computational costs. This clearly demonstrates the superiority of our proposed token-based transformer decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Art Methods</head><p>For RGB-D SOD, we compare our VST with 14 stateof-the-art RGB-D SOD methods, i.e., A2dele <ref type="bibr" target="#b52">[53]</ref>, JL-DCF <ref type="table">Table 3</ref>. Quantitative comparison of our proposed VST with other 12 SOTA RGB SOD methods on 6 benchmark datasets. "-R" and "-R2" means the ResNet50 and Res2Net backbone, respectively. CoNet <ref type="bibr" target="#b24">[25]</ref> HDFNet <ref type="bibr" target="#b47">[48]</ref> JLDCF <ref type="bibr" target="#b18">[19]</ref> UC-Net <ref type="bibr" target="#b74">[75]</ref> Image GT VST GateNet <ref type="bibr" target="#b82">[83]</ref> CSF <ref type="bibr" target="#b19">[20]</ref> LDF <ref type="bibr" target="#b67">[68]</ref> MINet <ref type="bibr" target="#b48">[49]</ref> ITSD <ref type="bibr" target="#b85">[86]</ref> EGNet <ref type="bibr" target="#b80">[81]</ref>   <ref type="bibr" target="#b18">[19]</ref>, SSF-RGBD <ref type="bibr" target="#b77">[78]</ref>, UC-Net <ref type="bibr" target="#b74">[75]</ref>, S 2 MA <ref type="bibr" target="#b37">[38]</ref>, PGAR <ref type="bibr" target="#b5">[6]</ref>, DANet <ref type="bibr" target="#b83">[84]</ref>, cmMS <ref type="bibr" target="#b28">[29]</ref>, ATSA <ref type="bibr" target="#b76">[77]</ref>, CMW <ref type="bibr" target="#b30">[31]</ref>, Cas-Gnn <ref type="bibr" target="#b42">[43]</ref>, HDFNet <ref type="bibr" target="#b47">[48]</ref>, CoNet <ref type="bibr" target="#b24">[25]</ref>, and BBS-Net <ref type="bibr" target="#b16">[17]</ref>. For RGB SOD, we compare our VST with 12 state-of-theart RGB SOD models, including GateNet <ref type="bibr" target="#b82">[83]</ref>, CSF <ref type="bibr" target="#b19">[20]</ref>, LDF <ref type="bibr" target="#b67">[68]</ref>, MINet <ref type="bibr" target="#b48">[49]</ref>, ITSD <ref type="bibr" target="#b85">[86]</ref>, EGNet <ref type="bibr" target="#b80">[81]</ref>, TSPOANet <ref type="bibr" target="#b40">[41]</ref>, AFNet <ref type="bibr" target="#b17">[18]</ref>, PoolNet <ref type="bibr" target="#b34">[35]</ref>, CPD <ref type="bibr" target="#b68">[69]</ref>, BASNet <ref type="bibr" target="#b54">[55]</ref>, and PiCANet <ref type="bibr" target="#b36">[37]</ref>. <ref type="table" target="#tab_0">Table 2</ref> and <ref type="table">Table 3</ref> show the quantitative comparison results for RGB-D and RGB SOD, respectively. The results show that our VST outperforms all previous state-of-the-art CNN-based SOD models on both RGB and RGB-D benchmark datasets, with comparable number of parameters and relatively small MACs, hence demonstrating the great effectiveness of our VST. We also show visual comparison results among best-performed models in <ref type="figure" target="#fig_5">Figure 3</ref>. It shows our proposed VST can accurately detect salient objects in very challenging scenarios, e.g., big salient objects, cluttered backgrounds, foreground and background having similar appearances, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we are the first to rethink SOD from a sequence-to-sequence perspective and develop a novel unified model based on a pure transformer, for both RGB and RGB-D SOD. To handle the difficulty of applying transformers in dense prediction tasks, we propose a new token upsampling method under the transformer framework and fuse multi-level patch tokens. We also design a multitask decoder by introducing task-related tokens and a novel patch-task-attention mechanism to jointly perform saliency and boundary detection. Our VST model achieves state-ofthe-art performance for both RGB and RGB-D SOD with-out relying on heavy computational costs, hence showing its great effectiveness. We also set a new paradigm for the open question of how to use transformer in dense prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary materials 6.1. Ablation Study on RGB SOD Datasets</head><p>We further report the results of ablation studies on four RGB SOD datasets, i.e., DUTS, HKU-IS, PASCAL-S, and SOD, in <ref type="table">Table 4</ref> to demonstrate the effectiveness of our VST model components.</p><p>The baseline model is using transformer encoder to extract patch tokens T E r and then directly using T E r to predict the saliency map with 1/16 scale by using MLP on each patch token. Based on the baseline, we insert RGB convertor right after the transformer encoder, shown as "+RC" in <ref type="table">Table 4</ref>. Compared to the baseline, RC brings performance gains especially on the DUTS and PASCAL-S datasets, which demonstrates its effectiveness. For other components, i.e., RT2T, multi-level token fusion, and multi-task transformer decoder, we get consistent conclusions with the ablation studies on RGB-D SOD datasets as follows.</p><p>First, using bilinear upsampling ("+RC+Bili") can significantly improve the model performance while using our proposed RT2T ("+RC+RT2T") can further bring performance gains, hence demonstrating the effectiveness of our proposed RT2T. Second, based on "+RC+RT2T", multilevel token fusion ("+RC+RT2T+F") can lead to better performance on all four datasets, which verifies its effectiveness. Third, using multi-task transformer decoder ("+RC+RT2T+F+TMD") can improve the model performance on all four datasets and it is also superior to the conventional two-stream decoder ("+RC+RT2T+F+C2D").</p><p>To this end, the results of ablation studies on both RGB and RGB-D SOD datasets strongly demonstrate the effectiveness of our proposed VST components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Layer Number Study</head><p>We conduct experiments to study the optimal numbers of different transformer layers, i.e., L C in the transformer convertor and L D in the multi-task transformer decoder, jointly considering computational costs and model performance. Note that there are three decoder modules at three scales in the multi-task transformer decoder, thus we set different transformer layer numbers for them, i.e., L D 3 for 1/16 scale, L D 2 for 1/8 scale, and L D 1 for 1/4 scale. The experimental results on four RGB-D SOD datasets, i.e., NJUD, DUTLF-Depth, STERE, and LFSD, are given in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>In our initial model setting, we set L C = L D 3 = 8. Since L D 2 and L D 1 are used at relatively large scales, we initially set both of them to 4, as shown in row I in <ref type="table" target="#tab_4">Table 5</ref>. Then, we start to change the numbers of different layers.</p><p>We first reduce L D 2 and L D 1 from 4 to 2 to save computational costs. The experimental results on row II show that it can get comparable performance with less computational costs compared with row I. Hence, we set L D 2 = L D 1 = 2 and start to change L D 3 from 8 to 6, 4, 2, respectively, which are shown in row III, IV, V in <ref type="table" target="#tab_4">Table 5</ref>. We find that as L D 3 decreases, the computation costs decrease gradually while the results are generally comparable. However, the model performance on row IV is better than that on row V on DUTLF-Depth and LFSD datasets. Thus, we set L D 3 = 4 and start to change L C from 8 to 6, 4, 2, respectively, which are shown in row VI, VII, VIII. It can be seen that the performance on row VII is the best and the model has acceptable computational costs. Hence, we set L C = L D 3 = 4 and L D 2 = L D 1 = 2 as our final model setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">More Visual Comparison with State-of-the-art Methods</head><p>We give more visual comparison results with the stateof-the-art RGB and RGB-D SOD methods in <ref type="figure" target="#fig_7">Figure 4</ref> and <ref type="figure" target="#fig_8">Figure 5</ref>, respectively. It shows that our VST model can handle well in many challenging scenarios, i.e., big salient objects, cluttered backgrounds, foregrounds and backgrounds with very similar appearance, etc, while existing methods are heavily disturbed in these scenarios. <ref type="table">Table 4</ref>. Ablation studies of our proposed model on RGB SOD datasets. "RC" means RGB Convertor. "Bili" denotes bilinear upsampling and "F" means multi-level token fusion. "TMD" denotes our proposed token-based multi-task decoder, while "C2D" means using conventional two-stream decoder to perform saliency and boundary detection without using task-related tokens. The best results are labeled in blue.   CSF <ref type="bibr" target="#b19">[20]</ref> LDF <ref type="bibr" target="#b67">[68]</ref> MINet <ref type="bibr" target="#b48">[49]</ref> ITSD <ref type="bibr" target="#b85">[86]</ref> EGNet <ref type="bibr" target="#b80">[81]</ref> TSPOA <ref type="bibr" target="#b40">[41]</ref> AFNet <ref type="bibr" target="#b17">[18]</ref> PoolNet <ref type="bibr" target="#b34">[35]</ref> CPD <ref type="bibr" target="#b68">[69]</ref> BASNet <ref type="bibr" target="#b54">[55]</ref> PiCANet <ref type="bibr" target="#b36">[37]</ref>  CoNet <ref type="bibr" target="#b24">[25]</ref> HDFNet <ref type="bibr" target="#b47">[48]</ref> Cas-Gnn <ref type="bibr" target="#b42">[43]</ref> CMW <ref type="bibr" target="#b30">[31]</ref> ATST <ref type="bibr" target="#b76">[77]</ref> cmMS <ref type="bibr" target="#b28">[29]</ref> DANet <ref type="bibr" target="#b83">[84]</ref> PGAR <ref type="bibr" target="#b5">[6]</ref> s 2 MA <ref type="bibr" target="#b37">[38]</ref> UC-Net <ref type="bibr" target="#b74">[75]</ref> SSFRGBD <ref type="bibr" target="#b77">[78]</ref> JLDCF <ref type="bibr" target="#b18">[19]</ref> A2dele <ref type="bibr" target="#b52">[53]</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overall architecture of our proposed VST model for both RGB and RGB-D SOD. Dotted line represents exclusive components for RGB-D SOD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The details of (a) T2T module<ref type="bibr" target="#b72">[73]</ref> and (b) our proposed reverse T2T module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(4,447 images), PASCAL-S [34] (850 images), DUT-O [72] (5,168 images), SOD [45] (300 images), and DUTS [63] (10,553 training images and 5,019 testing images). For RGB-D SOD, we use nine widely used benchmark datasets: STERE [46] (1,000 image pairs), LFSD [33] (100 image pairs), RGBD135 [9] (135 image pairs), SSD [88] (80 image pairs), NJUD [26] (1,985 image pairs), NLPR [51] (1,000 image pairs), DUTLF-Depth [52] (1,200 image pairs), SIP [16] (929 image pairs), and ReDWeb-S [39] (3,179 image pairs). We adopt four widely used evaluation metrics to evaluate our model performance comprehensively. Specifically, Structure-measure S m [14] evaluates region-aware and object-aware structural similarity. Maximum F-measure (maxF) jointly considers precision and recall under the optimal threshold. Maximum enhanced-alignment measure E max ξ<ref type="bibr" target="#b14">[15]</ref> simultaneously considers pixel-level errors and image-level errors. Mean Absolute Error (MAE) computes pixel-wise average absolute error. To evaluate the model complexity, we also report the multiply accumulate operations (MACs) and the number of parameters (Params).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>LFSD</head><label></label><figDesc>Sm ↑ 0.825 0.853 0.851 0.856 0.829 0.808 0.841 0.845 0.845 0.776 0.838 0.846 0.848 0.835 0.882 maxF↑ 0.828 0.863 0.863 0.860 0.831 0.794 0.840 0.858 0.859 0.779 0.843 0.858 0.852 0.828 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative comparison against state-of-the-art RGB-D (left) and RGB (right) SOD methods. (GT: ground truth)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.780 0.909 0.071 0.858 0.854 0.938 0.075 0.826 0.795 0.878 0.096 0.802 0.803 0.880 0.100 +RC 0.827 0.785 0.913 0.070 0.860 0.856 0.939 0.074 0.830 0.797 0.879 0.095 0.804 0.805 0.880 0.100 +RC+Bili 0.867 0.835 0.929 0.048 0.901 0.901 0.956 0.044 0.856 0.827 0.891 0.074 0.833 0.836 0.891 0.077 +RC+RT2T 0.881 0.856 0.934 0.043 0.914 0.918 0.961 0.037 0.864 0.838 0.896 0.070 0.844 0.850 0.894 0.069 +RC+RT2T+F 0.895 0.874 0.939 0.039 0.925 0.932 0.966 0.032 0.871 0.845 0.897 0.068 0.851 0.861 0.899 0.068 +RC+RT2T+F+TMD 0.896 0.877 0.939 0.037 0.928 0.937 0.968 0.030 0.873 0.850 0.900 0.067 0.854 0.866 0.902 0.065 +RC+RT2T+F+C2D 0.891 0.870 0.937 0.040 0.924 0.931 0.966 0.033 0.869 0.844 0.896 0.069 0.852 0.860 0.898 0.067</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison against state-of-the-art RGB SOD methods. (GT: ground truth)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison against state-of-the-art RGB-D methods. (GT: ground truth)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of our proposed VST with other 14 SOTA RGB-D SOD methods on 9 benchmark datasets. Red and blue denote the best and the second-best results, respectively. '-' indicates the code or result is not available.</figDesc><table><row><cell>Baseline</cell><cell></cell><cell cols="2">0.869 0.862</cell><cell>0.931</cell><cell cols="2">0.073 0.889 0.887</cell><cell>0.942</cell><cell cols="2">0.062 0.868 0.853</cell><cell>0.927</cell><cell cols="2">0.075 0.842 0.845</cell><cell>0.893</cell><cell>0.103</cell></row><row><cell>+CMT</cell><cell></cell><cell cols="2">0.873 0.867</cell><cell>0.934</cell><cell cols="2">0.072 0.889 0.890</cell><cell>0.942</cell><cell cols="2">0.063 0.869 0.854</cell><cell>0.928</cell><cell cols="2">0.075 0.849 0.855</cell><cell>0.900</cell><cell>0.100</cell></row><row><cell>+CMT+Bili</cell><cell></cell><cell cols="2">0.906 0.902</cell><cell>0.944</cell><cell cols="2">0.045 0.926 0.930</cell><cell>0.961</cell><cell cols="2">0.032 0.889 0.877</cell><cell>0.939</cell><cell cols="2">0.051 0.856 0.858</cell><cell>0.895</cell><cell>0.081</cell></row><row><cell cols="2">+CMT+RT2T</cell><cell cols="2">0.915 0.915</cell><cell>0.951</cell><cell cols="2">0.039 0.934 0.940</cell><cell>0.964</cell><cell cols="2">0.028 0.896 0.889</cell><cell>0.943</cell><cell cols="2">0.046 0.867 0.873</cell><cell>0.903</cell><cell>0.073</cell></row><row><cell cols="2">+CMT+RT2T+F</cell><cell cols="2">0.923 0.923</cell><cell>0.954</cell><cell cols="2">0.035 0.936 0.943</cell><cell>0.963</cell><cell cols="2">0.028 0.910 0.903</cell><cell>0.947</cell><cell cols="2">0.040 0.876 0.880</cell><cell>0.909</cell><cell>0.067</cell></row><row><cell cols="4">+CMT+RT2T+F+TMD 0.922 0.920</cell><cell>0.951</cell><cell cols="2">0.035 0.943 0.948</cell><cell>0.969</cell><cell cols="2">0.024 0.913 0.907</cell><cell>0.951</cell><cell cols="2">0.038 0.882 0.889</cell><cell>0.921</cell><cell>0.061</cell></row><row><cell cols="4">+CMT+RT2T+F+C2D 0.922 0.921</cell><cell>0.954</cell><cell cols="2">0.036 0.941 0.947</cell><cell>0.968</cell><cell cols="2">0.026 0.911 0.906</cell><cell>0.949</cell><cell cols="2">0.040 0.874 0.878</cell><cell>0.909</cell><cell>0.069</cell></row><row><cell>Dataset</cell><cell cols="12">Metric A2dele JL-DCF SSF-RGBD UC-Net S 2 MA PGAR DANet cmMS ATST CMW Cas-Gnn HDFNet CoNet BBS-Net VST</cell></row><row><cell></cell><cell></cell><cell>[53]</cell><cell>[19]</cell><cell>[78]</cell><cell>[75]</cell><cell>[38]</cell><cell>[6]</cell><cell>[84]</cell><cell cols="2">[29] [77] [31]</cell><cell>[43]</cell><cell>[48]</cell><cell>[25]</cell><cell>[17]</cell></row><row><cell cols="4">MACs (G) 41.86 861.45</cell><cell>46.56</cell><cell cols="6">16.16 141.19 44.65 66.25 134.77 42.17 208.03</cell><cell>-</cell><cell>91.77 20.89</cell><cell>31.2 30.99</cell></row><row><cell cols="4">Params (M) 30.34 143.52</cell><cell>32.93</cell><cell cols="6">31.26 86.65 16.2 26.68 92.02 32.17 85.65</cell><cell>-</cell><cell>44.15 43.66 49.77 83.83</cell></row><row><cell></cell><cell cols="3">Sm ↑ 0.871 0.902</cell><cell>0.899</cell><cell cols="7">0.897 0.894 0.909 0.899 0.900 0.885 0.870 0.911</cell><cell>0.908 0.896 0.921 0.922</cell></row><row><cell>NJUD</cell><cell cols="3">maxF↑ 0.874 0.904 E max ξ ↑ 0.916 0.944</cell><cell>0.896 0.935</cell><cell cols="7">0.895 0.889 0.907 0.898 0.897 0.893 0.871 0.916 0.936 0.930 0.940 0.935 0.936 0.930 0.927 0.948</cell><cell>0.911 0.893 0.919 0.920 0.944 0.937 0.949 0.951</cell></row><row><cell>[26]</cell><cell cols="3">MAE↓ 0.051 0.041</cell><cell>0.043</cell><cell cols="7">0.043 0.054 0.042 0.046 0.044 0.047 0.061 0.036</cell><cell>0.039 0.046 0.035 0.035</cell></row><row><cell></cell><cell cols="3">Sm ↑ 0.899 0.925</cell><cell>0.915</cell><cell cols="7">0.920 0.916 0.917 0.920 0.919 0.909 0.917 0.919</cell><cell>0.923 0.912 0.931 0.932</cell></row><row><cell>NLPR</cell><cell cols="3">maxF↑ 0.882 0.918 E max ξ ↑ 0.944 0.963</cell><cell>0.896 0.953</cell><cell cols="7">0.903 0.902 0.897 0.909 0.904 0.898 0.903 0.906 0.956 0.953 0.950 0.955 0.955 0.951 0.951 0.955</cell><cell>0.917 0.893 0.918 0.920 0.963 0.948 0.961 0.962</cell></row><row><cell>[51]</cell><cell cols="3">MAE↓ 0.029 0.022</cell><cell>0.027</cell><cell cols="7">0.025 0.030 0.027 0.027 0.028 0.027 0.029 0.025</cell><cell>0.023 0.027 0.023 0.024</cell></row><row><cell></cell><cell cols="3">Sm ↑ 0.885 0.906</cell><cell>0.915</cell><cell cols="7">0.871 0.904 0.899 0.899 0.912 0.916 0.797 0.920</cell><cell>0.908 0.923 0.882 0.943</cell></row><row><cell>DUTLF</cell><cell cols="3">maxF↑ 0.891 0.910</cell><cell>0.923</cell><cell cols="7">0.864 0.899 0.898 0.904 0.913 0.928 0.779 0.926</cell><cell>0.915 0.932 0.870 0.948</cell></row><row><cell>-Depth</cell><cell>E max ξ</cell><cell cols="2">↑ 0.928 0.941</cell><cell>0.950</cell><cell cols="7">0.908 0.935 0.933 0.939 0.940 0.953 0.864 0.953</cell><cell>0.945 0.959 0.912 0.969</cell></row><row><cell>[52]</cell><cell cols="3">MAE↓ 0.043 0.042</cell><cell>0.033</cell><cell cols="7">0.059 0.043 0.041 0.042 0.036 0.033 0.098 0.030</cell><cell>0.041 0.029 0.058 0.024</cell></row><row><cell></cell><cell cols="3">Sm ↑ 0.641 0.734</cell><cell>0.595</cell><cell cols="3">0.713 0.711 0.656</cell><cell>-</cell><cell cols="2">0.699 0.679 0.634</cell><cell>-</cell><cell>0.728 0.696 0.693 0.759</cell></row><row><cell>ReDWeb-S</cell><cell cols="3">maxF↑ 0.603 0.727 E max ξ ↑ 0.674 0.805</cell><cell>0.558 0.710</cell><cell cols="3">0.710 0.696 0.632 0.794 0.781 0.749</cell><cell>--</cell><cell cols="2">0.677 0.673 0.607 0.767 0.758 0.714</cell><cell>--</cell><cell>0.717 0.693 0.680 0.763 0.804 0.782 0.763 0.826</cell></row><row><cell>[39]</cell><cell cols="3">MAE↓ 0.160 0.128</cell><cell>0.189</cell><cell cols="3">0.130 0.139 0.161</cell><cell>-</cell><cell cols="2">0.143 0.155 0.195</cell><cell>-</cell><cell>0.129 0.147 0.150 0.113</cell></row><row><cell></cell><cell cols="3">Sm ↑ 0.879 0.903</cell><cell>0.837</cell><cell cols="7">0.903 0.890 0.894 0.901 0.894 0.896 0.852 0.899</cell><cell>0.900 0.905 0.908 0.913</cell></row><row><cell>STERE</cell><cell cols="3">maxF↑ 0.880 0.904 E max ξ ↑ 0.928 0.947</cell><cell>0.840 0.912</cell><cell cols="7">0.899 0.882 0.880 0.892 0.887 0.901 0.837 0.901 0.944 0.932 0.929 0.937 0.935 0.942 0.907 0.944</cell><cell>0.900 0.901 0.903 0.907 0.943 0.947 0.942 0.951</cell></row><row><cell>[46]</cell><cell cols="3">MAE↓ 0.045 0.040</cell><cell>0.065</cell><cell cols="7">0.039 0.051 0.045 0.044 0.045 0.038 0.067 0.039</cell><cell>0.042 0.037 0.041 0.038</cell></row><row><cell></cell><cell cols="3">Sm ↑ 0.803 0.860</cell><cell>0.790</cell><cell cols="7">0.865 0.868 0.832 0.864 0.857 0.850 0.798 0.872</cell><cell>0.879 0.851 0.863 0.889</cell></row><row><cell>SSD</cell><cell cols="3">maxF↑ 0.777 0.833 E max ξ ↑ 0.862 0.902</cell><cell>0.762 0.867</cell><cell cols="7">0.855 0.848 0.798 0.843 0.839 0.853 0.771 0.863 0.907 0.909 0.872 0.914 0.900 0.920 0.871 0.923</cell><cell>0.870 0.837 0.843 0.876 0.925 0.917 0.914 0.935</cell></row><row><cell>[88]</cell><cell cols="3">MAE↓ 0.070 0.053</cell><cell>0.084</cell><cell cols="7">0.049 0.053 0.068 0.050 0.053 0.052 0.085 0.047</cell><cell>0.046 0.056 0.052 0.045</cell></row><row><cell></cell><cell cols="3">Sm ↑ 0.886 0.931</cell><cell>0.904</cell><cell cols="7">0.934 0.941 0.886 0.924 0.934 0.917 0.934 0.894</cell><cell>0.926 0.914 0.934 0.943</cell></row><row><cell>RGBD135</cell><cell cols="3">maxF↑ 0.872 0.923 E max ξ ↑ 0.921 0.968</cell><cell>0.885 0.940</cell><cell cols="7">0.930 0.935 0.864 0.914 0.928 0.916 0.931 0.894 0.976 0.973 0.924 0.966 0.969 0.961 0.969 0.937</cell><cell>0.921 0.902 0.928 0.940 0.970 0.948 0.966 0.978</cell></row><row><cell>[9]</cell><cell cols="3">MAE↓ 0.029 0.021</cell><cell>0.026</cell><cell cols="7">0.019 0.021 0.032 0.023 0.018 0.022 0.022 0.028</cell><cell>0.022 0.024 0.021 0.017</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The parameters and MACs of TMD vs. C2D are 17.22 M vs. 20.35 M and 17.70 G vs. 28.27 G, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of using different numbers of transformer layers in our VST model. The final model setting is labeled in blue.</figDesc><table><row><cell>ID</cell><cell></cell><cell cols="2">Layer Num</cell><cell></cell><cell cols="2">MACs Params</cell><cell>NJUD [26]</cell><cell></cell><cell cols="2">DUTLF-Depth [52]</cell><cell>STERE [46]</cell><cell></cell><cell>LFSD [33]</cell></row><row><cell></cell><cell cols="2">L C L D 3</cell><cell>L D 2</cell><cell>L D 1</cell><cell>(G)</cell><cell>(M) Sm</cell><cell>maxF E max ξ</cell><cell>MAE Sm</cell><cell>maxF E max ξ</cell><cell>MAE Sm</cell><cell>maxF E max ξ</cell><cell>MAE Sm</cell><cell>maxF E max ξ</cell><cell>MAE</cell></row><row><cell>I</cell><cell>8</cell><cell>8</cell><cell>4</cell><cell>4</cell><cell cols="9">48.35 119.30 0.925 0.925 0.955 0.033 0.940 0.947 0.966 0.026 0.910 0.902 0.948 0.039 0.878 0.884 0.914 0.066</cell></row><row><cell>II</cell><cell>8</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell cols="9">36.78 113.39 0.923 0.922 0.955 0.035 0.943 0.947 0.968 0.025 0.911 0.904 0.948 0.039 0.874 0.878 0.908 0.069</cell></row><row><cell>III</cell><cell>8</cell><cell>6</cell><cell>2</cell><cell>2</cell><cell cols="9">36.20 110.43 0.921 0.920 0.952 0.036 0.940 0.945 0.966 0.026 0.910 0.904 0.948 0.040 0.875 0.883 0.911 0.067</cell></row><row><cell>IV</cell><cell>8</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell cols="9">35.61 107.47 0.921 0.920 0.951 0.036 0.942 0.947 0.968 0.026 0.911 0.904 0.949 0.040 0.876 0.880 0.912 0.068</cell></row><row><cell>V</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell cols="9">35.03 104.52 0.922 0.921 0.952 0.036 0.940 0.944 0.965 0.026 0.912 0.906 0.949 0.039 0.873 0.875 0.908 0.068</cell></row><row><cell>VI</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>33.30</cell><cell cols="8">95.65 0.923 0.921 0.952 0.036 0.943 0.948 0.968 0.024 0.913 0.906 0.949 0.039 0.875 0.878 0.912 0.067</cell></row><row><cell>VII</cell><cell>4</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>30.99</cell><cell cols="8">83.83 0.922 0.920 0.951 0.035 0.943 0.948 0.969 0.024 0.913 0.907 0.951 0.038 0.882 0.889 0.921 0.061</cell></row><row><cell cols="2">VIII 2</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>28.68</cell><cell cols="8">72.00 0.923 0.921 0.953 0.036 0.938 0.943 0.963 0.028 0.912 0.906 0.950 0.039 0.881 0.887 0.917 0.062</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Effectiveness of multi-level token fusion. We progressively fuse T 1 and T 2 in our decoder ("+CMT+RT2T+F") to supply low-level fine-grained information. We find that this strategy further improves the model performance. Hence, leveraging low-level tokens in transformer is as important as fusing low-level features in CNN-based models.Effectiveness of the multi-task transformer decoder. Based on "+CMT+RT2T+F", we further use our tokenbased multi-task decoder (TMD) to jointly perform saliency and boundary detection ("+CMT+RT2T+F+TMD"). It</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting local and global patch rarities for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Progressively complementarityaware fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Progressively guided alternate refinement network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dpanet: Depth potentiality-aware gated attention network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjian</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Internet Multimedia Computing and Service</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">R3net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced-alignment Measure for Binary Foreground Map Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking rgb-d salient object detection: Models, data sets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bbs-net: Rgb-d salient object detection with a bifurcated backbone strategy network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jldcf: Joint learning and densely-cooperative fusion framework for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Highly efficient salient object detection with 100k parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Qiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Stas Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayellet</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cnns-based rgb-d saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accurate rgb-d salient object detection via collaborative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic centersurround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rgb-d salient object detection with crossmodality modulation and selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icnet: Information conversion network for rgb-d based salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4873" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-modal weighting network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for realtime salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning selective self-mutual attention for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning selective mutual attention and contrast for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05537</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Endto-end lane shape prediction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Employing deep part-object relationships for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image by single stream recurrent convolution neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quntao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cascade graph neural networks for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6609" to="6617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vida</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhen</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical dynamic filtering network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale interactive network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: A benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A2dele: Adaptive and attentive depth distiller for efficient rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkun</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Basnet: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Basnet: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploiting global priors for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00759</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Salient object detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1734" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4019" to="4028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Salient object detection driven by fixation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1711" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Label decoupling framework for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visual attention detection in video sequences using spatiotemporal cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh Sadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Capsal: Leveraging captioning to boost semantics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6024" to="6033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Asymmetric two-stream architecture for accurate rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><forename type="middle">Xiao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Select, supplement and focus for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkun</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Egnet:edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Suppress and balance: A simple gated network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A single stream network for robust and real-time rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Interactive two-stream decoder for accurate and fast saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Rgb-d salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A three-pathway psychobiological framework of salient object detection using stereoscopic technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
