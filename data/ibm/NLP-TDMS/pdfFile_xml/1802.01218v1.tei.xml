<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Video Object Segmentation via Network Modulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
							<email>linjie.yang@snap.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Image and Video Processing Laboratory</orgName>
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
							<email>xiong828@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
							<email>jianchao.yang@snap.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Image and Video Processing Laboratory</orgName>
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Video Object Segmentation via Network Modulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video object segmentation targets at segmenting a specific object throughout a video sequence, given only an annotated first frame. Recent deep learning based approaches find it effective by fine-tuning a general-purpose segmentation model on the annotated frame using hundreds of iterations of gradient descent. Despite the high accuracy these methods achieve, the fine-tuning process is inefficient and fail to meet the requirements of real world applications. We propose a novel approach that uses a single forward pass to adapt the segmentation model to the appearance of a specific object. Specifically, a second meta neural network named modulator is learned to manipulate the intermediate layers of the segmentation network given limited visual and spatial information of the target object. The experiments show that our approach is 70× faster than fine-tuning approaches while achieving similar accuracy. Our model and code are released at https: //github.com/linjieyangsc/video_seg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation plays an important role in understanding visual content of an image as it assigns predefined object or scene labels to each pixel and thus translates the image into a segmentation map. When dealing with video content, a human can easily segment an object in the whole video without knowing its semantic meaning, which inspired a research topic named semi-supervised video segmentation. In a typical scenario of semi-supervised video segmentation, one is given the first frame of a video along with an annotated object mask, and the task is to accurately locate the object in all following frames <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b19">20]</ref>. The ability of performing accurate pixel-level video segmentation with minimum supervision (e.g., one annotated frame) can foster a large amount of applications, such as accurate object * This work is done when Yanran was an intern at Snap Inc. <ref type="figure">Figure 1</ref>. An overview of our approach. Our model is consisted of a modulator and a segmentation network. The modulator can adapt the segmentation model instantly to segment an arbitrary object through a video sequence. tracking for video understanding, interactive video editing, augmented reality, and video-based advertisement. When the supervision is limited to only one annotated frame, researchers refer to this scenario as one-shot learning. In the recent years, we have witnessed a rising amount of interests in developing one-shot learning techniques for video segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b3">4]</ref>. Most of these work share a similar two-stage paradigm: first, train a general-purpose Fully Convolutional Network (FCN) <ref type="bibr" target="#b30">[31]</ref> to segment the foreground object; Second, fine-tune this network based on the first frame of the video for several hundred forwardbackward iterations to adapt the model to the specific video sequence. Despite the high accuracies achieved by these approaches, the fine-tuning process is arguably time consuming, which makes it prohibited for real-time applications. Some of these approaches <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b22">[23]</ref> also utilize optical flow information, which is computationally heavy for state-ofthe-art algorithms <ref type="bibr" target="#b28">[29]</ref>  <ref type="bibr" target="#b14">[15]</ref>.</p><p>In order to alleviate the computational cost of semisupervised segmentation, we propose a novel approach to adapt the generic segmentation network to the appearance of a specific object instance in one single feed-forward pass. We propose to employ another meta neural network called modulator to learn to adjust the intermediate layers of the generic segmentation network given an arbitrary target object instance. <ref type="figure">Fig. 1</ref> shows an illustration of our approach. By extracting information from the image of the annotated object and the spatial prior of the object, the modulator produces a list of parameters, which are injected into the segmentation model for layer-wise feature manipulation. Without one-shot fine-tuning, our model is able to change the behavior of the segmentation network with minimum extracted information from the target object. We name this process network modulation.</p><p>Our proposed model is efficient, requiring only one forward pass from the modulator to produce all parameters needed for the segmentation model to adapt to the specific object instance. Network modulation guided by the spatial prior facilitates the model to track the object even with the presence of multiple similar instances. The whole pipeline is differentiable and can be learned end-to-end using the standard stochastic gradient descent. The experiments show that our approach outperforms previous approaches without one-shot fine-tuning by a large margin, and achieves comparable performance with these approaches after one-shot fine-tuning with a 70× speed up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semi-supervised video segmentation. Semi-supervised video object segmentation aims at tracking an object mask given from the first annotated frame throughout the rest of video. Many approaches have been proposed in the literature, including those propagating superpixels <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b34">[35]</ref>, patches <ref type="bibr" target="#b8">[9]</ref>, object proposals <ref type="bibr" target="#b24">[25]</ref>, or in bilateral space <ref type="bibr" target="#b21">[22]</ref>, and graphical model based optimization is usually performed to consider multiple frames simultaneously. With the success of FCN on static image segmentation <ref type="bibr" target="#b11">[12]</ref>, deep learning based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4]</ref> have been recently proposed for video segmentation and promising results have been achieved. To model the temporal motion information, some works heavily rely on optical flow <ref type="bibr" target="#b33">[34]</ref>  <ref type="bibr" target="#b3">[4]</ref>, and use CNNs to learn mask refinement of an object from current frame to the next one <ref type="bibr" target="#b22">[23]</ref>, or combine the training of CNN with bilateral filtering between adjacent frames <ref type="bibr" target="#b17">[18]</ref>. Chen et al. <ref type="bibr" target="#b3">[4]</ref> use a CNN to jointly estimate the optical flow and provide the learned motion representation to generate motion consistent segmentation across time. Different from these approaches, Caelles et al. <ref type="bibr" target="#b1">[2]</ref> combine offline and online training process on static images without using temporal information. While it saves the computation of optical flow and/or conditional random fields (CRF) <ref type="bibr" target="#b18">[19]</ref> involved in some previous methods, online fine-tuning still requires many iterations of optimization, which poses a challenge for real-world applications that need rapid inference.</p><p>Meta-learning for low-shot learning. Current success of deep learning relies on the ability of learning from largescale labeled datasets through gradient descent optimization. However, if we want our model to learn many tasks adapted to many environments, it is not affordable to learn each task for each setting from scratch. Instead, we want our deep learning system to be able to learn new tasks very fast and from very limited quantities of data. In the extreme of "one-shot learning", the algorithm needs to learn the new task with a single observation. One potential strategy for learning a versatile model is the notion of meta-learning, or learning to learn, which can date back to the late 80s. Recently, meta-learning has become a hot research topic with publications on neural network optimization <ref type="bibr" target="#b2">[3]</ref>, finding good network architectures, fast reinforcement learning, and few-shot image recognition <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref>. Ravi and Larochelle <ref type="bibr" target="#b27">[28]</ref> proposed a LSTM meta-learner to learn the update rules for few shot learning. The meta optimization over a large number of tasks in <ref type="bibr" target="#b9">[10]</ref> targets at learning a model that can quickly adapt to the new task with limited number of updates. Hariharan and Girschick <ref type="bibr" target="#b12">[13]</ref> trained a learner that generated new samples and used new samples for training new tasks. Our approach shares the similarity with meta-learning that it learns to update the segmentation model rapidly with another meta learner, i.e. the modulator.</p><p>Network manipulation Several previous work try to incorporate modules to manipulate the behavior of a deep neural network, either to manipulate spatial arrangement of data <ref type="bibr" target="#b15">[16]</ref> or filter connections <ref type="bibr" target="#b4">[5]</ref>. Our method is also heavily motivated by conditional batch normalization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>, where the behavior of the deep model is manipulated by batch normalization parameters conditioned on a guidance input, e.g. a style image for image stylization or a language sentence for visual question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Object Segmentation with Network Modulation</head><p>In our proposed framework, we utilize modulators to instantly adapt the segmentation network to a specific object, rather than performing hundreds of iterations of gradient descent. We can achieve similar accuracy by adjusting a limited number of parameters in the segmentation network, compared with the updating the whole network in one-shot learning approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref>. There are two important cues for video object segmentation: visual appearance and continuous motion in space. To use information from both vi- <ref type="figure">Figure 2</ref>. An illustration of our model with three components: a segmentation network, a visual modulator, and a spatial modulator. The two modulators produce a set of parameters that manipulates the intermediate feature maps of the segmentation network and adapt it to segment the specific object.</p><p>sual and spatial domains, we incorporate two network modulators, namely visual modulator and spatial modulator, to learn to adjust intermediate layers in the main segmentation network, based on the annotated first frame and spatial location of the object, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional batch normalization</head><p>Our approach is inspired by recent works using Conditional Batch Normalization (CBN) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>, where the scale and bias parameters of each batch-normalization layer are produced by a second controller network. These parameters are used to control the behavior of the main network for tasks such as image stylization and question answering. Mathematically, each CBN layer can be formulated as follows:</p><formula xml:id="formula_0">y c = γ c x c + β c ,<label>(1)</label></formula><p>where x c and y c are the input and output feature maps in the c th channel, and γ c and β c are the scale and bias parameters produced by the controller network, respectively. The mean and variance parameters are omitted for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual and spatial modulation</head><p>The CBN layer is a special case of the more general scale-and-shift operation on feature maps. Following each convolution layer, we define a new modulation layer with parameters generated by both visual and spatial modulators that are jointly trained. We design the two modulators such that the visual modulator produces channel-wise scale parameters to adjust the weights of different channels in the feature maps, while the spatial modulator generates element-wise bias parameters to inject spatial prior to the modulated features. Specifically, our modulation layer can be formulated as follows:</p><formula xml:id="formula_1">y c = γ c x c + β c ,<label>(2)</label></formula><p>where γ c and β c are modulation parameters from the visual and spatial modulators, respectively. γ c is a scalar for channel-wise weighting, while β c is a two-dimensional matrix to apply point-wise bias values. <ref type="figure">Fig. 2</ref> shows an illustration of the proposed approach, which consists of three networks: a fully-convolutional main segmentation network, a visual modulator network, and a spatial modulator network. The visual modulator network is a CNN that takes the annotated visual object image as input and produces a vector of scale parameters for all modulation layers, while the spatial modulator network is a very efficient network that produces bias parameters based on the spatial prior input. We will discuss the two modulators in more detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Visual modulator</head><p>The visual modulator is used to adapt the segmentation network to focus on a specific object instance, which is the annotated object in the first frame. The annotated object is referred to as visual guide hereafter for convenience. The visual modulator extracts semantic information such as category, color, shape, and texture, from the visual guide and generates corresponding channel-wise weights so as to retarget the segmentation network to segment the object. We use VGG16 <ref type="bibr" target="#b32">[33]</ref> neural network as the model for the visual modulator. We modify its last layer trained for Ima-geNet classification to match the number of parameters in the modulation layers for the segmentation network.</p><p>The visual modulator implicitly learns an embedding of different types of objects. It should produce similar parameters to adjust the segmentation network for similar objects while different parameters for different objects. This is indeed true as we show in Sec. 4.2 that the embedding of the modulator outputs correlates with object appearance very well. One big advantage of using such a visual modulator is that we can potentially transfer the knowledge learned with a large number of object classes, e.g., ImageNet, in order to learn a good embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Spatial modulator</head><p>Our spatial modulator takes a prior location of the object in the image as input. Since objects move continuously in a video, we set the prior to be the predicted location of the object mask in the previous frame. Specifically, we encode the location information as a heatmap with a two-dimensional Gaussian distribution on the image plane. The center and standard deviations of the Gaussian distribution are computed from the predicted mask of the previous frame. This heatmap is referred as spatial guide hereafter for convenience. The spatial modulator downsamples the spatial guide into different scales, to match the resolution of different feature maps in the segmentation network, and then applies a scale-and-shift operation on each downsampled heatmap to generate the bias parameters of the corresponding modulation layer. Mathematically,</p><formula xml:id="formula_2">β c =γ c m +β c<label>(3)</label></formula><p>where m is a down-sampled Gaussian heat map for the corresponding modulation layer,γ c andβ c are the scale-andshift parameters for the c-th channel, respectively. This is implemented with a computationally efficient 1 × 1 convolution. In the bottom of <ref type="figure">Fig. 2</ref>, we illustrate the structure of the spatial modulator. Our method shares some similarities with the previous work MaskTrack <ref type="bibr" target="#b22">[23]</ref> in utilizing information from the previous mask. Comparing with their approach that uses the exact foreground mask of the previous frame, we only use a very coarse location prior. It may seem that our method throws away more information from the previous frame. However, we argue that the rough position and size in the previous frame possess enough information to infer the object mask with the RGB image, and it prevents the model from relying too much on the mask and as a result the error propagation, which can be catastrophic when the object has large movements in the video. As a drawback of such overutilization of the mask, MaskTrack has to apply plenty of well-engineered data augmentation to prevent over-fitting, while we only apply simple shift and scaling as augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation details</head><p>Our FCN structure follows the one used by <ref type="bibr" target="#b1">[2]</ref>, which is a VGG16 <ref type="bibr" target="#b32">[33]</ref> model with a hyper-column structure <ref type="bibr" target="#b11">[12]</ref>. Intuitively, we should add modulation layers after each convolution layer in the FCN. However, we found that adding modulation layers in-between the early convolution layers actually makes the model perform worse. One possible reason is that early layers extract low-level features that are very sensitive to the scale-and-shift operations introduced by the modulator. In our implementation, we add modulation operations to all convolution layers in VGG16 except the first four layers, which results in nine modulation layers.</p><p>Similar to MaskTrack <ref type="bibr" target="#b22">[23]</ref>, we also utilize static images for training our model. Ideally, the visual modulator should learn a mapping from any object to modulation weights of different layers in a FCN, which requires the model to see all possible different objects. However, most video semantic segmentation datasets only contain a very limited number of categories. We tackle this challenge by using the largest public semantic segmentation dataset MS-COCO <ref type="bibr" target="#b20">[21]</ref>, which has 80 object categories. We select objects that are larger than 3% of the image size for training, resulting in a total number of 217, 516 objects. For preprocessing the input for the visual modulator, we first crop the object using the annotated mask, then set the background pixels to mean image values, and then resize the cropped image to a constant resolution of 224 × 224. The object is also augmented with up to 10% random scaling and 10 • random rotation. For preprocessing the spatial guide as input to the spatial modulator, we first compute the mean and standard deviation of the mask, and then augment the mask with up to 20% random shift and 40% random scaling. For the whole image fed into the FCN, we use a random size from 320, 400, and 480 with a square shape.</p><p>The visual modulator and segmentation network are both initialized with VGG16 model pretrained on the Im-ageNet <ref type="bibr" target="#b6">[7]</ref> classification task. The modulation parameters {γ c } are initialized to ones by setting the weights and biases of the last fully-connected layer of the visual modulator to zeros and ones, respectively. The weights of spatial modulator are initialized randomly. We used the same balanced cross-entropy loss as in <ref type="bibr" target="#b1">[2]</ref>. A mini-batch size of 8 is used. We use Adam optimizer with default momentum 0.9 and 0.999 for β 1 and β 2 , respectively. The model is first trained for 10 epochs with learning rate 10 −5 and then trained for another 5 epochs with learning rate 10 −6 .</p><p>Further, in order to model appearance variations of moving objects in videos, the model can be finetuned on video segmentation dataset such as DAVIS 2017 <ref type="bibr" target="#b26">[27]</ref>. To be more robust to appearance variations, we randomly pick a fore-ground object from the whole video sequence as the visual guide for each frame. The spatial guide is obtained from the ground truth mask of the object in the previous frame. The same data augmentations are applied as training on MS-COCO. The model is finetuned for 20 epochs with learning rate 10 −6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will introduce three parts of experiment: the comparison of our approach with previous methods, the visualization of the modulation parameters, and ablation study. Our model is trained on MS-COCO <ref type="bibr" target="#b20">[21]</ref> 2017 dataset, and is tested on several popular video segmentation datasets, including DAVIS <ref type="bibr" target="#b23">[24]</ref>  <ref type="bibr" target="#b26">[27]</ref> and YoutubeObjects <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semi-supervised Video Segmentation</head><p>In this section, we compare with traditional approaches including OFL <ref type="bibr" target="#b34">[35]</ref>, BVS <ref type="bibr" target="#b21">[22]</ref>, and deep learning-based approaches including PLM <ref type="bibr" target="#b31">[32]</ref>, MaskTrack <ref type="bibr" target="#b22">[23]</ref>, OS-VOS <ref type="bibr" target="#b1">[2]</ref>, VPN <ref type="bibr" target="#b17">[18]</ref>, SFL <ref type="bibr" target="#b3">[4]</ref>, and ConvGRU <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">DAVIS 2016 &amp; YoutubeObjects</head><p>First, we compare our approach with previous approaches on DAVIS 2016 and YoutubeObjects. Some approaches (MaskTrack <ref type="bibr" target="#b22">[23]</ref>, SFL <ref type="bibr" target="#b3">[4]</ref> and OSVOS <ref type="bibr" target="#b1">[2]</ref>) reported results both with and without model fine-tuning on the target sequences. We include both of them and denote the variants without fine-tuning as MaskTrack-B, SFL-B, and OSVOS-B, respectively. Our model has two variants,with the first only trained on static images (Stage 1) and the second finetuned on video data (Stage 1&amp;2). Since there are several popular add-ons for this line of research, such as optical flow and CRF <ref type="bibr" target="#b18">[19]</ref>, which both have a lot of variants and make a fair comparison hard, we only include the performances without optical flow and CRF if possible, and mark those with add-ons in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, by comparing our method with OFL <ref type="bibr" target="#b34">[35]</ref>, an expensive graphical model based approach, we achieve better accuracy on both DAVIS 2016 and YoutubeObjects. Comparing with deep learning approaches without model fine-tuning, and therefore, similar speed as ours, our method achieves the best accuracy on both DAVIS 2016 and YoutubeObjects. Comparing with the four approaches using model fine-tuning on target videos (PLM, MaskTrack, SFL, and OSVOS), our approach achieves better performance than PLM and MaskTrack, and is on-par with SFL. OS-VOS achieves higher accuracy but it also utilizes a boundary snapping approach which contributes 2.4% in mean IU. Our method is 70× faster than MaskTrack and OS-VOS, 50× faster than SFL. We measure the running time of MaskTrack-B, OSVOS-B, and our method on a NVIDIA Quadro M6000 GPU using Tensorflow <ref type="bibr" target="#b0">[1]</ref>. Speed of other methods are derived from the corresponding papers <ref type="bibr" target="#b0">1</ref> .</p><p>In our method, the adaptation of the segmentation model by the modulators is done with one forward pass for visual modulator, so it is much more efficient than the approaches with model fine-tuning on target videos. The visual modulator only needs to be computed once for the whole video, while the spatial modulator needs to be computed for every frame but the overhead is negligible, i.e., the average speed of our model on a video sequence is about the same as FCN itself. Our method is the second fastest of all compared methods, with only MaskTrack-B and OSVOS-B achieving similar speed but with much worse accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">DAVIS 2017</head><p>To further investigate the capability of our model, we conduct more experiments on DAVIS 2017 <ref type="bibr" target="#b26">[27]</ref>, which is the largest video segmentation dataset to date. DAVIS 2017 is more challenging than DAVIS 2016 and YoutubeObjects in that it has multiple objects for each video sequences and some of the objects are very similar. We compare our method with two most related approaches, MaskTrack <ref type="bibr" target="#b22">[23]</ref> and OSVOS <ref type="bibr" target="#b1">[2]</ref>. For fair comparison, we only use their single network and adds-on free versions. We directly use open source code of OSVOS and adapt MaskTrack model to Tensorflow <ref type="bibr" target="#b0">[1]</ref>. For each video sequence, OSVOS and MaskTrack are finetuned with 1000 iterations. To show that network modulation is capable of adapting different model structures to specific object instances, we also experiment with modified OSVOS and MaskTrack models by adding a visual modulator to each of them, which are named OSVOS-M and MaskTrack-M respectively. For these two models, we only update the weights of the visual modulators and keep the weights of the segmentation model fixed in training. <ref type="table" target="#tab_1">Table 2</ref> shows the results of different approaches on DAVIS 2017. We utilize the official evaluation metrics of DAVIS dataset: mean, recall, and decay of region similarity J and contour accuracy F, respectively. Note J mean is equivalent to mean IU we used above. Again, our model outperforms OSVOS-B and MaskTrack-B with a large margin, while obtaining comparable performance with the two methods with model fine-tuning. OSVOS-M and MaskTrack-M are both better than their baseline implementations with a 18% and 9.3% gain in J mean, respectively. Since the weights of the segmentation model are fixed, the accuracy gain comes solely from the modulator, which proves that the visual modulator is capable of improving different model structures by manipulating the scales of the intermediate feature maps. Noticeably, our  method obtains much lower decay rate for both region similarity and contour accuracy compared to OSVOS and Mask-Track. The accuracy changes of the different methods over time are illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>. In the beginning of the video, our method lags behind OSVOS and MaskTrack. However, when it proceeds to around 40% of the video, our method is on par with OSVOS and outperforms MaskTrack towards the end of the video. With one-shot fine-tuning, OSVOS and MaskTrack fit to the first frame very well. They are able to obtain high accuracy in the beginning of the video since these frames are all similar to the first one. But as time goes on and the object turns into different poses and appearances, it gets harder for the fine-tuned model to generalize to new object appearances. Our model is more robust to the appearance changes since it learns a feature embedding (see Section 4.2) for the annotated object which is more tolerant to pose and appearance changes compared to one-shot fine-tuning.</p><p>Some qualitative results of our methods compared with the two previous approaches are shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. Compared with MaskTrack, our method generally obtains more accurate boundaries, partially due to that the coarse spatial prior forces the model to explore more cues on the image rather than the mask in the previous frame. Compared with OSVOS, our method shows better results when there are multiple similar objects in the image, thanks to the tracking capability provided by the spatial modulator. On the other hand, our method is also shown to work well on unseen object categories in training data. In <ref type="figure" target="#fig_0">Fig. 3</ref>, the camel and the pigs are unseen object categories in MS-COCO dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Visualization of the modulation parameters</head><p>Our model implicitly learns an embedding with the modulation parameters from the visual modulator for the annotated objects. Intuitively, similar objects should have similar modulation parameters, while different objects should have dramatically different modulation parameters. To visual-  ize this embedding, we extract modulation parameters from 100 object instances in 10 object classes in MS-COCO, and visualize the parameters in a two-dimensional embedding space using multi-dimensional scaling in <ref type="figure" target="#fig_2">Fig. 5</ref>. We can see that objects in the same category are mostly clustered to- gether, and similar categories are closer to each other than dissimilar categories. For example, cats and dogs, cars and buses are mixed up due to their similar appearance, while bicycles and dogs, buses and horses are far from each other due to the big visual difference. Mammal classes (cats, dogs, cows, horses, human) are generally clustered together, and man-made objects (cars, buses, bicycles, motorcycles, trucks) are clustered together.</p><p>We also investigate the magnitude of the modulation parameters in different layers. The modulation parameters {γ c } changes according to the visual guide. Therefore, we compute the standard deviations of modulation parameters {γ c } in each modulation layer for images in MS-COCO validation set and illustrate them in <ref type="figure" target="#fig_3">Fig. 6</ref>. An interesting observation is that towards deeper level of the network, the variations of modulation parameters get larger. This shows that the manipulation of feature maps is more dramatic in the last few layers than in early layers of the network. The last few layers of a deep neural network usually learn highlevel semantic meanings <ref type="bibr" target="#b36">[37]</ref>, which could be used to adjust the segmentation model to a specific object more effectively.</p><p>We also look into the spatial modulator by extracting the scale parameters {γ c } in each layer of the spatial modulator and visualize them in <ref type="figure" target="#fig_4">Fig. 7</ref>. The magnitudes of {γ c } are the relative scales of the spatial guide added to the feature maps in the FCN. The scale of {γ c } is proportional to the impact of spatial prior on the intermediate feature maps. Interestingly, we observe sparsity in the values of {γ c }. Except the last convolution layer conv5 3, around 60% of the parameters have zero values, which means only 40% of the feature maps are affected by the spatial prior in these layers. In the layer conv5 3, around 70% of the feature maps interact with the spatial guide and most of them are added with a similar scale (note the peak around 0.4) of the spatial guide. This shows that the spatial prior is fused into the feature maps gradually, rather than being effective at the beginning of the network. After all feature extractions are done, the spatial modulator makes a large adjustment to the feature maps, which provides a strong prior of the location of the target object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We study the impact of different ingredients in our method. We conduct experiments on DAVIS 2017 and measure the performance using mean IU. For variants of model structures, we experiment with only using spatial or visual modulator. For data augmentation methods, we experiment with no random crop augmentation for the FCN input, and no affine transformation for the visual guide and the spatial guide. We experiment with CRF as a post-processing step. To investigate the effect of one-shot fine-tuning on our model, we also experiment with standard one-shot finetuning using a small number of iterations. Results are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>By adding a CRF post-processing, our method achieves mIU (mean IU) of 54.4. By one-shot fine-tuning with only 100 iterations for each sequence, our method achieves mIU of 60.8, which is 5.7 better than OSVOS with 1000 iterations. With fine-tuning, our method is still relatively efficient with average running time around 1 s/frame. Without visual modulator, our model deteriorates to 33.0, while without spatial modulator, our model obtains mIU of 40.1, which shows that the visual guide is more important than the spatial guide. For data augmentation, without random crop, the accuracy drops by 1.9. Without affine data augmentation on the visual guide, the accuracy further decreases by 1.1. Without augmentation on the spatial guide, our model only obtains mIU of 35.6, which is a dramatic drop from 49.5. The results indicates that the spatial guide augmentation is the most significant on the performance. Without perturbation, the model might rely on the location of the spatial prior too much that it cannot deal with moving objects in real video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we propose a novel framework to process one-shot video segmentation efficiently. To alleviate the slow speed of one-shot fine-tuning developed by previous FCN-based methods, we propose to use a network modulation approach mimicking the fine-tuning process with one forward pass of the modulator network. We show in experiments that by injecting a limited number of parameters computed by the modulators, the segmentation model can be repurposed to segment an arbitrary object. The proposed network modulation method is a general learning method for few-shot learning problems, which could be applied to other tasks such as visual tracking and image stylization. Our approach falls into the general category of meta-learning, and it would also be interesting to investigate other metalearning approaches for video segmentation. Another piece of future work would be to learn a recurrent representation of the modulation parameters to manipulate the FCN based on temporal information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Some qualitative results of our approach compared with two recent state-of-the-art approaches on DAVIS 2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The J mean performance of different methods over time on DAVIS 2017. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of learned modulation parameters for 100 objects from 10 categories: bicycle, motorcycle, car, bus, truck, dog, cat, horse, cow, person. Zoom in to see details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Histograms of standard deviations of γc from the visual modulator in different modulation layers. The annotated names are the corresponding convolution layers in VGG16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Histograms of magnitude ofγc from the spatial modulator in different modulation layers. The annotated names are the corresponding convolution layers in VGG16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of our approach with recent approaches on DAVIS 2016 and YoutubeObjects. Performance measured in mean IU.</figDesc><table><row><cell>Method</cell><cell cols="4">DAVIS 16 YoutubeObjs with FT OptFlow CRF Speed (s)</cell></row><row><cell>OFL [35]</cell><cell>68.0</cell><cell>67.5</cell><cell>-</cell><cell>42.2</cell></row><row><cell>BVS [22]</cell><cell>60.0</cell><cell>58.4</cell><cell>-</cell><cell>0.37</cell></row><row><cell>ConvGRU[34]</cell><cell>70.1</cell><cell>-</cell><cell></cell><cell>20</cell></row><row><cell>VPN[18]</cell><cell>70.2</cell><cell>-</cell><cell></cell><cell>0.63</cell></row><row><cell>MaskTrack-B [23]</cell><cell>63.2</cell><cell>66.5</cell><cell></cell><cell>0.24</cell></row><row><cell>SFL-B [4]</cell><cell>67.4</cell><cell>-</cell><cell></cell><cell>0.30</cell></row><row><cell>OSVOS-B [2]</cell><cell>52.5</cell><cell>44.7</cell><cell></cell><cell>0.14</cell></row><row><cell>Ours (Stage 1)</cell><cell>72.2</cell><cell>66.4</cell><cell></cell><cell>0.14</cell></row><row><cell>Ours (Stage 1&amp;2)</cell><cell>74.0</cell><cell>69.0</cell><cell></cell><cell>0.14</cell></row><row><cell>PLM [32]</cell><cell>70.0</cell><cell>-</cell><cell></cell><cell>0.50</cell></row><row><cell>MaskTrack [23]</cell><cell>69.8</cell><cell>71.7</cell><cell></cell><cell>12</cell></row><row><cell>SFL [4]</cell><cell>74.8</cell><cell>-</cell><cell></cell><cell>7.9</cell></row><row><cell>OSVOS [2]</cell><cell>79.8</cell><cell>74.1</cell><cell></cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of our approach and two state-of-the-art algorithm on DAVIS 2017 validation set.Methodwith FT J mean↑ J recall↑ J decay↓ F mean↑ F recall↑ F decay↓</figDesc><table><row><cell>OSVOS-B [2]</cell><cell>18.5</cell><cell>15.9</cell><cell>-0.8</cell><cell>30.0</cell><cell>20.0</cell><cell>0.1</cell></row><row><cell>MaskTrack-B [23]</cell><cell>35.3</cell><cell>37.8</cell><cell>39.3</cell><cell>36.4</cell><cell>36.0</cell><cell>42.0</cell></row><row><cell>OSVOS-M</cell><cell>36.4</cell><cell>34.8</cell><cell>14.8</cell><cell>39.5</cell><cell>35.3</cell><cell>9.1</cell></row><row><cell>MaskTrack-M</cell><cell>44.6</cell><cell>48.7</cell><cell>27.1</cell><cell>47.6</cell><cell>49.3</cell><cell>27.9</cell></row><row><cell>OSVOS [2]</cell><cell>55.1</cell><cell>60.2</cell><cell>28.2</cell><cell>62.1</cell><cell>71.3</cell><cell>29.3</cell></row><row><cell>MaskTrack [23]</cell><cell>51.2</cell><cell>59.7</cell><cell>28.3</cell><cell>57.3</cell><cell>65.5</cell><cell>29.1</cell></row><row><cell>Ours</cell><cell>52.5</cell><cell>60.9</cell><cell>21.5</cell><cell>57.1</cell><cell>66.1</cell><cell>24.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of our method on DAVIS 2017.</figDesc><table><row><cell></cell><cell>Variants</cell><cell>mIU ∆ mIU</cell></row><row><cell>Add-on</cell><cell>Ours + Online finetuning Ours + CRF</cell><cell>60.8 +8.3 54.4 +1.9</cell></row><row><cell></cell><cell>Ours</cell><cell>52.5</cell></row><row><cell>Model</cell><cell>no visual modulator no spatial modulator</cell><cell>33.0 -19.5 40.1 -12.4</cell></row><row><cell></cell><cell>-random crop</cell><cell>50.6 -1.9</cell></row><row><cell>Data</cell><cell cols="2">-visual guide augmentation 49.5 -1.1</cell></row><row><cell></cell><cell cols="2">-spatial guide augmentation 35.6 -13.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Speed of ConvGRU is estimated with the expensive optical flow they use, speed of PLM is derived through communication with the authors.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensor-Flow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 5</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to learn without gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1707.00683</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jumpcut:non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">exploring the structure of a real-time, arbitrary neural artistic stylization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization. CoRR, abs/1703.06868</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning visual reasoning without strong priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1707.03017</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
