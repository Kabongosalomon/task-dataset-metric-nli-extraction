<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adding Attentiveness to the Neurons in Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
							<email>zpengfei@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Reserach Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Reserach Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
							<email>zhanninggao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
							<email>nnzheng@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adding Attentiveness to the Neurons in Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Element-wise-Attention Gate (EleAttG)</term>
					<term>recurrent neural networks</term>
					<term>action recognition</term>
					<term>skeleton</term>
					<term>RGB video</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) are capable of modeling the temporal dynamics of complex sequential information. However, the structures of existing RNN neurons mainly focus on controlling the contributions of current and historical information but do not explore the different importance levels of different elements in an input vector of a time slot. We propose adding a simple yet effective Element-wise-Attention Gate (EleAttG) to an RNN block (e.g., all RNN neurons in a network layer) that empowers the RNN neurons to have the attentiveness capability. For an RNN block, an EleAttG is added to adaptively modulate the input by assigning different levels of importance, i.e., attention, to each element/dimension of the input. We refer to an RNN block equipped with an EleAttG as an EleAtt-RNN block. Specifically, the modulation of the input is content adaptive and is performed at fine granularity, being element-wise rather than input-wise. The proposed EleAttG, as an additional fundamental unit, is general and can be applied to any RNN structures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to the action recognition tasks on both 3D human skeleton data and RGB videos. Experiments show that adding attentiveness through EleAttGs to RNN blocks significantly boosts the power of RNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, recurrent neural networks <ref type="bibr" target="#b0">[1]</ref>, such as standard RNN (sRNN), its variant Long Short-Term Memory (LSTM) <ref type="bibr" target="#b1">[2]</ref>, and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b2">[3]</ref>, have been adopted to address many challenging problems with sequential time-series data, such as action recognition <ref type="bibr" target="#b3">[4]</ref>, machine translation <ref type="bibr" target="#b4">[5]</ref>, and image caption <ref type="bibr" target="#b5">[6]</ref>. They are powerful in exploring temporal dynamics and learning appropriate feature representations.  <ref type="figure">Fig. 1</ref>: Illustration of Element-wise-Attention Gate (EleAttG) (marked in red) for (a) a generic RNN block, where the RNN structure could be the standard RNN, LSTM, or GRU and (b) a GRU block which consists of a group of (e.g., N ) GRU neurons. In the diagram, each line carries a vector. The brown circles denote element-wise operation, e.g., element-wise vector product or vector addition. The yellow boxes denote the units of the original GRU with the output dimension of N . The red box denotes the EleAttG with an output dimension of D, which is the same as the dimension of the input x t .</p><p>The structure of recurrent neural networks facilitates the processing of sequential data. RNN neurons perform the same task at each step, with the output being dependent on the previous output, i.e., some historical information is memorized. Standard RNNs have difficulties in learning long-range dependencies due to the vanishing gradient problem <ref type="bibr" target="#b6">[7]</ref>. The LSTM <ref type="bibr" target="#b1">[2]</ref> or GRU <ref type="bibr" target="#b2">[3]</ref> architectures combat vanishing gradients through a gating mechanism. Gates provide a way to optionally let information through or stop softly, which balances the contributions of the information of the current time slot and historical information. There are some variants of RNNs with slightly different designs <ref type="bibr" target="#b6">[7]</ref>. Note a gate applies a single scaling factor to control the flow of the embedded information (as a whole) of the input rather than imposing controls on each element of the input. They are not designed to explore the potential different characteristics of the input elements.</p><p>Attention mechanisms which selectively focus on different parts of the data have been demonstrated to be effective for many tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. These inspire us to develop an Element-wise-Attention Gate (EleAttG) to augment the capability of RNN neurons. More specifically, for an RNN block, an EleAttG is designed to output an attention vector, with the same dimension as the input, which is then used to modulate the input elements. Note that similar to <ref type="bibr" target="#b13">[14]</ref>, we use an RNN block to represent an ensemble of N RNN neurons, which for example could be all the RNN neurons in an RNN layer. <ref type="figure">Fig. 1 (a)</ref> illustrates the EleAttG within a generic RNN block. <ref type="figure">Fig. 1 (b)</ref> shows a specific case when the RNN structure of GRU is used. The input x t is first modulated by the response of the EleAttG to output x t before other operations are applied to the RNN block. We refer to an RNN block equipped with an EleAttG as EleAtt-RNN block. Depending on the underlying RNN structure used (e.g., standard RNN, LSTM, GRU), the newly developed EleAtt-RNN will also be denoted as EleAtt-sRNN, EleAtt-LSTM, or EleAtt-GRU. An RNN layer with such EleAttG can replace the original RNN layer and multiple EleAtt-RNN layers can be stacked.</p><p>We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to action recognition. Specifically, for 3D skeleton-based human action recognition, we build our systems by stacking several EleAtt-RNN layers, using standard RNN, LSTM and GRU, respectively. EleAtt-RNNs consistently outperform the original RNNs for all the three types of RNNs. Our scheme based on EleAtt-GRU achieves state-of-the-art performance on three challenging datasets, i.e., the NTU <ref type="bibr" target="#b15">[15]</ref>, N-UCLA <ref type="bibr" target="#b16">[16]</ref>, and SYSU <ref type="bibr" target="#b17">[17]</ref> datasets. For RGB-based action recognition, we design our system by applying an EleAtt-GRU network to the sequence of frame-level CNN features. Experiments on both the JHMDB <ref type="bibr" target="#b18">[18]</ref> and NTU <ref type="bibr" target="#b15">[15]</ref> datasets show that adding EleAttGs brings significant gain.</p><p>The proposed EleAttG has the following merits. First, EleAttG is capable of adaptively modulating the input at a fine granularity, paying different levels of attention to different elements of the input, resulting in faster convergence in training and higher performance. Second, the design is very simple. For an RNN layer, only one line of code needs to be added in implementation. Third, the EleAttG is general and can be added to any underlying RNN structure, e.g., standard RNN, LSTM, GRU, and to any layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recurrent Neural Networks</head><p>Recurrent neural networks have many different structures. In 1997, to address the vanishing gradient problem of standard RNN, Hochreiter et al. proposed LSTM, which introduces a memory cell that allows "constant error carrousels" and multiplicative gate units that learn to open and close access to the constant error flow <ref type="bibr" target="#b1">[2]</ref>. Gers et al. made improvement by adding the "forget gate" that enables an LSTM cell to learn to reset itself (historical information) at appropriate times to prohibit the growth of the state indefinitely <ref type="bibr" target="#b19">[19]</ref>. A variant of LSTM is the peephole LSTM, which allows the gates to access the cell <ref type="bibr" target="#b20">[20]</ref>. GRU, which was proposed in 2014, is a simpler variant of LSTM. A GRU has a reset gate and an update gate which control the memory and the new input information. Between the LSTMs and GRUs, there is no clear winner <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b6">7]</ref>. For LSTM, a differential gating scheme is proposed in <ref type="bibr" target="#b22">[22]</ref> which leverages the derivative of the cell state to gate the information flow. Its effectiveness is demonstrated on action recognition.</p><p>In this work, we address the capability of RNNs from a new perspective. We propose a simple yet effective Element-wise-Attention Gate which adaptively modulates the input elements to explore their different importances for an RNN block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Mechanisms</head><p>Attention mechanisms which selectively focus on different parts of the data have been proven effective for many tasks such as machine translation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, image caption <ref type="bibr" target="#b9">[10]</ref>, object detection <ref type="bibr" target="#b10">[11]</ref>, and action recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Luong et al. examine some simple attention mechanisms for neural machine translation. At each time step, the model infers the attention weights and uses them to average the embedding vectors of the source words <ref type="bibr" target="#b7">[8]</ref>. For image caption, Xu et al. split an image into L parts with each part described by a feature vector. To allow the decoder which is built by LSTM blocks to selectively focus on certain parts of the image, the weighted average of all the feature vectors using learned attention weights is fed to the LSTM network at every time step <ref type="bibr" target="#b9">[10]</ref>. A similar idea is used for RGB-based action recognition in <ref type="bibr" target="#b11">[12]</ref>. The above attention models focus on how to average a set of feature vectors with suitable weights to generate a pooled vector of the same dimension as the input of RNN. They do not consider the fine-grained adjustment based on different levels of importance across the input dimensions. In addition, they address attention at the network level, but not RNN block level.</p><p>For skeleton-based action recognition, a global context-aware attention is proposed to allocate different levels of importance to different joints of different input frames <ref type="bibr" target="#b23">[23]</ref>. Since the global information of a sequence is required to learn the attention, the system suffers from time delay. Song et al. propose a spatiotemporal attention model without requiring global information <ref type="bibr" target="#b24">[24]</ref>. Before the main recognition network, a spatial attention subnetwork is added which modulates the skeleton input to selectively focus on discriminative joints at each time slot. However, their design is not general and has not been extended to higher RNN layers. In contrast, our proposed enhanced RNN, with EleAttG embedded as a fundamental unit of RNN block, is general, simple yet effective, which can be applied to any RNN block/layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Action Recognition</head><p>For action recognition, many studies focus on recognition from RGB videos <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref>. In recent years, 3D skeleton based human action recognition has been extensively studied and has been attracting increasing attention, thanks to its high level representation <ref type="bibr" target="#b30">[30]</ref>. Many traditional approaches focus on how to design efficient features to solve the problems of small inter-class variation, large view variations, and the modeling of complicated spatial and temporal evolution <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref>.</p><p>For 3D skeleton based human action recognition, RNN-based approaches have been attractive due to their powerful spatio-temporal dynamic modeling ability. Du et al. propose a hierarchical RNN model with the hierarchical body partitions as input to different RNN layers <ref type="bibr" target="#b3">[4]</ref>. To exploit the co-occurrence of discriminative joints of skeletons, Zhu et al. propose a deep regularized LSTM networks with group sparse regularization <ref type="bibr" target="#b35">[35]</ref>. Shahroudy et al. propose a partaware LSTM network by separating the original LSTM cell into five sub-cells corresponding to five major groups of human body <ref type="bibr" target="#b15">[15]</ref>. Liu et al. propose a spatio-temporal LSTM structure to explore the contextual dependency of joints in spatio-temporal domains <ref type="bibr" target="#b36">[36]</ref>. Li et al. propose an RNN tree network with a hierarchical structure which classifies the action classes that are easier to distinguish at the lower layers and the action classes that are harder to distinguish at higher layers <ref type="bibr" target="#b37">[37]</ref>. To address the large view variation of the captured data, Zhang et al. propose a view adaptive subnetwork which automatically selects the best observation viewpoints within an end-to-end network for recognition <ref type="bibr" target="#b38">[38]</ref>.</p><p>For RGB-based action recognition, to exploit the spatial correlations, convolutional neural networks are usually used to learn the features <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b27">27]</ref>. Some approaches explore the temporal dynamics of the sequential frames by simply averaging/multiplying the scores/features of the frames for fusion <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b39">39]</ref>. Some other approaches leverage RNNs to model temporal correlations, with frame-wise CNN features as input at every time slot <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b27">27]</ref>.</p><p>Our proposed EleAttG is a fundamental unit that aims to enhance the capability of an RNN block. We will demonstrate its effectiveness in both 3D skeleton based action recognition and RGB-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Standard RNN, LSTM, and GRU</head><p>Recurrent neural networks are capable of modeling temporal dynamics of a time sequence. They have a "memory" which captures historical information accumulated from previous time steps. To better understand the proposed EleAttG and its generalization capability, we briefly review the popular RNN structures, i.e., standard RNN, Long Short Term Memory (LSTM) <ref type="bibr" target="#b1">[2]</ref>, and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b2">[3]</ref>.</p><p>For a standard RNN layer, the output response h t at time t is calculated based on the input x t to this layer and the output h t−1 from the previous time slot</p><formula xml:id="formula_0">h t = tanh (W xh x t + W hh h t−1 + b h ) ,<label>(1)</label></formula><p>where W αβ denotes the matrix of weights between α and β, b h is the bias vector. The standard RNN suffers from the gradient vanishing problem due to insufficient, decaying error back flow <ref type="bibr" target="#b1">[2]</ref>. LSTM allevates this problem by enforcing constant error flow through "constant error carrousels" within the cell unit c t . The input gate i t , forget gate f t and output gate o t learn to open and close access to the constant error flow. For an LSTM layer, the recursive computations of activations of the units are</p><formula xml:id="formula_1">i t = σ (W xi x t + W hi h t−1 + b i ) , f t = σ (W xf x t + W hf h t−1 + b f ) , c t = f t c t−1 + i t tanh(W xc x t +W hc h t−1 +b c ) , o t = σ (W xo x t + W ho h t−1 + b o ) , h t = o t tanh (c t ) ,<label>(2)</label></formula><p>where denotes an element-wise product. Note that i t is a vector denoting the responses of a set of input gates of all the LSTM neurons in the layer.</p><p>GRU is an architecture that is similar to but much simpler than that of LSTM. A GRU has two gates, reset gate r t and update gate z t . When the response of the reset gate is close to 0, the hidden state h t is forced to ignore the previous hidden state and reset with the current input only. The update gate controls how much information from the previous hidden state will be carried over to the current hidden state h t . The hidden state acts in a way similar to the memory cell in LSTM. For a GRU layer, the recursive computations of activations of the units are</p><formula xml:id="formula_2">r t = σ (W xr x t + W hr h t−1 + b r ) , z t = σ (W xz x t + W hz h t−1 + b z ) , h t = tanh (W xh x t + W hh (r t h t−1 ) + b h ) , h t = z t h t−1 + (1 − z t ) h t .<label>(3)</label></formula><p>For all the above designs, we note that the gates can control the information flow. However, the controlling of the flow takes the input x t as a whole without adaptively treating different elements of the input differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Element-wise-Attention Gate for an RNN Block</head><p>For an RNN block, we propose an Element-wise-Attention Gate (EleAttG) to enable the RNN neurons to have the attentiveness capability. The response of an EleAttG is a vector a t with the same dimension as the input x t of the RNNs, which is calculated as</p><formula xml:id="formula_3">a t = φ (W xa x t + W ha h t−1 + b a ) ,<label>(4)</label></formula><p>where φ denotes the activation function of Sigmoid, i.e., φ(s) = 1/(1 + e −s ). W αβ denotes the matrix of weights between α and β, and b a denotes the bias vector. The current input x t and the hidden states h t−1 are used to determine the levels of importance of each element of the input x t . The attention response modulates the input to have an updated input x t as</p><formula xml:id="formula_4">x t = a t x t .<label>(5)</label></formula><p>The recursive computations of activations of the other units in the RNN block are then based on the updated input x t , instead of the original input x t , as illustrated in <ref type="figure">Fig. 1</ref>. For a standard RNN block with EleAttG (denoted as EleAtt-sRNN), the output responses h t at time t are calculated as</p><formula xml:id="formula_5">h t = tanh (W xh x t + W hh h t−1 + b h ) .<label>(6)</label></formula><p>Similarly, for an EleAtt-GRU block, the recursive computations of activations of the units are</p><formula xml:id="formula_6">r t = σ (W xr x t + W hr h t−1 + b r ) , z t = σ (W xz x t + W hz h t−1 + b z ) , h t = tanh (W xh x t + W hh (r t h t−1 ) + b h ) , h t = z t h t−1 + (1 − z t ) h t .<label>(7)</label></formula><p>The computations for an EleAtt-LSTM block can be obtained similarly. Most attention designs use Softmax as the activation function such that the sum of the attention values is 1 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">24]</ref>. In our design, we relax this sum-to-1 constraint by using the Sigmoid activation function, with response values ranging from 0 to 1. If the sum-to-1 constraint is not relaxed, the attention responses of the k th element will be affected by the changes of other elements' response values even when the levels of importance of this element are the same over consecutive time slots.</p><p>Note that in our design, an EleAttG is shared by all neurons in an RNN block/layer (see <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref> for the standard RNN block, (5) and <ref type="formula" target="#formula_6">(7)</ref> for the GRU block). Theoretically, each RNN neuron (instead of block) can have its own attention gate at the cost of increased computation complexity and a larger number of parameters. We focus on the shared design in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We perform comprehensive studies to evaluate the effectiveness of our proposed EleAtt-RNN with EleAttG by applying it to action recognition from 3D skeleton data and RGB video, respectively.</p><p>To demonstrate the generalization capability of EleAttG, we add EleAttG to the standard RNN, LSTM, and GRU structures, respectively.</p><p>For 3D skeleton based action recognition, we use three challenging datasets, i.e., the NTU RGB+D dataset (NTU) <ref type="bibr" target="#b15">[15]</ref>, the Northwestern-UCLA dataset (N-UCLA) <ref type="bibr" target="#b16">[16]</ref>, and the SYSU Human-Object Interaction dataset (SYSU) <ref type="bibr" target="#b17">[17]</ref>. The NTU is currently the largest dataset with diverse subjects, various viewpoints and small inter-class differences. Therefore, in-depth analyses are performed on the NTU dataset. For RGB-based action recognition, we take the CNN features extracted from existing, pre-trained models without finetuning on our datasets as the input to the RNN based recognition networks and evaluate the effectiveness of EleAttG on the NTU and the JHMDB datasets <ref type="bibr" target="#b18">[18]</ref>. We conduct most of our experiments based on GRU here, as it has simpler structure than LSTM and better performance than standard RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>NTU RGB+D Dataset (NTU) <ref type="bibr" target="#b15">[15]</ref>. NTU is currently the largest RGB+D+Skeleton dataset for action recognition, including 56880 videos of in total more than 4 million frames. There are 60 action classes performed by different subjects. Each subject has 25 body joints and each joint has 3D coordinates. Three cameras placed in different positions are used to capture the data at the same time. We follow the standard protocols proposed in <ref type="bibr" target="#b15">[15]</ref> including the Cross Subject (CS) and Cross View (CV) settings. For the CS setting, 40 subjects are equally split into training and testing groups. For the CV setting, the samples of cameras 2 and 3 are used for training while those of camera 1 are for testing.</p><p>Northwestern-UCLA dataset (N-UCLA) <ref type="bibr" target="#b16">[16]</ref>. N-UCLA is a small RGB+D+Ske dataset including 1494 sequences which records 10 actions performed by 10 subjects. 20 joints with 3D coordinates are provided in this dataset. Following <ref type="bibr" target="#b16">[16]</ref>, we use samples from the first two cameras as training data, and the samples from the third camera as testing data.</p><p>SYSU Human-Object Interaction dataset (SYSU) <ref type="bibr" target="#b17">[17]</ref>. SYSU is a small RGB+D+Skeleton dataset, including 480 sequences performed by 40 different subjects. It contains 12 actions. A subject has 20 joints with 3D coordinates. We follow the standard protocols proposed in <ref type="bibr" target="#b17">[17]</ref> for evaluation. They include two settings. For the Cross Subject (CS) setting, half of the subjects are used for training and the others for testing. For the Same Subject (SS) setting, half of the sequences of each subject are used for training and others for testing. The average performance of 30-fold cross validation is reported.</p><p>JHMDB dataset (JHMDB) <ref type="bibr" target="#b18">[18]</ref>. JHMDB is an RGB-based dataset which has 928 RGB videos with each video containing about 15-40 frames. It contains 21 actions performed by different actors. This dataset is challenging where the videos are collected on the Internet which also includes outdoor activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We perform our experiments on the deep learning platform of Keras <ref type="bibr" target="#b40">[40]</ref> with Theano <ref type="bibr" target="#b41">[41]</ref> as the backend. For the RNN networks, Dropout <ref type="bibr" target="#b42">[42]</ref> with the probability of 0.5 is used to alleviate overfitting. Gradient clipping similar to <ref type="bibr" target="#b43">[43]</ref> is used by constraining the maximum amplitude of the gradient to 1. Adam <ref type="bibr" target="#b44">[44]</ref> is used to train the networks from end-to-end. The initial learning rate is set to 0.005 for 3D skeleton-based action recognition and 0.001 for RGB-based action recognition. During training, the learning rate will be reduced by a factor of 10 when the training accuracy does not increase. We use cross-entropy as the loss function.</p><p>For 3D skeleton-based action recognition, similar to the classification network design in <ref type="bibr" target="#b38">[38]</ref>, we build our systems by stacking three RNN layers with EleAttGs and one fully connected (FC) layer for classification. We use 100 RNN neurons in each layer. Considering the large difference on the sizes of the datasets, we set the batch size for the NTU, N-UCLA, and SYSU datasets to 256, 32, and 32, respectively. We use the sequence-level pre-processing method in <ref type="bibr" target="#b38">[38]</ref> by setting the body center in the first frame as the coordinate origin to make the system invariant to the initial position of human body. To improve the robustness to view variations at the sequence level, we can perform data augmentation by randomly rotating the skeleton around the X, Y and Z axes by various degrees ranging from -35 to 35 during training. For the N-UCLA and SYSU datasets, we use the RNN models pre-trained on the NTU dataset to initialize the baseline schemes and the proposed schemes.</p><p>For RGB-based action recognition, we feed an RNN network with the features to further explore temporal dynamics. Since our purpose is to evaluate whether the proposed EleAttG can generally improve recognition accuracy, we extract CNN features using some available pre-trained models without finetuning for the specific dataset or task. For the JHMDB dataset, we use the TSN model from <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b45">45]</ref> which was trained on the HMDB dataset <ref type="bibr" target="#b46">[46]</ref> to extract a 1024 dimensional feature for each frame. For the NTU dataset which has more videos, we take the ResNet50 model <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref> which has been pre-trained on ImageNet as our feature extractor (2048 dimensional feature for each frame) considering the ResNet50 model is much faster than the TSN model. The implementation details of the RNN networks are similar to that discussed above. For the NTU dataset, we stack three EleAtt-GRU layers, with each layer consisting of 512 GRU neurons. For the JHMDB dataset, we use only one GRU layer (512 GRU neurons) with EleAttG to avoid overfitting, considering that the number of video samples is much smaller than that of the NTU dataset. The batch size is set to 256 for the NTU dataset and 32 for the JHMDB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effectiveness of Element-wise-Attention-Gates</head><p>Effectiveness on GRU network. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the effectiveness of EleAttG on the GRU network. Our final scheme with three EleAtt-GRU layers ("3-EleAtt-GRU") outperforms the baseline scheme "3-GRU(Baseline)" significantly, by 4.6% and 5.6% for the CS and CV settings, respectively. The performance increases as more GRU layers are replaced by the EleAtt-GRU layers. Generalization to other input signals. The proposed RNN block with EleAttG is generic and can be applied to different types of source data. To demonstrate this, we use CNN features extracted from RGB frames as the input of the RNNs  for RGB based action recognition. <ref type="table" target="#tab_0">Table 1</ref> shows the performance comparisons on the NTU and JHMDB dataset respectively. The implementation details have been described in Section 5.2. We can see that the "EleAtt-GRU" outperform the "Baseline-GRU" by about 2-4% on the NTU dataset, and 2% on the JHMDB dataset. Note that the performance is not optimized since we have not used the fine-tuned CNN model on this dataset for this task. Generalization on various RNN structures. The proposed EleAttG is generic and can be applied to various RNN structures. We evaluate the effects of EleAttGs on three representative RNN structures, i.e., the standard RNN (sRNN), LSTM, GRU respectively and show the results in <ref type="table" target="#tab_1">Table 2</ref>. Compared with LSTM and GRU, the standard RNN neurons do not have the gating designs which control the contributions of the current input to the network. The EleAttG can element-wise control the contribution of the current input, which remedies the lack of gate designs to some extent. The gate designs in LSTM and GRU can only control the information flow input-wise. In contrast, the proposed EleAttGs are capable of controlling the input element-wise, adding the attentiveness capability to RNNs. We can see that the adding of EleAttGs enhances performance significantly. Note that for sRNN, we build both the Baseline(1-sRNN) and our scheme using one sRNN layer rather than three as those for LSTM and GRU, in considering that the three layered sRNN baseline converges to a poorer performance, i.e., 33.6% and 42.8% for the CS and CV settings, which may be caused by the gradient vanishing of sRNN. Comparisons with state-of-the-arts on skeleton based action recognition. <ref type="table" target="#tab_2">Table 3</ref>, 4 and 5 show the performance comparisons with state-of-the-art approaches for the NTU, N-UCLA and SYSU datasets, respectively. "Baseline-GRU" denotes our baseline scheme which is built by stacking three GRU layers while "EleAtt-GRU" denotes our proposed scheme which replaces the GRU layers by the proposed GRU layers with EleAttGs. Implementation details can be found in Section 5.2. "EleAtt-GRU(aug.)" denotes that data argumentation by rotating skeleton sequences is performed during training. We achieve the best performances in comparison with other state-of-the-art approaches on all the three datasets. Our scheme "EleAtt-GRU" achieves significant gains over the baseline scheme "Baseline-GRU", of 4.6-5.6%, 4.7%, and 2.4-2.8% on the NTU, N-UCLA, and SYSU datasets, respectively. Visualization of the responses of EleAttG. To better understand the learned element-wise attention, we observe the responses of the EleAttG in the first GRU layer for the skeleton based action recognition. In the first layer, the input (with dimension of 3 × J) at a time slot corresponds to the J joints with each joint represented by the X, Y , and Z coordinate values. The physical meaning of the attention responses is clear. However, in a higher layer, the EleAttG modulates the input features on each element which is more difficult to interpret and visualize. Thus, we perform visualization based on the attention responses of the first GRU layer in <ref type="figure" target="#fig_2">Fig. 3</ref> for the actions of kicking and touching the neck. Actually, the absolute response values cannot represent the relative importances across dimensions very well. The statistical energies of the different elements of the original input are different. For example, the foot joint which is in general far away from the body center has a higher average energy than that of the body center joint. We can imagine that there is a static modulation a i on the i th element of the input, which can be calculated by the average energy before </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method accuracy</head><p>HOJ3D <ref type="bibr" target="#b32">[32]</ref> 54.5 AE <ref type="bibr" target="#b51">[51]</ref> 76.0 VA-LSTM <ref type="bibr" target="#b38">[38]</ref> 70.7 HBRNN-L <ref type="bibr" target="#b3">[4]</ref> 78.5</p><p>Baseline-GRU 84.3 EleAtt-GRU 89.0 EleAtt-GRU(aug.) 90.7  <ref type="bibr" target="#b17">[17]</ref> 75.5 76.9 ST-LSTM <ref type="bibr" target="#b36">[36]</ref> 76.5 -VA-LSTM <ref type="bibr" target="#b38">[38]</ref> 76.9 77.5</p><p>Baseline-GRU 82.  and after the modulation. For the i th element of an sample j with attention value a i,j , we use the relative response value a i,j = a i,j /a i for visualization to better reflect the importances among joints. Note that the sum of the relative responses for the X, Y , and Z of a joint is utilized for visualization. For the action of touching neck which is highly concerned with the joints on the arms and heads, the relative attention on those joints are larger. For kicking, the relative attention on the legs is large. These are consistent with a human's perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussions</head><p>Convergence of Learning. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the loss curves for the training set and validation set during the training process for the proposed EleAtt-GRU and the baseline Baseline-GRU, respectively. By adding the EleAttGs, the convergence becomes faster and the final loss is much lower. EleAtt-GRU is consistently better than the baseline. The modulation of input can control the information flow of each input element adaptively and make the subsequent learning within the neurons much easier.</p><p>Relaxing the sum-to-1 constraint on EleAttG responses.Unlike other works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b23">23]</ref>, we do not use Softmax, which enforces the sum of attention responses to be 1, as the activation function of EleAttG. Instead, we use the Sigmoid activation function to avoid introducing mutual influence of elements. We show the experimental comparisons between the cases with the sum-to-1 constraint (w/constraint) by using Softmax, and our case without such constraint (wo/constraint) by using Sigmoid in <ref type="table" target="#tab_6">Table 6</ref>. "EleAttG-n th " denotes that the n th GRU layer uses the GRU with EleAttG while the other layers still use the original GRU. "Baseline" denotes the baseline scheme with three GRU layers. We can see wo/constraint always performs better than that with constraint w/constraint. Adding EleAttG with constraint on the second or the third layer even decreases the accuracy by about 2.4-3.2% in comparison with the baselines. Number of parameters versus performance. For an RNN block, the adding of an EleAttG increases the number of parameters. Taking a GRU block of N neurons with the input dimension of D as an example, the numbers of parameters for the original GRU block and the proposed EleAttG-GRU block are 3N (D + N + 1), and 3N (D + N + 1) + D(D + N + 1), respectively. We calculate the computational complexity by counting the number of floating-point operations (FLOPs) including all multiplication and addition operations. At a time slot, adding attention to the layer as in 4 and 5 takes D(D + N + 1) multiplication operations and D(D + N ) addition operations. Then the complexity increases from N (6D+6N +5) to N (6D+6N +5)+D(2D+2N +1), which is approximately proportional to the number of parameters. <ref type="table" target="#tab_7">Table 7</ref> shows the effect of the number of parameters under different experimental settings on the NTU dataset. Note that "m-GRU(n)" denotes the baseline scheme which is built by m GRU blocks (layers) with each layer composed of n neurons. "m-EleAtt-GRU(100)" denotes our scheme which includes  m EleAtt-GRU layers with each layer composed of 100 neurons. We can see that the performance increases only a little when more neurons ("2-GRU(128)") or more layers ("3-GRU(100)") are used in comparison with the baseline "2-GRU(100)". In contrast, our scheme "2-EleAtt-GRU(100)", achieves significant gains of 3.1-4.1%. Similar observation can be found for three-layer case. With the similar number of parameters, adding EleAttG is much more effective than increasing the number of neurons or the number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose to empower the neurons in recurrent neural networks to have the attentiveness capability by adding the proposed EleAttG. It can explore the varying importance of different elements of the inputs. The EleAttG is simple yet effective. Experiments show that our proposed EleAttG can be used in any RNN structures (e.g standard RNN, LSTM and GRU) and any layers of the multi-layer RNN networks. In addition, for both human skeleton-based and RGB-based action recognitions, EleAttG boosts performance significantly. We expect that, as a fundamental unit, the proposed EleAttG will be effective for improving many RNN-based learning tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Effectiveness of proposed EleAttGs on the three layered GRU network for 3D skeleton based human action recognition on the NTU dataset. "m-EleAtt-GRU+n-GRU" denotes that the first m layers are EleAtt-GRU layers and the remaining n layers are the original GRU layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization based on the attention responses of the first GRU layer for the actions of kicking and touching neck. For each joint, the size of the yellow circle indicates the learned level of importance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Loss curves during training on the NTU dataset for the proposed scheme "EleAtt-GRU" and the baseline scheme "Baseline-GRU".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness of proposed EleAttGs in the GRU network for RGB-based action recognition on the NTU and JHMDB datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">NTU</cell><cell></cell><cell cols="2">JHMDB</cell><cell></cell></row><row><cell></cell><cell>CS</cell><cell>CV</cell><cell>Split1</cell><cell>Split2</cell><cell>Split3</cell><cell>Average</cell></row><row><cell>Baseline-GRU</cell><cell>61.3</cell><cell>66.8</cell><cell>60.6</cell><cell>59.2</cell><cell>62.9</cell><cell>60.9</cell></row><row><cell>EleAtt-GRU</cell><cell>63.3</cell><cell>70.6</cell><cell>64.5</cell><cell>59.2</cell><cell>65.0</cell><cell>62.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness of EleAttGs on three types of RNN structures on the NTU dataset. "EleAtt-X" denotes the scheme with EleAttGs based on the RNN structure of X.</figDesc><table><row><cell>RNN structure</cell><cell>Scheme</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Standard RNN</cell><cell>Baseline(1-sRNN) EleAtt-sRNN</cell><cell>51.6 61.6</cell><cell>57.6 67.2</cell></row><row><cell>LSTM</cell><cell>Baseline(3-LSTM) EleAtt-LSTM</cell><cell>77.2 78.4</cell><cell>83.0 85.0</cell></row><row><cell>GRU</cell><cell>Baseline(3-GRU) EleAtt-GRU</cell><cell>75.2 79.8</cell><cell>81.5 87.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparisons on the NTU dataset in accuracy (%).</figDesc><table><row><cell>Method</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Skeleton Quads [49]</cell><cell>38.6</cell><cell>41.4</cell></row><row><cell>Lie Group [31]</cell><cell>50.1</cell><cell>52.8</cell></row><row><cell>Dynamic Skeletons [17]</cell><cell>60.2</cell><cell>65.2</cell></row><row><cell>HBRNN-L [4]</cell><cell>59.1</cell><cell>64.0</cell></row><row><cell>Part-aware LSTM [15]</cell><cell>62.9</cell><cell>70.3</cell></row><row><cell>ST-LSTM + Trust Gate [36]</cell><cell>69.2</cell><cell>77.7</cell></row><row><cell>STA-LSTM [24]</cell><cell>73.4</cell><cell>81.2</cell></row><row><cell>GCA-LSTM [23]</cell><cell>74.4</cell><cell>82.8</cell></row><row><cell>URNN-2L-T [37]</cell><cell>74.6</cell><cell>83.2</cell></row><row><cell>Clips+CNN+MTLN [50]</cell><cell>79.6</cell><cell>84.8</cell></row><row><cell>VA-LSTM [38]</cell><cell>79.4</cell><cell>87.2</cell></row><row><cell>Baseline-GRU</cell><cell>75.2</cell><cell>81.5</cell></row><row><cell>EleAtt-GRU</cell><cell>79.8</cell><cell>87.1</cell></row><row><cell>EleAtt-GRU(aug.)</cell><cell>80.7</cell><cell>88.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons on the N-UCLA dataset in acc. (%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparisons on the SYSU dataset in acc. (%).</figDesc><table><row><cell>Method</cell><cell>CS</cell><cell>SS</cell></row><row><cell>LAFF [52]</cell><cell>54.2</cell><cell>-</cell></row><row><cell>DS</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons about relaxing the constraint to EleAttG on the NTU dataset in terms of accuracy (%). Method Baseline EleAttG-1 st EleAttG-2 nd EleAttG-3 rd</figDesc><table><row><cell>Protocols</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CS</cell><cell>w/ constraint wo/ constrain</cell><cell>75.2 75.2</cell><cell>75.0 78.7</cell><cell>72.7 77.3</cell><cell>72.0 76.4</cell></row><row><cell>CV</cell><cell>w/ constraint wo/ constrain</cell><cell>81.5 81.5</cell><cell>83.7 84.9</cell><cell>79.1 83.5</cell><cell>78.8 82.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Effect of the number of parameters on the NTU dataset.</figDesc><table><row><cell>Scheme</cell><cell># parameters</cell><cell>CS</cell><cell>CV</cell></row><row><cell>2-GRU(100)</cell><cell>0.14M</cell><cell cols="2">75.5 81.4</cell></row><row><cell>2-GRU(128)</cell><cell>0.21M</cell><cell cols="2">75.8 81.7</cell></row><row><cell>3-GRU(100)</cell><cell>0.20M</cell><cell cols="2">75.2 81.5</cell></row><row><cell>3-GRU(128)</cell><cell>0.31M</cell><cell cols="2">76.5 81.3</cell></row><row><cell>2-EleAtt-GRU(100)</cell><cell>0.20M</cell><cell cols="2">78.6 85.5</cell></row><row><cell>3-EleAtt-GRU(100)</cell><cell>0.28M</cell><cell cols="2">79.8 87.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A critical review of recurrent neural networks for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00019</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP, Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentive contexts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="944" to="954" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<title level="m">Action recognition using visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hierarchical attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06416</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://colah.github.io/posts/2015-08-Understanding-" />
		<title level="m">Lstm</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lstms</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV, IEEE</publisher>
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="4041" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="4694" to="4702" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Space-time representation of people based on 3d skeletal data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="85" to="105" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshop (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph based skeleton motion representation and similarity measurement for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive rnn tree for largescale human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1444" to="1452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2329" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belopolsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688472</idno>
		<title level="m">Theano: A python framework for fast computation of mathematical expressions. arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">473</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page" from="3104" to="3112" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://github.com/yjxiong/temporal-segment-networks" />
		<title level="m">TSN model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<editor>ICCV, IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Resnet50 model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<editor>ICPR, IEEE</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4513" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="4570" to="4579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Action Recognition with Depth Cameras</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="11" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Real-time rgb-d activity prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="280" to="296" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
