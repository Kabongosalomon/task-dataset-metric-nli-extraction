<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCAN: Learning to Classify Images without Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Leuven</settlement>
									<country>ESAT-PSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Leuven</settlement>
									<country>ESAT-PSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ETH Zurich/CVL</orgName>
								<orgName type="institution" key="instit2">TRACE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Leuven</settlement>
									<country>ESAT-PSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Leuven</settlement>
									<country>ESAT-PSI</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ETH Zurich/CVL</orgName>
								<orgName type="institution" key="instit2">TRACE</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCAN: Learning to Classify Images without Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Learning</term>
					<term>Self-Supervised Learning</term>
					<term>Image Classification</term>
					<term>Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6% on CI-FAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any groundtruth annotations. The code is made publicly available here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and prior work</head><p>Image classification is the task of assigning a semantic label from a predefined set of classes to an image. For example, an image depicts a cat, a dog, a car, an airplane, etc., or abstracting further an animal, a machine, etc. Nowadays, this task is typically tackled by training convolutional neural networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b46">47]</ref> on large-scale datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref> that contain annotated images, i.e. images with their corresponding semantic label. Under this supervised setup, the networks excel at learning discriminative feature representations that can subsequently be clustered into the predetermined classes. What happens, however, when there is no access to ground-truth semantic labels at training time? Or going further, the semantic classes, or even their total number, are not a priori known? The desired goal in this case is to group the images into clusters, such that images within the same cluster belong to the same or similar semantic classes, while images in different clusters are semantically dissimilar. Under this setup, unsupervised or self-supervised learning techniques have recently emerged in the literature as an alternative to supervised feature learning.</p><p>Representation learning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b15">16]</ref> use self-supervised learning to generate feature representations solely from the images, omitting the need for costly semantic annotations. To achieve this, they use pre-designed tasks, called pretext tasks, which do not require annotated data to learn the weights of a convolutional neural network. Instead, the visual features are learned by minimizing the objective function of the pretext task. Numerous pretext tasks have been explored in the literature, including predicting the patch context <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>, inpainting patches <ref type="bibr" target="#b38">[39]</ref>, solving jigsaw puzzles <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>, colorizing images <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b28">29]</ref>, using adversarial training <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, predicting noise <ref type="bibr" target="#b2">[3]</ref>, counting <ref type="bibr" target="#b35">[36]</ref>, predicting rotations <ref type="bibr" target="#b15">[16]</ref>, spotting artifacts <ref type="bibr" target="#b22">[23]</ref>, generating images <ref type="bibr" target="#b40">[41]</ref>, using predictive coding <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b19">20]</ref>, performing instance discrimination <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b31">32]</ref>, and so on. Despite these efforts, representation learning approaches are mainly used as the first pretraining stage of a two-stage pipeline. The second stage includes finetuning the network in a fully-supervised fashion on another task, with as end goal to verify how well the self-supervised features transfer to the new task. When annotations are missing, as is the case in this work, a clustering criterion (e.g. K-means) still needs to be defined and optimized independently. This practice is arguably suboptimal, as it leads to imbalanced clusters <ref type="bibr" target="#b3">[4]</ref>, and there is no guarantee that the learned clusters will align with the semantic classes.</p><p>As an alternative, end-to-end learning pipelines combine feature learning with clustering. A first group of methods (e.g. DEC <ref type="bibr" target="#b51">[52]</ref>, DAC <ref type="bibr" target="#b5">[6]</ref>, DeepCluster <ref type="bibr" target="#b3">[4]</ref>, DeeperCluster <ref type="bibr" target="#b4">[5]</ref>, or others <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b53">54]</ref>) leverage the architecture of CNNs as a prior to cluster images. Starting from the initial feature representations, the clusters are iteratively refined by deriving the supervisory signal from the most confident samples <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b51">52]</ref>, or through cluster re-assignments calculated offline <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. A second group of methods (e.g. IIC <ref type="bibr" target="#b23">[24]</ref>, IMSAT <ref type="bibr" target="#b20">[21]</ref>) propose to learn a clustering function by maximizing the mutual information between an image and its augmentations. In general, methods that rely on the initial feature representations of the network are sensitive to initialization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b53">54]</ref>, or prone to degenerate solutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, thus requiring special mechanisms (e.g. pretraining, cluster reassignment and feature cleaning) to avoid those situations. Most importantly, since the cluster learning depends on the network initialization, they are likely to latch onto low-level features, like color, which is unwanted for the objective of semantic clustering. To partially alleviate this problem, some works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref> are tied to the use of specific preprocessing (e.g. Sobel filtering).</p><p>In this work we advocate a two-step approach for unsupervised image classification, in contrast to recent end-to-end learning approaches. The proposed method, named SCAN (Semantic Clustering by Adopting Nearest neighbors), leverages the advantages of both representation and end-to-end learning approaches, but at the same time it addresses their shortcomings:</p><p>-In a first step, we learn feature representations through a pretext task. In contrast to representation learning approaches that require K-means clustering after learning the feature representations, which is known to lead to cluster degeneracy <ref type="bibr" target="#b3">[4]</ref>, we propose to mine the nearest neighbors of each image based on feature similarity. We empirically found that in most cases these nearest neighbors belong to the same semantic class (see <ref type="figure" target="#fig_1">Figure 2</ref>), rendering them appropriate for semantic clustering. -In a second step, we integrate the semantically meaningful nearest neighbors as a prior into a learnable approach. We classify each image and its mined neighbors together by using a loss function that maximizes their dot product after softmax, pushing the network to produce both consistent and discriminative (one-hot) predictions. Unlike end-to-end approaches, the learned clusters depend on more meaningful features, rather than on the network architecture. Furthermore, because we encourage invariance w.r.t. the nearest neighbors, and not solely w.r.t. augmentations, we found no need to apply specific preprocessing to the input.</p><p>Experimental evaluation shows that our method outperforms prior work by large margins across multiple datasets. Furthermore, we report promising results on the large-scale ImageNet dataset. This validates our assumption that separation between learning (semantically meaningful) features and clustering them is an arguably better approach over recent end-to-end works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The following sections present the cornerstones of our approach. First, we show how mining nearest neighbors from a pretext task can be used as a prior for semantic clustering. Also, we introduce additional constraints for selecting an appropriate pretext task, capable of producing semantically meaningful feature representations. Second, we integrate the obtained prior into a novel loss function to classify each image and its nearest neighbors together. Additionally, we show how to mitigate the problem of noise inherent in the nearest neighbor selection with a self-labeling approach. We believe that each of these contributions are relevant for unsupervised image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Representation learning for semantic clustering</head><p>In the supervised learning setup, each sample can be associated with its correct cluster by using the available ground-truth labels. In particular, the mapping between the images D = X 1 , . . . , X |D| and the semantic classes C can generally be learned by minimizing a cross-entropy loss. However, when we do not have access to such ground-truth labels, we need to define a prior to obtain an estimate of which samples are likely to belong together, and which are not.</p><p>End-to-end learning approaches have utilized the architecture of CNNs as a prior <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, or enforced consistency between images and their augmentations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref> to disentangle the clusters. In both cases, the cluster learning is  known to be sensitive to the network initialization. Furthermore, at the beginning of training the network does not extract high-level information from the image yet. As a result, the clusters can easily latch onto low-level features (e.g. color, texture, contrast, etc.), which is suboptimal for semantic clustering. To overcome these limitations, we employ representation learning as a means to obtain a better prior for semantic clustering.</p><p>In representation learning, a pretext task τ learns in a self-supervised fashion an embedding function Φ θ -parameterized by a neural network with weights θ -that maps images into feature representations. The literature offers several pretext tasks which can be used to learn such an embedding function Φ θ (e.g. rotation prediction <ref type="bibr" target="#b15">[16]</ref>, affine or perspective transformation prediction <ref type="bibr" target="#b56">[57]</ref>, colorization <ref type="bibr" target="#b28">[29]</ref>, in-painting <ref type="bibr" target="#b38">[39]</ref>, instance discrimination <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref>, etc.). In practice, however, certain pretext tasks are based on specific image transformations, causing the learned feature representations to be covariant to the employed transformation. For example, when Φ θ predicts the transformation parameters of an affine transformation, different affine transformations of the same image will result in distinct output predictions for Φ θ . This renders the learned feature representations less appropriate for semantic clustering, where feature representations ought to be invariant to image transformations. To overcome this issue, we impose the pretext task τ to also minimize the distance between images X i and their augmentations T [X i ], which can be expressed as:</p><formula xml:id="formula_0">min θ d(Φ θ (X i ), Φ θ (T [X i ])).<label>(1)</label></formula><p>Any pretext task <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> that satisfies Equation 1 can consequently be used.</p><p>For example, <ref type="figure" target="#fig_0">Figure 1</ref> shows the results when retrieving the nearest neighbors under an instance discrimination task <ref type="bibr" target="#b50">[51]</ref> which satisfies Equation 1. We observe that similar features are assigned to semantically similar images. An experimental evaluation using different pretext tasks can be found in Section 3.2.</p><p>To understand why images with similar high-level features are mapped closer together by Φ θ , we make the following observations. First, the pretext task output is conditioned on the image, forcing Φ θ to extract specific information from its input. Second, because Φ θ has a limited capacity, it has to discard information from its input that is not predictive of the high-level pretext task. For example, it is unlikely that Φ θ can solve an instance discrimination task by only encoding color or a single pixel from the input image. As a result, images with similar high-level characteristics will lie closer together in the embedding space of Φ θ .</p><p>We conclude that pretext tasks from representation learning can be used to obtain semantically meaningful features. Following this observation, we will leverage the pretext features as a prior for clustering the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A semantic clustering loss</head><p>Mining nearest neighbors. In Section 2.1, we motivated that a pretext task from representation learning can be used to obtain semantically meaningful features. However, naively applying K-means on the obtained features can lead to cluster degeneracy <ref type="bibr" target="#b3">[4]</ref>. A discriminative model can assign all its probability mass to the same cluster when learning the decision boundary. This leads to one cluster dominating the others. Instead, we opt for a better strategy.</p><p>Let us first consider the following experiment. Through representation learning, we train a model Φ θ on the unlabeled dataset D to solve a pretext task τ , i.e. instance discrimination <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. Then, for every sample X i ∈ D, we mine its K nearest neighbors in the embedding space Φ θ . We define the set N Xi as the neighboring samples of X i in the dataset D. <ref type="figure" target="#fig_1">Figure 2</ref> quantifies the degree to which the mined nearest neighbors are instances of the same semantic cluster. We observe that this is largely the case across four datasets 1 (CIFAR10 <ref type="bibr" target="#b26">[27]</ref>, CIFAR100-20 <ref type="bibr" target="#b26">[27]</ref>, STL10 <ref type="bibr" target="#b8">[9]</ref> and ImageNet <ref type="bibr" target="#b10">[11]</ref>) for different values of K. Motivated by this observation, we propose to adopt the nearest neighbors obtained through the pretext task τ as our prior for semantic clustering. Loss function. We aim to learn a clustering function Φ η -parameterized by a neural network with weights η -that classifies a sample X i and its mined neighbors N Xi together. The function Φ η terminates in a softmax function to perform a soft assignment over the clusters C = {1, . . . , C}, with Φ η (X i ) ∈ [0, 1] C . The probability of sample X i being assigned to cluster c is denoted as Φ c η (X i ). We learn the weights of Φ η by minimizing the following objective:</p><formula xml:id="formula_1">Λ = − 1 |D| X∈D k∈N X log Φ η (X), Φ η (k) + λ c∈C Φ c η log Φ c η , with Φ c η = 1 |D| X∈D Φ c η (X).<label>(2)</label></formula><p>Here, · denotes the dot product operator. The first term in Equation 2 imposes Φ η to make consistent predictions for a sample X i and its neighboring samples N Xi . Note that, the dot product will be maximal when the predictions are one-hot (confident) and assigned to the same cluster (consistent). To avoid Φ η from assigning all samples to a single cluster, we include an entropy term (the second term in Equation 2), which spreads the predictions uniformly across the clusters C. If the probability distribution over the clusters C is known in advance, which is not the case here, this term can be replaced by KL-divergence.</p><p>Remember that, the exact number of clusters in C is generally unknown. However, similar to prior work <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>, we choose C equal to the number of ground-truth clusters for the purpose of evaluation. In practice, it should be possible to obtain a rough estimate of the amount of clusters 2 . Based on this estimate, we can overcluster to a larger amount of clusters, and enforce the class distribution to be uniform. We refer to Section 3.4 for a concrete experiment. Implementation details. For the practical implementation of our loss function, we approximate the dataset statistics by sampling batches of sufficiently large size. During training we randomly augment the samples X i and their neighbors N Xi . For the corner case K = 0, only consistency between samples and their augmentations is imposed. We set K ≥ 1 to capture more of the cluster's variance, at the cost of introducing noise, i.e. not all samples and their neighbors belong to the same cluster. Section 3.2 experimentally shows that choosing K ≥ 1 significantly improves the results compared to only enforcing consistency between samples and their augmentations, as in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>. Discussion. Unlike <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b51">52]</ref> we do not include a reconstruction criterion into the loss, since this is not explicitly required by our target task. After all, we are only interested in a few bits of information encoded from the input signal, rather than the majority of information that a reconstruction criterion typically requires. It is worth noting that the consistency in our case is enforced at the level of individual samples through the dot product term in the loss, rather than on an approximation of the joint distribution over the classes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>. We argue that this choice allows to express the consistency in a more direct way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fine-tuning through self-labeling</head><p>The semantic clustering loss in Section 2.2 imposed consistency between a sample and its neighbors. More specifically, each sample was combined with K ≥ 1 neighbors, some of which inevitably do not belong to the same semantic cluster. These false positive examples lead to predictions for which the network is less certain. At the same time, we experimentally observed that samples with highly confident predictions (p max ≈ 1) tend to be classified to the proper cluster. In fact, the highly confident predictions that the network forms during clustering can be regarded as "prototypes" for each class (see Section 3.5). Unlike prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b51">52]</ref>, this allows us to select samples based on the confidence of the predictions in a more reliable manner. Hence, we propose a self-labeling approach <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46]</ref> to exploit the already well-classified examples, and correct for mistakes due to noisy nearest neighbors. In particular, during training confident samples are selected by thresholding the probability at the output, i.e. p max &gt; threshold. For every confident sample, a pseudo label is obtained by assigning the sample to its predicted cluster. A cross-entropy loss is used to update the weights for the obtained pseudo labels. To avoid overfitting, we calculate the cross-entropy loss on strongly augmented versions of the confident samples. The self-labeling step allows the network to correct itself, as it gradually becomes more certain, adding more samples to the mix. We refer to Section 3.2 for a concrete experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Semantic Clustering by Adopting Nearest neighbors (SCAN)</head><formula xml:id="formula_2">1: Input: Dataset D, Clusters C, Task τ , Neural Nets Φ θ and Φη, Neighbors ND = {}. 2: Optimize Φ θ with task τ . Pretext Task Step, Sec. 2.1 3: for Xi ∈ D do 4: ND ← ND ∪ NX i , with NX i = K neighboring samples of Φ θ (Xi</formula><p>Algorithm 1 summarizes all the steps of the proposed method. We further refer to it as SCAN, i.e. Semantic Clustering by Adopting Nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>Datasets. The experimental evaluation is performed on CIFAR10 <ref type="bibr" target="#b26">[27]</ref>, CIFAR100-20 <ref type="bibr" target="#b26">[27]</ref>, STL10 <ref type="bibr" target="#b8">[9]</ref> and ImageNet <ref type="bibr" target="#b10">[11]</ref>. We focus on the smaller datasets first. The results on ImageNet are discussed separately in Section 3.5. Some prior works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref> trained and evaluated on the complete datasets. Differently, we train and evaluate using the train and val split respectively. Doing so, allows to study the generalization properties of the method for novel unseen examples. Note that this does not result in any unfair advantages compared to prior work. The results are reported as the mean and standard deviation from 10 different runs. Finally, all experiments are performed using the same backbone, augmentations, pretext task and hyperparameters. Training setup. We use a standard ResNet-18 backbone. For every sample, the 20 nearest neighbors are determined through an instance discrimination task based on noise contrastive estimation (NCE) <ref type="bibr" target="#b50">[51]</ref>. We adopt the SimCLR <ref type="bibr" target="#b6">[7]</ref> implementation for the instance discrimination task on the smaller datasets, and the implementation from MoCo <ref type="bibr" target="#b7">[8]</ref> on ImageNet. The selected pretext task satisfies the feature invariance constraint from Equation 1 w.r.t. the transformations applied to augment the input images. In particular, every image is disentangled as a unique instance independent of the applied transformation. To speed up training, we transfer the weights, obtained from the pretext task to initiate the clustering step (Section 2.2). We perform the clustering step for 100 epochs using batches of size 128. The weight on the entropy term is set to λ = 5. A higher weight avoids the premature grouping of samples early on during training. The results seem to be insensitive to small changes of λ. After the clustering step, we train for another 200 epochs using the self-labeling procedure with threshold 0.99 (Section 2.3). A weighted cross-entropy loss compensates for the imbalance between confident samples across clusters. The class weights are inversely proportional to the number of occurrences in the batch after thresholding. The network weights are updated through Adam <ref type="bibr" target="#b24">[25]</ref> with learning rate 10 −4 and weight decay 10 −4 . The images are strongly augmented by composing four randomly selected transformations from RandAugment <ref type="bibr" target="#b9">[10]</ref> during both the clustering and selflabeling steps. The transformation parameters are uniformly sampled between fixed intervals. For more details visit the supplementary materials. Validation criterion During the clustering step, we select the best model based on the lowest loss. During the self-labeling step, we save the weights of the model when the amount of confident samples plateaus. We follow these practices as we do not have access to a labeled validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation studies</head><p>Method. We quantify the performance gains w.r.t. the different parts of our method through an ablation study on CIFAR10 in <ref type="table" target="#tab_1">Table 1</ref>. K-means clustering of the NCE pretext features results in the lowest accuracy (65.9%), and is characterized by a large variance (5.7%). This is to be expected since the cluster assignments can be imbalanced <ref type="figure">(Figure 3</ref>), and are not guaranteed to align with the ground-truth classes. Interestingly, applying K-means to the pretext features outperforms prior state-of-the-art methods for unsupervised classification based on end-to-end learning schemes (see Sec. 3.3). This observation supports our primary claim, i.e. it is beneficial to separate feature learning from clustering. Updating the network weights through the SCAN-loss -while augmenting the input images through SimCLR transformations -outperforms K-means (+15.9%). Note that the SCAN-loss is somewhat related to K-means, since both methods employ the pretext features as their prior to cluster the images. Differently, our loss avoids the cluster degeneracy issue. We also research the effect of using different augmentation strategies during training. Applying transformations from RandAgument (RA) to both the samples and their mined neighbors further improves the performance (78.7% vs. 81.8%). We hypothesize that strong augmentations help to reduce the solution space by imposing additional invariances.</p><p>Fine-tuning the network through self-labeling further enhances the quality of the cluster assignments (81.8% to 87.6%). During self-labeling, the network corrects itself as it gradually becomes more confident (see <ref type="figure">Figure 4</ref>). Importantly,   in order for self-labeling to be successfully applied, a shift in augmentations is required (see <ref type="table" target="#tab_1">Table 1</ref> or <ref type="figure">Figure 5</ref>). We hypothesize that this is required to prevent the network from overfitting on already well-classified examples. Finally, <ref type="figure">Figure 6</ref> shows that self-labeling procedure is not sensitive to the threshold's value.</p><p>Pretext task. We study the effect of using different pretext tasks to mine the nearest neighbors. In particular we consider two different implementations of the instance discrimination task from before <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b6">7]</ref>, and RotNet <ref type="bibr" target="#b15">[16]</ref>. The latter trains the network to predict image rotations. As a consequence, the distance between an image X i and its augmentations T [X i ] is not minimized in the embedding space of a model pretrained through RotNet (see <ref type="bibr">Equation 1</ref>). Differently, the instance discrimintation task satisfies the invariance criterion w.r.t. the used augmentations. <ref type="table" target="#tab_2">Table 2</ref> shows the results on CIFAR10. First, we observe that the proposed method is not tied to a specific pretext task. All cases report high accuracy (&gt; 70%). Second, pretext tasks that satisfy the invariance criterion are better suited to mine the nearest neighbors, i.e. 83.5% and 87.6% for inst. discr. versus 74.3% for RotNet. This confirms our hypothesis from Section 2.1, i.e. it is beneficial to choose a pretext task which imposes invariance between an image and its augmentations.  sensitive to the value of K, and even remain stable when increasing K to 50. This is beneficial, since we do not have to fine-tune the value of K on very new dataset. In fact, both robustness and accuracy improve when increasing the value of K upto a certain value. We also consider the corner case K = 0, when only enforcing consistent predictions for images and their augmentations. the performance decreases on all three datasets compared to K = 5, 56.3% vs 79.3% on CIFAR10, 24.6% vs 41.1% on CIFAR100-20 and 47.70% vs 69.8% on STL10. This confirms that better representations can be learned by also enforcing coherent predictions between a sample and its nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of neighbors.</head><p>Convergence. <ref type="figure" target="#fig_4">Figure 8</ref> shows the results when removing the false positives from the nearest neighbors, i.e. sample-pairs which belong to a different class.</p><p>The results can be considered as an upper-bound for the proposed method in terms of classification accuracy. A desirable characteristic is that the clusters quickly align with the ground truth, obtaining near fully-supervised performance on CIFAR10 and STL10 with a relatively small increase in the number of used neighbors K. The lower performance improvement on CIFAR100-20 can be explained by the ambiguity of the superclasses used to measure the accuracy. For example, there is not exactly one way to group categories like omnivores or carnivores together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with the state-of-the-art</head><p>Comparison. <ref type="table">Table 3</ref> compares our method to the state-of-the-art on three different benchmarks. We evaluate the results based on clustering accuracy (ACC), normalized mutual information (NMI) and adjusted rand index (ARI). The proposed method consistently outperforms prior work by large margins on all three metrics, e.g. +26.6% on CIFAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of accuracy. We also compare with the state-of-the-art in representation learning <ref type="bibr" target="#b6">[7]</ref> (Pretext + K-means). As shown in Section 3.2, our method outperforms the application of K-means on the pretext features. Finally, we also include results when tackling the problem in a fully-supervised manner. Our <ref type="table">Table 3</ref>: State-of-the-art comparison: We report the averaged results for 10 different runs after the clustering ( * ) and self-labeling steps ( †), and the best model. Opposed to prior work, we train and evaluate using the train and val split respectively, instead of using the full dataset for both training and testing. model obtains close to supervised performance on CIFAR-10 and STL-10. The performance gap is larger on CIFAR100-20, due to the use of superclasses.</p><p>Other advantages. In contrast to prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21]</ref>, we did not have to perform any dataset specific fine-tuning. Furthermore, the results on CIFAR10 can be obtained within 6 hours on a single GPU. As a comparison, training the model from <ref type="bibr" target="#b23">[24]</ref> requires at least a day of training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overclustering</head><p>So far we assumed to have knowledge about the number of ground-truth classes. The method predictions were evaluated using a hungarian matching algorithm. However, what happens if the number of clusters does not match the number of ground-truth classes anymore. <ref type="table">Table 3</ref> reports the results when we overestimate the number of ground-truth classes by a factor of 2, e.g. we cluster CIFAR10 into 20 rather than 10 classes. The classification accuracy remains stable for CIFAR10 (87.6% to 86.2%) and STL10 (76.7% to 76.8%), and improves for CIFAR100-20 (45.9% to 55.1%) <ref type="bibr" target="#b2">3</ref> . We conclude that the approach does not require knowledge of the exact number of clusters. We hypothesize that the increased performance on CIFAR100-20 is related to the higher intra-class variance. More specifically, CIFAR100-20 groups multiple object categories together in superclasses. In this case, an overclustering is better suited to explain the intra-class variance. <ref type="table">Table 4</ref>: Validation set results for 50, 100 and 200 randomly selected classes from ImageNet. The results with K-means were obtained using the pretext features from MoCo <ref type="bibr" target="#b7">[8]</ref>. We provide the results obtained by our method after the clustering step ( * ), and after the self-labeling step ( †). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">ImageNet</head><p>Setup. We consider the problem of unsupervised image classification on the large-scale ImageNet dataset <ref type="bibr" target="#b10">[11]</ref>. We first consider smaller subsets of 50, 100 and 200 randomly selected classes. The sets of 50 and 100 classes are subsets of the 100 and 200 classes respectively. Additional details of the training setup can be found in the supplementary materials.</p><p>Quantitative evaluation. <ref type="table">Table 4</ref> compares our results against applying Kmeans on the pretext features from MoCo <ref type="bibr" target="#b7">[8]</ref>. Surprisingly, the application of K-means already performs well on this challenging task. We conclude that the pretext features are well-suited for the down-stream task of semantic clustering. Training the model with the SCAN-loss again outperforms the application of K-means. Also, the results are further improved when fine-tuning the model through self-labeling. We do not include numbers for the prior state-ofthe-art <ref type="bibr" target="#b23">[24]</ref>, since we could not obtain convincing results on ImageNet when running the publicly available code. We refer the reader to the supplementary materials for additional qualitative results on ImageNet-50.</p><p>Prototypical behavior. We visualize the different clusters after training the model with the SCAN-loss. Specifically, we find the samples closest to the mean embedding of the top-10 most confident samples in every cluster. The results are shown together with the name of the matched ground-truth classes in <ref type="figure">Fig. 9</ref>. Importantly, we observe that the found samples align well with the classes of the dataset, except for 'oboe' and 'guacamole' (red). Furthermore, the discriminative features of each object class are clearly present in the images. Therefore, we regard the obtained samples as "prototypes" of the various clusters. Notice that the performed experiment aligns well with prototypical networks <ref type="bibr" target="#b44">[45]</ref>.</p><p>ImageNet -1000 classes. Finally, the model is trained on the complete Im-ageNet dataset. <ref type="figure" target="#fig_0">Figure 11</ref> shows images from the validation set which were assigned to the same cluster by our model. The obtained clusters are semantically   meaningful, e.g. planes, cars and primates. Furthermore, the clusters capture a large variety of different backgrounds, viewpoints, etc. We conclude that (to a large extent) the model predictions are invariant to image features which do not alter the semantics. On the other hand, based on the ImageNet ground-truth annotations, not all sample pairs should have been assigned to the same cluster. For example, the ground-truth annotations discriminate between different primates, e.g. chimpanzee, baboon, langur, etc. We argue that there is not a single correct way of categorizing the images according to their semantics in case of ImageNet. Even for a human annotator, it is not straightforward to cluster each image according to the ImageNet classes without prior knowledge.</p><p>Based on the ImageNet hierarchy we select class instances of the following superclasses: dogs, insects, primates, snake, clothing, buildings and birds. <ref type="figure">Fig-ure</ref> 10 shows a confusion matrix of the selected classes. The confusion matrix has a block diagonal structure. The results show that the misclassified examples tend to be assigned to other clusters from within the same superclass, e.g. the model confuses two different dog breeds. We conclude that the model has learned to group images with similar semantics together, while its prediction errors can be attributed to the lack of annotations which could disentangle the fine-grained differences between some classes.</p><p>Finally, <ref type="table" target="#tab_6">Table 5</ref> compares our method against recent semi-supervised learning approaches when using 1% of the images as labelled data. We obtain the following quantitative results on ImageNet: Top-1: 39.9%, Top-5: 60.0%, NMI: 72.0%, ARI: 27.5%. Our method outperforms several semi-supervised learning approaches, without using labels. This further demonstrates the strength of our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented a novel framework to unsupervised image classification. The proposed approach comes with several advantages relative to recent works which adopted an end-to-end strategy. Experimental evaluation shows that the proposed method outperforms prior work by large margins, for a variety of datasets. Furthermore, positive results on ImageNet demonstrate that semantic clustering can be applied to large-scale datasets. Encouraged by these findings, we believe that our approach admits several extensions to other domains, e.g. semantic segmentation, semi-supervised learning and few-shot learning.</p><p>Acknowledgment. The authors thankfully acknowledge support by Toyota via the TRACE project and MACCHINA (KU Leuven, C14/18/065). Furthermore, we would like to thank Xu Ji for her valuable insights and comments. Finally, we thank Kevis-Kokitsi Maninis, Jonas Heylen and Mark De Wolf for their feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Smaller datasets</head><p>We include additional qualitative results on the smaller datasets, i.e. CIFAR10 <ref type="bibr" target="#b26">[27]</ref>, CIFAR100-20 <ref type="bibr" target="#b26">[27]</ref> and STL10 <ref type="bibr" target="#b8">[9]</ref>. We used the models from the state-of-the-art comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Prototypical examples</head><p>Figure S1 visualizes a prototype image for every cluster on CIFAR10, CIFAR100-20 and STL-10. The object of interest is clearly recognizable in the images. It is worth noting that the prototypical examples on CIFAR10 and STL10 can be matched with the ground-truth classes of the dataset. This is not the case for CIFAR100-20, e.g. bus and bicycle belong to the vehicles 1 ground-truth class. This behavior can be easily understood since CIFAR-20 makes use of superclasses. As a consequence, it is difficult to explain the intra-class variance from visual appearance alone. Interestingly, we can reduce this mismatch through overclustering (see Sec 3.4.).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Training setup</head><p>We summarize the training setup for ImageNet below.</p><p>Pretext Task Similar to our setup on the smaller datasets, we select instance discrimination as our pretext task. In particular, we use the implementation from MoCo <ref type="bibr" target="#b7">[8]</ref>. We use a ResNet-50 model as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering</head><p>Step We freeze the backbone weights during the clustering step, and only train the final linear layer using the SCAN-loss. More specifically, we train ten separate linear heads in parallel. When initiating the self-labeling step, we select the head with the lowest loss to continue training. Every image is augmented using augmentations from SimCLR <ref type="bibr" target="#b6">[7]</ref>. We reuse the entropy weight from before (5.0), and train with batches of size 512, 1024 and 1024 on the subsets of 50, 100 and 200 classes respectively. We use an SGD optimizer with momentum 0.9 and initial learning rate 5.0. The model is trained for 100 epochs. On the full ImageNet dataset, we increase the batch size and learning rate to 4096 and 30.0 respectively, and decrease the number of neighbors to 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Labeling</head><p>Step We use the strong augmentations from RandAugment to finetune the weights through self-labeling. The model weights are updated for 25 epochs using SGD with momentum 0.9. The initial learning rate is set to 0.03 and kept constant. Batches of size 512 are used. Importantly, the model weights are updated through an exponential moving average with α = 0.999. We did not find it necessary to apply class balancing in the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ImageNet -Subsets</head><p>Confusion matrix <ref type="figure" target="#fig_10">Figure S3</ref> shows a confusion matrix on the ImageNet-50 dataset. Most of the mistakes can be found between classes that are hard to disentangle, e.g. 'Giant Schnauzer' and 'Flat-coated Retriever' are both black dog breeds, 'Guacamole' and 'Mashed Potato' are both food, etc. <ref type="figure" target="#fig_11">Figure S4</ref> shows a prototype image for every cluster on the ImageNet-50 subset. This figure extends <ref type="figure">Figure 9</ref> from the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototype examples</head><p>Remarkably, the vast majority of prototype images can be matched with one of the ground-truth classes.</p><p>Low confidence examples <ref type="figure">Figure S5</ref> shows examples for which the model produces low confidence predictions on the ImageNet-50 subset. In a number of cases, the low confidence output can be attributed to multiple objects being visible in the scene. Other cases can be explained by the partial visibility of the object, distracting elements in the scene, or ambiguity of the object of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 ImageNet -Full</head><p>We include additional qualitative results on the full ImageNet dataset. In particular, <ref type="figure">Figures S6, S7</ref> and S8 show images from the validation set that were assigned to the same cluster. These can be viewed together with <ref type="figure" target="#fig_0">Figure 11</ref> in the main paper. Additionally, we show some mistakes in <ref type="figure" target="#fig_13">Figure S9</ref>. The failure cases occur when the model focuses too much on the background, or when the network cannot easily discriminate between pairs of similarly looking images. However, in most cases, we can still attach some semantic meaning to the clusters, e.g. animals in cages, white fences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Datasets</head><p>Different from prior work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>, we do not train and evaluate on the full datasets. Differently, we use the standard train-val splits to study the generalization properties of our models. Additionally, we report the mean and standard deviation on the smaller datasets. We would like to encourage future works to adopt this procedure as well. <ref type="table" target="#tab_1">Table S1</ref> provides an overview of the number of classes, the number of images and the aspect ratio of the used datasets. The selected classes on ImageNet-50, ImageNet-100 and ImageNet-200 can be found in our git repository.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Augmentations</head><p>As shown in our experiments, it is beneficial to apply strong augmentations during training. The strong augmentations were composed of four randomly selected transformations from RandAugment <ref type="bibr" target="#b9">[10]</ref>, followed by Cutout <ref type="bibr" target="#b11">[12]</ref>. The transformation parameters were uniformly sampled between fixed intervals. <ref type="table" target="#tab_2">Table S2</ref> provides a detailed overview. We applied an identical augmentation strategy across all datasets.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Change Log</head><p>The following changes were made since version 1:</p><p>-Sections 1 and 2: Minor changes were made to the text. References were added to recent related works (CMC, SimCLR, MoCo, FixMatch). <ref type="figure" target="#fig_1">Fig. 2</ref> was updated using more recent implementations of the instance discrimination pretext task. -Section 3.1.: The experimental setup was updated. In particular, we use the implementations from SimCLR and MoCo to perform the instance discrimination task. An identical entropy weight is now used across all datasets. We train and evaluate on the train, val split respectively, rather than using the complete dataset for both training and testing as in prior work. Doing so allows to compare the results against semi-and fully-supervised methods. -Section 3.2.: All results were updated using the SimCLR implementation of the instance discrimination pretext task. Additional experiments were included to study the influence of applying various augmentation strategies, and to analyze the effect of the threshold value. -Section 3.5.: We added a comparison with semi-and fully-supervised methods on ImageNet. We revised our earlier results on the smaller ImageNet subsets due to a coding mistake. Importantly, the initial conclusions remain valid, while results on the full ImageNet dataset improved. We apologize for any inconvenience this might have caused.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Images (first column) and their nearest neighbors (other columns)<ref type="bibr" target="#b50">[51]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Neighboring samples tend to be instances of the same semantic class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Acc. and the number of confident samples during self-labeling. Self-labeling with SimCLR or RandAugment augmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 Fig. 7 :</head><label>77</label><figDesc>shows the influence of using a different number of nearest neighbors K during the clustering step. The results are not very Influence of the used number of neighbors K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Results without false positives in the nearest neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Prototypes obtained by sampling the confident samples. Zoom on seven superclasses in the confusion matrix on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 :</head><label>11</label><figDesc>Clusters extracted by our model on ImageNet (more in supplementary).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. S1 :</head><label>S1</label><figDesc>Prototype images on the smaller datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>20 A. 2</head><label>202</label><figDesc>Low confidence examplesFigure S2shows examples for which the network produces low confidence predictions. In most cases, it is hard to determine the correct class label. The difficultFig. S2: Low confidence predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>which are: only partially visible, occluded, under bad lighting conditions, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. S3 :</head><label>S3</label><figDesc>Confusion matrix on ImageNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. S4 :</head><label>S4</label><figDesc>Prototype images on ImageNet-50.Fig. S5: Low confidence examples on ImageNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. S8 :</head><label>S8</label><figDesc>Example clusters of ImageNet-1000 (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. S9 :</head><label>S9</label><figDesc>Incorrect clusters of ImageNet-1000 predicted by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation Method CIFAR10</figDesc><table><row><cell>Setup</cell><cell>ACC</cell></row><row><cell></cell><cell>(Avg ± Std)</cell></row><row><cell>Pretext + K-means</cell><cell>65.9 ± 5.7</cell></row><row><cell>SCAN-Loss (SimCLR)</cell><cell>78.7 ± 1.7</cell></row><row><cell>(1) Self-Labeling (SimCLR)</cell><cell>10.0 ± 0</cell></row><row><cell>(2) Self-Labeling (RA)</cell><cell>87.4 ± 1.6</cell></row><row><cell>SCAN-Loss (RA)</cell><cell>81.8 ± 1.7</cell></row><row><cell>(1) Self-Labeling (RA)</cell><cell>87.6 ± 0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation Pretext CIFAR10</figDesc><table><row><cell cols="2">Pretext Task Clustering</cell><cell>ACC</cell></row><row><cell></cell><cell></cell><cell>(Avg ± Std)</cell></row><row><cell>RotNet [16]</cell><cell>K-means</cell><cell>27.1 ± 2.1</cell></row><row><cell></cell><cell>SCAN</cell><cell>74.3 ± 3.9</cell></row><row><cell>Inst. discr. [51]</cell><cell>K-means</cell><cell>52.0 ± 4.6</cell></row><row><cell></cell><cell>SCAN</cell><cell>83.5 ± 4.1</cell></row><row><cell>Inst. discr. [7]</cell><cell>K-means</cell><cell>65.9 ± 5.7</cell></row><row><cell></cell><cell>SCAN</cell><cell>87.6 ± 0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with supervised, and semi-supervised learning methods using 1% of the labelled data on ImageNet.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Labels Top-1 Top-5</cell></row><row><cell>Supervised Baseline</cell><cell>ResNet-50</cell><cell>25.4</cell><cell>48.4</cell></row><row><cell>Pseudo-Label</cell><cell>ResNet-50</cell><cell>-</cell><cell>51.6</cell></row><row><cell cols="2">VAT + Entropy Min. [56] ResNet-50</cell><cell>-</cell><cell>47.0</cell></row><row><cell>InstDisc [51]</cell><cell>ResNet-50</cell><cell>-</cell><cell>39.2</cell></row><row><cell>BigBiGAN [15]</cell><cell>ResNet-50(4x)</cell><cell>-</cell><cell>55.2</cell></row><row><cell>PIRL [32]</cell><cell>ResNet-50</cell><cell>-</cell><cell>57.2</cell></row><row><cell>CPC v2 [20]</cell><cell>ResNet-161</cell><cell>52.7</cell><cell>77.9</cell></row><row><cell>SimCLR [7]</cell><cell>ResNet-50</cell><cell>48.3</cell><cell>75.5</cell></row><row><cell>SCAN (Ours)</cell><cell>ResNet-50</cell><cell>39.9</cell><cell>60.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S1 :</head><label>S1</label><figDesc>Datasets overviewDatasetClasses Train images Val images Aspect ratio</figDesc><table><row><cell>Albatross</cell><cell cols="2">Black Bear Dipper</cell><cell cols="3">Box Turtle Cairn Terrier Goose</cell><cell>Collie</cell><cell>Coyote</cell><cell>Snake</cell><cell>Proboscis</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Monkey</cell></row><row><cell></cell><cell cols="2">Irish Setter Mountain</cell><cell>Scorpion</cell><cell></cell><cell>Water Hen</cell><cell></cell><cell>Organ</cell></row><row><cell></cell><cell></cell><cell>Lion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Banana</cell></row><row><cell>Guenon Monkey Cowboy Hat</cell><cell></cell><cell></cell><cell></cell><cell>Schipperke</cell><cell></cell><cell>Giant Schnauzer</cell><cell></cell><cell>Guitar Threshing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Machine</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mashed</cell></row><row><cell></cell><cell>Mic Tobacco</cell><cell>Mitten</cell><cell>Wreck</cell><cell cols="2">Shoe Shop Purse Broom File Cabinet</cell><cell>Table Lamp Maillot</cell><cell>Potato Maze</cell><cell>Hatchet Manhole</cell><cell>Obelisk</cell></row><row><cell></cell><cell>Shop</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Vending</cell><cell>Washing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Unicycle</cell><cell></cell><cell>Machine</cell><cell>Machine</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Neck Brace</cell><cell></cell><cell></cell><cell>Woodworking</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Plane</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Steel Arch Bridge</cell><cell>Shower Curtain</cell><cell></cell><cell>Trash Bin</cell><cell>Soda Bottle</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S2 :</head><label>S2</label><figDesc>List of transformations. The strong transformations are composed by randomly selecting four transformations from the list, followed by Cutout.</figDesc><table><row><cell cols="3">Transformation Parameter Interval</cell></row><row><cell>Identity</cell><cell>-</cell><cell>-</cell></row><row><cell>Autocontrast</cell><cell>-</cell><cell>-</cell></row><row><cell>Equalize</cell><cell>-</cell><cell>-</cell></row><row><cell>Rotate</cell><cell>θ</cell><cell>[−30, 30]</cell></row><row><cell>Solarize</cell><cell>T</cell><cell>[0, 256]</cell></row><row><cell>Color</cell><cell>C</cell><cell>[0.05, 0.95]</cell></row><row><cell>Contrast</cell><cell>C</cell><cell>[0.05, 0.95]</cell></row><row><cell>Brightness</cell><cell>B</cell><cell>[0.05, 0.95]</cell></row><row><cell>Sharpness</cell><cell>S</cell><cell>[0.05, 0.95]</cell></row><row><cell>Shear X</cell><cell>R</cell><cell>[−0.1, 0.1]</cell></row><row><cell>Translation X</cell><cell>λ</cell><cell>[−0.1, 0.1]</cell></row><row><cell>Translation Y</cell><cell>λ</cell><cell>[−0.1, 0.1]</cell></row><row><cell>Posterize</cell><cell>B</cell><cell>[4, 8]</cell></row><row><cell>Shear Y</cell><cell>R</cell><cell>[−0.1, 0.1]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The details for each dataset are provided in the supplementary materials.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As an example, say you want to cluster various animal species observed in a national park. In this case, we can rely on prior domain knowledge to make an estimate.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since the overclustering case is evaluated using a many-to-one mapping, a direct comparison is not entirely fair. Still, we provide the comparison as an indication.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fig. S6: Example clusters of ImageNet-1000 (1).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised deep learning by neighbourhood discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Learning multiple layers of features from tiny images</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improvements to context based selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Sparse autoencoder. CS294A Lecture notes</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cross-domain self-supervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<title level="m">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optimized cartesian k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Aet vs. aed: Unsupervised representation learning by auto-encoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02351</idno>
	</analytic>
	<monogr>
		<title level="m">Stacked what-where auto-encoders</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Imagenet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0224" />
			<biblScope unit="volume">558</biblScope>
			<biblScope unit="page" from="0" to="224" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
