<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
							<email>dosovits@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
							<email>riedmiller@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted as a workshop contribution at ICLR 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -and building on other recent work for finding simple network structures -we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches. * Both authors contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION AND RELATED WORK</head><p>The vast majority of modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: They use alternating convolution and max-pooling layers followed by a small number of fully connected layers (e.g. <ref type="bibr" target="#b8">Jarrett et al. (2009)</ref>; <ref type="bibr" target="#b12">Krizhevsky et al. (2012)</ref>; <ref type="bibr">Ciresan et al.)</ref>. Within each of these layers piecewise-linear activation functions are used. The networks are typically parameterized to be large and regularized during training using dropout. A considerable amount of research has over the last years focused on improving the performance of this basic pipeline. Among these two major directions can be identified. First, a plethora of extensions were recently proposed to enhance networks which follow this basic scheme. Among these the most notable directions are work on using more complex activation functions <ref type="bibr" target="#b4">(Goodfellow et al., 2013;</ref><ref type="bibr" target="#b15">Lin et al., 2014;</ref> techniques for improving class inference <ref type="bibr" target="#b22">(Stollenga et al., 2014;</ref><ref type="bibr" target="#b19">Srivastava &amp; Salakhutdinov, 2013)</ref> as well as procedures for improved regularization <ref type="bibr" target="#b18">Springenberg &amp; Riedmiller, 2013;</ref><ref type="bibr" target="#b24">Wan et al., 2013)</ref> and layer-wise pre-training using label information <ref type="bibr" target="#b14">(Lee et al., 2014</ref>). Second, the success of CNNs for large scale object recognition in the ImageNet challenge  has stimulated research towards experimenting with the different architectural choices in CNNs. Most notably the top entries in the 2014 ImageNet challenge deviated from the standard design principles by either introducing multiple convolutions in between pooling layers  or by building heterogeneous modules performing convolutions and pooling at multiple scales in each layer <ref type="bibr" target="#b23">(Szegedy et al., 2014)</ref>.</p><p>Since all of these extensions and different architectures come with their own parameters and training procedures the question arises which components of CNNs are actually necessary for achieving state of the art performance on current object recognition datasets. We take a first step towards answering this question by studying the most simple architecture we could conceive: a homogeneous network solely consisting of convolutional layers, with occasional dimensionality reduction by using a stride of 2. Surprisingly, we find that this basic architecture -trained using vanilla stochastic gradient descent with momentum -reaches state of the art performance without the need for complicated activation functions, any response normalization or max-pooling. We empirically study the effect of transitioning from a more standard architecture to our simplified CNN by performing an ablation study on CIFAR-10 and compare our model to the state of the art on CIFAR-10, CIFAR-100 and the ILSVRC-2012 ImageNet dataset. Our results both confirm the effectiveness of using small convolutional layers as recently proposed by  and give rise to interesting new questions about the necessity of pooling in CNNs. Since dimensionality reduction is performed via strided convolution rather than max-pooling in our architecture it also naturally lends itself to studying questions about the invertibility of neural networks <ref type="bibr" target="#b3">(Estrach et al., 2014)</ref>. For a first step in that direction we study properties of our network using a deconvolutional approach similar to <ref type="bibr" target="#b26">Zeiler &amp; Fergus (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL DESCRIPTION -THE ALL CONVOLUTIONAL NETWORK</head><p>The models we use in our experiments differ from standard CNNs in several key aspects. Firstand most interestingly -we replace the pooling layers, which are present in practically all modern CNNs used for object recognition, with standard convolutional layers with stride two. To understand why this procedure can work it helps to recall the standard formulation for defining convolution and pooling operations in CNNs. Let f denote a feature map produced by some layer of a CNN. It can be described as a 3-dimensional array of size W × H × N where W and H are the width and height and N is the number of channels (in case f is the output of a convolutional layer, N is the number of filters in this layer). Then p-norm subsampling (or pooling) with pooling size k (or half-length k/2) and stride r applied to the feature map f is a 3-dimensional array s(f ) with the following entries:</p><formula xml:id="formula_0">s i,j,u (f ) =   k/2 h=− k/2 k/2 w=− k/2 |f g(h,w,i,j,u) | p   1/p ,<label>(1)</label></formula><p>where g(h, w, i, j, u) = (r · i + h, r · j + w, u) is the function mapping from positions in s to positions in f respecting the stride, p is the order of the p-norm (for p → ∞, it becomes the commonly used max pooling). If r &gt; k, pooling regions do not overlap; however, current CNN architectures typically include overlapping pooling with k = 3 and r = 2. Let us now compare the pooling operation defined by Eq. 1 to the standard definition of a convolutional layer c applied to feature map f given as:</p><formula xml:id="formula_1">c i,j,o (f ) = σ   k/2 h=− k/2 k/2 w=− k/2 N u=1 θ h,w,u,o · f g(h,w,i,j,u)   ,<label>(2)</label></formula><p>where θ are the convolutional weights (or the kernel weights, or filters), σ(·) is the activation function, typically a rectified linear activation ReLU σ(x) = max(x, 0), and o ∈ [1, M ] is the number of output feature (or channel) of the convolutional layer. When formalized like this it becomes clear that both operations depend on the same elements of the previous layer feature map. The pooling layer can be seen as performing a feature-wise convolution 1 in which the activation function is replaced by the p-norm. One can therefore ask the question whether and why such special layers need to be introduced into the network. While a complete answer of this question is not easy to give (see the experiments and discussion for further details and remarks) we assume that in general there exist three possible explanations why pooling can help in CNNs: 1) the p-norm makes the representation in a CNN more invariant; 2) the spatial dimensionality reduction performed by pooling makes covering larger parts of the input in higher layers possible; 3) the feature-wise nature of the pooling operation (as opposed to a convolutional layer where features get mixed) could make optimization easier. Assuming that only the second part -the dimensionality reduction performed by poolingis crucial for achieving good performance with CNNs (a hypothesis that we later test in our experiments) one can now easily see that pooling can be removed from a network without abandoning the spatial dimensionality reduction by two means:</p><p>1. We can remove each pooling layer and increase the stride of the convolutional layer that preceded it accordingly.</p><p>2. We can replace the pooling layer by a normal convolution with stride larger than one (i.e. for a pooling layer with k = 3 and r = 2 we replace it with a convolution layer with corresponding stride and kernel size and number of output channels equal to the number of input channels)</p><p>The first option has the downside that we significantly reduce the overlap of the convolutional layer that preceded the pooling layer. It is equivalent to a pooling operation in which only the top-left feature response is considered and can result in less accurate recognition. The second option does not suffer from this problem, since all existing convolutional layers stay unchanged, but results in an increase of overall network parameters. It is worth noting that replacing pooling by convolution adds inter-feature dependencies unless the weight matrix θ is constrained. We emphasize that that this replacement can also be seen as learning the pooling operation rather than fixing it; which has previously been considered using different parameterizations in the literature 2 <ref type="bibr" target="#b13">(LeCun et al., 1998;</ref><ref type="bibr" target="#b6">Gülçehre et al., 2014;</ref><ref type="bibr" target="#b9">Jia et al., 2012)</ref>. We will evaluate both options in our experiments, ensuring a fair comparison w.r.t. the number of network parameters. Although we are not aware of existing studies containing such controlled experiments on replacing pooling with convolution layers it should be noted that the idea of removing pooling is not entirely unprecedented: First, the nomenclature in early work on CNNs LeCun et al. (1998) (referring to pooling layers as subsampling layers already) suggests the usage of different operations for subsampling. Second, albeit only considering small networks, experiments on using only convolution layers (with occasional subsampling) in an architecture similar to traditional CNNs already appeared in work on the "neural abstraction pyramid" <ref type="bibr" target="#b0">Behnke (2003)</ref>.</p><p>The second difference of the network model we consider to standard CNNs is that -similar to models recently used for achieving state-of-the-art performance in the ILSVRC-2012 competition (Simonyan &amp; Zisserman, 2014; Szegedy et al., 2014) -we make use of small convolutional layers with k &lt; 5 which can greatly reduce the number of parameters in a network and thus serve as a form of regularization. Additionally, to unify the architecture further, we make use of the fact that if the image area covered by units in the topmost convolutional layer covers a portion of the image large enough to recognize its content (i.e. the object we want to recognize) then fully connected layers can also be replaced by simple 1-by-1 convolutions. This leads to predictions of object classes at different positions which can then simply be averaged over the whole image. This scheme was first described by <ref type="bibr" target="#b15">Lin et al. (2014)</ref> and further regularizes the network as the one by one convolution has much less parameters than a fully connected layer. Overall our architecture is thus reduced to consist only of convolutional layers with rectified linear non-linearities and an averaging + softmax layer to produce predictions over the whole image. <ref type="table">Table 1</ref>: The three base networks used for classification on CIFAR-10 and CIFAR-100.</p><formula xml:id="formula_2">Model A B C Input 32 × 32 RGB image 5 × 5 conv. 96 ReLU 5 × 5 conv. 96 ReLU 3 × 3 conv. 96 ReLU 1 × 1 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 max-pooling stride 2 5 × 5 conv. 192 ReLU 5 × 5 conv. 192 ReLU 3 × 3 conv. 192 ReLU 1 × 1 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 max-pooling stride 2 3 × 3 conv. 192 ReLU 1 × 1 conv. 192 ReLU 1 × 1 conv. 10</formula><p>ReLU global averaging over 6 × 6 spatial dimensions 10 or 100-way softmax</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In order to quantify the effect of simplifying the model architecture we perform experiments on three datasets: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b11">(Krizhevsky &amp; Hinton, 2009</ref>) and ILSVRC-2012 ImageNet <ref type="bibr" target="#b2">(Deng et al., 2009</ref>) . Specifically, we use CIFAR-10 to perform an in-depth study of different models, since a large model on this dataset can be trained with moderate computing costs of ≈ 10 hours on a modern GPU. We then test the best model found on CIFAR-10 and CIFAR-100 with and without augmentations and perform a first preliminary experiment on the ILSVRC-2012 ImageNet dataset. We performed all experiments using the Caffe (Jia et al., 2014) framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EXPERIMENTAL SETUP</head><p>In experiments on CIFAR-10 and CIFAR-100 we use three different base network models which are intended to reflect current best practices for setting up CNNs for object recognition. Architectures of these networks are described in <ref type="table">Table 1</ref>. Starting from model A (the simplest model) the depth and number of parameters in the network gradually increases to model C. Several things are to be noted here. First, as described in the table, all base networks we consider use a 1-by-1 convolution at the top to produce 10 outputs of which we then compute an average over all positions and a softmax to produce class-probabilities (see Section 2 for the rationale behind this approach). We performed additional experiments with fully connected layers instead of 1-by-1 convolutions but found these models to consistently perform 0.5% − 1% worse than their fully convolutional counterparts. This is in line with similar findings from prior work <ref type="bibr" target="#b15">(Lin et al., 2014)</ref>. We hence do not report these numbers here to avoid cluttering the experiments. Second, it can be observed that model B from the table is a variant of the Network in Network architecture proposed by <ref type="bibr" target="#b15">Lin et al. (2014)</ref> in which only one 1-by-1 convolution is performed after each "normal" convolution layer. Third, model C replaces all 5 × 5 convolutions by simple 3 × 3 convolutions. This serves two purposes: 1) it unifies the architecture to consist only of layers operating on 3 × 3 spatial neighborhoods of the previous layer feature map (with occasional subsampling); 2) if max-pooling is replaced by a convolutional layer, then 3 × 3 is the minimum filter size to allow overlapping convolution with stride 2. We also highlight that model C resembles the very deep models used by  in this years ImageNet competition. <ref type="table">Table 2</ref>: Model description of the three networks derived from base model C used for evaluating the importance of pooling in case of classification on CIFAR-10 and CIFAR-100. The derived models for base models A and B are built analogously. The higher layers are the same as in <ref type="table">Table 1</ref> .</p><formula xml:id="formula_3">Model Strided-CNN-C ConvPool-CNN-C All-CNN-C Input 32 × 32 RGB image 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU with stride r = 2 3 × 3 conv. 96 ReLU 3 × 3 max-pooling stride 2 3 × 3 conv. 96 ReLU with stride r = 2 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU with stride r = 2 3 × 3 conv. 192 ReLU 3 × 3 max-pooling stride 2 3 × 3 conv. 192 ReLU with stride r = 2 . . .</formula><p>For each of the base models we then experiment with three additional variants. The additional (derived) models for base model C are described in in <ref type="table">Table 2</ref>. The derived models for base models A and B are built analogously but not shown in the table to avoid cluttering the paper. In general the additional models for each base model consist of:</p><p>• A model in which max-pooling is removed and the stride of the convolution layers preceding the max-pool layers is increased by 1 (to ensure that the next layer covers the same spatial region of the input image as before). This is column "Strided-CNN-C" in the table.</p><p>• A model in which max-pooling is replaced by a convolution layer. This is column "All-CNN-C" in the table. • A model in which a dense convolution is placed before each max-pooling layer (the additional convolutions have the same kernel size as the respective pooling layer). This is model "ConvPool-CNN-C" in the table. Experiments with this model are necessary to ensure that the effect we measure is not solely due to increasing model size when going from a "normal" CNN to its "All-CNN" counterpart.</p><p>Finally, to test whether a network solely using convolutions also performs well on a larger scale recognition problem we trained an up-scaled version of ALL-CNN-B on the ILSVRC 2012 part of the ImageNet database. Although we expect that a larger network using only 3 × 3 convolutions and having stride 1 in the first layer (and thus similar in style to ) would perform even better on this dataset, training it would take several weeks and could thus not be completed in time for this manuscript. <ref type="table">Table 3</ref>: Comparison between the base and derived models on the CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CLASSIFICATION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">CIFAR-10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 classification error</head><formula xml:id="formula_4">Model Error (%) # parameters without data augmentation Model A 12.47% ≈ 0.9 M Strided-CNN-A 13.46% ≈ 0.9 M ConvPool-CNN-A 10.21% ≈ 1.28 M ALL-CNN-A 10.30% ≈ 1.28 M Model B 10.20% ≈ 1 M Strided-CNN-B 10.98% ≈ 1 M ConvPool-CNN-B 9.33% ≈ 1.35 M ALL-CNN-B 9.10% ≈ 1.35 M Model C 9.74% ≈ 1.3 M Strided-CNN-C 10.19% ≈ 1.3 M ConvPool-CNN-C 9.31% ≈ 1.4 M ALL-CNN-C 9.08% ≈ 1.4 M</formula><p>In our first experiment we compared all models from Section 3.1 on the CIFAR-10 dataset without using any augmentations. All networks were trained using stochastic gradient descent with fixed momentum of 0.9. The learning rate γ was adapted using a schedule S = e 1 , e 2 , e 3 in which γ is multiplied by a fixed multiplier of 0.1 after e 1 .e 2 and e 3 epochs respectively. To keep the amount of computation necessary to perform our comparison bearable 3 we only treat γ as a changeable hyperparameter for each method. The learning rate schedule and the total amount of training epochs were determined in a preliminary experiment using base model A and then fixed for all other experiments. We used S = [200, 250, 300] and trained all networks for a total of 350 epochs. It should be noted that this strategy is not guaranteed to result in the best performance for all methods and thus care must be taken when interpreting the the following results from our experiments. The learning rate γ was individually adapted for each model by searching over the fixed set γ ∈ [0.25, 0.1, 0.05, 0.01].</p><p>In the following we only report the results for the best γ for each method. Dropout  was used to regularize all networks. We applied dropout to the input image as well as after each pooling layer (or after the layer replacing the pooling layer respectively). The dropout probabilities were 20% for dropping out inputs and 50% otherwise. We also experimented with additional dropout (i.e. dropout on all layers or only on the 1 × 1 convolution layer) which however did not result in increased accuracy 4 . Additionally all models were regularized with weight decay λ = 0.001. In experiments with data augmentation we perform only the augmentations also used in previous work <ref type="bibr" target="#b4">(Goodfellow et al., 2013;</ref><ref type="bibr" target="#b15">Lin et al., 2014)</ref> in order to keep our results comparable. These include adding horizontally flipped examples of all images as well as randomly translated versions (with a maximum translation of 5 pixels in each dimension). In all experiments images were whitened and contrast normalized following <ref type="bibr" target="#b4">Goodfellow et al. (2013)</ref>.</p><p>The results for all models that we considered are given in <ref type="table">Table 3</ref>. Several trends can be observed from the table. First, confirming previous results from the literature <ref type="bibr" target="#b20">(Srivastava et al., 2014)</ref> the simplest model (model A) already performs remarkably well, achieving 12.5% error. Second, simply removing the max-pooling layer and just increasing the stride of the previous layer results in diminished performance in all settings. While this is to be expected we can already see that the drop in performance is not as dramatic as one might expect from such a drastic change to the network architecture. Third, surprisingly, when pooling is replaced by an additional convolution layer with stride r = 2 performance stabilizes and even improves on the base model. To check that this is not only due to an increase in the number of trainable parameters we compare the results to the "ConvPool" versions of the respective base model. In all cases the performance of the model without any pooling and the model with pooling on top of the additional convolution perform about on par. Surprisingly, this suggests that while pooling can help to regularize CNNs, and generally does not hurt performance, it is not strictly necessary to achieve state-of-the-art results (at least for current small scale object recognition datasets). In addition, our results confirm that small 3 × 3 convolutions stacked after each other seem to be enough to achieve the best performance.</p><p>Perhaps even more interesting is the comparison between the simple all convolutional network derived from base model C and the state of the art on CIFAR-10 shown in <ref type="table" target="#tab_0">Table 4</ref> , both with and without data augmentation. In both cases the simple network performs better than the best previously reported result. This suggests that in order to perform well on current benchmarks "almost all you need" is a stack of convolutional layers with occasional stride of 2 to perform subsampling.  <ref type="bibr" target="#b5">(Graham, 2015)</ref>. The number of parameters is given in million parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 classification error</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">CIFAR-100</head><p>We performed an additional experiment on the CIFAR-100 dataset to confirm the efficacy of the best model (the All-CNN-C) found for CIFAR-10. As is common practice we used the same model as on CIFAR-10 and also kept all hyperparameters (the learning rate as well as its schedule) fixed. Again note that this does not necessarily give the best performance. The results of this experiment are given in <ref type="table" target="#tab_0">Table 4</ref> (right). As can be seen, the simple model using only 3 × 3 convolutions again performs comparable to the state of the art for this dataset even though most of the other methods either use more complicated training schemes or network architectures. It is only outperformed by the fractional max-pooling approach <ref type="bibr" target="#b5">(Graham, 2015)</ref> which uses a much larger network (on the order of 50M parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">CIFAR-10 WITH ADDITIONAL DATA AUGMENTATION</head><p>After performing our experiments we became aware of recent results by <ref type="bibr" target="#b5">Graham (2015)</ref> who report a new state of the art on CIFAR-10/100 with data augmentation. These results were achieved using very deep CNNs with 2 × 2 convolution layers in combination with aggressive data augmentation in which the 32 × 32 images are placed into large 126 × 126 pixel images and can hence be heavily scaled, rotated and color augmented. We thus implemented the Large-All-CNN, which is the all convolutional version of this network (see <ref type="table" target="#tab_2">Table 5</ref> in the appendix for details) and report the results of this additional experiment in Table 4 (bottom right). As can be seen, Large-All-CNN achieves performance comparable to the network with max-pooling. It is only outperformed by the fractional max-pooling approach when performing multiple passes through the network. Note that these networks have vastly more parameters (&gt; 50 M) than the networks from our previous experiments. We are currently re-training the Large-All-CNN network on CIFAR-100, and will include the results in <ref type="table" target="#tab_0">Table 4</ref> once training is finished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CLASSIFICATION OF IMAGENET</head><p>We performed additional experiments using the ILVRC-2012 subset of the ImageNet dataset. Since training a state of the art model on this dataset can take several weeks of computation on a modern GPU, we did not aim for best performance, but rather performed a simple 'proof of concept' experiment. To test if the architectures performing best on CIFAR-10 also apply to larger datasets, we trained an upscaled version of the All-CNN-B network (which is also similar to the architecture proposed by <ref type="bibr" target="#b15">Lin et al. (2014)</ref>). It has 12 convolutional layers (conv1-conv12) and was trained for 450, 000 iterations with batches of 64 samples each, starting with a learning rate of γ = 0.01 and dividing it by 10 after every 200, 000 iterations. A weight decay of λ = 0.0005 was used in all layers. The exact architecture used is given in <ref type="table" target="#tab_3">Table 6</ref> in the Appendix.</p><p>This network achieves a Top-1 validation error of 41.2% on ILSVRC-2012, when only evaluating on the center 224 × 224 patch, -which is comparable to the 40.7% Top-1 error reported by <ref type="bibr" target="#b12">Krizhevsky et al. (2012)</ref> -while having less than 10 million parameters (6 times less than the network of <ref type="bibr" target="#b12">Krizhevsky et al. (2012)</ref>) and taking roughly 4 days to train on a Titan GPU. This supports our intuition that max-pooling may not be necessary for training large-scale convolutional networks. However, a more thorough analysis is needed to precisely evaluate the effect of max-pooling on ImageNet-scale networks. Such a complete quantitative analysis using multiple networks on Ima-geNet is extremely computation-time intensive and thus out of the scope of this paper. In order to still gain some insight into the effects of getting rid of max-pooling layers, we will try to analyze the representation learned by the all convolutional network in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DECONVOLUTION</head><p>In order to analyze the network that we trained on ImageNet -and get a first impression of how well the model without pooling lends itself to approximate inversion -we use a 'deconvolution' approach. We start from the idea of using a deconvolutional network for visualizing the parts of an image that are most discriminative for a given unit in a network, an approach recently proposed by <ref type="bibr" target="#b26">Zeiler &amp; Fergus (2014)</ref>. Following this initial attempt -and observing that it does not always work well without max-pooling layers -we propose a new and efficient way of visualizing the concepts learned by higher network layers.</p><p>The deconvolutional network ('deconvnet') approach to visualizing concepts learned by neurons in higher layers of a CNN can be summarized as follows. Given a high-level feature map, the 'deconvnet' inverts the data flow of a CNN, going from neuron activations in the given layer down to an image. Typically, a single neuron is left non-zero in the high level feature map. Then the resulting reconstructed image shows the part of the input image that is most strongly activating this neuron (and hence the part that is most discriminative to it). A schematic illustration of this procedure is shown in <ref type="figure">Figure 1 a)</ref>. In order to perform the reconstruction through max-pooling layers, which are in general not invertible, the method of Zeiler and Fergus requires first to perform a forward pass of the network to compute 'switches' -positions of maxima within each pooling region. These switches are then used in the 'deconvnet' to obtain a discriminative reconstruction. By using the switches from a forward pass the 'deconvnet' (and thereby its reconstruction) is hence conditioned on an image and does not directly visualize learned features. Our architecture does not include maxpooling, meaning that in theory we can 'deconvolve' without switches, i.e. not conditioning on an input image. This way we get insight into what lower layers of the network learn. Visualizations of a) b) c) activation: backpropagation:</p><p>backward 'deconvnet': guided backpropagation: <ref type="figure">Figure 1</ref>: Schematic of visualizing the activations of high layer neurons. a) Given an input image, we perform the forward pass to the layer we are interested in, then set to zero all activations except one and propagate back to the image to get a reconstruction. b) Different methods of propagating back through a ReLU nonlinearity. c) Formal definition of different methods for propagating a output activation out back through a ReLU unit in layer l; note that the 'deconvnet' approach and guided backpropagation do not compute a true gradient but rather an imputed version.</p><p>features from the first three layers are shown in <ref type="figure" target="#fig_0">Figure 2</ref> . Interestingly, the very first layer of the network does not learn the usual Gabor filters, but higher layers do.</p><p>For higher layers of our network the method of Zeiler and Fergus fails to produce sharp, recognizable, image structure. This is in agreement with the fact that lower layers learn general features with limited amount of invariance, which allows to reconstruct a single pattern that activates them. However, higher layers learn more invariant representations, and there is no single image maximally activating those neurons. Hence to get reasonable reconstructions it is necessary to condition on an input image.</p><p>An alternative way of visualizing the part of an image that most activates a given neuron is to use a simple backward pass of the activation of a single neuron after a forward pass through the network; thus computing the gradient of the activation w.r.t. the image. The backward pass is, by design, partially conditioned on an image through both the activation functions of the network and the maxpooling switches (if present). The connections between the deconvolution and the backpropagation conv1 conv2 conv3 approach were recently discussed in . In short the both methods differ mainly in the way they handle backpropagation through the rectified linear (ReLU) nonlinearity.</p><p>In order to obtain a reconstruction conditioned on an input image from our network without pooling layers we propose a modification of the 'deconvnet', which makes reconstructions significantly more accurate, especially when reconstructing from higher layers of the network. The 'deconvolution' is equivalent to a backward pass through the network, except that when propagating through a nonlinearity, its gradient is solely computed based on the top gradient signal, ignoring the bottom input. In case of the ReLU nonlinearity this amounts to setting to zero certain entries based on the top gradient. The two different approaches are depicted in <ref type="figure">Figure 1 b)</ref>, rows 2 and 3. We propose to combine these two methods: rather than masking out values corresponding to negative entries of the top gradient ('deconvnet') or bottom data (backpropagation), we mask out the values for which at least one of these values is negative, see row 4 of <ref type="figure">Figure 1 b)</ref>. We call this method guided backpropagation, because it adds an additional guidance signal from the higher layers to usual backpropagation. This prevents backward flow of negative gradients, corresponding to the neurons which decrease the activation of the higher layer unit we aim to visualize. Interestingly, unlike the 'deconvnet', guided backpropagation works remarkably well without switches, and hence allows us to visualize intermediate layers <ref type="figure" target="#fig_2">(Figure 3)</ref>  To compare guided backpropagation and the 'deconvnet' approach, we replace the stride in our network by 2 × 2 max-pooling after training, which allows us to obtain the values of switches. We then visualize high level activations using three methods: backpropagation, 'deconvnet' and guided backpropagation. A striking difference in image quality is visible in the feature visualizations of the highest layers of the network, see <ref type="figure">Figures 4</ref> and 5 in the Appendix. Guided backpropagation works equally well with and without switches, while the 'deconvnet' approach fails completely in the absence of switches. One potential reason why the 'deconvnet' underperforms in this experiment is that max-pooling was only 'artificially' introduced after training. As a control <ref type="figure">Figure 6</ref> shows visualizations of units in the fully connected layer of a network initially trained with max-pooling. Again guided backpropagation produces cleaner visualizations than the 'deconvnet' approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>To conclude, we highlight a few key observations that we made in our experiments:</p><p>• With modern methods of training convolutional neural networks very simple architectures may perform very well: a network using nothing but convolutions and subsampling matches or even slightly outperforms the state of the art on CIFAR-10 and CIFAR-100. A similar architecture shows competitive results on ImageNet.</p><p>• In particular, as opposed to previous observations, including explicit (max-)pooling operations in a network does not always improve performance of CNNs. This seems to be especially the case if the network is large enough for the dataset it is being trained on and can learn all necessary invariances just with convolutional layers.</p><p>• We propose a new method of visualizing the representations learned by higher layers of a convolutional network. While being very simple, it produces sharper visualizations of descriptive image regions than the previously known methods, and can be used even in the absence of 'switches' -positions of maxima in max-pooling regions.</p><p>We want to emphasize that this paper is not meant to discourage the use of pooling or more sophisticated activation functions altogether. It should rather be understood as an attempt to both search for the minimum necessary ingredients for recognition with CNNs and establish a strong baseline on often used datasets. We also want to stress that the results of all models evaluated in this paper could potentially be improved by increasing the overall model size or a more thorough hyperparameter search. In a sense this fact makes it even more surprising that the simple model outperforms many existing approaches. Each row corresponds to one filter. The visualization using "guided backpropagation" is based on the top 10 image patches activating this filter taken from the ImageNet dataset. Note that image sizes are not preserved (in order to save space).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A LARGE ALL-CNN MODEL FOR CIFAR-10</head><p>The complete model architecture for the large All-CNN derived from the spatially sparse network of Benjamin Graham (see <ref type="bibr" target="#b5">Graham (2015)</ref> for an explanation) is givenin <ref type="table" target="#tab_2">Table 5</ref> . Note that the network uses leaky ReLU units instead of ReLUs as we found these to speed up training. As can be seen it also requires a much larger input size in which the 32 × 32 pixel image is centered (and then potentially augmented by applying multiple transformations such as scaling). As a result the subsampling performed by the convolutional layers with stride 2 can hence be applied much more slowly. Also note that this network only consists of 2 × 2 convolutions with occasional subsampling until the spatial dimensionality is reduced to 1 × 1. It does hence not employ global average pooling at the end of the network. In a sense this architecture hence represents the most simple convolutional network usable for this task. 2 × 2 conv. 320 LeakyReLU, stride 1 conv2 2 × 2 conv. 320 LeakyReLU, stride 1 conv3 2 × 2 conv. 320 LeakyReLU, stride 2 conv4 2 × 2 conv. 640 LeakyReLU, stride 1, dropout 0.1 conv5 2 × 2 conv. 640 LeakyReLU, stride 1, dropout 0.1 conv6 2 × 2 conv. 640 LeakyReLU, stride 2 conv7 2 × 2 conv. 960 LeakyReLU, stride 1, dropout 0.2 conv8 2 × 2 conv. 960 LeakyReLU, stride 1, dropout 0.2 conv9 2 × 2 conv. 960 LeakyReLU, stride 2 conv10 2 × 2 conv. 1280 LeakyReLU, stride 1, dropout 0.3 conv11 2 × 2 conv. 1280 LeakyReLU, stride 1, dropout 0.3 conv12 2 × 2 conv. 1280 LeakyReLU, stride 2 conv13 2 × 2 conv. 1600 LeakyReLU, stride 1, dropout 0.4 conv14 2 × 2 conv. 1600 LeakyReLU, stride 1, dropout 0.4 conv15 2 × 2 conv. 1600 LeakyReLU, stride 2 conv16 2 × 2 conv. 1920 LeakyReLU, stride 1, dropout 0.5 conv17 1 × 1 conv. 1920 LeakyReLU, stride 1, dropout 0.5 softmax 10-way softmax</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMAGENET MODEL</head><p>The complete model architecture for the network trained on the ILSVRC-2102 ImageNet dataset is given in <ref type="table" target="#tab_3">Table 6</ref> .  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visualizations of patterns learned by the lower layers (conv1-conv3) of the network trained on ImageNet. Each single patch corresponds to one filter. Interestingly, Gabor filters only appear in the third layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>as well as the last layers of our network (Figures 4 and 5 in the Appendix). In a sense, the bottom-up signal in form of the pattern of bottom ReLU activations substitutes the switches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of patterns learned by the layer conv6 (top) and layer conv9 (bottom) of the network trained on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>the features learned by the last convolutional layer 'conv12' as well as the pre-softmax layer 'global pool' are depicted inFigure 4andFigure 5respectively. To allow fair comparison of 'deconvnet' and guided backpropagation, we additionally show inFigure 6visualizations from a model with max-pooling trained on ImageNet. Visualization of descriptive image regions with different methods from the single largest activation in the last convolutional layer conv12 of the network trained on ImageNet. Reconstructions for 4 different images are shown. Visualization of descriptive image regions with different methods from the single largest activation in the pre-softmax layer global pool of the network trained on ImageNet.backpropagation 'deconvnet' guided backpropagation Visualization of descriptive image regions with different methods from the single largest activation in the last layer fc8 of the Caffenet reference network trained on Ima-geNet. Reconstructions for 4 different images are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>The</cell></row></table><note>Test error on CIFAR-10 and CIFAR-100 for the All-CNN compared to the state of the art from the literature. The All-CNN is the version adapted from base model C (i.e. All-CNN-C).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Architecture of the Large All-CNN network for CIFAR-10.</figDesc><table><row><cell></cell><cell>Large All-CNN for CIFAR-10</cell></row><row><cell>Layer name</cell><cell>Layer description</cell></row><row><cell>input</cell><cell>Input 126 × 126 RGB image</cell></row><row><cell>conv1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>Architecture of the ImageNet network.</figDesc><table><row><cell></cell><cell>ImageNet model</cell></row><row><cell>Layer name</cell><cell>Layer description</cell></row><row><cell>input</cell><cell>Input 224 × 224 RGB image</cell></row><row><cell>conv1</cell><cell>11 × 11 conv. 96 ReLU units, stride 4</cell></row><row><cell>conv2</cell><cell>1 × 1 conv. 96 ReLU, stride 1</cell></row><row><cell>conv3</cell><cell>3 × 3 conv. 96 ReLU, stride 2</cell></row><row><cell>conv4</cell><cell>5 × 5 conv. 256 ReLU, stride 1</cell></row><row><cell>conv5</cell><cell>1 × 1 conv. 256 ReLU, stride 1</cell></row><row><cell>conv6</cell><cell>3 × 3 conv. 256 ReLU, stride 2</cell></row><row><cell>conv7</cell><cell>3 × 3 conv. 384 ReLU, stride 1</cell></row><row><cell>conv8</cell><cell>1 × 1 conv. 384 ReLU, stride 1</cell></row><row><cell>conv9</cell><cell>3 × 3 conv. 384 ReLU, stride 2, dropout 50 %</cell></row><row><cell>conv10</cell><cell>3 × 3 conv. 1024 ReLU, stride 1</cell></row><row><cell>conv11</cell><cell>1 × 1 conv. 1024 ReLU, stride 1</cell></row><row><cell>conv12</cell><cell>1 × 1 conv. 1000 ReLU, stride 1</cell></row><row><cell>global pool</cell><cell>global average pooling (6 × 6)</cell></row><row><cell>softmax</cell><cell>1000-way softmax</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">That is, a convolution where θ h,w,u,o = 1 if u equals o and zero otherwise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although in order to implement "proper pooling" in the same sense as commonly considered in the literature a special nonlinearity (e.g. a squaring operation) needs to be considered. A simple convolution layer with rectified linear activation cannot by itself implement a p-norm computation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Training one network on CIFAR-10 can take up to 10 hours on a modern GPU.4  In the case were dropout of 0.5 is applied to all layers accuracy even dropped, suggesting that the gradients become too noisy in this case</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We acknowledge funding by the ERC Starting Grant VideoLearn (279401); the work was also partly supported by the BrainLinks-BrainTools Cluster of Excellence funded by the German Research Foundation (DFG, grant number EXC 1086).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Hierarchical neural networks for image interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High-performance neural networks for visual object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ueli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arxiv:cs/arXiv:1102.0183</idno>
		<ptr target="http://arxiv.org/abs/1102.0183" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signal recovery from pooling representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">B</forename><surname>Estrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yoshua. Maxout networks. In ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fractional max-pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arxiv:cs/arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learned-norm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyunghyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<idno>arxiv:cs/1207.0580v3</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc&amp;apos;aurelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: Receptive field learning for pooled image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saining</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Representation Learning Workshop, NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR: Conference Track</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arxiv:cs/arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>1312.6034</idno>
		<ptr target="http://arxiv.org/abs/1312.6034" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving deep neural networks with probabilistic maxout units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6116</idno>
		<ptr target="http://arxiv.org/abs/1312.6116" />
	</analytic>
	<monogr>
		<title level="m">ICLR: Workshop Track</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compete to compute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kazerounian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohrob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep networks with internal selective attention through feedback connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marijn</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arxiv:cs/arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sixin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
