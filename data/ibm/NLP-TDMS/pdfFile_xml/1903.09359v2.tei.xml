<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Face Reconstruction from A Single Image Assisted by 2D Face Images in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
							<email>zhaojian90@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<addrLine>5 CASIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Xie</surname></persName>
							<email>mxie@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
							<email>zhaoyang10@nudt.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>He</surname></persName>
							<email>lingxiao.he@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D Face Reconstruction from A Single Image Assisted by 2D Face Images in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Dense face alignment (odd rows) and 3D face reconstruction (even rows) results from our proposed method. For alignment, only 68 key points are plotted for clear display; for 3D reconstruction, reconstructed shapes are rendered with head light for better view. Our method offers strong robustness and good performance even in presence of large poses (the 3th, 4th and 5th columns) and occlusions (the 6th, 7th and 8th columns). Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>3D face reconstruction from a single 2D image is a challenging problem with broad applications. Recent methods typically aim to learn a CNN-based 3D face model that regresses coefficients of 3D Morphable Model (3DMM) from 2D images to render 3D face reconstruction or dense face alignment. However, the shortage of training data with 3D annotations considerably limits performance of those methods. To alleviate this issue, we propose a novel 2D-assisted self-supervised learning (2DASL) method that can effectively use "in-the-wild" 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmarks as additional information, 2DSAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the 2D and 3D landmark self-prediction consistency, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark predictions. Using these four self-supervision schemes, the 2DASL method significantly relieves demands on the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on multiple challenging datasets show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Dense face alignment (odd rows) and 3D face reconstruction (even rows) results from our proposed method. For alignment, only 68 key points are plotted for clear display; for 3D reconstruction, reconstructed shapes are rendered with head light for better view. Our method offers strong robustness and good performance even in presence of large poses (the 3th, 4th and 5th columns) and occlusions (the 6th, 7th and 8th columns). Best viewed in color.</p><p>Abstract 3D face reconstruction from a single 2D image is a challenging problem with broad applications. Recent methods typically aim to learn a CNN-based 3D face model that regresses coefficients of 3D Morphable Model (3DMM) from 2D images to render 3D face reconstruction or dense face alignment. However, the shortage of training data with 3D annotations considerably limits performance of those methods. To alleviate this issue, we propose a novel 2D-assisted self-supervised learning (2DASL) method that can effectively use "in-the-wild" 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmarks as additional information, 2DSAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the 2D and 3D landmark self-prediction consistency, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark predictions. Using these four self-supervision schemes, the 2DASL method significantly relieves demands on the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on multiple challenging datasets show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D face reconstruction is an important task in the field of computer vision and graphics. For instance, the recovery of 3D face geometry from a single image can help address many challenges (e.g., large pose and occlusion) for 2D face alignment through dense face alignment <ref type="bibr" target="#b29">[30]</ref>. Traditional 3D face reconstruction methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref> are mainly based on optimization algorithms, e.g., iterative closest point <ref type="bibr" target="#b1">[2]</ref>, to obtain coefficients for the 3D Morphable Model (3DMM) model and render the corresponding 3D faces from a single face image <ref type="bibr" target="#b48">[49]</ref>. However, such methods are usually time-consuming due to the high optimization complexity and suffer from local optimal solution and bad initialization. Recent works thus propose to use CNNs to learn to regress the 3DMM coefficients and significantly improve the reconstruction quality and efficiency.</p><p>CNN-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45]</ref> have achieved remarkable success in 3D face reconstruction and dense face alignment. However, obtaining an accurate 3D face CNN regression model (from input 2D images to 3DMM coefficients) requires a large amount of training faces with 3D annotations, which are expensive to collect and even not achievable in some cases. Even some 3D face datasets, like 300W-LP <ref type="bibr" target="#b48">[49]</ref>, are publicly available, they generally lack diversity in face appearance, expression, occlusions and environment conditions, limiting the generalization performance of resulted 3D face regression models. A model trained on such datasets cannot deal well with various potential cases in-the-wild that are not present in the training examples. Although some recent works bypass the 3DMM parameter regression and use image-to-volume <ref type="bibr" target="#b19">[20]</ref> or image-to-image <ref type="bibr" target="#b14">[15]</ref> strategy instead, the ground truths are all still needed and generated from 3DMM using 300W-LP, still lacking diversity.</p><p>In order to overcome the intrinsic limitation of existing 3D face recovery models, we propose a novel learning method that leverages 2D "in-the-wild" face images to effectively supervise and facilitate the 3D face model learning. With the method, the trained 3D face model can perform 3D face reconstruction and dense face alignment well. This is inspired by the observation that a large number of 2D face datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref> are available with obtainable 2D landmark annotations, that could provide valuable information for 3D model learning, without requiring new data with 3D annotations.</p><p>Since these 2D images do not have any 3D annotations, it is not straightforward to exploit them in 3D face model learning. We design a novel self-supervised learning method that is able to train a 3D face model with weak supervision from 2D images. In particular, the proposed method takes the sparse annotated 2D landmarks as input and fully leverage the consistency within the 2Dto-2D and 3D-to-3D self-mapping procedure as supervi-sion. The model should be able to recover 2D landmarks from predicted 3D ones via direct 3D-to-2D projection. Meanwhile, the 3D landmarks predicted from the annotated and recovered 2D landmarks via the model should be the same. Additionally, our proposed method also exploits cycle-consistency over the 2D landmark predictions, i.e., taking the recovered 2D landmarks as input, the model should be able to generate 2D landmarks (by projecting its predicted 3D landmarks) that have small difference with the annotated ones. By leveraging these self-supervision derived from 2D face images without 3D annotations, our method could substantially improve the quality of learned 3D face regression model, even though there is lack of 3D samples and no 3D annotations for the 2D samples. To facilitate the overall learning procedure, our method also exploits self-critic learning. It takes as input both the latent representation and 3DMM coefficients of an face image and learns a critic model to evaluate the intrinsic consistency between the predicted 3DMM coefficients and the corresponding face image, offering another supervision for 3D face model learning.</p><p>Our proposed method is principled, effective and fully exploits available data resources. As shown in <ref type="figure">Fig. 1</ref>, our method can produce 3D reconstruction and dense face alignment results with strong robustness to large poses and occlusions. Our code, models and online demos will be available upon acceptance. Our contributions are summarized as follows:</p><p>• We propose a new scheme that aims to fully utilize the abundant "in-the-wild" 2D face images to assist 3D face model learning. This is new and different from most common practices that pursues to improve 3D face model by collecting more data with 3D annotations for model training.</p><p>• We introduce a new method that is able to train 3D face models with 2D face images by self-supervised learning. The devised multiple forms of self-supervision are effective and data efficient.</p><p>• We develop a new self-critic learning based approach which could effectively improve the 3D face model learning procedure and give a better model, even though the 2D landmark annotations are noisy.</p><p>• Comparison on the AFLW2000-3D and AFLW-LFPA datasets shows that our method achieves excellent performance on both tasks of 3D face reconstruction and dense face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>3D Face Reconstruction Various approaches have been proposed to tackle the inherently ill-posed problem of 3D face reconstruction from a single image. In <ref type="bibr" target="#b6">[7]</ref>, Vetter and Blanz observe that both the geometric structure and the texture of human faces can be approximated by a linear combination of orthogonal basis vectors obtained by PCA over 100 male and 100 female identities. Based on this, they propose the 3DMM to represent the shape and texture of a 3D face. After that, large amount of efforts have been proposed to improve 3DMM modeling mechanism. Most of them devote to regressing the 3DMM coefficients by solving the non-linear optimization problem to establish the correspondences of the points between a single face image and the canonical 3D face model, including facial landmarks <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17]</ref> and local features <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39]</ref>. Recently, various attempts have been made to estimate the 3DMM coefficients from a single face image using CNN as a regressor, as opposed to non-linear optimization. In <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, cascaded CNN structures are used to regress the 3DMM coefficients, which are time-consuming due to multi-stage. Besides, end-to-end approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b22">23]</ref> are also proposed to directly estimate the 3DMM coefficients in a holistic manner. More recently, works are proposed to use CNN directly obtain the reconstructed 3D face bypassing the 3DMM coefficients regression. In <ref type="bibr" target="#b19">[20]</ref>, Jackson et al. propose to map the image pixels to a volumetric representation of the 3D facial geometry through CNN-based regression. While their method is not restricted to the 3DMM space any more, it needs a complex network structure and a lot of time to predict the voxel information. In a later work <ref type="bibr" target="#b14">[15]</ref>, Feng et al. store the 3D facial geometry into UV position map and train an imageto-image CNN to directly regress the complete 3D facial structure along with semantic information from a single image.</p><p>Face Alignment Traditional 2D face alignment methods aim at locating a sparse set of fiducial facial landmarks. Initial progresses have been made with the classic Active Appearance Model (AAM) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref> and Constrained Local Model (CLM) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref>. Recently, CNN-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8]</ref> have achieved state-of-the-art performance on 2D landmark localization. However, 2D face alignment only regresses visible landmarks on faces, which are unable to address large pose or occlusion situations, where partial face regions are invisible. With the development of this field, 3D face alignment have been proposed, aiming to fit a 3DMM <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16]</ref> or register a 3D facial template <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b12">13]</ref> to a 2D face image, which makes it possible to deal with the invisible points. The original 3DMM fitting method <ref type="bibr" target="#b5">[6]</ref> fits the 3D model by minimizing the pixel-wise difference between image and the rendered face model. It is the first method that can address arbitrary poses, which, however, suffers from the one-minute-per-image computational cost. After that, some methods estimate 3DMM coefficients and then project the estimated 3D landmarks onto 2D space, such methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b24">25]</ref> could significantly improve the efficiency. Recently, the task of dense face alignment starts to attract more and more research attention, aiming to achieve very dense 3D alignment for large pose face images (including invisible parts). In <ref type="bibr" target="#b29">[30]</ref>, Liu et al. use multi-constraints to train a CNN model, jointly estimating the 3DMM coefficient and provides very dense 3D alignment. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">48]</ref> directly learn the correspondence between a 2D face image and a 3D template via a deep CNN, while only visible face-region is considered. Overall, CNN-based methods have achieved great success in both 3D face reconstruction and dense face alignment. However, they need a huge amount of 3D annotated images for training. Unfortunately, currently face datasets with 3D annotations are very limited. As far as we know, only the 300W-LP <ref type="bibr" target="#b48">[49]</ref> dataset has been widely used for training. However, the 300W-LP is generated by profiling faces of 300W <ref type="bibr" target="#b39">[40]</ref> into larger poses, which is not strictly unconstrained and can not cover all possible scenes in-thewild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>In this section we introduce the proposed 2D-Aided Selfsupervised Learning (2DASL) method for simultaneous 3D face reconstruction and dense face alignment. We first review the popular 3D morphable model that we adopt to render the 3D faces. Then we explain our method in details, in particular the novel cycle-consistency based self-supervised learning and the self-critic learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D morphable model</head><p>We adopt the 3D morphable model (3DMM) <ref type="bibr" target="#b6">[7]</ref> to recover the 3D facial geometry from a single face image. The 3DMM renders 3D face shape S ∈ R 3N that stores 3D coordinates of N mesh vertices with linear combination over a set of PCA basis. Following <ref type="bibr" target="#b48">[49]</ref>, we use 40 basis from the Basel Face Model (BFM) <ref type="bibr" target="#b33">[34]</ref> to generate the face shape component and 10 basis from the Face Warehouse dataset <ref type="bibr" target="#b9">[10]</ref> to generate the facial expression component. The rendering of a 3D face shape is thus formulated as:</p><formula xml:id="formula_0">S = S + A s α s + A exp α exp ,</formula><p>where S ∈ R 3N is the mean shape, A s ∈ R 3N ×40 is the shape principle basis trained on the 3D face scans, α s ∈ R 40 is the shape representation coefficient; A exp ∈ R 3N ×10 is the expression principle basis and α exp ∈ R 10 denotes the corresponding expression coefficient. The target of singleimage based 3D face modeling is to predict the coefficients α exp and α s for 3D face rendering from a single 2D image.</p><p>After obtaining the 3D face shape S, it can be projected onto the 2D image plane with the scale orthographic projection to generate a 2D face from specified viewpoint:</p><formula xml:id="formula_1">V = f * P r * Π * S + t,</formula><p>where V stores the 2D coordinates of the 3D vertices projected onto the 2D plane, f is the scale factor, P r is the orthographic projection matrix 1 0 0 0 1 0 , Π is the projection matrix consisting of 9 parameters, and t is the translation vector. Putting them together, we have in total 62 pa-rameters α = [f, t, Π, α s , α exp ] to regress for the 3D face regressor model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model overview</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, the proposed 2DASL model contains 3 modules, i.e., a CNN-based regressor that predicts 3DMM coefficients from the input 2D image, an encoder that transforms the input image into a latent representation, and a self-critic that evaluates the input (latent representation, 3DMM coefficients) pairs to be consistent or not.</p><p>We use ResNet-50 <ref type="bibr" target="#b17">[18]</ref> to implement the CNN regressor. The encoder contains 6 convolutional layers, each followed by a ReLU and a max pooling layer. The critic consists of 4 fully-connected layers with 512, 1024, 1024 and 1 neurons respectively, followed by a softmax layer to output a score on the consistency degree of the input pair. The CNN regressor takes a 4-channel tensor as input that concatenates a 3-channel RGB face image and a 1-channel 2D Facial Landmark Map (FLM). The FLM is a binary-value image, where the locations corresponding to facial landmarks take the value of 1 and others take the value of −1.</p><p>Our proposed 2DSAL method trains the model using two sets of images, i.e., the images with 3DMM ground truth annotations and the 2D face images with only 2D facial landmark annotations provided by an off-the-shelf facial landmark detector <ref type="bibr" target="#b7">[8]</ref>. The model is trained by minimizing the following one conventional 3D-supervision and four selfsupervision losses.</p><p>The first one is the weighted coefficient prediction loss L 3d over the 3D annotated images that measures how accurate the model can predict 3DMM coefficients. The second one is the 2D landmark consistency loss L 2d-con that measures how well the predicted 3D face shapes can recover the 2D landmark locations for the input 2D images. The third one is the 3D landmark consistency loss L 3d-con . The fourth one is the cycle consistency loss L cyc . The last one is the self-critic loss L sc that estimates the realism of the predicted 3DMM coefficients for 3D face reconstruction, conditioned on the face latent representation. Thus the overall training loss is:</p><formula xml:id="formula_2">L = L 3d + λ 1 L 2d-con + λ 2 L 3d-con + λ 3 L cyc + λ 4 L sc ,</formula><p>where λ's are the weighting coefficients for different losses. The details of these losses are described in the following sections one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weighted 3DMM coefficient supervision</head><p>Following <ref type="bibr" target="#b48">[49]</ref>, we deploy the ground truth 3DMM coefficients to supervise the model training where the contribution of each 3DMM coefficient is re-weighted according to their importance. It trains the model to predict closer coefficientsα to its 3DMM ground truth α * . Instead of calculating the conventional 2 loss, we explicitly consider im-  <ref type="figure">Figure 3</ref>: Illustration on the self-supervision introduced by our 2DSAL for utilizing sparse 2D landmark information. The 2D landmark prediction can be viewed as a self-mapping: X 2d → Y 2d (forward training) constrained by L2d-con. To further supervise the model training, we introduce the Lcyc by mapping back from Y 2d →X 2d (backward training). The L3d-con is employed to constrain landmarks matching in 3D space during the cycle training.</p><p>Here i indexes the landmark. Best viewed in color.</p><p>portance of each coefficient and re-weigh their contribution to the loss computation accordingly. Thus we obtain the weighted coefficient prediction loss as follows:</p><formula xml:id="formula_3">L 3d = (α * −α) W (α * −α),</formula><p>where, W = diag(w 1 , . . . , w 62 ),</p><formula xml:id="formula_4">w i = 1 i w i H(α i ) − H(α * ) .</formula><p>Here w i indicates importance of the i th coefficient, computed from how much error it introduces to locations of 2D landmarks after projection. Here H(·) is the sparse landmark projection from rendered 3D shape, α * is the ground truth andα i is the coefficient whose i th element comes from the predicted parameter and the others come from α * . With such a reweighting scheme, during training, the CNN model would first focus on learning the coefficients with larger weight (e.g., the ones for rotation and translation). After decreasing their error and consequently their weights, the model will change to optimize the other coefficients (e.g., the ones for shape and expression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">2D assisted self-supervised learning</head><p>To leverage the 2D face images with only annotation of sparse 2D landmark points offered by detector <ref type="bibr" target="#b7">[8]</ref>, we develop the following self-supervision scheme that offers three different self-supervision losses, including the 2D landmark consistency loss L 2d-con , the 3D landmark consistency loss L 3d-con and the cycle-consistency loss L cyc . <ref type="figure">Fig. 3</ref> gives a systematic overview. The intuition behind this scheme is: if the 3D face estimation model is trained well, it should present consistency in the following three aspects. First, the 2D landmarks Y 2d recovered from the predicted 3D landmarks X 3d via 3D-2D projection should have small difference with the input 2D landmarks X 2d . Second,  <ref type="figure">Figure 4</ref>: Illustration of the weight mask used for computing L2d-con. We assign the highest weight to the red points, the medium weight to the pinky points, the yellow points has the lowest weight. Best viewed in color. the predicted 3D landmarks X 3d from the input 2D landmarks X 2d should be consistent with the 3D landmarksX 3d recovered from the predicted 2D landmarks Y 2d by passing it through the same 3D estimation model. Third, the pro-jectedX 2d fromX 3d should be consistent with the original input X 2d , i.e., forming a consistent cycle.</p><p>Thus, we define following two landmark consistency losses in our model correspondingly. The L 3d-con is formulated as:</p><formula xml:id="formula_5">L 3d-con = 68 i=1 x 3d i −x 3d i ,</formula><p>where x 3d i is the i th 3D landmark output from the forward pass (see red arrow in <ref type="figure">Fig. 3</ref>),x 3d i is the i th landmark predicted from the backward pass (see green arrow in <ref type="figure">Fig. 3)</ref>.</p><p>For computing the L 2d-con , we first create a weight mask V = {v 1 , v 2 , ..., v N } based the contribution of each point. Since the contour landmarks of a 2D face are inaccurate to represent the corresponding points of 3D face, we discard them and sample 18 landmarks from the 68 2D facial landmarks. The weight mask is shown in <ref type="figure">Fig. 4</ref>. Here, the mouth center landmark is the midpoint of two mouth corner points. The L 2d-con is defined as:</p><formula xml:id="formula_6">L 2d-con = 18 i=1 v i × x 2d i − y 2d i ,</formula><p>where x 2d i is the i th 2D landmark of the input face, y 2d i is the i th 2D landmark inferred from the output LMP, and v i is its corresponding weight. The weight values are specified in <ref type="figure">Fig. 4</ref>. We use the following relative weights in our experiments: (red points) : (pinky points) : (yellow points) = 4:2:1 that are set empirically.</p><p>We model the 2D facial landmarks prediction as a selfmapping process, and denote F : X 2d → Y 2d as the forward mapping, Q : Y 2d → X 2d as the backward mapping. The backward mapping brings the output landmarks y i back to its original position x i , i.e., x → F (x) → Q(F (X)) ≈ x. We constrain this mapping using the cycle consistency loss:</p><formula xml:id="formula_7">L cyc = L 2d-con (x 2d ,x 2d ),</formula><p>where x 2d are the input 2D facial landmarks, andx 2d are the landmarks output from Q(F (X)). <ref type="figure">Figure 5</ref>: Qualitative results on AFLW2000-3D dataset. The predictions by 2DASL show that our predictions are more accurate than ground truth in some cases (only 68 points are plotted to show). Green: landmarks predicted by our 2DASL. Red: ground truth from <ref type="bibr" target="#b48">[49]</ref>. The thumbnails on the top right corner of each image are the dense alignment results. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Self-critic learning</head><p>We further introduce a self-critic scheme to weakly supervise the model training with the "in-the-wild" 2D face images. Given a set of face images I = {I 1 , . . . , I n } without any 3D annotations and a set of face images J = {(J 1 , α * 1 ), . . . , (J m , α * m )} with accurate 3DMM annotations, the CNN regressor model R : I i → α i would output 62 coefficients for each image. We use another model as the critic C(·) to evaluate whether the predicted coefficients are consistent with the input images as the pairs of (J i , α * i ). Since each coefficient is closely related to its corresponding face image, the critic model would learn to distinguish the realism of the coefficients conditioned on the latent representation of the input face images. To this end, we feed the input images to an encoder to obtain the latent representation z and then concatenate with their corresponding 3DMM coefficients as the inputs to the critic C(·). The critic is trained in the same way as the adversarial learning by optimizing the following loss:</p><formula xml:id="formula_8">L sc = E I∈I [log(D([z * , α * ])) + log(1 − D([z, R(I)]))],</formula><p>where z * is the latent representation of a 3D annotated image J, α * is the 3DMM ground truth, I is the input "inthe-wild" face image, and z is its latent representation. The above self-critic loss encourages the model to output 3D faces that lie on the manifold of human faces, and predict landmarks that have the same distribution with the true facial landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate 2DASL qualitatively and quantitatively under various settings for 3D face reconstruction and dense face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training details and datasets</head><p>Our proposed 2DASL is implemented with Pytorch <ref type="bibr" target="#b32">[33]</ref>. We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 × 10 −5 and decays exponentially, the discriminator uses the Adam as optimizer with the fixed learning rate 1 × 10 −4 . The batch size is set as 32. λ 1 , λ 2 , λ 3 and λ 4 are set as 0.005, 0.005, 1 and 0.005 respectively. We use a two-stage strategy to train our model. In the first stage, we train the model using the overall loss L. In the second stage, we fine-tune our model using the Vertex Distance Cost, following <ref type="bibr" target="#b48">[49]</ref>.</p><p>The dataset 300W-LP <ref type="bibr" target="#b48">[49]</ref> is used to train our model. This dataset contains more than 60K face images with annotated 3DMM coefficients. The "in-the-wild" face images are all from the UMDFaces dataset <ref type="bibr" target="#b3">[4]</ref> that contains 367,888 still face images for 8,277 subjects. The 2D facial landmarks of all the face images are detected by an advanced 2D facial landmarks detector <ref type="bibr" target="#b7">[8]</ref>. The input images are cropped to the size 120 × 120. We use the test datasets below to evaluate our method: AFLW2000-3D <ref type="bibr" target="#b48">[49]</ref> is constructed by selecting the first 2000 images from AFLW <ref type="bibr" target="#b25">[26]</ref>. Each face is annotated with its corresponding 3DMM coefficients and the 68 3D facial landmarks. We use this dataset to evaluate our method on both 3D face reconstruction and dense face alignment. <ref type="bibr" target="#b21">[22]</ref> is another extension of AFLW. It is constructed by picking images from AFLW according to the poses. It contains 1,299 test images with a balanced distribution of yaw angle. Each image is annotated with 34 facial landmarks. We use this dataset to evaluate performance for the dense face alignment task. The 34 landmarks are used as the ground truth to measure the accuracy of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFLW-LFPA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dense face alignment</head><p>We first compare the qualitative results from our method and corresponding ground truths in <ref type="figure">Fig. 5</ref>. Although all the state-of-the-art methods of dense face alignment conduct evaluation on AFLW2000-3D, the ground truth of AFLW2000-3D is controversial <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48]</ref>, since its annotation pipeline is based on the Landmarks Marching method in <ref type="bibr" target="#b49">[50]</ref>. As can be seen, our results are more accurate than the ground truth in some cases. This is mainly because 2DASL involves a number of the "in-the-wild" images for training, enabling the model to perform well in cases even unseen in the 3D annotated training data. For fair comparison, we adopt the normalized mean error (NME) <ref type="bibr" target="#b48">[49]</ref> as the metric to evaluate the alignment performance. The NME is the mean square error normalized by face bounding box size. Since some images in AFLW2000-3D contains more than 2 faces, and the face detector sometimes gives the wrong face for evaluation (not the test face with ground truth), leading to high NME. Therefore, we discard the worst 20 cases of each method and only 1,980 images from AFLW2000-3D are used for evaluation. We evaluate our 2DASL using a sparse set of 68 facial landmarks and also the dense points with both 2D and 3D coordinates, and compare it with other state-of-the-arts. The 68 sparse facial landmarks can be viewed as sampling from the dense facial points. Since PRNet <ref type="bibr" target="#b14">[15]</ref> and VRN-Guided <ref type="bibr" target="#b19">[20]</ref> are not 3DMM based, and the point cloud of these two methods are not corresponding to 3DMM, we only compare with them on the sparse 68 landmarks. The results are shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, where we can see our 2DASL achieves the lowest NME (%) on the evaluation of both 2D and 3D coordinates among all the methods. For 3DMM-based methods: 3DDFA <ref type="bibr" target="#b48">[49]</ref> and DeFA <ref type="bibr" target="#b29">[30]</ref>, our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates.</p><p>To further investigate performance of our 2DASL across poses and datasets, we report the NME of faces with small, medium and large yaw angles on AFLW2000-3D dataset and the mean NME on both AFLW2000-3D and AFLW-LPFA datasets. The comparison results are shown in Tab. 1. Note that all the images from these two datasets are used for evaluation to keep consistent with prior works. The results of the compared method are directly from their published papers. As can be observed, our method achieves the lowest mean NME on both of the two datasets, and the lowest NME across all poses on AFLW2000-3D. Our 2DASL even performs better than PRNet <ref type="bibr" target="#b14">[15]</ref>, reducing NME by 0.09 and 0.08 on AFLW2000-3D and AFLW-LFPA, respectively. Es-  pecially on large poses (from 60 • to 90 • ), 2DASL achieves 0.2 lower NME than PRNet. We believe more "in-the-wild" face images used for training ensures better performance of 2DASL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D face reconstruction</head><p>In this section, we evaluate our 2DASL on the task of 3D face reconstruction on AFLW2000-3D by comparing with 3DDFA and DeFA. The VRN-Guided and PRNet are not compared because of the mis-match of point cloud between them and our method. Following <ref type="bibr" target="#b14">[15]</ref>, we first employ the Iterative Closest Points (ICP) algorithm to find the corresponding nearest points between the reconstructed 3D face and the ground truth point cloud. We then calculate the NME normalized by the face bounding box size. <ref type="figure" target="#fig_5">Fig. 7 (a)</ref> shows the comparison results on AFLW2000-3D. As can be seen, the 3D reconstruction results of 2DASL outperforms 3DDFA by 0.39, and 2.29 for DeFA, which are significant improvements.</p><p>We show some visual results of our 2DASL and compare with PRNet and VRN-Guided in <ref type="figure" target="#fig_5">Fig. 7 (b)</ref>. As can be seen, the reconstructed shape of our 2DASL are more smooth, however, both PRNet and VRN-Guided introduce some artifacts into the reconstructed results, which makes the reconstructed faces look unnaturally.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>In this section, we perform ablation study on AFLW2000-3D by evaluating several variants of our model: (1) 2DASL (base), which only takes the RGB images as input without self-supervision and self-critic supervision;</p><p>(2) 2DASL (cyc), which takes as input the combination of RGB face images and the corresponding 2D FLMs with self-supervison, however without self-critic supervision; (3) 2DASL (sc), which takes as input the RGB face images only using self-critic learning. (4) 2DASL (cyc+sc), which contains both self-supervision and self-critic supervision. For each variant, we use the L 2d-con with (w/) or without (w/o) weight mask. Therefore, there are in total 6 variants.</p><p>The ablation study results are shown in Tab. 2. Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages, respectively. Both self-critic and the self-supervision are effective to improve the performance. If the self-critic learning is not used, the NME increases by 0.04/0.18 for with/without weight mask, respectively. While the self-supervision scheme reduce NME by 0.1 when the weight mask is used, and 0.23 if the weight mask is removed, no significant improvement is observed. The best result is achieved when both these two modules are used. Moreover, in our experiments, we found taking the FLMs as input can accelerate the convergence of training process. Therefore, the first training stage just takes one or two epochs to reach a good model.</p><p>To explore how the performance is affected by the number of "in-the-wild" face images involved in training, we train our model using different numbers. Since the UMD-Faces dataset <ref type="bibr" target="#b3">[4]</ref> divides the whole dataset into 3 batches, each contains 77,228, 115,126, and 175,534 images respectively. We use the 3 batches and also the whole dataset to train our model. The results are reported in Tab. 3, where we can see the more data that used for aiding training, the lower NME is achieved by 2DASL.   <ref type="table">Table 3</ref>: The results (NME (%)) of 2DASL by training with different number of "in-the-wild" face images. "Num. # ITW" indicates the number of the "in-the-wild" face images used for training. The numbers in bold are the best results of each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel 2D-Assisted Selfsupervised Learning (2DASL) method for 3D face reconstruction and dense face alignment based on the 3D Morphable face Model. The sparse 2D facial landmarks are taken as input of CNN regressor and learn themselves via 3DMM coefficients regression. To supervise and facilitate the 3D face model learning, we introduce four selfsupervision losses, including the self-critic which is employed to weakly supervise the training samples that without 3D annotations. Our 2DASL make the abundant "inthe-wild" face images could be used to aid 3D face analysis without any 2D-to-3D supervision. Experiments on two challenging face datasets illustrate the effectiveness of 2DASL on both 3D face reconstruction and dense face alignment by comparing with other state-of-the-art meth-ods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The pipeline of our 2DASL. It aims to train a CNN regressor model. The model takes as input the face images with 3D annotations and other images with only 2D Facial Landmark Map (FLM), and predicts coefficients αj (for 3D annotated image) and αi (for images only with 2D landmarks) for 3DMM for 3D reconstruction and dense alignment. There are dual training paths. The upper path trains the model through 3D annotation supervision. The bottom path trains the model through self-critic supervision based on the 2D face images. In particular, 2D images are transformed by an encoder to the latent representation, and the self-critic module evaluates whether the predicted coefficients are consistent with the latent representations, by taking ground truth pairs as reference. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Error Distribution Curves (EDC) of face alignment results on AFLW2000-3D. The worst 20 cases of each method are discarded. The horizontal axis are the NME (%) in ascending order. The vertical axis are the number of images. Evaluation is performed on the 68 2D landmarks (a), 68 3D landmarks (b), all 2D points (c) and all 3D points (d). The mean NME (%) of each method is shown in the bottom legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a) EDC of face reconstruction results on AFLW2000-3D dataset. The worst 20 cases of each method are discarded. The mean NME (%) of each method is shown in the bottom legend. (b) Some 3D reconstruction results of 2DASL (columns 2 &amp; 3) , PRNet (columns 4 &amp; 5) and VRN-Guided (columns 6 &amp; 7). Images of the first column are the original face images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• to 30 • 30 • to 60 • 60 • to 90</figDesc><table><row><cell>Methods</cell><cell cols="4">AFLW2000-3D 0 • Mean</cell><cell>AFLW-LFPA Mean</cell></row><row><cell>SDM [32]</cell><cell>3.67</cell><cell>4.94</cell><cell>9.67</cell><cell>6.12</cell><cell>-</cell></row><row><cell>3DDFA [49]</cell><cell>3.78</cell><cell>4.54</cell><cell>7.93</cell><cell>5.42</cell><cell>-</cell></row><row><cell>3DDFA + SDM [49]</cell><cell>3.43</cell><cell>4.24</cell><cell>7.17</cell><cell>5.42</cell><cell>-</cell></row><row><cell>PAWF [24]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.72</cell></row><row><cell>Yu et al. [48]</cell><cell>3.62</cell><cell>6.06</cell><cell>9.56</cell><cell>-</cell><cell>-</cell></row><row><cell>3DSTN [5]</cell><cell>3.15</cell><cell>4.33</cell><cell>5.98</cell><cell>4.49</cell><cell>-</cell></row><row><cell>DeFA [30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.50</cell><cell>3.86</cell></row><row><cell>PRNnet [15]</cell><cell>2.75</cell><cell>3.51</cell><cell>4.61</cell><cell>3.62</cell><cell>2.93</cell></row><row><cell>2DASL (ours)</cell><cell>2.75</cell><cell>3.44</cell><cell>4.41</cell><cell>3.53</cell><cell>2.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on AFLW2000-3D (68 2D landmarks) and AFLW-LFPA (34 2D visible landmarks). The NME (%) for faces with different yaw angles are reported. The numbers in bold are the best results on each dataset, the lower is the better.</figDesc><table /><note>-" indicates the corresponding result is unavailable.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study results (NME %). "w/ Mask" means using the weight mask for L2d-con, while "w/o Mask" refers to without weight mask. The numbers in bold are the best results of each variant.</figDesc><table><row><cell cols="5">Num. # ITW 77,228 115,126 175,534 367,888</cell></row><row><cell>Stage 1</cell><cell>4.13</cell><cell>4.01</cell><cell>3.85</cell><cell>3.79</cell></row><row><cell>Stage 2</cell><cell>3.92</cell><cell>3.75</cell><cell>3.62</cell><cell>3.53</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6799" to="6808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimal step nonrigid icp algorithms for surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Umdfaces: An annotated face dataset for training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Faster than real-time facial alignment: A 3d spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3980" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<title level="m">Face recognition based on fitting a 3d morphable model. T-PAMI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM-TOG</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-VCG</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d face alignment in the wild: A landmark-free, nose-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>De Bittencourt Zavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Bellon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="581" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end 3d face reconstruction with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5908" to="5917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape augmented regression for 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="604" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully automated and highly accurate dense correspondence for facial surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fitting 3d morphable face models using local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1195" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense 3d face alignment from 2d videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The first 3d face alignment in the wild (3dfaw) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose-invariant 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via cnnbased dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single view-based 3d face reconstruction robust to self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unconstrained facial landmark localization with backbone-branches fullyconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment with a deformable hough transform model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration. PyTorch: Tensors and dynamic neural networks in Python with strong GPU acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A recurrent encoder-decoder network for sequential face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="38" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">3d face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1259" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Estimating 3d shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<idno>CVPR. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d face alignment without correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sánta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A nonlinear discriminative approach to aam fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7346" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Optimization problems for fast aam fitting in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning dense facial correspondences in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
