<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Noisy Student Training for Automatic Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
							<email>danielspark@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
							<email>jiaye@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
							<email>weihan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
							<email>chungchengc@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<email>boboli@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
							<email>yonghui@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Noisy Student Training for Automatic Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech recognition</term>
					<term>semi-supervised learning</term>
					<term>data augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, a semi-supervised learning method known as "noisy student training" has been shown to improve image classification performance of deep networks significantly. Noisy student training is an iterative self-training method that leverages augmentation to improve network performance. In this work, we adapt and improve noisy student training for automatic speech recognition, employing (adaptive) SpecAugment as the augmentation method. We find effective methods to filter, balance and augment the data generated in between self-training iterations. By doing so, we are able to obtain word error rates (WERs) 4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h subset of LibriSpeech as the supervised set and the rest (860h) as the unlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the clean/noisy LibriSpeech test sets by using the unlab-60k subset of Libri-Light as the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%). Index Terms: speech recognition, semi-supervised learning, data augmentation When the supervised performance of the task is low, we find gradational self-training over NST generations effective:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We aim to improve semi-supervised learning in automatic speech recognition (ASR) by adapting a method proven successful for image classification that we refer to as "noisy student training" (NST) <ref type="bibr" target="#b0">[1]</ref>. Noisy student training is an iterative selftraining method that makes use of unlabeled data to improve accuracy. In noisy student training, a series of models are trained in succession, such that for each model, the preceding model in the series serves as a teacher model on the unlabeled portion of the dataset. The distinguishing feature of noisy student training is the exploitation of augmentation, where the teacher produces quality labels by reading in clean input, while the student is forced to reproduce those labels with heavily augmented input features. To ensure the integrity of the labels provided by the teacher, the data generated by the teacher is typically filtered by removing low confidence examples, and balanced by matching the labeled distribution.</p><p>We adapt noisy student training for the purpose of speech recognition by introducing the following measures:</p><p>1. We use gradational filtering, where the filtering criterion is sytematically relaxed to grow the semi-supervised dataset as the model performance improves.</p><p>2. We introduce gradational augmentation, where augmentation strength is increased with NST iterations.</p><p>We are able to achieve state-of-the-art performance on two ASR tasks by using noisy student training. On the first task, LibriSpeech 100-860, the clean 100h subset of LibriSpeech <ref type="bibr" target="#b5">[6]</ref> is used as the labeled set and the remaining 860h of LibriSpeech is used as the unlabeled set. We view this as a low-supervisedperformance task, the baseline word error rates (WERs) being 5.5%/16.9% on the test-clean/test-other sets of LibriSpeech. By employing a noisy student training pipeline with gradational filtering and augmentation, we are able to achieve test-clean/testother WERs 4.2%/8.6% on this task.</p><p>The second task is LibriSpeech-LibriLight, where the entirety of LibriSpeech is used as the labeled set, and the unlab-60k subset of LibriLight <ref type="bibr" target="#b6">[7]</ref>, a large untranscribed speech dataset based on audio books, is used as the unlabeled set. The baseline model <ref type="bibr" target="#b7">[8]</ref> is able to achieve test-clean/test-other WERs 1.9%/4.1% showing high supervised performance. For this set, we obtain WERs 1.7%/3.4% on the test-clean/test-other sets by noisy student training without gradational filtering or augmentation, as we do not find gradational self-training effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Self-training, where a teacher model is used to generate labels for the unlabeled set the student can train on, is one of the earliest methods in semi-supervised learning (e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>). Selftraining has a long history of being applied to ASR through numerous studies, e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Noise has been studied and shown to have positive effects in selftraining for machine translation and image classification recently <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>, directly motivating our work. Gradational growth of the labelled dataset in self-training has been used in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Another direction of semi-supervised learning research is to use consistency-based methods (e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>). In this approach, a separate task/loss to (pre-)train the acoustic model (or a part thereof) is introduced to help the model learn a good representation of the input, e.g., <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Our work builds upon self-training and has three main contributions. First, we make use of new augmentation methods in speech to improve the task performance. Second, we introduce a normalized filtering score for shallow-fused models for filtering generated transcripts. Third, we apply gradational filtering where the filtering criterion for unlabeled data used for training is systematically relaxed with the self-training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Noisy Student Training for ASR</head><p>The NST algorithm we propose assumes a labeled set S, an unlabeled set U and a fixed LM trained on a separate text corpus. Then, NST generates a series of ASR models as follows: </p><formula xml:id="formula_0">1. Train M0 on S with SpecAugment. Set M = M0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Balance filtered data f (M (U )) to obtain b · f (M (U )). We denote each cycle of the noisy student training a generation. We denote the filtered and balanced dataset b · f (M (U )) the "semi-supervised portion" of the training set. We now elaborate on the details of the presented components. SpecAugment: We use SpecAugment <ref type="bibr" target="#b1">[2]</ref> to augment the input data at each step of NST. We employ adaptive time masking <ref type="bibr" target="#b2">[3]</ref>, where the size of the time masks scales linearly with the length of the input utterance, depending on the task. Language Model Fusion: To produce better transcripts for the student, we shallow-fuse <ref type="bibr" target="#b3">[4]</ref> the teacher networks with an LM trained on a fixed text corpus. We introduce a coverage penalty term <ref type="bibr" target="#b31">[32]</ref> in the fusion score with parameter c for LAS models <ref type="bibr" target="#b32">[33]</ref>, while we use a constant non-blank reward ρ to shift the logits produced by the LM for RNN-T <ref type="bibr" target="#b33">[34]</ref> networks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. These parameters are tuned at each generation via grid search to minimize the dev-set WERs. The transcripts of the unlabeled set are generated from the fused model using beam search. Filtering: Given that the generated transcripts were found based on a fusion score that does not have an interpretation as a log-probability, it is unclear what to use to judge the quality of the transcripts generated by a teacher network. We introduce a filtering score given as a function of the shallow fusion score S and the token length of a transcript generated by the fused teacher model. The normalized filtering score is defined to be</p><formula xml:id="formula_1">s(S, ) = (S − µ − β)/(σ √ ) ,<label>(1)</label></formula><p>with parameters µ, β and σ fit on the dev-set. µ, β are fit via linear regression on the value pairs ( i, Si) of the transcripts generated from the dev-set utterances. σ is obtained by computing the standard deviation of (Si − µ i − β)/ √ i. These parameters are fit for each new generation of teacher models.</p><p>Using this score, we filter the transcript-utterance pairs generated by the trained models in a gradational manner, i.e., we lower the filtering cutoff scutoff for which only the utterances with scores s(S, ) &gt; scutoff are kept, as the self-training cycle is iterated. This strategy is expected to work best when the teacher model becomes markedly better with NST iterations. Balancing: The distribution of tokens of the transcripts of the generated set f (M (U )) can differ significantly from that of the supervised training set distribution. We may choose to weigh the samples in the newly generated dataset to bridge this gap.</p><p>We use sub-modular sampling <ref type="bibr" target="#b4">[5]</ref> with the "cost-benefit score" to balance the dataset. Sub-modular sampling is a method that samples (with replacement) a set of sentences from a sentence pool so that the token distribution of the sampled set is close to a target distribution. This is done by optimizing the KL divergence between the token distributions of the sampled set and the target distribution in a greedy way, where sentences are collected by batches of size B. Each batch is chosen from the pool of sentences by selecting the top-B sentences in terms of cost-benefit, i.e., the decrease in KL divergence by adding that sentence to the current set of sampled transcripts, divided by its number of tokens. After a batch of samples have been collected and added to the sampled set, the KL divergence between the token distribution of assembled sentences and the target distribution is computed, and the process is iterated.</p><p>We impose additional constraints on this algorithm by capping the sampling multiplicity of a sentence by 2 and requiring the total number of tokens in the sampled set to be bounded below by the total number of tokens in the supervised set. Furthermore, we take the batch size to be a tenth of the filtered dataset. We optimize the KL divergence with these restrictions to obtain the balanced set b · f (M (U )) from f (M (U )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixing:</head><p>We combine b · f (M (U )) with the supervised dataset S for training using two different methods of mixing. In batchwise mixing, we fix the ratio between supervised and semisupervised samples in each training batch. In non-batch-wise mixing, data is uniformly sampled from both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">LibriSpeech 100-860</head><p>LibriSpeech 100-860 is a semi-supervised task where the clean 100h subset of LibriSpeech <ref type="bibr" target="#b5">[6]</ref> is taken to be the supervised set, while the remaining 860h of audio is taken to be the unlabeled set. The unlabeled audio consists of 360h of clean data and 500h of noisy data. We tokenize the transcripts using a WPM model <ref type="bibr" target="#b36">[37]</ref> with vocabulary size 16k constructed from the clean 100h subset transcripts. We use 80-dimensional filter bank coefficients with delta and delta-delta acceleration as the input features. We use the model denoted LAS-6-1280 in <ref type="bibr" target="#b1">[2]</ref> (see also <ref type="bibr" target="#b37">[38]</ref>) as our acoustic model, which is a Listen, Attend and Spell network <ref type="bibr" target="#b32">[33]</ref> with a bi-directional LSTM encoder.</p><p>We train 6 generations of models numbered 0 to 5, where we count the baseline model trained with the supervised set as the zeroth generation. Each generation is trained with peak Adam learning rate of 0.001 and batch size of 512 on 32 Google Cloud TPU chips for 10 days. The checkpoint to be used for LM fusion is chosen based on its dev-set WER. The choices made for the components of NST are as follows: SpecAugment: For generation 0, we use two frequency masks with mask parameter (F ) 27, two time masks with mask parameter (T ) 40, and time warping with warp parameter (W ) 40 <ref type="bibr" target="#b1">[2]</ref>. At generations 2 and 4, we re-tuned the time mask parameter T on a proxy task (with model LAS-4-1024 and schedule LB from <ref type="bibr" target="#b1">[2]</ref>). The time masking parameter used for generations 0 and 1, 2 and 3, 4 and 5 were set to 40, 80 and 100, respectively. LM: We use a 3-layer LSTM language model with width 4096 trained on the LibriSpeech LM corpus tokenized with the 16k WPM. The LM has word-level perplexity 63.9 on the dev-set. Filtering: We apply gradational filtering from generation 1 to 5 with cutoffs, 1, 0.5, 0, −1 and −∞. We apply filtering separately to the clean-360h and other-500h audio, i.e., the scoring coefficients are fit separately on dev-clean and dev-other, and applied to clean-360h and other-500h respectively. Other: Neither balancing or batch-wise mixing is used.</p><p>The performance of the fused models through the generations are plotted in <ref type="figure" target="#fig_2">figure 1</ref>. Our best trained model is the generation-4 model. We compare the performance of our baseline model, as well as the generation-4 model before and after LM fusion, against other results in the literature in table 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LibriSpeech-LibriLight</head><p>The entire LibriSpeech <ref type="bibr" target="#b5">[6]</ref> training set is used as the supervised training set of this task, while the "unlab-60k" subset of Libri-Light <ref type="bibr" target="#b6">[7]</ref>, an unlabeled audio dataset derived from audio books, is used as the unlabeled set. We do not use the "duplicate" set, nor any supervised subset of LibriLight. We segment the audio using a maximum expected segmentation length of 36 seconds, but filter further down to choose segments that have less than 22 seconds of audio. This results in producing approximately a million utterances. The transcripts of the supervised set are tokenized using a WPM model with vocabulary size 1k constructed from the LibriSpeech training set transcripts. We use 80-dimensional filter bank coefficients of the utterances as single-channel input features. We use a family of Con-textNets <ref type="bibr" target="#b7">[8]</ref>, which are RNN-T models <ref type="bibr" target="#b33">[34]</ref> with deep CNN encoders, for this task. We use CN-w to denote a model with a 23-convolutional-block encoder and 2-layer decoder with cell size 1280 and width scaling factor w studied in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Five generations of models numbered 0 to 4, are trained, where the baseline model is taken to be the generation-zero model. The baseline ContextNet model has the same encoder as CN-2, but has a one-layer RNN decoder with dimension 640. Meanwhile, CN-w with w=1.25, 1.75, 2.25 and 2.5 have been set to be the ASR model from generation 1 through 4. Each generation is trained on 128 Google Cloud TPU chips for 1-3 days. The checkpoint of the model to be used for LM fusion is chosen based on the dev-set WER. The choices for the NST pipeline have been set as follows: SpecAugment: We use SpecAugment with adaptive time mask size <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. We use two frequency masks with mask parameter (F ) 27, and ten time masks with maximum time-mask ratio (pS) 0.05, where the maximum-size of the time mask is set to pS times the length of the utterance. Time warping is not used. LM: We use a 3-layer LSTM LM with width 4096 trained on the LibriSpeech LM corpus with the LibriSpeech 960h transcripts added, tokenized with the 1k WPM. The LM has wordlevel perplexity 68.3 on the dev-set transcripts. Other: We do not apply filtering, while balancing is applied with the parameters detailed in section 2. We use batchwise mixing, and gradually increase the portion of the semisupervised utterances-the ratio between the supervised and semi-supervised utterances are raised from 4:6 at generations 1, 2 to 3:7 at generation 3 to 2:8 at generation 4.</p><p>We have plotted the generational performance of the fused models in <ref type="figure" target="#fig_2">figure 1</ref>. Our best trained model is the generation-4 model whose performance is presented in table 2. Gradational Filtration: As noted in section 3.1, for Lib-riSpeech 100-860, we gradually relax the filtering criterion for the semi-supervised dataset based on their normalized filtering score. Recall that we generate transcripts for the dev-clean and dev-other sets using the generation 0 to 4 LibriSpeech 100-860 models to fit the filtering score parameters. We plot the portion of utterances and their tokens in these datasets that are above a given filtering score in <ref type="figure" target="#fig_0">figure 2</ref>. The thick dotted line is the cumulative density of the standard normal distribution we have plotted as a reference, while the ten solid curves come from transcripts generated at each generation from the two dev-sets. The distribution of utterances are remarkably stable throughout generations, the solid curves virtually lying on top of each other. We have plotted the WER of utterances above a given filtering score in <ref type="figure">figure 3</ref> for each generation. The score reasonably ranks the quality of the generated dev-set transcripts, except at generation 4, where the model generates a blank transcript to a long utterance, which makes the cumulative WER become large at high scores. We have marked the score used for filtering at each generation. The filtered dev-set WERs are comparable throughout the generations, while the size of the filtered dataset grows. This is the intended effect of gradational filtration. <ref type="figure">Figure 3</ref>: WER above filtering score for LibriSpeech 100-860 for dev-clean and dev-other with generation 0-4 models.</p><p>We find that slow evolution improves performance in the range we have explored. We have run an experiment where we evolve the model "faster," by training two generations in addition to the baseline, using filtering cutoffs 0 and −∞. We plot the evolution of dev-set WERs against the semi-supervised dataset size in <ref type="figure" target="#fig_4">figure 4</ref>. Models in the 6-gen pipeline outperforms those in the 3-gen pipeline at comparable dataset sizes. Augmentation: The increase of time mask size used for training at generations 2 and 4 is based on proxy task evaluation. In the proxy task, LAS-4-1024 models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>, which have about 1/3 of the parameters of LAS-6-1280, are trained for 100k steps with the combined training set at generations 2 and 4 with varying time mask size. The time mask size has been chosen based on the final WERs tabulated in table 3, where we prioritized performance on the clean set. The performances are measured without LM fusion. We find that a stronger augmentation is favored, as the dataset grows with gradational filtering. Mixing and Balancing: We conduct control experiments with batch-wise mixing and balancing for LibriSpeech 100-860. We have experimented with four combinations of balancing and 1:1 batch-wise mixing for the LibriSpeech 100-860 task, the results of which we present in <ref type="figure" target="#fig_5">figure 5</ref>. Balancing cuts down the semisupervised dataset size at each generation to about a half, diminishing the performance of the trained network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">LibriSpeech-LibriLight</head><p>We present control experiments for the LibriSpeech-LibriLight task. A slightly inferior baseline with model CN-1.25 with   Gradational Self-Training: We find that gradational filtration and augmentation are not as effective for this task as it was for LibriSpeech 100-860. We compare the performance of a network trained with gradational filtration over 5 generations with filtering scores 1, 0.5, 0, −1 and −∞ and one trained on the entire LibriLight dataset without filtering. We do not see the benefit of gradational filtration, as both models, after LM fusion have dev-clean/dev-other WERs 1.7%/3.7%, respectively. An explanation of this result is that the baseline model for these tasks is already extremely good, and the quality of filtered transcripts is not drastically better than the unfiltered transcripts. For example, the WER for the transcripts with score &gt; 1 on the dev-set is 1.5%, against 3.0% of the unfiltered transcripts at generation 0. This should be compared to the ∼ 10% difference in WER between the transcripts with score &gt; 1 against the unfiltered transcripts generated from the dev-other utterances at generation 0 for LibriSpeech 100-860 (gen-0 curve in <ref type="figure">figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have adapted and improved noisy student training for ASR. We have employed SpecAugment, language model fusion and sub-modular sampling into the noisy student training pipeline to adapt it for speech recognition. We have also introduced a normalized filtering score which we use for gradational self-training. The gradational methods prove to be beneficial for the low-supervised-performance task LibriSpeech 100-860, while it does not show much effect for the high-supervisedperformance task LibriSpeech-LibriLight. The various components introduced in this work were mixed and matched to improve the state-of-the-art performance of both tasks studied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 .</head><label>2</label><figDesc>Fuse M with LM and measure performance. 3. Generate labeled dataset M (U ) with fused model. 4. Filter generated data M (U ) to obtain f (M (U )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6 .</head><label>6</label><figDesc>Mix dataset b · f (M (U )) and S. Use mixed dataset to train new model M with SpecAugment. 7. Set M = M and go to 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>WERs (%) plotted against training generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Portion of utterances/tokens above filtering score. Solid curves are indexed by generation and filtered dev-set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>WER (%) vs semi-supervised dataset size for models in 6 and 3 generation noisy student training pipelines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>WERs plotted against the size of the semi-supervised dataset for combinations of balancing and batch-wise mixing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>LibriSpeech 100h WERs (%).</figDesc><table><row><cell>Method</cell><cell></cell><cell>Dev</cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell cols="4">clean other clean other</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lüscher et al., (2019) [39]</cell><cell>5.0</cell><cell>19.5</cell><cell>5.8</cell><cell>18.6</cell></row><row><cell>Kahn et al., (2019) [16]</cell><cell>7.78</cell><cell>28.15</cell><cell>8.06</cell><cell>30.44</cell></row><row><cell>Hsu et al., (2019) [19]</cell><cell cols="4">14.00 37.02 14.85 39.95</cell></row><row><cell>Ling et al., (2019) [31]</cell><cell></cell><cell></cell><cell>6.10</cell><cell>17.43</cell></row><row><cell>Semi-supervised (w/ LibriSpeech 860h)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kahn et al., (2019) [16]</cell><cell>5.41</cell><cell>18.95</cell><cell>5.79</cell><cell>20.11</cell></row><row><cell>Hsu et al., (2019) [19]</cell><cell>5.39</cell><cell>14.89</cell><cell>5.78</cell><cell>16.27</cell></row><row><cell>Ling et al., (2019) [31]</cell><cell></cell><cell></cell><cell>4.74</cell><cell>12.20</cell></row><row><cell>This Work</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline (LAS + SpecAugment)</cell><cell>5.3</cell><cell>16.5</cell><cell>5.5</cell><cell>16.9</cell></row><row><cell>+ NST before LM Fusion</cell><cell>4.3</cell><cell>9.7</cell><cell>4.5</cell><cell>9.5</cell></row><row><cell>+ NST with LM Fusion</cell><cell>3.9</cell><cell>8.8</cell><cell>4.2</cell><cell>8.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>LibriSpeech 960h WERs (%).</figDesc><table><row><cell>Method</cell><cell>Dev</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell cols="4">clean other clean other</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Synnaeve et al., (2019) [17]</cell><cell>2.10</cell><cell>4.79</cell><cell>2.33</cell><cell>5.17</cell></row><row><cell>Zhang et al., (2020) [36]</cell><cell></cell><cell></cell><cell>2.0</cell><cell>4.6</cell></row><row><cell>Han et al., (2020) [8]  †</cell><cell>1.9</cell><cell>3.9</cell><cell>1.9</cell><cell>4.1</cell></row><row><cell>Semi-supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Synnaeve et al., (2019) [17]</cell><cell>2.00</cell><cell>3.65</cell><cell>2.09</cell><cell>4.11</cell></row><row><cell>This Work (with baseline  †)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ContextNet + NST before LM Fusion</cell><cell>1.6</cell><cell>3.7</cell><cell>1.7</cell><cell>3.7</cell></row><row><cell>ContextNet + NST after LM Fusion</cell><cell>1.6</cell><cell>3.4</cell><cell>1.7</cell><cell>3.4</cell></row><row><cell cols="2">4. Discussion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1. LibriSpeech 100-860</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Proxy task WER (%) for LibriSpeech 100-860.</figDesc><table><row><cell cols="2">Time Mask Size (T )</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>120</cell></row><row><cell>Gen 2</cell><cell>dev-clean</cell><cell>5.7</cell><cell>5.6</cell><cell>5.5</cell><cell>9.2</cell><cell>-</cell></row><row><cell></cell><cell>dev-other</cell><cell cols="4">15.0 14.5 14.7 17.8</cell><cell>-</cell></row><row><cell>Gen 4</cell><cell>dev-clean</cell><cell>-</cell><cell>4.8</cell><cell>5.0</cell><cell>4.7</cell><cell>4.7</cell></row><row><cell></cell><cell>dev-other</cell><cell cols="5">-11.7 11.7 11.4 11.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>dev-clean/other WERs 1.8%/4.2% and test-clean/other WERs 1.9%/4.4% has been used for these experiments. Balancing and Filtering: Combinations of filtering with score 0 (which filters ∼ 50% of the utterances) and balancing have been tested on the first generation of NST. The pre-fusion WERs are presented in table 4. In contrast with LibriSpeech 100-860, the reduction in the semi-supervised set size by balancing does not seem to outweigh the benefits of making the token distribution closer to that of the supervised set, due to the size of the LibriLight dataset. We have chosen to use balancing, but not to filter in our pipeline based on these experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>WERs (%) for balancing and filtering configurations.</figDesc><table><row><cell cols="5">(Bal, Filt) (N, N) (N, Y) (Y, N) (Y, Y)</cell></row><row><cell>dev-clean</cell><cell>1.8</cell><cell>1.8</cell><cell>1.7</cell><cell>1.8</cell></row><row><cell>dev-other</cell><cell>4.5</cell><cell>4.4</cell><cell>4.3</cell><cell>4.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. We employ (adaptive) SpecAugment <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, an augmentation method for ASR that directly acts on the spectrogram of the input audio, for noisy student training.</p><p>2. We use shallow fusion with a language model (LM) <ref type="bibr" target="#b3">[4]</ref> on the teacher network to generate better transcripts for the student network to train on.</p><p>3. We propose a normalized filtering score for transcripts generated by teacher networks given as a function of the fusion score and number of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We use a variant of sub-modular sampling <ref type="bibr" target="#b4">[5]</ref> to weigh the utterance-transcript pairs generated by the teacher network to balance the token statistics of the dataset to be passed on to the student. Acknowledgements: We thank William Chan, Zhehuai Chen, Mike Chrzanowski, Ekin Dogus Cubuk, Arun Narayanan, Sankaran Panchapagesan, Bhuvana Ramabhadran, June Shangguan and Barret Zoph for helpful discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Specaugment on large scale datasets,&quot; in arxiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>in arxiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A submodular optimization approach to sentence set selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shinohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arxiv</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ContextNet: Convolutional Neural Networks with Larger Context for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive patternrecognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd annual meeting of the association for computational linguistics</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning extraction patterns for subjective expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical methods in natural language processing</title>
		<meeting>the 2003 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Utilizing untranscribed training data to improve performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zavaliagkos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Colthurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting><address><addrLine>Landsdowne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="301" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Analysis of low-resource acoustic model self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Novotney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep neural network features and semi-supervised training for low resource speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised training for end-to-end models via weak distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-toend asr: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lessons from building acoustic models with a million hours of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H K</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Selfsupervised speech recognition via local prior matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revisiting self-training for neural sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lightly supervised acoustic model training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luc Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA ITRW ASR2000</title>
		<meeting>ISCA ITRW ASR2000</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="150" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">S4l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extracting domain invariant features by unsupervised learning for robust automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Speech2vec: A sequenceto-sequence framework for learning word embeddings from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised speech representation learning using wavenet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">vq-wav2vec: Selfsupervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep contextualized acoustic representations for semi-supervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Model Unit Exploration for Sequence-to-Sequence Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">RWTH ASR systems for librispeech: Hybrid vs attention -w/o data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
