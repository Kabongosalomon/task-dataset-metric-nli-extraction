<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NePTuNe: Neural Powered Tucker Network for Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-15">15 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Sonkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NePTuNe: Neural Powered Tucker Network for Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-15">15 Apr 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs link entities through relations to provide a structured representation of real world facts. However, they are often incomplete, because they are based on only a small fraction of all plausible facts. The task of knowledge graph completion via link prediction aims to overcome this challenge by inferring missing facts represented as links between entities. Current approaches to link prediction leverage tensor factorization and/or deep learning. Factorization methods train and deploy rapidly thanks to their small number of parameters but have limited expressiveness due to their underlying linear methodology. Deep learning methods are more expressive but also computationally expensive and prone to overfitting due to their large number of trainable parameters. We propose Neural Powered Tucker Network (NePTuNe), a new hybrid link prediction model that couples the expressiveness of deep models with the speed and size of linear models. We demonstrate that NePTuNe provides state-of-the-art performance on the FB15K-237 dataset and near state-of-the-art performance on the WN18RR dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs encode information by means of triples of the form (h, r, t), where relation r represents the link between the head entity h and tail entity t. This rich structured information is useful for a wide range of natural language processing tasks, e.g., question answering, multi-hop reasoning, and information retrieval <ref type="bibr" target="#b7">(Hao et al., 2017;</ref><ref type="bibr" target="#b22">Zhang et al., 2018;</ref><ref type="bibr" target="#b6">Dietz et al., 2018)</ref>. However, knowledge graphs are often incomplete, meaning that they contain only a small subset of all known connections. Thus the task of knowledge graph completion via link prediction is of great importance, since populating knowledge graphs manually is both arduous and costly.</p><p>There are currently two families of link prediction methods at opposite ends of the expressiveness vs. scale/speed tradeoff. Tensor factorization methods represent the knowledge graph as a 3-mode binary tensor <ref type="bibr" target="#b12">(Nickel et al., 2011;</ref><ref type="bibr" target="#b21">Yang et al., 2015;</ref><ref type="bibr" target="#b18">Trouillon et al., 2017;</ref><ref type="bibr" target="#b2">Balazevic et al., 2019c)</ref>. These methods are easy and fast to train and deploy thanks to their small number of parameters. However, they have limited expressiveness due to their underlying linear methodology. Deep learning methods are more expressive but are computationally expensive and prone to overfitting due to their large number of trainable parameters <ref type="bibr" target="#b12">(Nickel et al., 2011;</ref><ref type="bibr" target="#b13">Socher et al., 2013)</ref>.</p><p>The state-of-the-art linear model for link prediction is TuckER <ref type="bibr" target="#b2">(Balazevic et al., 2019c)</ref>, which is based on the Tucker decomposition <ref type="bibr" target="#b20">(Tucker, 1966;</ref><ref type="bibr">Tucker and others, 1964)</ref> of a third-order binary tensor. Popular linear models like RESCAL <ref type="bibr" target="#b12">(Nickel et al., 2011</ref><ref type="bibr">), DistMult (Yang et al., 2015</ref>, ComplEx <ref type="bibr" target="#b17">(Trouillon et al., 2016), and</ref><ref type="bibr">SimplE (Kazemi and</ref><ref type="bibr" target="#b9">Poole, 2018)</ref> are special cases of TuckER. The pioneering deep learning model for link prediction is the Neural Tensor Network (NTN) <ref type="bibr" target="#b13">(Socher et al., 2013)</ref> which offers high expressiveness but comes with a major shortcoming. Since it uses a different weight matrix for modeling each relation, its number of parameters explodes with the graph size, resulting in computationally expensive training and overfitting issues.</p><p>In this paper, we propose Neural Powered Tucker Network (NePTuNe), a hybrid link prediction model that leverages desirable features from both TuckER and NTN. Carefully crafted nonlinearities make NePTuNe considerably more expressive than TuckER. Harnessing the principles of a shared core tensor intrinsic to the Tucker decomposition leads to parameter-sharing which simultaneously significantly reduces the number of parameters, remedies overfitting, and makes NePTuNe fast and efficient to train. Even more importantly, the Tucker formulation enables NePTuNe to be trained using the 1-N loss function by employing the re-arrangement property of n-mode multiplication. NePTuNe's capacity to use 1-N loss function is crucial, since the loss function has been shown to be greatly advantageous in training link prediction models. Importantly, deep learning models like NTN are incompatible with the 1-N loss.</p><p>Our experimental evaluations demonstrate that NePTuNe outperforms state-of-the-art linear and deep learning models on the FB15k-237 dataset <ref type="bibr" target="#b16">(Toutanova et al., 2015)</ref> and achieves second-best performance on the WN18RR <ref type="bibr" target="#b5">(Dettmers et al., 2018)</ref> dataset. We also show that NePTuNe has the same space and training and testing time complexity as the state-of-the-art linear TuckER model. Hence, NePTuNe successfully balances the expressiveness vs. scale/speed tradeoff introduced above. Code and instructions to reproduce our results are available here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Following (Balazevic et al., 2019c)'s notation, let E be the set of entities, R the set of relations, and K ⊆ K = E × R × E the knowledge graph. (h, r, t) ∈ K denotes a triple in the knowledge graph, where h, t ∈ E and r ∈ R. Let the scoring function φ(h, r, t) : E × R × E → R measure the plausibility of a triple (h, r, t) ∈ K. Boldface lower-case letters x denote a vector, while boldface upper-case letters X denote a matrix. Script upper-case letters X denote a three-mode tensor. The embeddings for the entities and relations are given by the matrices E ∈ R |E|×d and W ∈ R |R|×k , respectively.</p><p>We now introduce the TuckER and NTN models. As mentioned above, TuckER is based on the Tucker decomposition of a 3-mode tensor X , which is given by</p><formula xml:id="formula_0">X ≈ G × 1 A × 2 B × 3 C,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">X ∈ R I×J×K , G ∈ R L×M ×N , A ∈ R I×L , B ∈ R J×M , C ∈ R K×N ,</formula><p>and × n is the n-mode product. We can write TuckER's scoring function as</p><formula xml:id="formula_2">φ TuckER (h, r, t) = W × 1 e h × 2 w r × 3 e t ,<label>(2)</label></formula><p>where w r , e h , e t ∈ R d are embeddings for the relation, head, and tail entities, and W ∈ R d×k×d is the shared core tensor. <ref type="table">Table 1</ref> lists the scoring functions for some additional linear link prediction models.</p><p>NTN's scoring function is given by</p><formula xml:id="formula_3">φ NTN (h, r, t) = w T r · f act (W r × 1 e h × 3 e t ),<label>(3)</label></formula><p>where W r ∈ R d×d×k is the relation specific tensor, · denotes the dot (inner) product, and f act is a nonlinear scalar activation function such as ReLU or tanh.</p><p>Observe that NTN can be viewed as a nonlinear version of TuckER (albeit with a different core tensor per relation). By comparing (3) with (6), TuckER's scoring function can be written as NTN's scoring function but without a nonlinearity as</p><formula xml:id="formula_4">φ tuckER (h, r, t) = W × 1 e h × 2 w r × 3 e t (4) = W × 1 e h × 3 e t × 2 w r (5) = w T r · (W × 1 e h × 3 e t ),<label>(6)</label></formula><p>Model Scoring Function Relation Parameters Space Complexity RESCAL <ref type="bibr" target="#b12">(Nickel et al., 2011)</ref> e <ref type="bibr" target="#b21">(Yang et al., 2015)</ref> e <ref type="table">Table 1</ref>: Scoring function and space complexity (significant terms) of state-of-the-art linear link prediction models in comparison to NePTuNe. Information gathered from <ref type="bibr" target="#b2">(Balazevic et al., 2019c)</ref>.</p><formula xml:id="formula_5">T h W r e t W r ∈ R d 2 O(|E|d + |R|k 2 ) DistMult</formula><formula xml:id="formula_6">h , w r , e t w r ∈ R d O(|E|d + |R|d) ComplEX (Trouillon et al., 2016) Re( e h , w r ,ē t ) w r ∈ C d O(|E|d + |R|d) SimplE (Kazemi and Poole, 2018) 1 2 ( e h , w r , e t + e ′ h , w r , e ′ t ) w r ∈ R k O(|E|d + |R|d) TuckER (Balazevic et al., 2019c) W × 1 e h × 2 w r × 3 e t w r ∈ R k O(|E|d + |R|k) NePTuNe (ours) e T t · f act (W × 1 e h × 2 w r ) w r ∈ R k O(|E|d + |R|k)</formula><p>where <ref type="formula">(5)</ref> follows from the re-arrangement property of n-mode multiplication, and (6) follows from the properties of the dot product and n-mode multiplication. Viewing NTN against this backdrop of the Tucker decomposition sets the stage for defining a single shared core tensor W across relations, which is one of key defining traits of our proposed model, NePTuNe.</p><p>3 NePTuNe: Neural Powered Tucker Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The NePTuNe Model</head><p>The scoring function of our proposed model NePTuNe is given by</p><formula xml:id="formula_7">φ NePTuNe (h, r, t) = e T t · f act (W × 1 e h × 2 w r ).<label>(7)</label></formula><p>Like NTN, NePTuNe utilizes a nonlinear activation function f act . However, unlike NTN (which employs a different W r for each relation r ∈ R) and inspired by TuckER, NePTuNe uses a shared W across all relations. The shared core tensor W enables parameter-sharing among relations, which mitigates the problem of overfitting that plagues NTN and accelerates training.</p><p>Rewriting the TuckER scoring function from (4) as</p><formula xml:id="formula_8">φ tuckER (h, r, t) = e T t · (W × 1 e h × 2 w r )<label>(8)</label></formula><p>and comparing to <ref type="formula" target="#formula_7">(7)</ref>, we see that the NePTuNe scoring function shares the exact same structure save for the application of the nonlinear f act to the second term. NePTuNe also shares the same structure as NTN except that the order of e t and w r is reversed. This turns out to be critically important, because it enables us to apply the 1 − N loss function to train NePTuNe. We discuss this at length below in Section 3.3. In the absence of the nonlinearity (f act set to identity), the NePTuNe <ref type="formula" target="#formula_7">(7)</ref> and NTN (3) scoring functions are equivalent, since the n-th mode product in the Tucker decomposition is invariant to the order [(W × 1 e h × 2 w r ) × 3 e t = (W × 1 e h × 3 e t ) × 2 w r ]. However, with the nonlinearity, the scoring functions are not equivalent</p><formula xml:id="formula_9">[f act (W × 1 e h × 2 w r ) × 3 e t = f act (W × 1 e h × 3 e t ) × 2 w r ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>We train NePTuNe using a binary cross entropy (BCE) loss with 1 − N scoring. 1 − N scoring was proposed by <ref type="bibr" target="#b5">(Dettmers et al., 2018)</ref> to speed up training and performance of link prediction models. For an arbitrary head entity h and relation r, the 1 − N score with BCE loss is given by</p><formula xml:id="formula_10">L (h,r) = − |E| i=1 [y i log(p i ) + (1 − y i )log(1 − p i )] ,<label>(9)</label></formula><p>where y i = 1 for all tail entities t i ∈ E for which the triple (h, r, t i ) ∈ K and y i = 0 otherwise, and p i = sigmoid(φ NePTuNe (h, r, t i )). We also apply the inverse relation data-augmentation technique <ref type="bibr" target="#b5">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b11">Lacroix et al., 2018)</ref>, which adds a new triple (t, r −1 , h) in K for every (h, r, t) ∈ K by defining a new relation r −1 for each relation r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Time Complexity Analysis</head><p>We study the number of multiplication operations needed to compute L (h,r) in (9) for an arbitrary head entity h and relation r. The analysis can be reduced to computing the number of multiplication operations needed to compute the scoring function φ NePTuNe (h, r, t) for all t ∈ E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define the scoring function vector</head><formula xml:id="formula_11">Φ NePTuNe (h, r) ∈ R |E| with i th element Φ i NePTuNe (h, r) = φ NePTuNe (h, r, t i ). Then, Φ NePTuNe (h, r) = f act (W × 1 e h × 2 w r ) × 3 E (10) = f act ((W × 2 w r ) A × 1 e h ) b × 3 E<label>(11)</label></formula><p>since n-mode multiplication is invariant to order. The operation (W × 2 w r ) reduces the 3-D tensor W ∈ R k×d×d to a 2-D matrix A ∈ R d×d , which, followed by × 1 e h and the nonlinearity reduces A to a vector b ∈ R d .</p><formula xml:id="formula_12">Operation W × 2 w r requires O(d × d × k) multiplications; operation A × 1 e h requires O(d × d) multiplications; and operation b × 3 E requires O(d × |E|). Thus, computing Φ NePTuNe (h, r) involves O(d × d × k + d × d × 1 + 1 × d × |E|) multiplications.</formula><p>This analysis enables us to compare the effect of interchanging w r and e t to contrast the NePTuNe scoring function <ref type="formula" target="#formula_7">(7)</ref> with the NTN scoring function (3). The scoring function of NTN is given by</p><formula xml:id="formula_13">Φ NTN (h, r) = f act ((W r × 1 e h ) C × 3 E) d × 2 w r .<label>(12)</label></formula><formula xml:id="formula_14">Computing Φ NTN (h, r) requires O(d × d × k + d × d × |E| + k) multiplications. The bottleneck operation C × 3 E that requires d × d × |E| multiplications is what makes 1 − N scoring infeasible for NTN.</formula><p>The above time complexity analysis reveals that the design decision to interchange the order of the terms in the NePTuNe scoring function <ref type="formula" target="#formula_7">(7)</ref> vs. the NTN (3) is significant, since it enables NePTuNe to use 1 − N scoring, which has proven effective in accelerating the training of link prediction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Space Complexity Analysis</head><p>The parameters of NePTuNe are the shared core tensor W ∈ R d×d×k , entity embeddings E ∈ R |E|×d , and relation embeddings W ∈ R |R|×k , leading to the significant terms of NePTuNe's space complexity being of O(|E| × d + |R| × k).</p><p>Since the above space and time complexity analysis also holds for TuckER, we have verified that NePTuNe and TuckER are equivalent in these respects.   <ref type="bibr" target="#b4">(Chami et al., 2020)</ref>. NePTuNe outperforms the state-of-the-art linear TuckER model on the FB15k-237 dataset by a significant margin while also outperforming the state-of-the-art nonlinear RotH model on the WN18RR dataset in the Hits@1 metric. Importantly, NePTuNe makes no assumptions about the data, and hence its performance is consistent across both datasets. RotH, in contrast, was specifically designed for hierarchical datasets like WN18RR, which explains its subpar performance on FB15k-237.</p><p>representations for regularization. We optimize NePTuNe in terms of the the 1 − N score with BCE loss from (9) using Adam (Kingman and Ba, 2015) for 1000 training epochs. (We found that ReLU performed marginally better than tanh for the activation f act .) We measure performance using the standard filtered metrics of mean reciprocal rank (MRR), Hits@1, Hits@3, and Hits@10 <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref>. MRR is defined as the mean over all test triples of the inverse of the rank assigned to the true triple tensor. Hits@n is defined as the fraction of times the true triple is present among the top n ranked candidate triples. <ref type="table" target="#tab_1">Table 2</ref> compares NePTuNe with a range of state-of-the-art linear and nonlinear link prediction models. We see that NePTuNe has superior performance on FB15k-237 and outperforms the state-of-theart nonlinear RotH model <ref type="bibr" target="#b4">(Chami et al., 2020)</ref> in the Hits@1 metric on WN18RR. In contrast to RotH, which has been specifically designed to model the kinds of hierarchical relations that feature prominently in WN18RR, NePTuNe's model design is dataset-agnostic. This explains its consistent performance on both FB15k-237 and WN18RR. Finally, it is worth noting that NePTuNe has the same number of parameters as the state-of-the-art linear TuckER model yet significantly outperforms it on both datasets due to its increased expressive power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have developed NePTuNe, a compact and computationally efficient model for knowledge graph completion that combines the speed and robustness of linear with the expressivity of nonlinear approaches to link prediction. An important hallmark of NePTuNe is that it is dataset-agnostic, as demonstrated by its (near) state-of-the-art performance on two very different standard datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results comparing NePTuNe with a range of state-of-the-art linear and nonlinear link prediction models. Results for all methods other than NePTuNe are taken from the RotH paper</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">ExperimentsWe compare the performance of NePTuNe against a wide range of state-of-the-art linear and nonlinear link prediction models, including ConvE<ref type="bibr" target="#b5">(Dettmers et al., 2018)</ref>, MurP<ref type="bibr" target="#b1">(Balazevic et al., 2019b)</ref>, MurE<ref type="bibr" target="#b1">(Balazevic et al., 2019b)</ref>, RotatE<ref type="bibr" target="#b15">(Sun et al., 2019)</ref>, ComplEx-N3<ref type="bibr" target="#b11">(Lacroix et al., 2018)</ref>,Quaternion (ZHANG et al., 2019), TuckER<ref type="bibr" target="#b2">(Balazevic et al., 2019c)</ref>, and RotH<ref type="bibr" target="#b4">(Chami et al., 2020)</ref> on the standard datasets FB15k-237<ref type="bibr" target="#b16">(Toutanova et al., 2015)</ref> and WN18RR<ref type="bibr" target="#b5">(Dettmers et al., 2018)</ref>. FB15k-237 has 14,541 entities and 237 relations, while WN18RR has 40,943 entities and 11 relations.We extended the TuckER codebase<ref type="bibr" target="#b0">(Balazevic et al., 2019a)</ref> and make the code publicly available with the hyper-parameters needed to replicate the experiments here. As with TuckER, we use batch normalization<ref type="bibr" target="#b8">(Ioffe and Szegedy, 2015)</ref> and dropout<ref type="bibr" target="#b14">(Srivastava et al., 2014)</ref> on the input embeddings and hidden</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by NSF grants CCF-1911094, IIS-1838177, IIS-1730574, and IUSE-1842378; ONR grants N00014-18-12571, N00014-20-1-2787, and N00014-20-1-2534; AFOSR grant FA9550-18-1-0478; and a Vannevar Bush Faculty Fellowship, ONR grant N00014-18-1-2047.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Github -ibalazevic/tucker: Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<ptr target="https://github.com/ibalazevic/TuckER" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<title level="m">Multi-relational poincaré graph embeddings. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4463" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TuckER: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00545</idno>
		<title level="m">Low-dimensional hyperbolic knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Utilizing knowledge graphs for text-centric information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1387" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An end-toend model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization. conference paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dp Kingman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2863" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML&apos;11</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning, ICML&apos;11<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Christopher R Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bouchard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06879</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The extension of factor analysis to three-dimensional matrices. Contributions to mathematical psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="page">110119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational reasoning for question answering with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
