<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-10-04">4 Oct 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Ajou University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-04">4 Oct 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Super-Resolution, Deep Convolutional Neural Network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, deep learning methods have been successfully applied to single-image super-resolution tasks. Despite their great performances, deep learning methods cannot be easily applied to realworld applications due to the requirement of heavy computation. In this paper, we address this issue by proposing an accurate and lightweight deep network for image super-resolution. In detail, we design an architecture that implements a cascading mechanism upon a residual network. We also present variant models of the proposed cascading residual network to further improve efficiency. Our extensive experiments show that even with much fewer parameters and operations, our models achieve performance comparable to that of state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Super-resolution (SR) is a computer vision task that reconstructs a high-resolution (HR) image from a low-resolution (LR) image. Specifically, we are concerned with single image super-resolution (SISR), which performs SR using a single LR image. SISR is generally difficult to achieve due to the fact that computing the HR image from an LR image is a many-to-one mapping. Despite such difficulty, SISR is a very active area because it can offer the promise of overcoming resolution limitations, and could be used in a variety of applications such as video streaming or surveillance system.</p><p>Recently, convolutional neural network-based(CNN-based) methods have provided outstanding performance in SISR tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. From the SRCNN <ref type="bibr" target="#b5">[6]</ref> that has three convolutional layers to MDSR <ref type="bibr" target="#b25">[26]</ref> that has more than 160 layers, the depth of the network and the overall performance have dramatically grown over time. However, even though deep learning methods increase the quality of the SR images, they are not suitable for real-world scenarios. From this point of view, it is important to design lightweight deep learning models that are practical for real-world applications. One way to build a lean model is reducing the number of parameters. There are many ways to achieve this <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>, but the most simple and effective approach is to use a recursive network. For example, DRCN <ref type="bibr" target="#b20">[21]</ref> uses a recursive network to reduce redundant parameters, and DRRN <ref type="bibr" target="#b34">[35]</ref> improves DRCN by adding a residual architecture to it. These models decrease the number of model parameters effectively when compared to the standard CNN and show good performance. However, there are two downsides to these models: 1) They first upsample the input image before feeding it to the CNN model, and 2) they increase the depth or the width of the network to compensate for the loss due to using a recursive network. These points enable the model to maintain the details of the image when reconstructed, but at the expense of the increased number of operations and inference time.</p><p>Most of the works that aim to build a lean model focused primarily on reducing the number of parameters. However, as mentioned above, the number of operations is also an important factor to consider in real-world scenarios. Consider a situation where an SR system operates on a mobile device. Then, the execution speed of the system is also of crucial importance from a user-experience perspective. Especially the battery capacity, which is heavily dependent on the amount of computation performed, becomes a major problem. In this respect, reducing the number of operations in the deep learning architectures is a challenging and necessary step that has largely been ignored until now. Another scenario relates to applying SR methods to video streaming services. The demand for streaming media has skyrocketed and hence requires large storage to store massive multimedia data. It is therefore imperative to compress data using lossy compression techniques before storing. Then, an SR technique can be applied to restore the data to the original resolution. However, because latency is the most critical factor in streaming services, the decompression process (i.e., super-resolution) has to be performed in real-time. To do so, it is essential to make the SR methods lightweight in terms of the number of operations.</p><p>To handle these requirements and improve the recent models, we propose a Cascading residual network (CARN) and its variant CARN-Mobile (CARN-M). We first build our CARN model to increase the performance and extend it to CARN-M to optimize it for speed and the number of operations. Following the FSRCNN <ref type="bibr" target="#b6">[7]</ref>, CARN family take the LR images and compute the HR counterparts as the output of the network. The middle parts of our models are designed based on the ResNet <ref type="bibr" target="#b12">[13]</ref>. The ResNet architecture has been widely used in deep learning-based SR methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref> because of the ease of training and superior performance. In addition to the ResNet architecture, CARN uses a cascading mechanism at both the local and the global level to incorporate the features from multiple layers. This has the effect of reflecting various levels of input representations in order to receive more information. In addition to the CARN model, we also provide the CARN-M model that allows the designer to tune the trade-off between the performance and the heaviness of the model. It does so by means of the efficient residual block (residual-E) and recursive network architecture, which we describe in more detail in Section 3.</p><p>In summary, our main contributions are as follows: 1) We propose CARN, a neural network based on the cascading modules, which achieves high performance on SR task ( <ref type="figure" target="#fig_0">Fig. 1</ref>). Our cascading modules, effectively boost the performance via multi-level representation and multiple shortcut connections. 2) We also propose CARN-M for efficient SR by combining the efficient residual block and the recursive network scheme. 3) We show through extensive experiments, that our model uses only a modest number of operations and parameters to achieve competitive results. Our CARN-M, which is the more lightweight SR model, shows comparable results to others with much fewer operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Since the success of AlexNet <ref type="bibr" target="#b22">[23]</ref> in image recognition task <ref type="bibr" target="#b4">[5]</ref>, many deep learning approaches have been applied to diverse computer vision tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>. The SISR task is one such task, and we present an overview of the deep learningbased SISR in section 2.1. Another area we deal with in this paper is model compression. Recent deep learning models focus on squeezing model parameters and operations for application in low-power computing devices, which has many practical benefits in real-world applications. We briefly review in section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning Based Image Super-Resolution</head><p>Recently, deep learning based models have shown dramatic improvements in the SISR task. Dong et al. <ref type="bibr" target="#b5">[6]</ref> first proposed a deep learning-based SR method, SRCNN, which outperformed traditional algorithms. However, SRCNN has a large number of operations compared to its depth, since network takes upsampled images as input. Taking a different approach from SRCNN, FSRCNN <ref type="bibr" target="#b6">[7]</ref> and ESPCN <ref type="bibr" target="#b32">[33]</ref> upsample images at the end of the networks. By doing so, it leads to the reduction in the number of operations compared to the SRCNN. However, the overall performance could be degraded if there are not enough layers after the upsampling process. Moreover, they cannot manage multi-scale training, as the input image size differs for each upsampling scale.</p><p>Despite the fact that the power of deep learning comes from deep layers, the aforementioned methods have settled for shallow layers because of the difficulty  <ref type="bibr" target="#b19">[20]</ref> proposed VDSR, which uses residual learning to map the LR images x to their residual images r. Then, VDSR produces the SR images y by adding the residual back into the original, i.e., y = x + r. On the other hand, LapSRN <ref type="bibr" target="#b23">[24]</ref> uses a Laplacian pyramid architecture to increase the image size gradually. By doing so, LapSRN effectively performs SR on extremely low-resolution cases with a fewer number of operations compared to VDSR. Another issue of deep learning-based SR is how to reduce the parameters and operation. For example, DRCN <ref type="bibr" target="#b20">[21]</ref> uses a recursive network to reduce parameters by engaging in redundant usages of a small number of parameters. DRRN <ref type="bibr" target="#b34">[35]</ref> improves DRCN by combining the recursive and residual network schemes to achieve better performance with fewer parameters. However, DRCN and DRRN use very deep networks to compensate for the loss of performance and hence these require heavy computing resources. Hence, we aim to build a model that is lightweight in both size and computation. We will briefly discuss previous works that address such model efficiency issues in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Neural Network</head><p>There has been rising interest in building small and efficient neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. These approaches can be categorized into two groups: 1) Compressing pretrained networks, and 2) designing small but efficient models. Han et al. <ref type="bibr" target="#b10">[11]</ref> proposed deep compressing techniques, which consist of pruning, vector quanti- zation, and Huffman coding to reduce the size of a pretrained network. In the latter category, SqueezeNet <ref type="bibr" target="#b18">[19]</ref> builds an AlexNet-based architecture and achieves comparable performance level with 50× fewer parameters. MobileNet <ref type="bibr" target="#b15">[16]</ref> builds an efficient network by applying depthwise separable convolution introduced in Sifre et al. <ref type="bibr" target="#b33">[34]</ref>. Because of this simplicity, we also apply this technique in the residual block with some modification to achieve a lean neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>As mentioned in Section 1, we propose two main models: CARN and CARN-M. CARN is designed to be a high-performing SR model while suppressing the number of operations compared to the state-of-the-art methods. Based on CARN, we design CARN-M, which is a much more efficient SR model in terms of both parameters and operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cascading Residual Network</head><p>Our CARN model is based on ResNet <ref type="bibr" target="#b13">[14]</ref>. The main difference between CARN and ResNet is the presence of local and global cascading modules. <ref type="figure">Fig. 2</ref> (b) graphically depicts how the global cascading occurs. The outputs of intermediary layers are cascaded into the higher layers, and finally converge on a single 1x1 convolution layer. Note that the intermediary layers are implemented as cascading blocks, which host local cascading connections themselves. Such local cascading operations are shown in <ref type="figure">Figure 2</ref>(c) and (d). Local cascading is almost identical to a global one, except that the unit blocks are plain residual blocks.</p><p>To express the implementation formally, let f be a convolution function and τ be an activation function. Then, we can define the i-th residual block R i , which has two convolutions followed by a residual addition, as</p><formula xml:id="formula_0">R i (H i−1 ; W i R ) = τ (f (τ (f (H i−1 ; W i,1 R )); W i,2 R ) + H i−1 ).<label>(1)</label></formula><p>Here, H i is the output of the i-th residual block, W i R is the parameter set of the residual block, and W i,j R is the parameter of the j-th convolution layer in the i-th block. With this notation, we denote the output feature of the final residual block of ResNet as H u , which becomes the input to the upsampling block.</p><formula xml:id="formula_1">H u = R u . . . R 1 f (X; W c ) ; W 1 R . . . ; W u R .<label>(2)</label></formula><p>Note that because our model has a single convolution layer before each residual block, the first residual block gets f (X; W c ) as input, where W c is the parameter of the convolution layer.</p><p>In contrast to ResNet, our CARN model has a local cascading block illustrated in block (c) of <ref type="figure" target="#fig_1">Fig. 3</ref> instead of a plain residual block. In here, we denote B i,j as the output of the j-th residual block in the i-th cascading block, and W i c as the set of parameters of the i-th local cascading block. Then, the i-th local cascading block B i local is defined as</p><formula xml:id="formula_2">B i local H i−1 ; W i l ≡ B i,U ,<label>(3)</label></formula><p>where B i,U is defined recursively from the B i,u 's as:</p><formula xml:id="formula_3">B i,0 = H i−1 B i,u = f I, B i,0 , . . . , B i,u−1 , R u B i,u−1 ; W u R ; W i,u c for u = 1, . . . , U .</formula><p>Finally, we can define the output feature of the final cascading block H b by combining both the local and global cascading. Here, H 0 is the output of the first convolution layer. And we we fix u = b = 3 for our CARN and CARN-M.</p><formula xml:id="formula_4">H 0 = f (X; W c ) H b = f H 0 , . . . , H b−1 , B u local H b−1 ; W b B )] for b = 1, . . . , B.<label>(4)</label></formula><p>The main difference between CARN and ResNet lies in the cascading mechanism. As shown in <ref type="figure">Fig. 2</ref>, CARN has global cascading connections represented as the blue arrows, each of which is followed by a 1×1 convolution layer. Cascading on both the local and global levels has two advantages: 1) The model incorporates features from multiple layers, which allows learning multi-level representations. 2) Multi-level cascading connection behaves as multi-level shortcut connections that quickly propagate information from lower to higher layers (and vice-versa, in case of back-propagation).</p><p>CARN adopts a multi-level representation scheme as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>, but we apply this arrangement to a variety of feature levels to boost performance, as shown in equation 4. By doing so, our model reconstructs the LR image based on multilevel features. This facilitates the model to restore the details and contexts of the image simultaneously. As a result, our models effectively improve not only primitive objects but also complex objects.</p><p>Another reason for adopting the cascading scheme is two-fold: First, the propagation of information follows multiple paths <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref>. Second, by adding extra convolution layers, our model can learn to choose the right pathway with the given input information flows. However, the strength of multiple shortcuts is degraded when we use only one of local or global cascading, especially the local connection. We elaborate the details and present a case study on the effects of cascading mechanism in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficient Cascading Residual Network</head><p>To improve the efficiency of CARN, we propose an efficient residual (residual-E) block. We use a similar approach to the MobileNet <ref type="bibr" target="#b15">[16]</ref>, but use group convolution instead of depthwise convolution. Our residual-E block consists of two 3×3 group and one pointwise convolution, as shown in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>. The advantage of using group convolution over the depthwise convolution is that it makes the efficiency of the model tunable. The user can choose the group size appropriately since the group size and performance are in a trade-off relationship. The analysis on the cost efficiency of using the residual-E block is as follows.</p><p>Let K be the kernel size and C in , C out be the number of input and output channels. Because we retain the feature resolution of the input and output by padding, we can denote F to be both the input and output feature size. Then, the cost of a plain residual block is as 2 × (K · K · C in · C out · F · F ). Note that we only count the cost of convolution layers and ignore the addition or activation because both the plain and the efficient residual blocks have the same amount of cost in terms of addition and activation.</p><p>Let G be the group size. Then, the cost of a residual-E block, which consist of two group convolutions and one pointwise convolution, is as given in equation 5.</p><formula xml:id="formula_5">2 × K · K · C in · C out G · F · F + C in · C out · F · F<label>(5)</label></formula><p>By changing the plain residual block to our efficient residual block, we can reduce the computation by the ratio of</p><formula xml:id="formula_6">2 × K · K · C in · Cout G · F · F + C in · C out · F · F 2 × (K · K · C in · C out · F · F ) = 1 G + 1 2K 2 .<label>(6)</label></formula><p>Because our model uses a kernel of size 3×3 for all group convolutions, and the number of channels is constantly 64, using an efficient residual block instead of a standard residual block can reduce the computation from 1.8 up to 14 times depending on the group size. To find the best trade-off between performance and computation, we performed an extensive case study in Section 4.4.</p><p>To further reduce the parameters, we apply a technique similar to the one used by the recursive network. That is, we make the parameters of the Cascading blocks shared, effectively making the blocks recursive. <ref type="figure" target="#fig_1">Fig. 3 (d)</ref> shows our block after applying the recursive scheme. This approach reduces the parameters by up to three times of their original number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison to Recent Models</head><p>Comparison to SRDenseNet. SRDenseNet <ref type="bibr" target="#b36">[37]</ref> uses dense block and skip connection. The differences from our model are: 1) We use global cascading, which is more general than the skip connection. In SRDenseNet, all levels of features are combined at the end of the final dense block, but our global cascading scheme connects all blocks, which behaves as multi-level skip connection. 2) SRDenseNet preserves local information of dense block via concatenation operations, while we gather it progressively by 1 × 1 convolution layers. The use of additional 1×1 convolution layers results in a higher representation power. Comparison to MemNet. The motivation of MemNet <ref type="bibr" target="#b35">[36]</ref> and ours is similar. However, there are two main differences from our mechanism. 1) Inside of the memory blocks of MemNet, the output features of each recursive units are concatenated at the end of the network and then fused with 1×1 convolution. On the other hand, we fuse the features at every possible point in the local block, which can boost up the representation power via the additional convolution layers and nonlinearity. In general, this representation power is often not met because of the difficulty of training. However, we overcome this problem by using both local and global cascading mechanisms. We will discuss the details on Section 4.4. 2) MemNet takes upsampled images as input so the number of multi-adds is larger than ours. The input to our model is a LR image and we upsample it at the end of the network in order to achieve computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>There exist diverse single image super-resolution datasets, but the most widely used ones are the 291 image set by Yang et al. <ref type="bibr" target="#b38">[39]</ref> and the Berkeley Segmentation Dataset <ref type="bibr" target="#b1">[2]</ref>. However, because these two do not have sufficient images for training a deep neural network, we additionally use the DIV2K dataset <ref type="bibr" target="#b0">[1]</ref>. The DIV2K dataset is a newly-proposed high-quality image dataset, which consists of 800 training images, 100 validation images, and 100 test images. Because of the richness of this dataset, recent SR models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> use DIV2K as well. We use the standard benchmark datasets such as Set5 <ref type="bibr" target="#b2">[3]</ref>, Set14 <ref type="bibr" target="#b38">[39]</ref>, B100 <ref type="bibr" target="#b28">[29]</ref> and Urban100 <ref type="bibr" target="#b17">[18]</ref> for testing and benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and Training Details</head><p>We use the RGB input patches of size 64×64 from the LR images for training. We sample the LR patches randomly and augment them with random horizontal flips and 90 degree rotation. We train our models with the ADAM optimizer <ref type="bibr" target="#b21">[22]</ref> by setting β 1 = 0.9, β 2 = 0.999, and = 10 −8 in 6×10 5 steps. The minibatch size is 64, and the learning rate begins with 10 −4 and is halved every 4 × 10 5 steps. All the weights and biases are initialized by θ ∼ U (−k, k) with k = 1/ √ c in where, c in is the number of channels of input feature map. The most well-known and effective weight initialization methods are given by Glorot et al. <ref type="bibr" target="#b9">[10]</ref> and He et al. <ref type="bibr" target="#b11">[12]</ref>. However, such initialization routines tend to set the weights of our multiple narrow 1×1 convolution layers very high, resulting in an unstable training. Therefore, we sample the initial values from a uniform distribution to alleviate the initialization problem.</p><p>To train our model in a multi-scale manner, we first set the scaling factor to one of ×2, ×3, and ×4 because our model can only process a single scale for each batch. Then, we construct and argument our input batch, as described above. We use the L1 loss as our loss function instead of the L2. The L2 loss is widely used in the image restoration task due to its relationship with the peak signal-to-noise ratio (PSNR). However, in our experiments, L1 provides better convergence and performance. The downside of the L1 loss is that the convergence speed is relatively slower than that of L2 without the residual block. However, this drawback could be mitigated by using a ResNet style model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-art Methods</head><p>We compare the proposed CARN and CARN-M with state-of-the-art SR methods on two commonly-used image quality metrics: PSNR and the structural similarity index (SSIM) <ref type="bibr" target="#b37">[38]</ref>. One thing to note here is that we represent the number of operations by Mult-Adds. Mult-Adds is the number of composite multiply-accumulate operations for a single image. We assume the HR image size to be 720p (1280×720) to calculate Multi-Adds. In <ref type="figure" target="#fig_2">Fig. 4</ref>, we compare our CARN family against the various benchmark algorithms in terms of the Mult-Adds and the number of the parameters on the Set14 ×4 dataset. Here, our CARN model outperforms all state-of-the-art models that have less than 5M parameters. Especially, CARN has similar number of parameters to that of DRCN <ref type="bibr" target="#b20">[21]</ref>, SelNet <ref type="bibr" target="#b3">[4]</ref> and SRDenseNet <ref type="bibr" target="#b36">[37]</ref>, but we outperform all three models.</p><p>The MDSR <ref type="bibr" target="#b25">[26]</ref> achieves better performance than ours, which is not surprising because MDSR has 8M parameters which are nearly six times more parameters than ours. The CARN-M model also outperforms most of the benchmark methods and shows comparable results against the heavy models.</p><p>Moreover, our models are most efficient in terms of the computation cost: CARN shows second best results with 90.9G Mult-Adds, which is on par with SelNet <ref type="bibr" target="#b3">[4]</ref>. This efficiency mainly comes from the late-upsample approach that many recent models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37]</ref> used. In addition, our novel cascading mechanism shows increased performance compared to other similar approaches. For example, CARN outperforms its most similar model SelNet by a margin of 0.11 PSNR using similar number of operations. Also, the CARN-M model obtains comparable results against computationally-expensive models, while only requiring the similar number of the operations with respect to SRCNN. <ref type="table" target="#tab_1">Table 1</ref> also shows the quantitative comparisons of the performances over the benchmark datasets. Note that MDSR is excluded from this table, because we only compare models that have roughly similar number of parameters as ours; MDSR has a parameter set whose size is four times larger than that of the second-largest model. Our CARN exceeds all the previous methods on numerous benchmark dataset. CARN-M model achieves comparable results using very few operations. We would also like to emphasize that although CARN-M has more parameters than SRCNN or DRRN, it is tolerable in real-world scenarios. The sizes of SRCNN and CARN-M are 200KB and 1.6MB, respectively, all of which are acceptable on recent mobile devices.</p><p>To make our models even more lightweight, we apply the multi-scale learning approach. The benefit of using multi-scale learning is that it can process multiple scales using a single trained model. This helps us alleviate the burden of heavy-weight model size when deploying the SR application on mobile devices; CARN(-M) only needs a single fixed model for multiple scales, whereas even the state-of-the-art algorithms require to train separate models for each supported scale. This property is well-suited for real-world products because the size of the applications has to be fixed while the scale of given LR images could vary. Using the multi-scale learning to our models increases the number of parameters, since the network has to contain possible upsampling layers. On the other hand, VDSR and DRRN do not require this extra burden, even if multi-scale learning is performed, because they upsample the image before processing it.</p><p>In <ref type="figure" target="#fig_4">Fig. 6</ref>, we visually illustrate the qualitative comparisons over three datasets (Set14, B100 and Urban100) for ×4 scale. It can be seen that our model works better than others and accurately reconstructs not only stripes and line patterns, but also complex objects such as hand and street lamps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis</head><p>To further investigate the performance behavior of the proposed methods, we analyze our models via ablation study. First, we show how local and global cascading modules affect the performance of CARN. Next, we analyze the tradeoff between performance vs. parameters and operations. Cascading Modules. <ref type="table" target="#tab_2">Table 2</ref> presents the ablation study on the effect of local and global cascading modules. In this table, the baseline is ResNet, CARN-NL is CARN without local cascading and CARN-NG is CARN without global cascading. The network topologies are all same, but because of the 1×1 convolution layer, the overall number of parameters is increased by up to 10%.</p><p>We see that the model with only global cascading (CARN-NL) shows better performance than the baseline because the global cascading mechanism effectively carries mid-to high-level frequency signals from shallow to deep layers. Furthermore, by gathering all features before the upsampling layers, the model can better leverage multi-level representations. By incorporating multi-level representations, the CARN model can consider a variety of information from many different receptive fields when reconstructing the image. Somewhat surprisingly, using only local cascading blocks (CARN-NG) harms the performance. As discussed in He et al. <ref type="bibr" target="#b14">[15]</ref>, multiplicative manipulations such as 1×1 convolution on the shortcut connection can hamper information propagation, and thus lead to complications during optimization. Similarly, cascading connections in the local cascading blocks of CARN-NG behave as shortcut connections inside the residual blocks. Because these connections consist of concatenation and 1×1 convolutions, it is natural to expect performance degradation. That is, the advantage of multi-level representation is limited to the inside of each local cascading block. Therefore, there appears to be no benefit of using the cascading connection because of the increased number of multiplication operations in the cascading connection. However, CARN uses both local and global cascading levels and outperforms all three models. This is because the global cascading mechanism eases the information propagation issues that CARN-NG suffers from. In detail, information propagates globally via global cascading, and information flows in the local cascading blocks are fused with the ones that come through global connections. By doing so, information is transmitted by multiple shortcuts and thus mitigates the vanishing gradient problem. In other words, the advantage of multi-level representation is leveraged by the global cascading connections, which help the information to propagate to higher layers.</p><p>Efficiency Trade-off. <ref type="figure" target="#fig_3">Fig. 5</ref> depicts the trade-off study of PSNR vs. parameters, and PSNR vs. operations in relation to the efficient residual block and recursive network. In this experiment, we evaluate all possible group sizes of the efficient residual block for both the recursive and non-recursive cases. In both graphs, the blue line represents the model that does not use the recursive scheme and the orange line is the model that uses recursive cascading block.</p><p>Although all efficient models perform worse than the CARN, which shows 28.70 PSNR, the number of parameters and operations are decreased dramatically. For example, the G64 shows a five-times reduction in both parameters and operations. However, unlike the comparable result that is shown in Howard et al. <ref type="bibr" target="#b15">[16]</ref>, the degradation of performance is more pronounced in our case.</p><p>Next, we observe the case which uses the recursive scheme. As illustrated in <ref type="figure" target="#fig_3">Fig. 5b</ref>, there is no change in the Mult-Adds but the performance worsens, which . We evaluate all models on Set14 with ×4 scale. GConv represents the group size of group convolution and R means the model with the recursive network scheme (i.e., G4R represents group four with recursive cascading blocks).</p><p>seems reasonable given the decreased number of parameters in the recursive scheme. On the other hand, <ref type="figure" target="#fig_3">Fig. 5a</ref> shows that using the recursive scheme makes the model achieve better performance with fewer parameters. Based on these observations, we decide to choose the group size as four in the efficient residual block and use the recursive network scheme as our CARN-M model. By doing so, CARN-M reduces the number of parameters by five times and the number of operations by nearly four times with a loss of 0.29 PSNR compared to CARN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a novel cascading network architecture that can perform SISR accurately and efficiently. The main idea behind our architecture is to add multiple cascading connections starting from each intermediary layer to the others. Such connections are made on both the local (block-wise) and global (layer-wise) levels, which allows for the efficient flow of information and gradient. Our experiments show that employing both types of connections greatly outperforms those using only one or none at all. We wish to further develop this work by applying our technique to video data. Many streaming services require large storage to provide high-quality videos. In conjunction with our approach, one may devise a service that stores low-quality videos that go through our SR system to produce high-quality videos on-the-fly. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Super-resolution result of our methods compared with existing methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Residual Block (b) Residual-E Block (c) Cascading Block (d) Recursive Block Simplified structures of (a) residual block (b) efficient residual block (residual-E), (c) cascading block and (d) recursive cascading block. The ⊕ operations in (a) and (b) are element-wise addition for residual learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Trade-off between performance vs. number of operations and parameters on Set14 ×4 dataset. The x-axis and the y-axis denote the Multi-Adds and PSNR, and the size of the circle represents the number of parameters. The Mult-Adds is computed by assuming that the resolution of HR image is 720p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Results of using efficient residual block and recursive network in terms of PSNR vs. parameters(left) and PSNR vs. operations(right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Visual qualitative comparison on ×4 scale datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results of deep learning-based SR algorithms. Red/blue text: best/second-best.</figDesc><table><row><cell cols="2">Scale Model</cell><cell cols="2">Params MultAdds</cell><cell cols="3">Set5 PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM Set14 B100 Urban100</cell></row><row><cell></cell><cell>SRCNN [6]</cell><cell>57K</cell><cell cols="4">52.7G 36.66/0.9542 32.42/0.9063 31.36/0.8879 29.50/0.8946</cell></row><row><cell></cell><cell>FSRCNN [7]</cell><cell>12K</cell><cell cols="4">6.0G 37.00/0.9558 32.63/0.9088 31.53/0.8920 29.88/0.9020</cell></row><row><cell></cell><cell>VDSR [20]</cell><cell>665K</cell><cell cols="4">612.6G 37.53/0.9587 33.03/0.9124 31.90/0.8960 30.76/0.9140</cell></row><row><cell></cell><cell>DRCN [21]</cell><cell cols="5">1,774K 17,974.3G 37.63/0.9588 33.04/0.9118 31.85/0.8942 30.75/0.9133</cell></row><row><cell></cell><cell>CNF [31]</cell><cell>337K</cell><cell cols="4">311.0G 37.66/0.9590 33.38/0.9136 31.91/0.8962</cell><cell>-</cell></row><row><cell>2</cell><cell>LapSRN [24] DRRN [35]</cell><cell cols="5">813K 297K 6,796.9G 37.74/0.9591 33.23/0.9136 32.05/0.8973 31.23/0.9188 29.9G 37.52/0.9590 33.08/0.9130 31.80/0.8950 30.41/0.9100</cell></row><row><cell></cell><cell>BTSRN [8]</cell><cell>410K</cell><cell cols="2">207.7G 37.75/-</cell><cell>33.20/-</cell><cell>32.05/-</cell><cell>31.63/-</cell></row><row><cell></cell><cell>MemNet [36]</cell><cell cols="5">677K 2,662.4G 37.78/0.9597 33.28/0.9142 32.08/0.8978 31.31/0.9195</cell></row><row><cell></cell><cell>SelNet [4]</cell><cell>974K</cell><cell cols="4">225.7G 37.89/0.9598 33.61/0.9160 32.08/0.8984</cell><cell>-</cell></row><row><cell></cell><cell>CARN (ours)</cell><cell>1,592K</cell><cell cols="4">222.8G 37.76/0.9590 33.52/0.9166 32.09/0.8978 31.92/0.9256</cell></row><row><cell></cell><cell>CARN-M (ours)</cell><cell>412K</cell><cell cols="4">91.2G 37.53/0.9583 33.26/0.9141 31.92/0.8960 31.23/0.9193</cell></row><row><cell></cell><cell>SRCNN [6]</cell><cell>57K</cell><cell cols="4">52.7G 32.75/0.9090 29.28/0.8209 28.41/0.7863 26.24/0.7989</cell></row><row><cell></cell><cell>FSRCNN [7]</cell><cell>12K</cell><cell cols="4">5.0G 33.16/0.9140 29.43/0.8242 28.53/0.7910 26.43/0.8080</cell></row><row><cell></cell><cell>VDSR [20]</cell><cell>665K</cell><cell cols="4">612.6G 33.66/0.9213 29.77/0.8314 28.82/0.7976 27.14/0.8279</cell></row><row><cell></cell><cell>DRCN [21]</cell><cell cols="5">1,774K 17,974.3G 33.82/0.9226 29.76/0.8311 28.80/0.7963 27.15/0.8276</cell></row><row><cell></cell><cell>CNF [31]</cell><cell>337K</cell><cell cols="4">311.0G 33.74/0.9226 29.90/0.8322 28.82/0.7980</cell><cell>-</cell></row><row><cell>3</cell><cell>DRRN [35]</cell><cell cols="5">297K 6,796.9G 34.03/0.9244 29.96/0.8349 28.95/0.8004 27.53/0.8378</cell></row><row><cell></cell><cell>BTSRN [8]</cell><cell>410K</cell><cell cols="2">176.2G 34.03/-</cell><cell>29.90/-</cell><cell>28.97/-</cell><cell>27.75/-</cell></row><row><cell></cell><cell>MemNet [36]</cell><cell cols="5">677K 2,662.4G 34.09/0.9248 30.00/0.8350 28.96/0.8001 27.56/0.8376</cell></row><row><cell></cell><cell>SelNet [4]</cell><cell>1,159K</cell><cell cols="4">120.0G 34.27/0.9257 30.30/0.8399 28.97/0.8025</cell><cell>-</cell></row><row><cell></cell><cell>CARN (ours)</cell><cell>1,592K</cell><cell cols="4">118.8G 34.29/0.9255 30.29/0.8407 29.06/0.8034 28.06/0.8493</cell></row><row><cell></cell><cell>CARN-M (ours)</cell><cell>412K</cell><cell cols="4">46.1G 33.99/0.9236 30.08/0.8367 28.91/0.8000 27.55/0.8385</cell></row><row><cell></cell><cell>SRCNN [6]</cell><cell>57K</cell><cell cols="4">52.7G 30.48/0.8628 27.49/0.7503 26.90/0.7101 24.52/0.7221</cell></row><row><cell></cell><cell>FSRCNN [7]</cell><cell>12K</cell><cell cols="4">4.6G 30.71/0.8657 27.59/0.7535 26.98/0.7150 24.62/0.7280</cell></row><row><cell></cell><cell>VDSR [20]</cell><cell>665K</cell><cell cols="4">612.6G 31.35/0.8838 28.01/0.7674 27.29/0.7251 25.18/0.7524</cell></row><row><cell></cell><cell>DRCN [21]</cell><cell cols="5">1,774K 17,974.3G 31.53/0.8854 28.02/0.7670 27.23/0.7233 25.14/0.7510</cell></row><row><cell></cell><cell>CNF [31]</cell><cell>337K</cell><cell cols="4">311.0G 31.55/0.8856 28.15/0.7680 27.32/0.7253</cell><cell>-</cell></row><row><cell></cell><cell>LapSRN [24]</cell><cell>813K</cell><cell cols="4">149.4G 31.54/0.8850 28.19/0.7720 27.32/0.7280 25.21/0.7560</cell></row><row><cell>4</cell><cell>DRRN [35]</cell><cell cols="5">297K 6,796.9G 31.68/0.8888 28.21/0.7720 27.38/0.7284 25.44/0.7638</cell></row><row><cell></cell><cell>BTSRN [8]</cell><cell>410K</cell><cell cols="2">165.2G 31.85/-</cell><cell>28.20/-</cell><cell>27.47/-</cell><cell>25.74/-</cell></row><row><cell></cell><cell>MemNet [36]</cell><cell cols="5">677K 2,662.4G 31.74/0.8893 28.26/0.7723 27.40/0.7281 25.50/0.7630</cell></row><row><cell></cell><cell>SelNet [4]</cell><cell>1,417K</cell><cell cols="4">83.1G 32.00/0.8931 28.49/0.7783 27.44/0.7325</cell><cell>-</cell></row><row><cell></cell><cell cols="2">SRDenseNet [37] 2,015K</cell><cell cols="4">389.9G 32.02/0.8934 28.50/0.7782 27.53/0.7337 26.05/0.7819</cell></row><row><cell></cell><cell>CARN (ours)</cell><cell>1,592K</cell><cell cols="4">90.9G 32.13/0.8937 28.60/0.7806 27.58/0.7349 26.07/0.7837</cell></row><row><cell></cell><cell>CARN-M (ours)</cell><cell>412K</cell><cell cols="4">32.5G 31.92/0.8903 28.42/0.7762 27.44/0.7304 25.62/0.7694</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effects of the global and local cascading modules measured on the Set14 ×4 dataset. CARN-NL represents CARN without local cascading and CARN-NG without global cascading. CARN is our final model.</figDesc><table><row><cell>Local Cascading Global Cascading</cell><cell cols="4">Baseline CARN-NL CARN-NG CARN ! ! ! !</cell></row><row><cell># Params.</cell><cell>1,444K</cell><cell>1,481K</cell><cell>1,555K</cell><cell>1,592K</cell></row><row><cell>PSNR</cell><cell>28.43</cell><cell>28.45</cell><cell>28.42</cell><cell>28.52</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement.</head><p>This research was supported through the National Research Foundation of Korea </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network with selection units for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Balanced two-stage residual networks for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-level and multi-scale feature aggregation using pretrained convolutional neural networks for music auto-tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1208" to="1212" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image super resolution based on fusing multiple convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient subpixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
