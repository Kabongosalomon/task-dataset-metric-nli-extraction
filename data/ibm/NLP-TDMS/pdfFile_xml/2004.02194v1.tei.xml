<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iterative Context-Aware Graph Inference for Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Guo</surname></persName>
							<email>guodan@hfut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Iterative Context-Aware Graph Inference for Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both objectbased (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, cross-modal semantic understanding between vision and language has attracted more and more interests, such as image captioning <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b20">20]</ref>, referring expression <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b21">21]</ref>, and visual question answering (VQA) <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39]</ref>. In these works, the co-reference between vision and language is usually performed in a single round. Taking VQA as an example, given an image and * Corresponding authors.  <ref type="figure">Figure 1</ref>. Different graph structures for visual dialog. In our solution (c), we focus on a question-conditioned context-aware graph, including both fine-grained visual-objects and textual-history semantics.</p><p>a question, the agent identifies the interest areas related to the question and infers an answer. In contrast, visual dialog <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b16">16</ref>] is a multi-round extension for VQA. The interactions between the image and multi-round questionanswer pairs (history) are progressively changing, and the relationships among the objects in the image are influenced by the current question. Visual dialog is a challenging task due to these underlying semantic dependencies in the textual and visual contexts. Therefore, how to effectively realize the context-aware relational reasoning is vital to the task. For relational reasoning, the graph structure is employed to exploit the context-aware co-reference of image and history. Except for the graph structure referring to different multi-modal entities as shown in <ref type="figure">Fig. 1</ref>, prior graph-based models considered the fixed graph attention or embedding, such as fixed fully-connected graph (FGA <ref type="bibr" target="#b28">[28]</ref>), fixed once graph attention evolution (FGA <ref type="bibr" target="#b28">[28]</ref>) and fixed unidirectional message passing (GNN <ref type="bibr" target="#b42">[42]</ref>). In this paper, we are inspired by the nature of the visual dialog task, i.e., the dynamic multi-modal co-references in multi-round conversations. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the flexibility and adaptivity of our graph-based method, which iteratively evolves by adaptive top-K and adaptive-directional message passing. The significance of our method is that it exploits the image-history co-reference in a dynamic adaptive graph learning mode.</p><p>In order to comprehend the complex multi-modal coreference relationships over time, we propose a Context-Aware Graph (CAG) neural network. As shown in <ref type="figure">Fig. 1</ref> (c), each node in our graph is a multi-modal context representation, which contains both visual objects and textual history contexts; each edge contains the fine-grained visual interactions of the scene objects in the image. In CAG, all the nodes and edges are iteratively updated through an adaptive top-K message passing mechanism. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, in every message passing step, each graph node adaptively selects the most K-relevant nodes, and only receives the messages from them. It means that our CAG solution is an asymmetric dynamic directed graph, which observes adaptive message passing in the graph structure. Note that iterative CAG graph inference is shown as an effective realization of humans' multi-step reasoning <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b12">12]</ref>. Finally, after the multi-turn graph inference, we impose a graph attention on all the nodes to obtain the final graph embedding for the answer prediction. <ref type="figure" target="#fig_1">Fig. 2</ref> provides an overview of the proposed CAG. Specifically, CAG consists of three components: (1) Graph Construction (Sec. 3.1), which constructs the contextaware graph based on the representations of dialog-history and objects in the image; (2) Iterative Dynamic Directed-Graph Inference (Sec. 3.2), the context-aware graph is iteratively updated via T -step dynamic directed-graph inference; (3) Graph Attention Embedding (Sec. 3.3), which applies a graph attention to aggregate the rich node semantics. Then, we jointly utilize the generated graph, the encoded question, and the history context features to infer the final answer.</p><p>The contributions are summarized as follows. We propose a Context-Aware Graph (CAG) neural network for visual dialog, which targets at discovering the partially relevant contexts and building the dynamic graph structure. <ref type="bibr" target="#b0">(1)</ref> We build a fine-grained graph representation with various visual objects and attentive history semantics. The context cues on each node not only refer to the joint visual-textual semantic learning, but also involve iterative relational reasoning among image I, question Q and history H. (2) To eliminate the useless relations among the nodes, we design an adaptive top-K message passing mechanism and a graph attention to pick up more relevant context nodes. Each node has different related neighbors (different relations). As for the same node, the inbound and outbound messages vary from iteration to iteration. (3) Extensive experiments are conducted on VisDial v0.9 and v1.0 datasets, and CAG achieves new state-of-the-art performances out of the pervious graph-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relate Work</head><p>Visual Dialog. For the visual dialog task <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b16">16]</ref>, current encoder-decoder based works can be divided into three facets. (1) Fusion-based models. Late fusion (LF) <ref type="bibr" target="#b5">[5]</ref> and hierarchical recurrent network (HRE) <ref type="bibr" target="#b5">[5]</ref> directly encoded the multi-modal inputs and decoded the answer. (2) Attention-based models. To improve performance, various attention mechanisms have been widely used in the task, including history-conditioned image attention (HCIAE) <ref type="bibr" target="#b23">[23]</ref>, sequential co-attention (CoAtt) <ref type="bibr" target="#b34">[34]</ref>, dual visual attention (DVAN) <ref type="bibr" target="#b9">[9]</ref>, and recurrent dual attention (ReDAN) <ref type="bibr" target="#b7">[7]</ref>. (3) Visual co-reference resolution models. Some attentionbased works focused on explicit visual co-reference resolution. Seo et al. <ref type="bibr" target="#b29">[29]</ref> designed an attention memory (AMEM) to store previous visual attention distrubution. Kottur et al. <ref type="bibr" target="#b15">[15]</ref> utilized neural module networks <ref type="bibr" target="#b2">[2]</ref> to handle visual co-reference resolution at word-level. Niu et al. <ref type="bibr" target="#b24">[24]</ref> proposed a recursive visual attention (RvA) mechanism to recursively reviews history to refine visual attention. Graph Neural Network (GNN). Graph neural networks have attracted attention in various tasks <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b26">26]</ref>. The core idea is to combine the graphical structural representation with neural networks, which is suitable for reasoning-style tasks. Liu et al. <ref type="bibr" target="#b31">[31]</ref> proposed the first GNN-based approach for VQA, which built a scene graph of the image and parsed the sentence structure of the question, and calculated their similarity weights. Later, Norcliffe-Brown et al. <ref type="bibr" target="#b25">[25]</ref> modeled a graph representation conditioned on the question, and exploited a novel graph convolution to capture the interactions among different detected object nodes. As for visual dialog, there are merely two related works. Zheng et al. <ref type="bibr" target="#b42">[42]</ref> proposed an EM-style GNN to conduct the textual co-reference; it regarded the caption and the previous question-answer (QA) pairs as observed nodes, and the current answer was deemed as an unobserved node inferred using EM algorithm on the textual contexts. Schwartz et al. <ref type="bibr" target="#b28">[28]</ref> proposed a factor graph attention mechanism, which constructed the graph over all the multi-modal features and estimated their interactions. <ref type="figure">Fig. 1</ref> illustrates the difference between our work and other two graph-based models <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b42">42]</ref>. In this paper, we build a fine-grained context-aware graph, which involves the context co-reference in and between specific objects and history snippets under the guidance of word-level attended question semantics. Apart from the fine-grained objectlevel features of I (visual contexts) on the node representations, both sentence-level and word-level textual semantics of H and Q are utilized in the graph. We implement iterative dynamic message propagation on edges to aggregate the relationships among nodes for answer prediction. In a nutshell, we realize the cross-modal semantic understanding by context-aware relational reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>The visual dialog task refers to relational learning, which involves complicated semantic dependencies among implicit contexts of image, question and history. How to model the context-aware reasoning is critical. In this paper, we propose a dynamic directed-graph inference to iteratively review the multi-modal context cues. Given an image I and the dialog history H = {C, (q 1 , a 1 ) , ..., (q −1 , a −1 )}, where C is the image caption, (q, a) is any question-answer pair and is the turn number of current dialog. The goal of the model is to infer an exact answer for the current question Q by ranking a list of 100 candidate answers A = {a <ref type="bibr" target="#b0">(1)</ref> , ..., a (100) }. The following sub-sections describe the details of the proposed CAG model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Construction</head><p>Feature Representation. Given an image I, we extract the object-level features using Faster-RCNN <ref type="bibr" target="#b0">[1]</ref> and apply a single-layer MLP with activation tanh to encode them into a visual feature sequence V = {v 1 , ..., v n } ∈ R d×n , where n is the number of detected objects. For the current question Q, we first transform it into word embedding vectors W Q = (w 1 , ..., w m ) ∈ R dw×m , where m denotes the number of tokens in Q. Then we use an LSTM to encode W Q into a sequence U Q = (h q 1 , ..., h q m ) ∈ R d×m , and take the last vector h q m as the sentence-level representation of question Q, denoted as q s = h q m . Similarly, we adopt another LSTM to extract the features U H = (h 0 , ..., h −1 ) ∈ R d× of history H at sentence-level, where h 0 is the embedding feature of image caption C.</p><p>As questions in a dialog usually have at least one pronoun (e.g., "it", "they", "he"), the dialogue agent is required to discover the relevant textual contexts in the previous history snippets. We employ a question-conditioned attention to aggregate the textual context cues of history, which can be deemed as textual co-reference. The whole process is formulated as follows:</p><formula xml:id="formula_0">           z h = tanh((W q q s )1 + W h U H ); α h = sof tmax(P h z h ); u = −1 j=0 α h,j U H j ,<label>(1)</label></formula><p>where W q , W h ∈ R d×d and P h ∈ R 1×d are learnable parameters, 1 ∈ R 1× is a vector with all elements set to 1, and α h,j and U H j are respective the j-th element of α h and U H . u ∈ R d×1 denotes the history-related textual context and is further used to construct the context-aware graph. Graph Representation. Visual dialog is an on-going conversation. The relations among different objects in the image frequently dynamically vary according to the conversational content. In order to deeply comprehend the conversational content, we build a context-aware graph, which takes both visual and textual contexts into account. The graph structure (relations among objects) will be later iteratively inferred via an adaptive top-K message passing mechanism in Sec. 3.2. Here we construct a graph G = {N , E}, where the i-th node N i denotes a joint context feature, corresponding to the i-th visual object feature v i and its related context feature c i ; the directed edge E j→i represents the relational dependency from node N j to node N i (i, j ∈ [1, n]). Considering the iterative step t, the graph is denoted as</p><formula xml:id="formula_1">G (t) = {N (t) , E (t) }.</formula><p>There are two cases of N (t) :</p><formula xml:id="formula_2">N (t) = (N (t) 1 , ..., N (t) n ); N (t=1) i = [v i ; u]; N (t&gt;1) i = [v i ; c (t) i ],<label>(2)</label></formula><p>where [; ] is the concatenation operation, textual context u is calculated by Eq. 1, and</p><formula xml:id="formula_3">N (t) ∈ R 2d×n . For node N (t) i</formula><p>in the iterative step t, the visual feature v i is fixed, and we focus on the context learning of c (t) i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Iterative Dynamic Directed-Graph Inference</head><p>Visual dialog contains implicit relationships among the image, question and history. From a technical point of view, the key nature of visual dialog is multi-step reasoning and image-history co-reference. Thus, we address it by adaptively capturing the related visual-historical cues in a dynamic multi-modal co-reference mode. Here, we first exploit attended question commands {q (t) w } to instruct message passing on the edges in the graph structure, t ∈ [1, T ], where T is the number of iterations; imitating humans reviewing different keywords multiple times. Then, it is worth noting that in our solution, the "dynamic directed-graph inference" process, considers flexible, effective, and relevant message propagation. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, in each iterative step, the context-aware graph is updated through two facets: (1) adjacent correlation learning. Under the instruction of the current question command q (t) w , each node adaptively selects the top-K most relevant nodes as its neighbors based on an adjacent correlation matrix; (2) top-K message passing. To capture the dynamic realtions in the graph, each node receives messages from its top-K neighbors and aggregates these messages to update its context feature. Question-conditioned Relevance Feedback via Adjacent Correlation Learning. To infer a correct answer, we have to discover accurate semantics of question Q. In each iterative step t, reviewing different words in Q is helpful to locate attentive keywords. Based on the word-level feature sequence of the question U Q = (h q 1 , ..., h q m ), we employ a self-attention to obtain the word attention distribution α</p><formula xml:id="formula_4">(t) q . Then, the word embedding sequence W Q = (w 1 , ..., w m ) is jointly aggregated with α (t) q to get a new attended ques- tion feature q (t) w .            z (t) q = L2N orm(f (t) q (U Q )); α (t) q = sof tmax(P (t) q z (t) q ); q (t) w = m j=1 α (t) q,j w j ,<label>(3)</label></formula><p>where f (t) q (.) denotes a two-layer MLP and P</p><formula xml:id="formula_5">(t) q ∈ R 1×d . The parameters of f (t) q (.) and P (t) q are independently learned in the t-th iterative step. q (t) w ∈ R dw×1</formula><p>is defined as the t-th question command at the word level.  After obtaining the question command q (t) w , we measure the correlation among different nodes in the graph. We design an adjacency correlation matrix of the graph G (t) as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-K neighbors:</head><formula xml:id="formula_6">{node i1 , …, iK } Node: [ v i ; c i (t) ] in G (t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-K message passing in CAG</head><formula xml:id="formula_7">A (t) ∈ R n×n , in which each value A (t)</formula><p>l→i represents the connection weight of the edge E (t) l→i . We learn the correlation matrix A (t) by computing the similarity of each pair of two nodes in N (t) under the guidance of q</p><formula xml:id="formula_8">(t) w . A (t) = (W 1 N (t) ) ((W 2 N (t) ) (W 3 q (t) w )),<label>(4)</label></formula><p>where W 1 ∈ R d×2d , W 2 ∈ R d×2d and W 3 ∈ R d×dw are learnable parameters, and denotes the hadamard product, i.e., element-wise multiplication. A fact is that there are always only a part of the detected objects in the image related to the question (i.e., sparse relationships). Moreover, among these objects, each object is always irrelated with most of other objects. Therefore, each node in the graph is required to connect with the most relevant neighbor nodes. In order to learn a set of relevant neighbors S</p><formula xml:id="formula_9">(t) i of node N (t) i , i ∈ [1, n], t ∈ [1, T ],</formula><p>we adopt a ranking strategy as:</p><formula xml:id="formula_10">S (t) i = topK(A (t) i ),<label>(5)</label></formula><p>where topK returns the indices of the K largest values of an input vector, and A (t) i denotes the i-th row of the adjacent matrix. Thus, each node has its independent neighbors S (t) i . As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (topK, K = 2), even the same node can have different neighbors in different iterative steps. It means that our solution is an adaptive dynamic graph inference process. Our CAG graph is an asymmetric directed-graph. Relational Graph Learning via Top-K Message Passing. The graph structure is now relationalaware. Each node can be affected by its K neighbor nodes. We propagate the relational cues to each node via message passing. Taking node N (t) i in the t-th step as an example, it receives the messages from its most K relevant neighbor nodes {N</p><formula xml:id="formula_11">(t) j }, where j ∈ S (t) i .</formula><p>To evaluate the influences of relevant neighbors, B (t) j→i normalizes the connection weight of edge E (t)</p><formula xml:id="formula_12">j→i (i.e., N (t) j → N (t) i ). A (t)</formula><p>j→i denotes the adjacent correlation weight of edge E i . The whole process is formulated as follows:</p><formula xml:id="formula_13">               [B (t) j→i ] = sof tmax j∈S (t) i ([A (t) j→i ]); m (t) j→i = (W4N (t) j ) (W5q (t) w ); M (t) i = j∈S (t) i B (t) j→i m (t) j→i ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_14">W 4 ∈ R d×2d and W 5 ∈ R d×dw are learnable param- eters. M (t) i ∈ R d×1 denotes the summarized message to N (t) i , and update N (t) i to N (t+1) i as follows: c (t+1) i = W 6 [c (t) i ; M (t) i ]; N (t+1) i = [v i ; c (t+1) i ],<label>(7)</label></formula><p>where W 6 ∈ R d×2d is a learnable parameter. Parameters W 1 ∼ W 6 in Eqs. 4 ∼ 7 are shared for each iteration. After performing T -step message passing iterations, the final node representation is denoted as N (T +1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph Attention Embedding</head><p>Up to now, the context learning on each node in the graph N (T +1) not only integrates original visual and textual features, but also involves iterative context-aware relational learning. As the majority of questions merely pay attention to a small part of objects in the image scene, we apply a question-conditioned graph attention mechanism to attend all the nodes. The graph attention is learned as follows:</p><formula xml:id="formula_15">           z g = tanh((W g1 q s )1 + W g2 N (T +1) ); α g = sof tmax(P g z g ); e g = n j=1 α g,j N (T +1) j ,<label>(8)</label></formula><p>where W g1 ∈ R d×d and W g2 ∈ R d×2d are learnable parameters. e g ∈ R 2d×1 denotes the attended graph embedding.</p><p>Finally, we fuse it with the history-related textual context u and the sentence-level question feature q s to output the multi-modal embedding e: e = tanh(W e [e g ; u; q s ]).</p><p>The output embedding e is then fed into the discriminative decoder <ref type="bibr" target="#b23">[23]</ref> to choose the answer with the highest probability as the final prediction. The details of the training settings are explained in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Datasets. Experiments are conducted on benchmarks Vis-Dial v0.9 and v1.0 <ref type="bibr" target="#b5">[5]</ref>. VisDial v0.9 contains 83k and 40k dialogs on COCO-train and COCO-val images <ref type="bibr" target="#b19">[19]</ref> respectively, totally 1.2M QA pairs. VisDial v1.0 is an extension of VisDial v0.9, which adds additional 10k dialogs on Flickr images. The new train, validation, and test splits contains 123k, 2k and 8k dialogs, respectively. Each dialog in Vis-Dial v0.9 consists of 10-round QA pairs for each image.</p><p>In the test split of VisDial v1.0, each dialog has flexible m rounds of QA pairs, where m is in the range of 1 to 10. Implementation Details. The proposed method is implemented on the platform of Pytorch. We build the vocabulary that contains the words occurring at least 4 times in the training split. And the captions, questions, and answers are truncated to 40, 20 and 20, respectively. Each word in the dialog is embedded into a 300-dim vector by the GloVe embedding initialization <ref type="bibr" target="#b27">[27]</ref>. We adopt Adam optimizer <ref type="bibr" target="#b14">[14]</ref> and initialize the learning rate with 4 × 10 −4 . The learning rate is multiplied by 0.5 after every 10 epochs. We set all the LSTMs in the model with 1-layer and 512 hidden states, and apply Dropout <ref type="bibr" target="#b30">[30]</ref> with ratio 0.3 for attention layers and the last fusion layer. Finally, the model is trained with a multi-class N -pair loss <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b23">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study of CAG</head><p>We evaluate two main hyperparameters in our model CAG -the selected neighbor number K and the number of iterative steps T , and validate the influence of visual features and main components of CAG. Neighbor Number (K). We test different neighbor numbers K ∈ {1, 2, 4, 8, 16, 36}. As shown in <ref type="figure">Fig. 4</ref>, K = 8 is an optimal parameter setting. Performances drop significantly for K &lt; 8. It means that if the selected neighbor nodes are insufficient, the relational messages can not be fully propagated. While setting the neighbor number K &gt; 8, the node receiving redundant irrelevant messages from neighbors can disturb the reasoning ability of the model. Thus, we set the neighbor number K = 8 in the following experiments. Iterative step (T ). T indicates the number of relational reasoning steps to arrive the answer. We test different steps T to analysis the influences of iterative inferences. As shown in <ref type="table">Table 1</ref>, the performance of CAG is gradually improved with the increasing T . We have the best performance while T = 3, lifting R@1 from 53.25 (T = 1) to 54.64. The proposed iterative graph inference is effective. Visualization results in <ref type="figure">Fig. 5</ref>   <ref type="figure">Figure 4</ref>. Performance comparison of the neighbor number K on VisDial val v0.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Step T Mean↓ MRR↑ R@1↑ R@5↑ R@10↑ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison Results</head><p>Baseline methods. In our experiment, compared methods can be grouped into three types: (1) Fusion-based Models (LF <ref type="bibr" target="#b5">[5]</ref> and HRE <ref type="bibr" target="#b5">[5]</ref>); (2) Attention-based Models (HREA <ref type="bibr" target="#b5">[5]</ref>, MN <ref type="bibr" target="#b5">[5]</ref>, HCIAE <ref type="bibr" target="#b23">[23]</ref>, AMEM <ref type="bibr" target="#b29">[29]</ref>, CoAtt <ref type="bibr" target="#b34">[34]</ref>, CorefNMN <ref type="bibr" target="#b15">[15]</ref>, DVAN <ref type="bibr" target="#b9">[9]</ref>, RVA <ref type="bibr" target="#b24">[24]</ref>, Synergistic <ref type="bibr" target="#b10">[10]</ref>, DAN <ref type="bibr" target="#b13">[13]</ref>, and HACAN <ref type="bibr" target="#b37">[37]</ref>); and (3) Graph-based Methods (GNN <ref type="bibr" target="#b42">[42]</ref> and FGA <ref type="bibr" target="#b28">[28]</ref>). Results on VisDial v0.9. As shown in <ref type="table" target="#tab_2">Table 3</ref>, CAG consistently outperforms most of methods. Compared with fusion-based models LF <ref type="bibr" target="#b5">[5]</ref> and HRE <ref type="bibr" target="#b5">[5]</ref>, the R@1 performance of our CAG is significantly improved, lifting each other by 10.8% and 9.9%. For attention-based models, compared to DAN <ref type="bibr" target="#b13">[13]</ref>, CAG outperforms it at all evaluation metrics. HACAN <ref type="bibr" target="#b37">[37]</ref> reports the recent best results. It first pre-trained with N -pair loss, and then used the wrong answers to "tamper" the truth-history for data augment. Finally, the truth-and fake-history were used to fine-tune its model via reinforcement learning. Without the fine-tuning tactic, our model CAG still outperforms HACAN on Mean, R@5, and R@10.</p><p>Here, we mainly compare our method with the graphbased models. GNN <ref type="bibr" target="#b42">[42]</ref> is a recently proposed method, which constructs a graph exploring the dependencies among the textual-history. In contrast, our CAG builds a graph over both visual-objects and textual-history contexts. Compared with GNN, our model achieves 5.7% improvements on the  <ref type="bibr" target="#b33">[33]</ref> is proposed to evaluate quantitative semantics, which penalizes low ranking correct answers. Other metrics accord to evaluate the rank of the ground-truth in the candidate answer list. NDCG tackles more than one plausible answers in the answer set. Compared with the attention-based models, as above mentioned, HACAN <ref type="bibr" target="#b37">[37]</ref> trained the model twice and Synergistic <ref type="bibr" target="#b10">[10]</ref> sorted the candidate answers twice. Without resorting or fine-tuning, under end-to-end training, our model still performs better performance on the Mean value. Compared with the graph-based models, our model has greatly improved the NDCG value. CAG outperforms GNN <ref type="bibr" target="#b42">[42]</ref> and FGA [28] by 3.8% and 4.5% respectively. This also proves that our graph can infer more plausible answers. In addition, we give more intuitive visualization results of CAG to explain the reasoning process detailed in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>To further demonstrate the interpretability of our solution, we show an iterative graph inference example in <ref type="figure">Fig. 5</ref>. Two most salient objects ("snowboarder" and "pant") in the graph attention maps are selected to display the inference processes. In iterative step t = 1, the question focuses on the word "snowboarder" (target). By reviewing the dialog context, "snowboarder" is related with "midair" and "photographer". The above two salient objects receive messages from their relevant neighbor object nodes. Then in iterative step t = 2, the question changes the attention to both words of "snowboarder" and "wearing" (related objects). These two object nodes dynamically update their neighbor nodes under the guidance of the current question command q (t=2) w . In the last step t = 3, question Q focuses on the word "wearing" (relation). The edge connections in the graph are further refined by receiving messages from wearing-related nodes. Through multi-step message passing, our context-aware graph progressively finds out much more implicit question-related visual and textual semantics. Finally, the graph attention map overlaying the image I also demonstrates the effectiveness of the graph inference.</p><p>We provide another example in <ref type="figure">Fig. 6</ref> to display relational reasoning in multi-round QA pairs. The edge relations and the nodes' attention weights dynamically vary corresponding to the current question. Our context-aware graph effectively models this dynamic inference process via adaptive top-K message passing. Each node only receives strong messages from the most relevant nodes. The graph attention maps overlaying the image at different rounds further validate the adaptability of our graph on relational reasoning.</p><p>In addition, we display the visualization of attentive word clouds on VisDial v1.0. <ref type="figure" target="#fig_6">Fig. 7</ref> describes the wordlevel attention distribution of question Q in different iterative steps. In iterative step t = 1, the proposed CAG  inclines to pronouns in the questions, e.g., "there", "it", "you", "they". CAG tries to tackle the textual co-reference in the initial relational reasoning. Then, in step t = 2, CAG prefers to attend nouns related to target objects or associated objects in the image, e.g., "people", "building", "tree". This means CAG trends to infer the relationships among different related objects, namely visual-reference. In the time step t = 3, the model considers the words that describe the attributes or relations of the objects, e.g., "color", "wearing", "other", "on". All these appearances indicate that we reasonably and actively promote the iterative inference process using the context-aware graph CAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a fine-grained Context-Aware Graph (CAG) neural network for visual dialog, which contains both visual-objects and textual-history context semantics. An adaptive top-K message passing mechanism is proposed to iteratively explore the context-aware representations of nodes and update the edge relationships for a better answer inferring. Our solution is a dynamic directed-graph inference process. Experimental results on the VisDial v0.9 t = 1 t = 2 t = 3 and v1.0 datasets validate the effectiveness of the proposed approach and display explainable visualization results.</p><formula xml:id="formula_17">f (t) q = tanh(W f1 U Q ) σ(W f2 U Q ); z (t) q = L2N orm(f (t) q (U Q )),<label>(A1)</label></formula><p>where W f1 , W f2 ∈ R d×d are learnable parameters.</p><p>The operation f </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Fine-grained Graph Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Node components: visual and textual contexts</head><p>One argue maybe that why we realize the graph initialization with history-related context u and implement the joint visual-textual context learning? The motivation is that without history-related context u, the dialog agent can't understand the previous dialogue topic well, nor it can further solve the current visual reference well.</p><p>Technically, one challenge of visual dialog is to explore the latent relations among image, history and question. We reformulate the idea of the dynamic graph learning in the paper as follows. In iterative step t, as the definition of node N</p><formula xml:id="formula_18">(t) i = [v i ; c (t) i ]</formula><p>, v i denotes the visual feature of object obj i , and c (t) i records the relevant context related to obj i . c i considers both {v i } and u, and is guided by the question command q involving u plays an important role in the graph inference. Both ablation studies in <ref type="table">Table 1</ref> and qualitative results in Sec. A.3 detailed below demonstrate the effectiveness. In addition, <ref type="figure" target="#fig_6">Fig. 7</ref> also verifies the significance of u in step t=1. The introduce of u is helpful to tackle the visual-textual co-reference related to the question, such as parsing pronouns in the question (e.g., "he", "it" and "there") and grounding the relevant objects in the image.</p><formula xml:id="formula_19">(t) w .          N (1) i = [v i ; c (1) i ] = [v i ; u]; c (t+1) i = MP q (t) w {N (t) j }| top−K N (t) i c (t) i ; N (t+1) i = [v i ; c (t+1) i ], i ∈ [1, n], t ∈ [1, T ],<label>(A2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Adjacent correlation matrix learning</head><p>Another argue maybe that why impose the question command q (t) w on only one node side of the matrix A (t) in Eq. 4, which is not a symmetrical operation as mutual correlation calculation. We define a classical mutual (symmetrical) correlation calculation as CAG-DualQ as follows:</p><formula xml:id="formula_20">               CAG : A (t) = (W 1 N (t) ) (W 2 N (t) ) (W 3 q (t)</formula><p>w ) ; CAG-DualQ :</p><formula xml:id="formula_21">A (t) = (W 1 N (t) ) (W 3 q (t) w ) (W 2 N (t) ) (W 3 q (t) w ) . (A3) where W 1 and W 2 ∈ R d×2d , W 3 and W 3 ∈ R d×dw are learnable parameters.</formula><p>We implement the ablation study. As shown in <ref type="table">Table 4</ref>, CAG-DualQ performs worse than CAG. It is interpretable. As illustrated in <ref type="figure" target="#fig_10">Fig. 8</ref>, the Y-axis of the matrix A (t) marks the receiving nodes, and the X-axis denotes the distributing nodes. To infer an exact answer, for a node, we use the question command q (t) w to activate its neighbors. In other words, the i-th row of the matrix A    <ref type="table">Table 4</ref>. Ablation studies of different adjacent correlation matrix learning strategies on VisDial val v0.9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Qualitative results of CAG vs. CAG w/o u</head><p>As the ablation study shown in <ref type="table">Table 1</ref>, the performance of CAG w/o u drops a lot compared to CAG. Here, we provide explainable qualitative results in <ref type="figure">Fig. 9</ref> to further validate the effectiveness of history-related context u in CAG. There are two different examples. As shown in <ref type="figure">Fig. 9 (a)</ref>, for question Q: "He wearing helmet?", CAG w/o u directly locates words "he" (two people) and "helmet" in the image, and then the object "helmet" infers to a wrong "he" (the man who wears the helmet), while CAG consistently attends on the correct "he" (the subject "man" who is hitting the baseball with the bat in the previous dialogue). Besides, as shown in <ref type="figure">Fig. 9 (b)</ref>, although CAG w/o u infers the correct answer, but we observe that there is much more reasonable inference using CAG than CAG w/o u. For the question Q: "Is there other vehicles?", CAG does not attend the bus in the center of picture and devote to searching other vehicles, while CAG w/o u focuses on all the vehicles.</p><p>In a nutshell, CAG w/o u is accustomed to attend all the objects appeared in the question, while CAG tries to ground the relevant objects discussed in the entire dialogue. If without the history reference, the dialogue agent can not perform the pronoun explanation (e.g., the visual grounding of "he", "it" and "there", ect.) well, and then the subsequent iterative inferences are affected. Therefore, the history-related context u is necessary for the visual-textual co-reference reasoning in our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Additional qualitative results of visual-reference</head><p>We provide additional four visualization results in <ref type="figure">Fig. 10</ref>. These qualitative results also demonstrate that CAG has interpretable textual and visual attention distribution, reliable context-aware graph learning, and reasonable inference process. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall framework of Context-Aware Graph. Our context-aware graph is constructed with visual contexts {v obj } and textual context u. The dynamic relations among the nodes are iteratively inferred via Top-K neighbors' Message Passing under the guidance of word-level question command q (t) w . For example, the red and blue nodes in the graph respectively have different top-2 related neighbor nodes, and different directions of the message passing flow on the connected edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Adaptive Top-K Message Passing Unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(t) j→i . As shown in Eq. 6, B (t) j→i normalizes the weights of the neighbour set {A (t) j→i } (j ∈ S (t) i ) with a sof tmax function. In addition, under the guidance of the question command q (t) w , m (t) j→i calculates the inbound message of neighbour N (t) j to N (t) i . At last, N (t) i sums up all the inbound messages to get the final message feature M (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 . 12 Q 3 : 19 Q 5 :Figure 6 .</head><label>51231956</label><figDesc>Visualization results of iterative context-aware graph inference. It shows the word-level attention on question Q, and dynamic graph inference of the top-2 attended objects (red and blue bounding boxes) in image I. The number on each edge denotes the normalized connection weight, displaying the message influence propagated from neighbors. There are some abbreviations as follows: question (Q), generated answer (A), caption (C) and the ground-truth (GT ). Does it look like he is in the middle of a match ? Is he getting ready to hit the ball ? Visualization result of a progressive multi-round dialog inference. Each column displays the graph attention map overlaying the image I and the last step of message passing process of the most salient object. In these graph attention maps, bounding boxes correspond to the top-3 attended object nodes, and the numbers along with the bounding boxes represent the node attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of attentive word cloud of all the questions {Q} in different iterative steps on VisDial v1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>q</head><label></label><figDesc>uses the tangent &amp; sigmoid activation gates to learn a new word-level feature sequence of question Q. Then, the L2N orm operation normalizes each word's new feature embedding vector on the feature dimension. With the L2 normalization, z (t) q ∈ R d×m can equitably evaluate each word in the word sequence of Q. Figs. 5∼7 (especiallyFig. 7validate the adaptability of the word-level attention in different iterative steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>i</head><label></label><figDesc>calculates the correlation weights of node N (t) i and its neighbors {N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(t) j } under the only once guidance of q (t) w . It is reasonable to introduce the question cue on one node side of A (t) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>A schematic diagram of adjacent correlation matrix learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>24 CFigure 9 .Q 1 : 2 : 3 :Q 4 :Figure 10 .</head><label>249123410</label><figDesc>: A man is hitting the baseball with the bat. Q &amp; A 1 : How old is man? Probably in his 30s. Q &amp; A 2 : What is his race? White. Q &amp; A 3 : He pro player? No, he is in park. C: A blue bus on the street of a busy city. Q &amp; A 1 : Is it a tour bus? No. Q &amp; A 2 : Any passengers? Yes. Q &amp; A 3 : Is it a transit bus? Public. Qualitative results of CAG and CAG w/o u. Observing the graph attention map overlaying each image, the bounding boxes with the attention scores correspond to the top-3 relevant object nodes in the final graph. We pick out the top-2 objects to display the dynamic graph inference. CAG and CAG w/o u can refer to different top-2 objects. Without history context u, the agent could misunderstand the dialogue topic, and the visual reference cannot be solved well. Is the picture in color? A 1 : Yes. Q Can you see any furniture? A 2 : Yes. Q Is there a couch in the room? A 3 : Yes. What color is the couch? A 4 : Blue. Do they have any headbands on ? Q: Do they have any headbands on ? Q: Do they have any headbands on ? Additional visualization examples on VisDial 0.9. In these attention maps, red and blue bounding boxes correspond to the top-2 attended object nodes in the graph attention learning, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>66.2 66.5 66.8 67.1 67.4 Top1 Top2 Top4 Top8 Top16 Top36 MRR 52.5 52.9 53.3 53.7 54.1 54.5 Top1 Top2 Top4 Top8 Top16 Top36 R@1 82.5 82.8 83.1 83.4 83.7 Top1 Top2 Top4 Top8 Top16 Top36 R@5 90.6 90.8 91 91.2 91.4 Top1 Top2 Top4 Top8 Top16 Top36 R@10</head><label></label><figDesc>further validate this result. When T &gt; 3, the performance drops slightly. It means that if the relations in the graph have been fully inferred, further inference does not help. The questions in the VisDial datasets are collected</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Infer drops MRR significantly from 67.56 to 65.73. It indicates that the graph inference effectively performs well for relational reasoning. Learning the implicit relations among the nodes is helpful to predict the final answer. CAG w/o u drops R@1 significantly from 54.64 to 51.83. It indicates that the joint visual-textual context learning is necessary. Relations among the nodes can not be fully inferred without textual cues u. CAG w/o Q-att, which replaces question commands {q } with the sentence-level feature q s , drops R@1 from 54.64 to 53.74. It also can be explained. Figs. 5∼7 demonstrate that the attentive words always vary during the inference process. It usually firstly identifies the target in the question, than focuses on related objects, and finally observes attributes and relations in both visual and</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell>Mean↓ MRR↑ R@1↑ R@5↑ R@10↑</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Attention-based Models</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HCIAE [23]</cell><cell>4.81</cell><cell>62.22</cell><cell>48.48</cell><cell>78.75</cell><cell>87.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AMEM [29]</cell><cell>4.86</cell><cell>62.27</cell><cell>48.53</cell><cell>78.66</cell><cell>87.43</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CoAtt [34]</cell><cell>4.47</cell><cell>63.98</cell><cell>50.29</cell><cell>80.71</cell><cell>88.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DVAN-VGG [9]</cell><cell>4.38</cell><cell>63.81</cell><cell>50.09</cell><cell>80.58</cell><cell>89.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RvA-VGG [24]</cell><cell>4.22</cell><cell>64.36</cell><cell>50.40</cell><cell>81.36</cell><cell>89.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HACAN-VGG [37]</cell><cell>4.32</cell><cell>64.51</cell><cell>50.72</cell><cell>81.18</cell><cell>89.23</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Graph-based Models</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GNN [42]</cell><cell>4.57</cell><cell>62.85</cell><cell>48.95</cell><cell>79.65</cell><cell>88.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FGA w/o Ans [28]</cell><cell>4.63</cell><cell>62.94</cell><cell>49.35</cell><cell>79.31</cell><cell>88.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CAG-VGG (Ours)</cell><cell>4.13</cell><cell>64.91</cell><cell>51.45</cell><cell>81.60</cell><cell>90.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CAG (Ours)</cell><cell>3.75</cell><cell>67.56</cell><cell>54.64</cell><cell>83.72</cell><cell>91.48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Table 2. Performance comparison on VisDial val v0.9 with VGG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">features. Our model with VGG features is denoted as CAG-VGG.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">textual context cues to infer the answer. CAG w/o G-att,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">which removes the final graph attention module, drops R@1</cell></row><row><cell></cell><cell>T = 1</cell><cell>4.02</cell><cell>66.32</cell><cell>53.25</cell><cell>82.54</cell><cell>90.55</cell><cell cols="2">from 54.64 to 53.99. Although each node involves rela-</cell></row><row><cell>CAG</cell><cell>T = 2 T = 3</cell><cell>3.91 3.75</cell><cell>66.93 67.56</cell><cell>53.76 54.64</cell><cell>83.11 83.72</cell><cell>90.96 91.48</cell><cell cols="2">tional reasoning, not all the nodes are relevant to the current</cell></row><row><cell></cell><cell>T = 4</cell><cell>3.83</cell><cell>67.28</cell><cell>54.11</cell><cell>83.46</cell><cell>91.17</cell><cell></cell></row><row><cell>CAG w/o Infer</cell><cell>-</cell><cell>4.11</cell><cell>65.73</cell><cell>52.56</cell><cell>82.38</cell><cell>90.36</cell><cell></cell></row><row><cell>CAG w/o u</cell><cell>T = 3</cell><cell>4.19</cell><cell>65.26</cell><cell>51.83</cell><cell>81.55</cell><cell>90.21</cell><cell></cell></row><row><cell cols="2">CAG w/o Q-att T = 3</cell><cell>3.91</cell><cell>66.70</cell><cell>53.74</cell><cell>82.75</cell><cell>90.89</cell><cell></cell></row><row><cell cols="2">CAG w/o G-att T = 3</cell><cell>3.86</cell><cell>66.98</cell><cell>53.99</cell><cell>83.08</cell><cell>91.04</cell><cell></cell></row><row><cell>CAG</cell><cell>T = 3</cell><cell>3.75</cell><cell>67.56</cell><cell>54.64</cell><cell>83.72</cell><cell>91.48</cell><cell></cell></row><row><cell></cell><cell>(t)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 1. Ablation studies of different iterative steps T and the main components on VisDial val v0.9.from relative simple free-form human dialogue. The setting of T = 3 already performs well. In the following experi- ment, we set T = 3. Main Component Comparison. A few variants are pro- posed for ablation study. CAG w/o Infer denotes that CAG removes the whole dynamic directed-graph inference in Sec. 3.2. It means that the relations in the graph includ- ing all the nodes and edges will not be updated and inferred. CAG w/o u denotes that CAG without textual-history con- text u, where the whole graph merely describes visual con- text cues. CAG w/o Q-att denotes CAG without word-level attention on question Q. CAG w/o G-att removes the graph attention module, where all the node representations are av- erage pooled to get the final graph embedding. As shown in Table 1, compared with CAG, CAG w/owquestion. Therefore, paying attention to relevant nodes in the relational graph is helpful to infer an exact answer. Test with VGG Features. As some existing methods eval- uated with VGG features, to be fair, we test our model with VGG features too. Table 2 shows that our CAG-VGG still outperforms the previous methods that only utilize VGG features. Compared to CAG-VGG, CAG gets a significant performance boost. It indicates the object-region features provide richer visual semantics than VGG features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Main comparisons on both VisDial v0.9 and v1.0 datasets using the discriminative decoder [23]. R@1 metric. FGA [28] is the state-of-the-art graph-based method for visual dialog, which treats the candidate answer embedding feature A as new context cue and introduces it into the multi-modal encoding training. This operation improves their results a lot (FGA w/o Ans vs. FGA). Without candidate answer embedding, our model still performs better results, lifting R@1 from 51.43 to 54.64, and decreasing the Mean from 4.35 to 3.75. These comparisons indicate that in our solution, the fine-grained visual-textual joint semantics are helpful for answer inferring. Results on VisDial v1.0. A new metric NDCG (Normalized Discounted Cumulative Gain)</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">VisDial v0.9 (val)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">VisDial v1.0 (test-std)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="11">Mean↓ MRR↑ R@1↑ R@5↑ R@10↑ Mean↓ NDCG↑ MRR↑ R@1↑ R@5↑ R@10↑</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Fusion-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LF [5]</cell><cell>5.78</cell><cell>58.07</cell><cell>43.82</cell><cell>74.68</cell><cell>84.07</cell><cell>5.95</cell><cell>45.31</cell><cell>55.42</cell><cell>40.95</cell><cell>72.45</cell><cell>82.83</cell></row><row><cell>HRE [5]</cell><cell>5.72</cell><cell>58.46</cell><cell>44.67</cell><cell>74.50</cell><cell>84.22</cell><cell>6.41</cell><cell>45.46</cell><cell>54.16</cell><cell>39.93</cell><cell>70.45</cell><cell>81.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Attention-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HREA [5]</cell><cell>5.66</cell><cell>58.68</cell><cell>44.82</cell><cell>74.81</cell><cell>84.36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MN [5]</cell><cell>5.46</cell><cell>59.65</cell><cell>45.55</cell><cell>76.22</cell><cell>85.37</cell><cell>5.92</cell><cell>47.50</cell><cell>55.49</cell><cell>40.98</cell><cell>72.30</cell><cell>83.30</cell></row><row><cell>HCIAE [23]</cell><cell>4.81</cell><cell>62.22</cell><cell>48.48</cell><cell>78.75</cell><cell>87.59</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AMEM [29]</cell><cell>4.86</cell><cell>62.27</cell><cell>48.53</cell><cell>78.66</cell><cell>87.43</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CoAtt [34]</cell><cell>4.47</cell><cell>63.98</cell><cell>50.29</cell><cell>80.71</cell><cell>88.81</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CorefNMN [15]</cell><cell>4.45</cell><cell>64.10</cell><cell>50.92</cell><cell>80.18</cell><cell>88.81</cell><cell>4.40</cell><cell>54.70</cell><cell>61.50</cell><cell>47.55</cell><cell>78.10</cell><cell>88.80</cell></row><row><cell>DVAN [9]</cell><cell>3.93</cell><cell>66.67</cell><cell>53.62</cell><cell>82.85</cell><cell>90.72</cell><cell>4.36</cell><cell>54.70</cell><cell>62.58</cell><cell>48.90</cell><cell>79.35</cell><cell>89.03</cell></row><row><cell>RVA [24]</cell><cell>3.93</cell><cell>66.34</cell><cell>52.71</cell><cell>82.97</cell><cell>90.73</cell><cell>4.18</cell><cell>55.59</cell><cell>63.03</cell><cell>49.03</cell><cell>80.40</cell><cell>89.83</cell></row><row><cell>Synergistic [10]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.17</cell><cell>57.32</cell><cell>62.20</cell><cell>47.90</cell><cell>80.43</cell><cell>89.95</cell></row><row><cell>DAN [13]</cell><cell>4.04</cell><cell>66.38</cell><cell>53.33</cell><cell>82.42</cell><cell>90.38</cell><cell>4.30</cell><cell>57.59</cell><cell>63.20</cell><cell>49.63</cell><cell>79.75</cell><cell>89.35</cell></row><row><cell>HACAN [37]</cell><cell>3.97</cell><cell>67.92</cell><cell>54.76</cell><cell>83.03</cell><cell>90.68</cell><cell>4.20</cell><cell>57.17</cell><cell>64.22</cell><cell>50.88</cell><cell>80.63</cell><cell>89.45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Graph-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GNN [42]</cell><cell>4.57</cell><cell>62.85</cell><cell>48.95</cell><cell>79.65</cell><cell>88.36</cell><cell>4.57</cell><cell>52.82</cell><cell>61.37</cell><cell>47.33</cell><cell>77.98</cell><cell>87.83</cell></row><row><cell>FGA w/o Ans [28]</cell><cell>4.63</cell><cell>62.94</cell><cell>49.35</cell><cell>79.31</cell><cell>88.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FGA [28]</cell><cell>4.35</cell><cell>65.25</cell><cell>51.43</cell><cell>82.08</cell><cell>89.56</cell><cell>4.51</cell><cell>52.10</cell><cell>63.70</cell><cell>49.58</cell><cell>80.97</cell><cell>88.55</cell></row><row><cell>CAG (Ours)</cell><cell>3.75</cell><cell>67.56</cell><cell>54.64</cell><cell>83.72</cell><cell>91.48</cell><cell>4.11</cell><cell>56.64</cell><cell>63.49</cell><cell>49.85</cell><cell>80.63</cell><cell>90.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>History Attention Distribution Q: What is the snowboarder wearing ? Q: What is the snowboarder wearing ?</head><label></label><figDesc></figDesc><table><row><cell>Original Image</cell><cell></cell><cell>t = 1</cell><cell></cell><cell></cell><cell></cell><cell>t = 2</cell><cell></cell><cell></cell><cell></cell><cell>t = 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Q: What is the snowboarder wearing ? C:</cell><cell>0.41</cell><cell></cell><cell></cell></row><row><cell>0.14 0.07</cell><cell>0.07</cell><cell>0.42</cell><cell>0.14 0.16</cell><cell>0.07 0.06</cell><cell>0.07</cell><cell>0.34</cell><cell>0.31 0.15</cell><cell>0.1 0.04</cell><cell>0.02</cell><cell>0.18</cell><cell>0.46 0.20</cell><cell>Q 1 : Q 3 :</cell><cell>0.17 0.18</cell><cell>Q 2 : Q 4 :</cell><cell>0.09 0.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Attention Map</cell><cell></cell></row><row><cell>Dialog History</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C: A person jumping midair on a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>snowboard while a person photographs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q 1 : How many people are there?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A 1 : 2 people.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q 2 : Is it sunny? A 2 : Sunny but partly cloudy.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q 3 : What gender are the people? A 3 : 2 males but hard to confirm. Q4: What the photographer wearing?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">A/GT: White ski pant, white jacket, pink bib, black boots,</cell></row><row><cell>A4: A black sweater, pants.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">googles, baseball hat.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Natural Science Foundation of China (NSFC) under Grants 61725203, 61876058, 61732008, 61622211, and U19B2038.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>This supplementary document is organized as follows:</p><p>• We explain two different textual attention mechanisms in Sec. A.1, especially for the word-level attention on question Q at different iterative processes.</p><p>• Sec. A.2 elaborates the motivation of the fine-grained graph construction and the relational measurement between different nodes.</p><p>• In the paper, Figs. 5∼7 illustrate the visualization results on image I and question Q. Here, Sec. A.3 mainly demonstrates the influence of history-related context u in <ref type="figure">Fig. 9</ref>, and supplements additional qualitative results of visual reference in <ref type="figure">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Textual Attention Mechanisms</head><p>• Sentence-level attention on history H: The latent attention variable z h in Eq. 1 tackles the textual coreference between q s and U H , where both q s and U H are sentence-level semantics.</p><p>• Word-level attention on question Q: For computing word-level attention on Q, the latent attention variable z (t) q in Eq. 3 measures the question itself. We unfold the MLP operation in Eq. 3 as follows:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SCA-CNN: spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6298" to="6306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5503" to="5512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-step reasoning via recurrent dual attention for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6463" to="6474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual visual attention network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-questionanswer synergistic network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual attention networks for visual reference resolution in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gi-Cheon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clevr-dialog: A diagnostic dataset for multi-round reasoning in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="582" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Focal visual-text attention for memex question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1893" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-aware visual policy network for sequence-level image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive reconstruction network for weakly supervised referring expression grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dechao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2611" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structure inference net: Object detection using scene-level context and instance-level relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6985" to="6994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive visual attention in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning conditioned graph structures for interpretable visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Norcliffe-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stathis</forename><surname>Vafeias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8344" to="8353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised cross-media retrieval using domain adaptation with scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Factor graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3233" to="3241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A theoretical analysis of NDCG type ranking measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multitask learning for crossdomain image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1047" to="1061" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Making history matter: History-advantage sequence training for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatiotemporal-textual co-attention network for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zheng-Jun Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>2s):53:1-53:18</idno>
	</analytic>
	<monogr>
		<title level="j">The ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context-aware visual policy network for fine-grained image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqin</forename><surname>Zhen-Jun Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weakly supervised phrase localization with multi-scale anchored transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5696" to="5705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reasoning visual dialogs with structural and partial observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
