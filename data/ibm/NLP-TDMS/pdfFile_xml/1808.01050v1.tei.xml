<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhmmad</forename><surname>Tayyab</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishan</forename><surname>Athrey</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somaya</forename><surname>Al-Maadeed</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<address>
									<country>Qatar University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Crowd Counting · Localization · Convolution Neural Networks · Composition Loss</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With multiple crowd gatherings of millions of people every year in events ranging from pilgrimages to protests, concerts to marathons, and festivals to funerals; visual crowd analysis is emerging as a new frontier in computer vision. In particular, counting in highly dense crowds is a challenging problem with far-reaching applicability in crowd safety and management, as well as gauging political significance of protests and demonstrations. In this paper, we propose a novel approach that simultaneously solves the problems of counting, density map estimation and localization of people in a given dense crowd image. Our formulation is based on an important observation that the three problems are inherently related to each other making the loss function for optimizing a deep CNN decomposable. Since localization requires high-quality images and annotations, we introduce UCF-QNRF dataset that overcomes the shortcomings of previous datasets, and contains 1.25 million humans manually marked with dot annotations. Finally, we present evaluation measures and comparison with recent deep CNN networks, including those developed specifically for crowd counting. Our approach significantly outperforms state-of-the-art on the new dataset, which is the most challenging dataset with the largest number of crowd annotations in the most diverse set of scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Counting dense crowds is significant both from socio-political and safety perspective. At one end of the spectrum, there are large ritual gatherings such as during pilgrimages that typically have large crowds occurring in known and pre-defined locations. Although they generally have passive crowds coming together for peaceful purposes, disasters have known to occur, for instance, during Love Parade <ref type="bibr" target="#b8">[9]</ref> and Hajj <ref type="bibr" target="#b0">[1]</ref>. For active crowds, such as expressive mobs in demonstrations and protests, counting is important both from political and safety standpoint. It is very common for different sides to claim divergent numbers for crowd gathering, inclined towards their political standing on the concerned issue. Beyond subjectivity and preference for certain political or  <ref type="figure">Fig. 1</ref>: This figure highlights the problems due to low resolution images from two existing dense crowd datasets: (a) shows a case where the annotations were not done on parts of the images as it is virtually impossible to distinguish heads of neighboring people, while (b) shows a case where some of the locations / counts are erroneous and therefore not suitable for localization. The UCF-QNRF dataset proposed in this paper overcomes such issues.</p><p>social outcomes, the disparate counting estimates from opposing parties have a basis in numerical cognition as well. In humans, the results on subitizing <ref type="bibr" target="#b20">[21]</ref> suggest that once the number of observed objects increases beyond four, the brain switches from the exact Parallel Individuation System (PIS) to the inaccurate but scalable Approximate Number System (ANS) to count objects <ref type="bibr" target="#b10">[11]</ref>. Thus, computer vision based crowd counting offers alternative fast and objective estimation of the number of people in such events. Furthermore, crowd counting is extendable to other domains, for instance, counting cells or bacteria from microscopic images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>, animal crowd estimates in wildlife sanctuaries <ref type="bibr" target="#b1">[2]</ref>, or estimating the number of vehicles at transportation hubs or traffic jams <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this paper, we propose a novel approach to crowd counting, density map estimation and localization of people in a given crowd image. Our approach stems from the observation that these three problems are very interrelated -in fact, they can be decomposed with respect to each other. Counting provides an estimate of the number of people / objects without any information about their location. Density maps, which can be computed at multiple levels, provide weak information about location of each person. Localization does provide accurate location information, nevertheless, it is extremely difficult to estimate directly due to its very sparse nature. Therefore, we propose to estimate all three tasks simultaneously, while employing the fact that each is special case of another one. Density maps can be 'sharpened' till they approximate the localization map, whose integral should equal to the true count.</p><p>Furthermore, we introduce a new and the largest dataset to-date for training and evaluating dense crowd counting, density map estimation and localization methods, particularly suitable for training very deep Convolutional Neural Networks (CNNs). Though counting has traditionally been considered the primary focus of research, density map estimation and localization have significance and utility beyond counting. In particular, two applications are noteworthy: initialization / detection of people for tracking in dense crowds <ref type="bibr" target="#b12">[13]</ref>; and rectifying counting errors from an automated computer vision algorithm. That is, a real user or analyst who desires to estimate the exact count for a real image without any error, the results of counting alone are insufficient. The  single number for an entire image makes it difficult to assess the error or the source of the error. However, the localization can provide an initial set of dot locations of the individuals, the user then can quickly go through the image and remove the false positives and add the false negatives. The count using such an approach will be much more accurate and the user can get 100% precise count for the query image. This is particularly important when the number of image samples are few, and reliable counts are desired. Prior to 2013, much of the work in crowd counting focused on low-density scenarios. For instance, UCSD dataset <ref type="bibr" target="#b3">[4]</ref> contains 2, 000 video frames with 49, 885 annotated persons. The dataset is low density and low resolution compared to many recent datasets, where train and test splits belong to a single scene. WorldExpo'10 dataset <ref type="bibr" target="#b28">[29]</ref>, contains 108 low-to-medium density scenes and overcomes the issue of diversity to some extent. UCF dataset <ref type="bibr" target="#b11">[12]</ref> contains 50 different images with counts ranging between 96 and 4, 633 per image. Each image has a different resolution, camera angle, and crowd density. Although it was the first dataset for dense crowd images, it has problems with annotations ( <ref type="figure">Figure 1</ref>) due to limited availability of high-resolution crowd images at the time. The ShanghaiTech crowd dataset <ref type="bibr" target="#b29">[30]</ref> contains 1, 198 annotated images with a total of 330, 165 annotations. This dataset is divided into two parts: Part A contains 482 images and Part B with 716 images. The number of training images are 300 and 400 in both parts, respectively. Only the images in Part A contain high-density crowds, with 482 images and 250K annotations. <ref type="table" target="#tab_1">Table 1</ref> summarizes the statistics of the multi-scene datasets for dense crowd counting. The proposed UCF-QNRF dataset has the most number of high-count crowd images and annotations, and a wider variety of scenes containing the most diverse set of viewpoints, densities and lighting variations. The resolution is large compared to WorldExpo'10 <ref type="bibr" target="#b28">[29]</ref> and ShanghaiTech <ref type="bibr" target="#b29">[30]</ref>, as can be seen in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. The average density, i.e., the number of people per pixel over all images is also the lowest, signifying high-quality large images. Lower per-pixel density is partly due to inclusion of background regions, where there are many high-density regions as well as zero-density regions. Part A of Shanghai dataset has high-count crowd images as well, however, they are severely cropped to contain crowds only. On the other hand, the new UCF-QNRF dataset contains buildings, vegetation, sky and roads as they are present in realistic scenarios captured in the wild. This makes this dataset more realistic as well as difficult. Similarly, <ref type="figure" target="#fig_1">Figure 2</ref>(a) shows the diversity in counts among the datasets. The distribution of proposed dataset is similar to UCF CC 50 <ref type="bibr" target="#b11">[12]</ref>, however, the new dataset is 30 and 20 times larger in terms of number of images and annotations, respectively, compared to UCF CC 50 <ref type="bibr" target="#b11">[12]</ref>. We hope the new dataset will significantly increase research activity in visual crowd analysis and will pave way for building deployable practical counting and localization systems for dense crowds. The rest of the paper is organized as follows. In Sec. 2 we review related work, and present the proposed approach for simultaneous crowd counting, density map estimation and localization in Sec. 3. The process for collection and annotation of the UCF-QNRF dataset is covered in Sec. 4, while the three tasks and evaluation measures are motivated in Sec. 5. The experimental evaluation and comparison are presented in Sec. 6. We conclude with suggestions for future work in Sec. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Crowd counting is active an area of research with works tackling the three aspects of the problem: counting-by-regression <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b27">[28]</ref>, density map estimation <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref> and localization <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>Earlier regression-based approaches mapped global image features or a combination of local patch features to obtain counts <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Since these methods only produce counts, they cannot be used for density map estimation or localization. The features were hand-crafted and in some cases multiple features were used <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> to handle low resolution, perspective distortion and severe occlusion. On the other hand, CNNs inherently learn multiple feature maps automatically, and therefore are now being extensively used for crowd counting and density map estimation.</p><p>CNN based approaches for crowd counting include <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Zhang et al. <ref type="bibr" target="#b28">[29]</ref> train a CNN alternatively to predict density map and count in a patch, and then average the density map for all the overlapping patches to obtain density map for the entire image. Lebanoff and Idrees <ref type="bibr" target="#b15">[16]</ref> introduce a normalized variant of the Euclidean loss function in a deep network to achieve consistent counting performance across all densities. The authors in <ref type="bibr" target="#b29">[30]</ref> use three column CNN, each with different filter sizes to capture responses at different scales. The count for the image is obtained by summing over the predicted density map. Sindagi and Patel <ref type="bibr" target="#b25">[26]</ref> presented a CNNbased approach that incorporates global and local contextual information in an image to generate density maps. The global and local contexts are obtained by learning to classify the input image patches into various density levels, later fused with the output of a multi-column CNN to obtain the final density map. Similarly, in the approach by Sam et al. <ref type="bibr" target="#b23">[24]</ref>, image patches are relayed to the appropriate CNN using a switching mechanism learnt during training. The independent CNN regressors are designed to have different receptive fields while the switch classifier is trained to relay the crowd scene patch to the best CNN regressor.</p><p>For localization in crowded scenes, Rodriguez et al. <ref type="bibr" target="#b21">[22]</ref> use density map as a regularizer during the detection. They optimize an objective function that prefers density map generated on detected locations to be similar to predicted density map <ref type="bibr" target="#b16">[17]</ref>. This results in both better precision and recall. The density map is generated by placing a Gaussian kernel at the location of each detection. Zheng et al. <ref type="bibr" target="#b17">[18]</ref> first obtain density map using sliding window over the image through <ref type="bibr" target="#b16">[17]</ref>, and then use integer programming to localize objects on the density maps. Similarly, in the domain of medical imaging, Sirinukunwattana et al. <ref type="bibr" target="#b26">[27]</ref> introduced spatially-constrained CNNs for detection and classification of cancer nuclei. In this paper, we present results and analysis for simultaneous crowd counting, density map estimation, and localization using Composition Loss on the proposed UCF-QNRF dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep CNN with Composition Loss</head><p>In this section, we present the motivation for decomposing the loss of three interrelated problems of counting, density map estimation and localization, followed by details about the deep Convolutional Neural Network which can enable training and estimation of the three tasks simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Composition Loss</head><p>Let x = [x, y] denote a pixel location in a given image, and N be the number of people annotated with {x i : i = 1, 2, . . . N } as their respective locations. Dense crowds typically depict heads of people as they are the only parts least occluded and mostly visible. In localization maps, only a single pixel is activated, i.e., set to 1 per head, while all other pixels are set to 0. This makes localization maps extremely sparse and therefore difficult to train and estimate. We observe that successive computation of 'sharper' density maps which are relatively easier to train can aid in localization as well. Moreover, all three tasks should influence count, which is the integral over density or localization map. We use the Gaussian Kernel and adapt it for our problem of simultaneous solution for the three tasks.</p><p>Due to perspective effect and possibly variable density of the crowd, a single value of bandwidth, σ, cannot be used for the Gaussian kernel, as it might lead to well-defined separation between people close to the camera or in regions of low density, while excess  The proposed Composition Loss is implemented through multiple dense blocks after branching off the base network. We also test the effect of additional constraint on the density and localization maps (shown with amber and orange blocks) such that the count after integral in each should also be consistent with the groundtruth count.</p><p>blurring in other regions. Many images of dense crowds depict crowds in their entirety, making automatic perspective rectification difficult. Thus, we propose to define σ i for each person i as the minimum of the 2 distance to its nearest neighbor in spatial domain of the image or some maximum threshold, τ . This ensures that the location information of each person is preserved precisely irrespective of default kernel bandwidth, τ . Thus, the adaptive Gaussian kernel is given by,</p><formula xml:id="formula_0">D(x, f(·)) = N i=1 1 √ 2πf(σ i ) exp − (x − x i ) 2 + (y − y i ) 2 2f(σ i ) 2 ,<label>(1)</label></formula><p>where the function f is used to produce a successive set of 'sharper' density maps. We</p><formula xml:id="formula_1">define f k (σ) = σ 1/k . Thus, D k = D(x, f k (·)</formula><p>). As can be seen when k = 1, D k is a very smoothed-out density map using nearest-neighbor dependent bandwidth and τ , whereas as k −→ ∞, D k approaches the binary localization map with a Dirac Delta function placed at each annotated pixel. Since each pixel has a unit area, the localization map assumes a unit value at the annotated location. For our experiments we used three density levels with last one being the localization map. It is also interesting to note that the various connections between density levels and base CNN also serve to provide intermediate supervision which aid in training the filters of base CNN towards counting and density estimation early on in the network.</p><p>Hypothetically, since integral over each estimatedD k yields a count for that density level, the final count can be obtained by taking the mean of counts from the density and localization maps as well as regression output from base CNN. This has two potential advantages: 1) the final count relies on multiple sources -each capturing count at a different scale. 2) During training the mean of four counts should equal the true count, which implicitly enforces an additional constraint thatD k should not only capture the density and localization information, but that each of their counts should also sum to the groundtruth count. For training, the loss function of density and localization maps is the mean square error between the predicted and ground truth maps, i.e. L k = MSE(D k , D k ), where k = 1, 2, and ∞, and regression loss, L c , is Euclidean loss between predicted and groundtruth counts, while the final loss is defined as the weighted mean all four losses.  For density map estimation and localization, we branch out from DenseBlock2 and feed it to our Density Network (see <ref type="table" target="#tab_3">Table 2</ref>). The density network introduces 2 new dense blocks and three 1 × 1 convolutional layers. Each dense block has features computed at the previous step, concatenated with all the density levels predicted thus far as input, and learns features aimed at computing the current density / localization map. We used 1 × 1 convolutions to get the output density map from these features. Density Level 1 is computed directly from DenseBlock2 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DenseNet with Composition Loss</head><p>We used Adam solver with a step learning rate in all our experiments. We used 0.001 as initial learning rate and reduce the learning rate by a factor of 2 after every 20 epochs. We trained the entire network for 70 epoch with a batch size of 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The UCF-QNRF Dataset</head><p>Dataset Collection. The images for the dataset were collected from three sources: Flickr, Web Search and the Hajj footage. The Hajj images were carefully selected so that there are multiple images that capture different locations, viewpoints, perspective effects and times of the day. For Flickr and Web Search, we manually generated the following queries: CROWD, HAJJ, SPECTATOR CROWD, PILGRIMAGE, PROTEST CROWD and CONCERT CROWD. These queries were then passed onto the Flickr and Google Image Search APIs. We selected desired number of images for each query to be 2000 for Flickr and 200 for Google Image Search. The search sorted all the results by RELE-VANCE incorporating both titles and tags, and for Flickr we also ensured that only those images were downloaded for which original resolutions were permitted to be downloaded (through the URL O specifier). The static links to all the images were extracted and saved for all the query terms, which were then downloaded using the respective APIs. The images were also checked for duplicates by computing image similarities followed by manual verification and discarding of duplicates.</p><p>Initial Pruning. The initial set of images were then manually checked for desirability. Many of the images were pruned due to one or more of the following reasons:</p><p>-Scenes that did not depict crowds at all or low-density crowds -Objects or visualizations of objects other than humans -Motion blur or low resolution -Very high perspective effect that is camera height is similar to average human height -Images with watermarks or those where text occupied more than 10% of the image In high-density crowd images, it is mostly the heads that are visible. However, people who appear far away from the camera become indistinguishable beyond a certain distance, which depends on crowd density, lighting as well as resolution of the camera sensor. During pruning, we kept those images where the heads were separable visually. Such images were annotated with the others, however, they were cropped afterwards to ensure that regions with problematic annotations or those with none at all due to difficulty in recognizing human heads were discarded.</p><p>We performed the entire annotation process in two stages. In the first stage, unannotated images were given to the annotators, while in the second stage, the images were given to verifiers who corrected any mistakes or errors in annotations. There were 14 annotators and 4 verifiers, who clocked 1, 300 and 200 hours respectively. In total, the entire procedure involved 2, 000 human-hours spent through to its completion.</p><p>Statistics. The dataset has 1, 535 jpeg images with 1, 251, 642 annotations. The train and test sets were created by sorting the images with respect to absolute counts, and selecting every 5th image into the test set. Thus, the training and test set consist of 1201 and 334 images, respectively. The distribution of images from [Flickr, Web, Hajj] for the train and test are [1078, 84, 39] and [306, <ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7]</ref>, respectively. In the dataset, the minimum and maximum counts are 49 and 12, 865, respectively, whereas the median and mean counts are 425 and 815.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Definition and Quantification of Tasks</head><p>In this section, we define the three tasks and the associated quantification measures.</p><p>Counting: The first task involves estimation of count for a crowd image i, given by c i . Although this measure does not give any information about location or distribution of people in the image, this is still very useful for many applications, for instance, estimating size of an entire crowd spanning several square kilometers or miles. For the application of counting large crowds, Jacob's Method <ref type="bibr" target="#b13">[14]</ref> due to Herbert Jacob is typically employed which involves dividing the area A into smaller sections, finding the average number of people or density d in each section, computing the mean densityd and extrapolating the results to entire region. However, with automated crowd counting, it is now possible to obtain counts and density for multiple images at different locations, thereby, permitting the more accurate integration of density over entire area covered by crowd. Moreover, counting through multiple aerial images requires cartographic tools to map the images onto the earth to compute ground areas. The density here is defined as the number of people in the image divided by ground area covered by the image. We propose to use the same evaluation measures as used in literature for this task: the Mean Absolute Error (C-MAE), Mean Squared Error (C-MSE) with the addition of Normalized Absolute Error (C-NAE). Density Map Estimation amounts to computing per-pixel density at each location in the image, thus preserving spatial information about distribution of people. This is particularly relevant for safety and surveillance, since very high density at a particular location in the scene can be catastrophic <ref type="bibr" target="#b0">[1]</ref>. This is different from counting since an image can have counts within safe limits, while containing regions that have very high density. This can happen due to the presence of empty regions in the image, such as walls and sky for mounted cameras; and roads, vehicles, buildings and forestation in aerial cameras. The metrics for evaluating density map estimation are similar to counting, except that they are per-pixel, i.e., the per-pixel Mean Absolute Error (DM-MAE) and Mean Squared Error (DM-MSE). Finally, we also propose to compute the 2D Histogram Intersection (DM-HI) distance after normalizing both the groundtruth and estimated density maps. This discards the effect of absolute counts and emphasizes the error in distribution of density compared to the groundtruth. Localization: The ideal approach to crowd counting would be to detect all the people in an image and then count the number of detections. But since dense crowd images contain severe occlusions among individuals and fewer pixels per person for those away from the camera, this is not a feasible solution. This is why, most approaches to crowd counting bypass explicit detection and perform direct regression on input images. However, for many applications, the precise location of individuals is needed, for instance, to initialize a tracking algorithm in very high-density crowd videos.</p><p>To quantify the localization error, estimated locations are associated with the ground truth locations through 1-1 matching using greedy association, followed by computation of Precision and Recall at various distance thresholds <ref type="figure" target="#fig_1">(1, 2, 3, . . . , 100 pixels)</ref>. The overall performance of the localization task is then computed through area under the Precision-Recall curve, L-AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Next, we present the results of experiments for the three tasks defined in Section 5.   <ref type="table">Table 3</ref>: We show counting results obtained using state-of-the-art methods in comparison with the proposed approach. Methods with '*' regress counts without computing density maps.</p><p>For counting, we evaluated the new UCF-QNRF dataset using the proposed method which estimates counts, density maps and locations of people simultaneously with several state-of-the-art deep neural networks <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> as well as those specifically developed for crowd counting <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b23">[24]</ref>. To train the networks, we extracted patches of sizes 448, 224 and 112 pixels at random locations from each training image. While deciding on image locations to extract patch from, we assigned higher probability of selection to image regions with higher count. We used mean square error of counts as the loss function. At test time, we divide the image into a grid of 224 × 224 pixel cells -zero-padding the image for dimensions not divisible by 224 -and evaluate each cell using the trained network. Final image count is given by aggregating the counts in all cells. <ref type="table">Table 3</ref> summarizes the results which shows the proposed network significantly outperforms the competing deep CNNs and crowd counting approaches. In <ref type="figure" target="#fig_4">Figure 4</ref>, we show the images with the lowest and highest error in the test set, for counts obtained through different components of the Composition Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Density Map Estimation</head><p>Method DM-MAE DM-MSE DM-HI MCNN <ref type="bibr" target="#b29">[30]</ref> 0.006670 0.0223 0.5354 SwitchCNN <ref type="bibr" target="#b23">[24]</ref> 0.005673 0.0263 0.5301 CMTL <ref type="bibr" target="#b24">[25]</ref> 0.005932 0.0244 0.5024 Proposed 0.00044 0.0017 0.9131 <ref type="table">Table 4</ref>: Results for Density map estimation:</p><p>We show results on Histogram intersection (HI), obtained using existing state-of-the-art methods compared to the proposed approach.</p><p>For density map estimation, we describe and compare the proposed approach with several methods that directly regress crowd density during training. Among the deep learning methods, MCNN <ref type="bibr" target="#b29">[30]</ref> consists of three columns of convolution networks with different filter sizes to capture different head sizes and combines the output of all the columns to make a final density estimate. SwitchCNN <ref type="bibr" target="#b23">[24]</ref> uses a similar three column network; however, it also employs a switching network that decides which column should exclusively handle the input patch. CMTL [25] employs a multi-task network that computes a high level prior over the image patch (crowd count classification) and density estimation. These networks are specifically designed for crowd density estimation and their results are reported in first three rows of <ref type="table">Table 4</ref>. The results of proposed approach are shown in the bottom row of <ref type="table">Table 4</ref>. The proposed approach outperforms existing approaches by an order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Localization</head><p>For the localization task, we adopt the same network configurations used for density map estimation to perform localization. To get the accurate head locations, we postprocess the outputs by finding the local peaks / maximums based on a threshold, also known as non-maximal suppression. Once the peaks are found, we match the predicted location with the ground truth location using 1-1 matching, and compute precision and recall. We use different distance thresholds as the pixel distance, i.e., if the detection is within the a particular distance threshold of the groundtruth, it is treated as True Positive, otherwise it is a False Positive. Similarly, if there is no detection within a groundtruth location, it becomes a False Negative.</p><p>The results of localization are reported in <ref type="table" target="#tab_5">Table 5</ref>. This table shows that DenseNet <ref type="bibr" target="#b9">[10]</ref> and Encoder-Decoder <ref type="bibr" target="#b2">[3]</ref> outperform ResNet <ref type="bibr" target="#b7">[8]</ref> and MCNN <ref type="bibr" target="#b29">[30]</ref>, while the proposed approach is superior to all the compared methods. The performance on the localization task is dependent on post-processing, which can alter results. Therefore, finding optimal strategy for localization from neural network output or incorporating the post-processing into the network is an important direction for future research. We also show some qualitative results of localization in <ref type="figure" target="#fig_5">Figure 5</ref>. The red dots represent the groundtruth while yellow circles are the locations estimated by the our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation Study</head><p>We performed an ablation study to validate the efficacy of composition loss introduced in this paper, as well as various choices in designing the network. These results are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Av. Precision Av. Recall L-AUC MCNN <ref type="bibr" target="#b29">[30]</ref> 59.93% 63.50% 0.591 ResNet74 <ref type="bibr" target="#b7">[8]</ref> 61.60% 66.90% 0.612 DenseNet63 <ref type="bibr" target="#b9">[10]</ref> 70.19% 58.10% 0.637 Encoder-Decoder <ref type="bibr" target="#b2">[3]</ref> 71.80% 62.98% 0.670 Proposed 75.8% 59.75% 0.714  shown in <ref type="table" target="#tab_7">Table 6</ref>. Next, we describe and provide details for the experiment corresponding to each row in the table.</p><p>BaseNetwork: This row shows the results with base network of our choice, which is DenseNet201. A fully-connected layer is appended to the last layer of the network followed by a single neuron which outputs the count. The input patch size is 224 × 224.</p><p>DenseBlock4: This experiment studies the effect of connecting the Density Network (  Concatenate: Here, we take the sum of the two density and one localization map to obtain 3 counts. We then concatenate these counts to the output of fully-connected layer of the base network to predict count from the single neuron. Thus, we leave to the optimization algorithm to find appropriate weights for these 3 values along with the rest of 1920 features of the fully-connected layer.</p><p>Mean: We also tested the effect of using equal weights for counts obtained from the base network and three density levels. We take sum of each density / localization map and take the mean of 4 values (2 density map sums, one localization sum, and one count from base network). We treat this mean value as final count output -both during training and testing. Thus, this imposes the constraint that not only the density and localization map correctly predict the location of people, but also their counts should be consistent with groundtruth counts irrespective of predicted locations.</p><p>Proposed: In this experiment, the Density Network is connected with the DenseBlock2 of base network, however, the Density Network simply outputs two density and one localization maps, none of which are connected to count output (see <ref type="figure" target="#fig_3">Figure 3</ref>).</p><p>In summary, these results show that the Density Network contributes significantly to performance on the three tasks. It is better to branch out from the middle layers of the base network, nevertheless the idea of multiple connections back and forth from the base network and Density Network is an interesting direction for further research. Furthermore, enforcing counts from all sources to be equal to the groundtruth count slightly worsens the counting performance. Nevertheless, it does help in estimating better density and localization maps. Finally, the decrease in error rates from the right to left in <ref type="table" target="#tab_7">Table 6</ref> highlights the positive influence of the proposed Composition Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper introduced a novel method to estimate counts, density maps and localization in dense crowd images. We showed that these three problems are interrelated, and can be decomposed with respect to each other through Composition Loss which can then be used to train a neural network. We solved the three tasks simultaneously with the counting performance benefiting from the density map estimation and localization as well. We also proposed the large-scale UCF-QNRF dataset for dense crowds suitable for the three tasks described in the paper. We provided details of the process of dataset collection and annotation, where we ensured that only high-resolution images were curated for the dataset. Finally, we presented extensive set of experiments using several recent deep architectures, and show how the proposed approach is able to achieve good performance through detailed ablation study. We hope the new dataset will prove useful for this type of research, with applications in safety and surveillance, design and expansion of public infrastructures, and gauging political significance of various crowd events.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>(a) This graph shows the relative distribution of image counts among the four datasets. The proposed UCF-QNRF dataset has a fair number of images from all five count ranges. (b) This graph shows a 2D histogram of image resolution for all the images in the new dataset. The x-axis shows the number of rows, while y-axis is the number of columns. Each bin (500 × 500 pixels) is color-coded with the number of images that have the corresponding resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The figure shows the proposed architecture for estimating count, density and localization maps simultaneously for a given patch in an image. At the top is the base DenseNet which regresses only the counts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>This figure shows pairs of images where the left image in the pair has the lowest counting error while the right image has the highest counting error with respect to the four components of the Composition Loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Two examples of localization using the proposed approach. Ground truth is depicted in red and predicted locations after threshold are shown in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of statistics of different datasets.</figDesc><table><row><cell>UCF CC 50 (44MB); World-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>This table showsthe filter dimensions and output of the three density layer blocks appended to the network inFig. 3.</figDesc><table /><note>We use DenseNet [10] as our base network. It consists of 4 Dense blocks where each block has a number of consecutive 1 × 1 and 3 × 3 convolutional layers. Each dense block (except for the last one) is followed by a Transition layer, which reduces the num- ber of feature-maps by applying 1 × 1 convolutions followed by 2 × 2 average pooling with stride 2. In our experiments we used DenseNet-201 architecture. It has {6, 12, 48, 32} sets of 1 × 1 and 3 × 3 convolutional layers in the four dense blocks, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>This table shows the localization results averaged over four distance thresholds for different methods. We show Average Precision, Average Recall and AUC metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc>) containing the different density levels with DenseBlock4 of the base DenseNet instead of DenseBlock2. Since DenseBlock4 outputs feature maps of size 7 × 7, we therefore used deconvolution layer with stride 4 to upsample the features before feeding in to our Density Network.DenseBlock3: This experiment is similar to DenseBlock4, except that we connect our Density Network to Denseblock3 of the base network. DenseBlock3 outputs feature maps which are 14 × 14 in spatial dimensions, whereas we intend to predict density maps of spatial dimension 28 × 28, so we upsample the feature maps by using deconvolution layer before feeding them to the proposed Density Network.D 1 only: This row represents the results if we use Density Level 1 only in the Density Network along with regression of counts in the base network. The results are much</figDesc><table><row><cell>Experiment</cell><cell cols="9">Count MAE MSE NAE MAE MSE NAE MAE MSE NAE MAE MSE NAE D∞ D2 D1</cell></row><row><cell cols="2">BaseNetwork 163 227 0.395 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="10">DenseBlock4 148 265 0.385 382 765 0.956 879 1235 3.892 2015 4529 4.295</cell></row><row><cell cols="10">DenseBlock3 144 236 0.363 295 687 0.721 805 1159 3.256 1273 2936 3.982</cell></row><row><cell>D1 only</cell><cell>141 233 0.261 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">-1706 2496 5.677</cell></row><row><cell cols="2">D1 &amp; D2 only 137 208 0.251 -</cell><cell>-</cell><cell>-</cell><cell cols="6">691 1058 2.459 1887 3541 6.850</cell></row><row><cell>Concatenate</cell><cell cols="9">139 223 0.264 258 508 0.634 718 1096 3.570 1910 4983 6.574</cell></row><row><cell>Mean</cell><cell cols="9">150 341 0.271 405 710 1.135 1015 2099 2.916 1151 3170 3.283</cell></row><row><cell>Proposed</cell><cell cols="9">132 191 0.258 236 408 0.506 682 922 2.027 1629 3600 4.396</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>This table shows the results of ablation study. D ∞ corresponds to the results of counting using localization map estimation, while D 2 and D 1 represent results from the two density maps, respectively. worse compared to the proposed method which uses multiple levels in the Composition Loss.D 1 and D 2 only: Similar to D 1 only, this row represents the results if we use Density Levels 1 and 2 and do not use the D ∞ in the Density Network. Incorporation of another density level improves results slightly in contrast to a single density level.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.guardian.co.uk/world/2006/jan/13/saudiarabia" />
		<title level="m">A history of hajj tragedies. The Guardian</title>
		<imprint>
			<date type="published" when="2006-07-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to count with regression forest and structured labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fiaschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crowd disasters as systemic failures: analysis of the love parade disaster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mukerji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Data Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two systems of non-symbolic numerical cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Hyde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in human neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tracking in dense crowds using prominence and neighborhood motion concurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">To count a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Columbia Journalism Review</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="36" to="40" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A viewpoint invariant approach for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1187" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<title level="m">Counting in dense crowds using deep learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Small instance detection by integer programming on object density maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Are subitizing and counting implemented as separate or functionally overlapping processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mechelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Butterworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="435" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Density-aware person detection and tracking in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crowd counting using multiple local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>DICTA&apos;09.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>14th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1196" to="1206" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multicolumn convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
