<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Multilingual Alignment using Wasserstein Barycenter</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lian</surname></persName>
							<email>x9lian@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Jain</surname></persName>
							<email>k22jain@uwaterloo.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Borealis AI</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Truszkowski</surname></persName>
							<email>jakub.truszkowski@borealisai.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Borealis AI</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
							<email>ppoupart@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Borealis AI</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
							<email>yaoliang.yu@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Multilingual Alignment using Wasserstein Barycenter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study unsupervised multilingual alignment, the problem of finding word-to-word translations between multiple languages without using any parallel data. One popular strategy is to reduce multilingual alignment to the much simplified bilingual setting, by picking one of the input languages as the pivot language that we transit through. However, it is wellknown that transiting through a poorly chosen pivot language (such as English) may severely degrade the translation quality, since the assumed transitive relations among all pairs of languages may not be enforced in the training process. Instead of going through a rather arbitrarily chosen pivot language, we propose to use the Wasserstein barycenter as a more informative "mean" language: it encapsulates information from all languages and minimizes all pairwise transportation costs. We evaluate our method on standard benchmarks and demonstrate state-of-the-art performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many natural language processing tasks, such as part-ofspeech tagging, machine translation and speech recognition, rely on learning a distributed representation of words. Recent developments in computational linguistics and neural language modeling have shown that word embeddings can capture both semantic and syntactic information. This led to the development of the zero-shot learning paradigm as a way to address the manual annotation bottleneck in domains where other vector-based representations must be associated with word labels. This is a fundamental step to make natural language processing more accessible. A key input for machine translation tasks consists of embedding vectors for each word. <ref type="bibr" target="#b5">Mikolov et al. [2013b]</ref> were the first to release their pre-trained model and gave a distributed representation of words. After that, more software for training and using word embeddings emerged.</p><p>The rise of continuous word embedding representations has revived research on the bilingual lexicon alignment problem <ref type="bibr" target="#b6">[Rapp, 1995;</ref><ref type="bibr" target="#b4">Fung, 1995]</ref>, where the initial goal was to learn a small dictionary of a few hundred words by leveraging statistical similarities between two languages. Mikolov et al.</p><p>[2013a] formulated bilingual word embedding alignment as a quadratic optimization problem that learns an explicit linear mapping between word embeddings, which enables us to even infer meanings of out-of-dictionary words <ref type="bibr" target="#b8">[Zhang et al., 2016;</ref><ref type="bibr" target="#b3">Dinu et al., 2015;</ref><ref type="bibr" target="#b5">Mikolov et al., 2013a]</ref>. <ref type="bibr" target="#b8">Xing et al. [2015]</ref> showed that restricting the linear mapping to be orthogonal further improves the result. These pioneering works required some parallel data to perform the alignment. Later on, <ref type="bibr" target="#b6">[Smith et al., 2017;</ref><ref type="bibr" target="#b1">Artetxe et al., 2017;</ref><ref type="bibr" target="#b1">Artetxe et al., 2018a]</ref> reduced the need of supervision by exploiting common words or digits in different languages, and more recently, unsupervised methods that rely solely on monolingual data have become quite popular <ref type="bibr" target="#b5">[Gouws et al., 2015;</ref><ref type="bibr">Zhang et al., 2017b;</ref><ref type="bibr">Zhang et al., 2017a;</ref><ref type="bibr" target="#b5">Lample et al., 2018;</ref><ref type="bibr" target="#b1">Artetxe et al., 2018b;</ref><ref type="bibr" target="#b3">Dou et al., 2018;</ref><ref type="bibr">Hoshen and Wolf, 2018;</ref>.</p><p>Encouraged by the success on bilingual alignment, the more ambitious task that aims at simultaneously and unsupervisedly aligning multiple languages has drawn a lot of attention recently.</p><p>A naive approach that performs all pairwise bilingual alignment separately would not work well, since it fails to exploit all language information, especially when there are low resource ones. A second approach is to align all languages to a pivot language, such as English <ref type="bibr" target="#b6">[Smith et al., 2017]</ref>, allowing us to exploit recent progresses on bilingual alignment while still using information from all languages.</p><p>More recently, <ref type="bibr" target="#b2">[Chen and Cardie, 2018;</ref><ref type="bibr" target="#b6">Taitelbaum et al., 2019b;</ref><ref type="bibr">Taitelbaum et al., 2019a;</ref><ref type="bibr" target="#b1">Alaux et al., 2019;</ref><ref type="bibr" target="#b7">Wada et al., 2019]</ref> proposed to map all languages into the same language space and train all language pairs simultaneously. Please refer to the related work section for more details.</p><p>In this work, we first show that the existing work on unsupervised multilingual alignment (such as <ref type="bibr" target="#b1">[Alaux et al., 2019]</ref>) amounts to simultaneously learning an arithmetic "mean" language from all languages and aligning all languages to the common mean language, instead of using a rather arbitrarily pre-determined input language (such as English). Then, we argue for using the (learned) Wasserstein barycenter as the pivot language as opposed to the previous arithmetic barycenter, which, unlike the Wasserstein barycenter, fails to preserve distributional properties in word embeddings. Our approach exploits available information from all languages to enforce coherence among language spaces by enabling accurate com-positions between language mappings. We conduct extensive experiments on standard publicly available benchmark datasets and demonstrate competitive performance against current state-of-the-art alternatives. The source code is available at https://github.com/alixxxin/multi-lang. 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multilingual Lexicon Alignment</head><p>In this section we set up the notations and define our main problem: the multilingual lexicon alignment problem.</p><p>Given m languages L 1 , . . . , L m , each represented by a vocabulary V i consisting of n i respective words. Following <ref type="bibr" target="#b5">Mikolov et al. [2013a]</ref>, we assume a monolingual word embedding X i = [x i,1 , . . . , x i,ni ] ∈ R ni×di for each language L i has been trained independently on its own data. We are interested in finding all pairwise mappings T i→k : R di → R d k that translate a word x i,ji in language L i to a corresponding word x k,j k = T i→k (x i,ji ) in language L k . In the following, for the ease of notation, we assume w.l.o.g. that n i ≡ n and d i ≡ d. Note that we do not have access to any parallel data, i.e., we are in the much more challenging unsupervised learning regime.</p><p>Our work is largely inspired by that of Alaux et al. <ref type="bibr">[2019]</ref>, which we review below first. Along the way we point out some crucial observations that motivated our further development. <ref type="bibr" target="#b1">Alaux et al. [2019]</ref> employ the following joint alignment approach that minimizes the total sum of mis-alignment costs between every pair of languages:</p><formula xml:id="formula_0">min Qi∈O d ,P ik ∈Pn m i=1 m k=1,k =i X i Q i − P ik X k Q k 2 ,<label>(1)</label></formula><p>where Q i ∈ O d is a d × d orthogonal matrix and P ik ∈ P n is an n × n permutation matrix 2 . Since Q i is orthogonal, this approach ensures transitivity among word embeddings: Q i maps the i-th word embedding space X i into a common space X, and conversely Q −1 i = Q i maps X back to X i . Thus, Q i Q k maps X i to X k , and if we transit through an intermediate word embedding space X t , we still have the desired transitive property</p><formula xml:id="formula_1">Q i Q t · Q t Q k = Q i Q k .</formula><p>The permutation matrix P ik serves as an "inferred" correspondence between words in language L i and language L k . Naturally, we would again expect some form of transitivity in these pairwise correspondences, i.e., P ik · P kt ≈ P it , which, however, is not enforced in (1). A simple way to fix this is to decouple P ik into the product P i P k , in the same way as how we dealt with Q i . This leads to the following variant:</p><formula xml:id="formula_2">argmin Qi∈O d ,Pi∈Pn m i=1 m k=1 P i X i Q i − P k X k Q k 2 (2) = argmin Qi∈O d ,Pi∈Pn m i=1 P i X i Q i − 1 m m k=1 P k X k Q k 2 (3) = argmin Qi∈O d ,Pi∈Pn min X∈R n×d m i=1 P i X i Q i −X 2<label>(4)</label></formula><p>where Eq. 3 follows from the definition of variance andX in Eq. 4 admits the closed-form solution:</p><formula xml:id="formula_3">X = 1 m m k=1 P k X k Q k .<label>(5)</label></formula><p>Thus, had we known the arithmetic "mean" languageX beforehand, the joint alignment approach of Alaux et al. <ref type="bibr">[2019]</ref> would reduce to a separate alignment of each language X i to the "mean" languageX that serves as the pivot. An efficient optimization strategy would then consist of alternating between separate alignment (i.e., computing Q i and P i ) and computing the pivot language (i.e., (5)). We now point out two problems in the above formulation. First, a permutation assignment is a 1-1 correspondence that completely ignores polysemy in natural languages, that is, a word in language L i can correspond to multiple words in language L k . To address this, we propose to relax the permutation P i into a coupling matrix that allows splitting a word into different words. Second, the pivot language in (5), being a simple arithmetic average, may be statistically very different from any of the m given languages, see <ref type="figure" target="#fig_0">Figure 1</ref> and below. Besides, intuitively it is perhaps more reasonable to allow the pivot language to have a larger dictionary so that it can capture all linguistic regularities in all m languages. To address this, we propose to use the Wasserstein barycenter as the pivot language.</p><p>The advantage of using Wasserstein barycenter instead of the arithmetic average is that the Wasserstein metric gives a natural geometry for probability measures supported on a geometric space. In <ref type="figure" target="#fig_0">Figure 1</ref>, we demonstrate the difference between Wasserstein Barycenter and arithmetic average of two input distributions.</p><p>It is intuitively clear that the Wasserstein barycenter preserves the geometry of the input distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>We take a probabilistic approach, treating each language L i as a probability distribution over its word embeddings:</p><formula xml:id="formula_4">π i = n j=1 p ij δ x i j (6)</formula><p>where p ij is the probability of occurrence of the j-th word x i j in language L i (often approximated by the relative frequency of word x i j in its training documents), and δ x i j is the unit mass at x i j . We project word embeddings into a common space through the orthogonal matrix Q i ∈ O d . Taking a word x i from each language L i , we associate a cost c(Q 1 x 1 , . . . , Q m x m ) ∈ R + for bundling these words in our joint translation. To allow polysemy, we find a joint distribution π with fixed marginals π i so that the average cost</p><formula xml:id="formula_5">c(Q 1 x 1 , . . . , Q m x m ) dπ(x 1 , . . . , x m )<label>(7)</label></formula><p>is minimized. If we fix Q i , then the above problem is known as multi-marginal optimal transport <ref type="bibr">[Gangbo andŚwikech, 1998</ref>].</p><p>To simplify the computation, we take the pairwise approach of Alaux et al. <ref type="bibr">[2019]</ref>, where we set the joint cost c as the total sum of all pairwise costs:</p><formula xml:id="formula_6">c(x 1 , . . . , x m ) = i,k x i − x j 2 .<label>(8)</label></formula><p>Interestingly, with this choice, we can significantly simplify the numerical computation of the multi-marginal optimal transport.</p><p>We recall the definition of Wasserstein barycenter ν of m given probability distributions π 1 , . . . , π m :</p><formula xml:id="formula_7">ν = arg min µ m i=1 λ i · W 2 2 (π i , µ),<label>(9)</label></formula><p>where λ ≥ 0 are the weights, and the (squared) Wasserstein distance W 2 2 is given as:</p><formula xml:id="formula_8">W 2 2 (π i , µ) = min Πi∈Γ(πi,µ) x − y 2 dΠ i (x, y).<label>(10)</label></formula><p>The notation Γ(π i , µ) denotes all joint probability distributions (i.e. couplings) Π i with (fixed) marginal distributions π i and µ. As proven by <ref type="bibr" target="#b0">Agueh and Carlier [2011]</ref>, with the pairwise distance (8), the multi-marginal problem in (7) and the barycenter problem in (9) are formally equivalent. Hence, from now on we will focus on the latter since efficient computational algorithms for it exist. We use the push-forward notation (Q i ) # π i to denote the distribution of Q i x i when x i follows the distribution π i . Thus, we can write our approach succinctly as:</p><formula xml:id="formula_9">min µ min Qi∈O d m i=1 λ i · W 2 2 [(Q i ) # π i , µ],<label>(11)</label></formula><p>Algorithm 1: Barycenter Alignment Input:</p><formula xml:id="formula_10">Language distribution L i = (X i , p i ) m i=1 , p Output: Translation for L k and L m for i = 1, . . . , m do X i ← X i − mean(X i ) {C i } ← cosine dist(X i,j , X i,k )∀j, k Π i ← GW(C i , C 1 , p i , p 1 ) U ΣV ← SVD(X i Π i X 1 ) Q i ← U V X i ← X i Q i while not converged do ν ← WB(π 1 , · · · , π m ; λ 1 , · · · , λ m ) for i = 1, . . . , m do Π i ← OT(π i , ν) U ΣV ← SVD(X i Π i Y) Q i ← U V X i ← X i Q i ; return (Π 1 , . . . , Π m , Q 1 , . . . , Q m )</formula><p>where the barycenter µ serves as the pivot language in some common word embedding space. Unlike the arithmetic average in <ref type="formula" target="#formula_3">(5)</ref>, the Wasserstein barycenter can have a much larger support (dictionary size) than the m given language distributions.</p><p>We can again apply the alternating minimization strategy to solve <ref type="formula" target="#formula_0">(11)</ref>: fixing all orthogonal matrices Q i , we find the Wasserstein barycenter using an existing algorithm of <ref type="bibr" target="#b3">[Cuturi and Doucet, 2014]</ref> or <ref type="bibr" target="#b2">[Claici et al., 2018]</ref>; fixing the Wasserstein barycenter µ, we solve each orthogonal matrix Q i separately:</p><formula xml:id="formula_11">min Qi∈O d min Πi∈Γ(πi,µ) Q i x − y 2 dΠ i (x, y).<label>(12)</label></formula><p>For fixed coupling Π i ∈ R n×s , where s is the dictionary size for the barycenter µ, the integral can be simplified as:</p><formula xml:id="formula_12">jl (Π i ) jl Q i x i j − y l 2 ≡ − X i Π i Y, Q i .<label>(13)</label></formula><p>Thus, using the well-known theorem of Schönemann <ref type="bibr">[1966]</ref>, Q i is given by the closed-form solution</p><formula xml:id="formula_13">U i V i , where U i Σ i V i = X i Π i Y is the singular value decomposition.</formula><p>Our approach is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>To speed up the computation, we took a similar approach as Alaux et al.</p><p>[2019] and initialized space alignment matrices with the Gromov-Wasserstein approach [Alvarez-Melis and Jaakkola, 2018] applied to the first 5k vectors ( Alaux et al.</p><p>[2019] used the first 2k vectors) and with regularization parameter of 5e −5 . After the initialization, we use the space alignment matrices to map all languages into the language space of the first language. Multiplying all language embedding vectors with the corresponding space alignment matrix, we realign all languages into a common language space. In the common space, we compute the Wasserstein barycenter of all projected language distributions. The support locations for the barycenter are initialized with random samples from a standard normal distribution. The next step is to compute the optimal transport plans from the barycenter distribution to all language distributions. After obtaining optimal transport plans T i from the barycenter to every language L i , we can imply translations from L i to L j from the coupling T i T j . The coupling is not necessarily a permutation matrix, and indicates the probability with which a word corresponds to another. Method and code for computing accuracies of bilingual translation pairs are borrowed from Alvarez-Melis and Jaakkola <ref type="bibr">[2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare the results of our method on MUSE with the following methods: 1) Procrustes Matching with RSLS as similarity function to imply translation pairs <ref type="bibr" target="#b5">[Lample et al., 2018]</ref>; 2) the state-of-the-art bilingual alignment method, Gromov-Wasserstein alignment (GW) <ref type="bibr" target="#b1">[Alvarez-Melis and Jaakkola, 2018]</ref>; 3) the state-of-the-art multilingual alignment method (UMH) <ref type="bibr" target="#b1">[Alaux et al., 2019]</ref>; 4) bilingual alignment with multilingual auxiliary information (MPPA) <ref type="bibr" target="#b6">[Taitelbaum et al., 2019b]</ref>; and 5) unsupervised multilingual word embeddings trained with multilingual adversarial training <ref type="bibr" target="#b2">[Chen and Cardie, 2018]</ref>.</p><p>We compare the results of our method on XLING dataset with Ranking-Based Optimization <ref type="formula">(</ref> The translation accuracies for Gromov-Wasserstein are computed using the source code released by <ref type="bibr" target="#b1">[Alvarez-Melis and Jaakkola, 2018]</ref>. For the multilingual alignment method (UMH) <ref type="bibr" target="#b1">[Alaux et al., 2019]</ref>, and the two multilingual adversarial methods <ref type="bibr" target="#b2">[Chen and Cardie, 2018]</ref>, <ref type="bibr" target="#b6">[Taitelbaum et al., 2019b]</ref>, we directly compare our accuracies to previous methods as reported in <ref type="bibr" target="#b5">[Glavas et al., 2019]</ref>.  <ref type="bibr">et al., 2018]</ref>. For most language pairs, our method Barycenter Alignment (BA) outperforms all current unsupervised methods. Our barycenter approach infers a "potential universal language" from input languages. Transiting through that universal language, we infer translation for all pairs of languages. From the experimental results in <ref type="table" target="#tab_1">Table 2</ref>, we can see that our approach is clearly at an advantage and it benefits from using the information from all languages. Our method achieves statistically significant improvement for 22 out of 30 language pairs (p ≤ 0.05, McNemar's test, one-sided). <ref type="table" target="#tab_5">Table 3</ref> shows mean average precision (MAP) for 10 bilingual tasks on the XLING dataset <ref type="bibr" target="#b5">[Glavas et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In <ref type="table">Table 1</ref>, we show several German to English translations and compare the results to Gromov-Wasserstein direct bilingual alignment. Our method is capable of incorporating both the semantic and syntactic information of one word. For example, the top ten predicted English translations for the German word München, are "Cambridge, Oxford, Munich, London, Birmingham, Bristol, Edinburgh, Dublin, Hampshire, Baltimore". In this case, we hit the English translation Munich. What's more important in this example is that all predicted English words are the name of some city. Therefore, our method is capable of implying München is a city name. Another example is the German word sollte, which means "should" in English. The top five words predicted for sollte are syntactically correct -would, could, will, should and might are all modal verbs. The last three examples show polysemous words, and in all these cases our method performs better than the Gromov-Wasserstein. For German word erschienen, our algorithm predicts all three words released, appeared, and published in the top ten translations as compared to Gromov-Wasserstein which only predicts published .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we show the impact of some of our design choices and hyperparameters. One of the parameters is the number of support locations. In theory, the optimal barycenter distribution could have as many support locations as the sum of the total number of support locations for all input distributions. In <ref type="figure">Figure 2</ref>, we show the impact on translation performance when we have a different number of support locations. Let n j be the number of words we have in language L j . We picked the three most representative cases: the average number of words = m j=1 n j /m, twice the average number of words = 2 m j=1 n j /m, and the total number of words = m j=1 n j . As we increase the number of support locations for the barycenter distribution, we can see in <ref type="figure">Figure 2</ref> that the performance for language translation improves. However, when we increase the number of support locations for the barycenter, the algorithm becomes costly. Therefore, in an effort to balance accuracy and computational complexity, we decided to use 10000 support locations (twice the average number of words).</p><p>We also conducted a set of experiments to determine whether the inclusion of distant languages increases bilingual translation accuracy. Excluding two non-Indo-European languages Finnish and Turkish, we calculated the barycenter of Croatian (HR), English (EN), French (FR), German (DE), and Italian (IT). <ref type="figure" target="#fig_2">Figure 3</ref> contains results for common bilingual pairs. The red bar show the bilingual translation accuracy when translating through the barycenter for all languages including Finnish and Turkish, whereas blue bar indicate the accuracy of translations that use the barycenter of the five Indo-European languages. <ref type="figure">Figure 2</ref>: Accuracies for language pairs using different numbers of support locations for the barycenter. In our experimental setup, we have 5000 words in each language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>We briefly describe related work on supervised and unsupervised techniques for bilingual and multilingual alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Supervised Bilingual Alignment</head><p>Mikolov et al.</p><p>[2013a] formulated the problem of aligning word embeddings as a quadratic optimization problem to find an explicit linear map Q between the word embeddings X 1 and X 2 of two languages.</p><formula xml:id="formula_14">min Q ||X 1 Q − P X 2 || 2 2 (14)</formula><p>This setting is supervised since the assignment matrix P that maps words of one language to another is known. Later, <ref type="bibr">[Xing</ref>    <ref type="bibr">al., 2015]</ref> showed that the results can be improved by restricting the linear mapping Q to be orthogonal. This corresponds to Orthogonal Procrustes <ref type="bibr" target="#b6">[Schönemann, 1966]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unsupervised Bilingual Alignment</head><p>In the unsupervised setting, the assignment matrix P between words is unknown, and we resort to the joint optimization:</p><formula xml:id="formula_15">min Q min P ||X 1 Q − P X 2 || 2 2 .<label>(15)</label></formula><p>As a result, the optimization problem becomes non-convex and therefore more challenging. The problem can be relaxed into a (convex) semidefinite program.This method provides high accuracy at the expense of high computation complexity. Therefore, it is not suitable for large scale problems. Another way to solve (15) is to use Block Coordinate Relaxation, where we iteratively optimize each variable with other variables fixed. When Q is fixed, optimizing P can be done with the Hungarian algorithm in O(n 3 ) time (which is prohibitive since n is the number of words). <ref type="bibr" target="#b3">Cuturi and Doucet [2014]</ref> developed an efficient approximation (complexity O(n 2 )) achieved by adding a negative entropy regularizer.Observing that both P and Q preserve the intra-language distances, Alvarez-Melis and Jaakkola <ref type="bibr">[2018]</ref> cast the unsupervised bilingual alignment problem as a Gromov-Wasserstein optimal transport problem, and give a solution with minimum hyper-parameter to tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multilingual Alignment</head><p>In multilingual alignment, we seek to align multiple languages together while taking advantage of inter-dependencies to ensure consistency among them. A common approach consists of mapping each language to a common space X 0 by minimizing some loss function l:</p><formula xml:id="formula_16">min Qi∈O d ,Pi∈Pn i l(X i Q i , P i X 0 )<label>(16)</label></formula><p>The common space may be a pivot language such as English <ref type="bibr" target="#b6">[Smith et al., 2017;</ref><ref type="bibr" target="#b5">Lample et al., 2018;</ref>. Nakashole and Flauger <ref type="bibr">[2017]</ref> and Alaux et al. <ref type="bibr">[2019]</ref> showed that constraining coherent word alignments between triplets of nearby languages improves the quality of induced bilingual lexicons. <ref type="bibr" target="#b2">Chen and Cardie [2018]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we discussed previous attempts to solve the multilingual alignment problem, compared similarity between the approaches and pointed out a problem with existing formulations. Then we proposed a new method using the Wasserstein barycenter as a pivot for the multilingual alignment problem. At the core of our algorithm lies a new inference method based on an optimal transport plan to predict the similarity between words. Our barycenter can be interpreted as a virtual universal language, capturing information from all languages. The algorithm we proposed improves the accuracy of pairwise translations compared to the current state-of-the-art method..   Each iteration of our barycenter algorithm optimizes the barycenter weights and then the support locations. In this section, we investigate the speed of convergence of our approach. In <ref type="figure" target="#fig_4">figure 4</ref>, we plot the translation accuracy for all language pairs as a function of the number of iterations. As we can see, the accuracy stabilizes after roughly 5 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Hierarchical Approach</head><p>Training a joint barycenter for all languages captures shared information across all languages. We hypothesized that distant languages might potentially impair performance for some language pairs. To leverage existing knowledge of similarities between languages, we constructed a language tree whose topology was consistent with the widely agreed phylogeny of Indo-European languages (see e.g. <ref type="bibr" target="#b5">[Gray and Atkinson, 2003]</ref>). For each non-leaf node, we set it the barycenter for all its children. We traverse the language tree in depth-first order and store the mappings corresponding to each edge. The translation between any two languages can be implied by traversing through the tree structure and multiplying the mappings corresponding to each edge. <ref type="table">Table 4</ref> shows the results for the hierarchical barycenter. We see that the hierarchical approach yields slightly better performance for some language pairs, particularly for closely related languages such as Spanish and Portuguese or Italian and Spanish. For most language pairs, it does not improve over the weighted barycenter. More details about the hierarchical approach are available in first author's thesis <ref type="bibr" target="#b5">[Lian, Xin, 2020]</ref>.   <ref type="table">Table 4</ref>: Accuracy results for translation pairs between all pairs of languages for all evaluated methods. The column GW-benchmark contains results from Gromov-Wasserstein direct bilingual alignment. Unweighted is the barycenter approach without optimizing on support location weights. Hierarchical contains results from traversing through edges and infer translation mapping through hierarchical barycenters. The weighted column is what Algorithm 1 returns, optimizing both on support locations and weights on the support.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparing the Wasserstein barycenter and arithmetic mean (bottom panel) for two input distributions (top panel).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>RCSLS) [Joulin et al., 2018], solution to the Procrustes Problem (PROC) [Artetxe et al., 2018b; Lample et al., 2018; Glavas et al., 2019], Gromov-Wasserstein alignment (GW) [Alvarez-Melis and Jaakkola, 2018], and VECMAP [Artetxe et al., 2018b]. RCLS and PROC are supervised methods, while GW and VECMAP are both unsupervised methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>This graph shows the accuracy of bilingual translation pairs. The red bar indicate translation accuracy using the barycenter of all languages (HR, EN, FI, FR, DE, RU, IT, TR), while the blue bar correspond to the barycenter of (HR, EN, FR, DE, IT, RU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>et</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Translation accuracies for language pairs as a function of the number of iterations. The barycenter stabilizes after the 5-th iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>evaluate our algorithm on two standard publicly available datasets:MUSE [Lample et al., 2018]  and XLING<ref type="bibr" target="#b5">[Glavas et al., 2019]</ref>. The MUSE benchmark is a high-quality dictionary containing up to 100k pairs of words and has now become a standard benchmark for cross-lingual alignment tasks<ref type="bibr" target="#b5">[Lample et al., 2018]</ref>. On this dataset, we conducted an experiment with 6 European languages: English, French, Spanish, Italian, RU), and Turkish (TR). In this set of languages, we have languages coming from three different Indo-European branches, as well as two non-Indo-European languages (FI from Uralic and TR from the Turkic family)<ref type="bibr" target="#b5">[Glavas et al., 2019]</ref>.</figDesc><table /><note>Portuguese, and German. The MUSE dataset contains a direct translation for any pair of languages in this set. We also conducted an experiment with the XLING dataset with a more diverse set of languages: Croatian (HR), English (EN), Finnish (FI), French (FR), German (DE), Italian (IT), Russian (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>depicts precision@1 results for all bilingual tasks on the MUSE benchmark [Lample</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>extended the work of [Lample et al., 2018] to the multilingual case using adversarial algorithms. Taitelbaum et al. extended Procrustes Matching to the multi-Pairwise case [Taitelbaum et al., 2019b], and also designed an improved representation of the source word using auxiliary languages [Taitelbaum et al., 2019a].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>88.22 * 83.23 * 76.63 * 91.08 96.04 91.04 * 82.91 * 76.99</figDesc><table><row><cell>German</cell><cell>English</cell><cell></cell><cell></cell><cell></cell><cell cols="2">GW Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BA Prediction</cell></row><row><cell>München</cell><cell>Munich</cell><cell></cell><cell></cell><cell cols="6">London, Dublin, Oxford, Birmingham, Wellington</cell><cell cols="3">Cambridge, Oxford, Munich, London, Birmingham</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Glasgow, Edinburgh, Cambridge, Toronto, Hamilton</cell><cell cols="3">Rristol, Edinburgh, Dublin, Hampshire, Baltimore</cell></row><row><cell>sollte</cell><cell>should</cell><cell></cell><cell></cell><cell cols="5">would, could, might, will, needs,</cell><cell></cell><cell></cell><cell cols="2">would, could, will, supposed,</cell></row><row><cell>erschienen</cell><cell>released it-es</cell><cell></cell><cell>it-fr</cell><cell cols="5">supposed, put, willing, wanted, meant published, editions, publication, edition it-pt it-en it-de es-it</cell><cell cols="2">es-fr</cell><cell cols="2">might, meant, needs, expected, able, should published, editions, volumes, publication es-pt es-en es-de</cell></row><row><cell>GW</cell><cell>appeared 92.63</cell><cell cols="2">91.78</cell><cell cols="5">printed, volumes, compilation 89.47 80.38 74.03 89.35</cell><cell cols="2">91.78</cell><cell>92.82</cell><cell>released, titled, printed 81.52 75.03</cell></row><row><cell>GW o</cell><cell cols="2">published -</cell><cell>-</cell><cell>-</cell><cell cols="3">publications, releases, titled 75.2 -</cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">appeared, edition, compilation -80.4 -</cell></row><row><cell cols="9">aufgenommen PA MAT+MPPA 87.5 admitted recorded 87.3 taken, included 87.1 87.7 viel lots, lot MAT+MPSR 88.2 88.1 much UMH 87.0 86.7 BA 92.32 92.54  *  90.14 81.84  *  75.65  *  89.38 recorded, taken, recording, selected roll, placed, performing 81.0 76.9 67.5 83.5 eligible, motion, assessed 81.2 77.7 67.1 83.7 much, lot, little, more, less 82.3 77.4 69.5 84.5 bit, too, plenty, than, better 80.4 79.9 67.5 83.3</cell><cell cols="2">85.8 85.9 86.9 85.1 92.19</cell><cell cols="2">recorded, taken, recording, admitted 87.3 82.9 68.3 selected, sample, included 86.8 83.5 66.5 track, featured, mixed 87.8 83.7 69.0 much, lot, little, less, too 86.3 85.3 68.7 more, than, bit, far, lots 92.85 83.5  *  78.25  *</cell></row><row><cell cols="13">Table 1: German-to-English translation prediction comparing results by 1) using GW alignment to imply direct bilingual mapping and 2) fr-it fr-es fr-pt fr-en fr-de pt-it pt-es pt-fr pt-en pt-de</cell></row><row><cell cols="13">using Barycenter Alignment method described in Algorithm 1. We show top-10 translations of both methods. Last three examples show the GW 88.0 90.3 87.44 82.2 74.18 90.62 96.19 89.9 81.14 74.83</cell></row><row><cell cols="6">polysemous words and their corresponding translations. GW o ---82.1</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PA</cell><cell>83.2</cell><cell></cell><cell>82.6</cell><cell>78.1</cell><cell>82.4</cell><cell>69.5</cell><cell></cell><cell>81.1</cell><cell cols="2">91.5</cell><cell>84.3</cell><cell>80.3</cell><cell>63.7</cell></row><row><cell cols="2">MAT+MPPA 83.1</cell><cell></cell><cell>83.6</cell><cell>78.7</cell><cell>82.2</cell><cell>69.0</cell><cell></cell><cell>82.6</cell><cell cols="2">92.2</cell><cell>84.6</cell><cell>80.2</cell><cell>63.7</cell></row><row><cell cols="2">MAT+MPSR 83.5</cell><cell></cell><cell>83.9</cell><cell>79.3</cell><cell>81.8</cell><cell>71.2</cell><cell></cell><cell>82.6</cell><cell cols="2">92.7</cell><cell>86.3</cell><cell>79.9</cell><cell>65.7</cell></row><row><cell>UMH</cell><cell>82.5</cell><cell></cell><cell>82.7</cell><cell>77.5</cell><cell>83.1</cell><cell>69.8</cell><cell></cell><cell>81.1</cell><cell cols="2">91.7</cell><cell>83.6</cell><cell>82.1</cell><cell>64.4</cell></row><row><cell>BA</cell><cell cols="3">88.38 90.77  en-it en-es</cell><cell>en-fr</cell><cell>en-pt</cell><cell>en-de</cell><cell cols="2">de-it</cell><cell cols="2">de-es</cell><cell>de-fr</cell><cell>de-pt</cell><cell>de-en Average</cell></row><row><cell>GW</cell><cell>80.84</cell><cell cols="2">82.35</cell><cell>81.67</cell><cell>83.03</cell><cell>71.73</cell><cell cols="2">75.41</cell><cell cols="2">72.18</cell><cell>77.14</cell><cell>74.38</cell><cell>72.85</cell><cell>82.84</cell></row><row><cell>GW o</cell><cell>78.9</cell><cell></cell><cell>81.7</cell><cell>81.3</cell><cell>-</cell><cell>71.9</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>72.8</cell><cell>78.04</cell></row><row><cell>PA</cell><cell>77.3</cell><cell></cell><cell>81.4</cell><cell>81.1</cell><cell>79.9</cell><cell>73.5</cell><cell></cell><cell>69.5</cell><cell cols="2">67.7</cell><cell>73.3</cell><cell>59.1</cell><cell>72.4</cell><cell>77.98</cell></row><row><cell cols="2">MAT+MPPA 78.5</cell><cell></cell><cell>82.2</cell><cell>82.7</cell><cell>81.3</cell><cell>74.5</cell><cell></cell><cell>70.1</cell><cell cols="2">68.0</cell><cell>75.2</cell><cell>61.1</cell><cell>72.9</cell><cell>78.47</cell></row><row><cell cols="2">MAT+MPSR 78.8</cell><cell></cell><cell>82.5</cell><cell>82.4</cell><cell>81.5</cell><cell>74.8</cell><cell></cell><cell>72.0</cell><cell cols="2">69.6</cell><cell>76.7</cell><cell>63.2</cell><cell>72.9</cell><cell>79.29</cell></row><row><cell>UMH</cell><cell>78.9</cell><cell></cell><cell>82.5</cell><cell>82.7</cell><cell>82.0</cell><cell>75.1</cell><cell></cell><cell>68.7</cell><cell cols="2">67.2</cell><cell>73.5</cell><cell>59.0</cell><cell>75.5</cell><cell>78.46</cell></row><row><cell>BA</cell><cell>81.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*** 84.26* 82.94* 84.65* 74.08* 78.09* 75.93* 78.93* 77.18* 75.85* 84.24</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Pairs of languages in multilingual alignment problem results for English, German, French, Spanish, Italian, and Portuguese. All reported results are precision@1 percentage. The method achieving the highest precision for each bilingual pair is highlighted in bold. Methods we are comparing to in the table are: Procrustes Matching with CSLS metric to infer translation pairs (PA) [Lample et al.</figDesc><table><row><cell>, 2018];</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Mean average precision (MAP) accuracies of several current methods on XLING dataset. [Zhang et al., 2017a] Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon Induction. In EMNLP, 2017. [Zhang et al., 2017b] Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Adversarial Training for Unsupervised Bilingual Lexicon Induction. In ACL, 2017.</figDesc><table><row><cell>7 Appendix</cell></row><row><cell>7.1 Barycenter Convergence</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Preliminary results appeared in first author thesis<ref type="bibr" target="#b5">[Lian, Xin, 2020]</ref> 2 Alaux et al.[2019] also introduced weights α ik &gt; 0 to encode the relative importance of the language pair (i, k).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their critical comments and we are grateful for funding support from NSERC and Mitacs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlier</forename><surname>Agueh</surname></persName>
		</author>
		<idno type="DOI">https:/epubs.siam.org/doi/abs/10.1137/100805741</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="904" to="924" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaux</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<publisher>Alvarez-Melis and Jaakkola</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised Multilingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie ; Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Claici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Wasserstein Barycenters. In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Bilingual Lexicon Induction via Latent Variable Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dinu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compiling bilingual lexicon entries from a non-parallel english-chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfrid</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Gangbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swikech</surname></persName>
		</author>
		<idno type="DOI">10.1002/(SICI)1097-0312(199801)51:1&lt;23::AID-CPA2&gt;3.0.CO;2-H</idno>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="23" to="45" />
		</imprint>
	</monogr>
	<note>Optimal maps for the multidimensional Monge-Kantorovich problem</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language-tree divergence times support the anatolian theory of indo-european origin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Glavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconception</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">426</biblScope>
			<biblScope unit="page">435</biblScope>
		</imprint>
		<respStmt>
			<orgName>Gray and Atkinson</orgName>
		</respStmt>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Schönemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02289451</idno>
	</analytic>
	<monogr>
		<title level="m">Identifying Word Translations in Non-parallel Texts</title>
		<editor>Taitelbaum et al., 2019a] Hagai Taitelbaum, Gal Chechik, and Jacob Goldberger</editor>
		<imprint>
			<date type="published" when="1966-03" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
