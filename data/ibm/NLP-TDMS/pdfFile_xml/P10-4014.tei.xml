<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for Free Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2010-07-13">13 July 2010. 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
							<email>zhongzhi@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>13 Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>13 Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for Free Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the ACL 2010 System Demonstrations</title>
						<meeting>the ACL 2010 System Demonstrations <address><addrLine>Uppsala, Sweden; c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="78" to="83"/>
							<date type="published" when="2010-07-13">13 July 2010. 2010</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word sense disambiguation (WSD) systems based on supervised learning achieved the best performance in SensE-val and SemEval workshops. However, there are few publicly available open source WSD systems. This limits the use of WSD in other applications, especially for researchers whose research interests are not in WSD. In this paper, we present IMS, a supervised English all-words WSD system. The flexible framework of IMS allows users to integrate different preprocessing tools, additional features, and different classifiers. By default, we use linear support vector machines as the classifier with multiple knowledge-based features. In our implementation , IMS achieves state-of-the-art results on several SensEval and SemEval tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation ( <ref type="bibr" target="#b2">Chan et al., 2007a;</ref><ref type="bibr" target="#b0">Carpuat and Wu, 2007)</ref> and information retrieval ( <ref type="bibr" target="#b18">Stokoe et al., 2003)</ref>.</p><p>In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach ( <ref type="bibr" target="#b15">Palmer et al., 2001;</ref><ref type="bibr">Sny- der and Palmer, 2004;</ref><ref type="bibr" target="#b16">Pradhan et al., 2007</ref>). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines.</p><p>However, developing such a system requires much effort. As a result, very few open source WSD systems are publicly available -the only other publicly available WSD system that we are aware of is SenseLearner ( <ref type="bibr" target="#b8">Mihalcea and Csomai, 2005</ref>). Therefore, for applications which employ WSD as a component, researchers can only make use of some baselines or unsupervised methods. An open source supervised WSD system will promote the use of WSD in other applications.</p><p>In this paper, we present an English all-words WSD system, IMS (It Makes Sense), built using a supervised learning approach. IMS is a Java implementation, which provides an extensible and flexible platform for researchers interested in using a WSD component. Users can choose different tools to perform preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following <ref type="bibr" target="#b6">Lee and Ng (2002)</ref>, we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, SEMCOR <ref type="bibr" target="#b11">(Miller et al., 1994)</ref>, and the DSO corpus ( <ref type="bibr" target="#b13">Ng and Lee, 1996)</ref>.</p><p>A previous implementation of the IMS system, NUS-PT ( <ref type="bibr" target="#b3">Chan et al., 2007b</ref>), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks.</p><p>The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation results of IMS on SensE-val/SemEval English tasks. Finally, we conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>In this section, we first outline the IMS system, and introduce the default preprocessing tools, the feature types, and the machine learning method used in our implementation. Then we briefly explain the collection of training data for content words. <ref type="figure" target="#fig_1">Figure 1</ref> shows the system architecture of IMS. The system accepts any input text. For each content word w (noun, verb, adjective, or adverb) in the input text, IMS disambiguates the sense of w and outputs a list of the senses of w, where each sense s i is assigned a probability according to the likelihood of s i appearing in that context. The sense inventory used is based on WordNet <ref type="bibr" target="#b12">(Miller, 1990</ref>) version 1.7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Architecture</head><p>IMS consists of three independent modules: preprocessing, feature and instance extraction, and classification. Knowledge sources are generated from input texts in the preprocessing step. With these knowledge sources, instances together with their features are extracted in the instance and feature extraction step. Then we train one classification model for each word type. The model will be used to classify test instances of the corresponding word type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Preprocessing</head><p>Preprocessing is the step to convert input texts into formatted information. Users can integrate different tools in this step. These tools are applied on the input texts to extract knowledge sources such as sentence boundaries, part-of-speech tags, etc. The extracted knowledge sources are stored for use in the later steps.</p><p>In IMS, preprocessing is carried out in four steps:</p><p>• Detect the sentence boundaries in a raw input text with a sentence splitter.</p><p>• Tokenize the split sentences with a tokenizer.</p><p>• Assign POS tags to all tokens with a POS tagger.</p><p>• Find the lemma form of each token with a lemmatizer.</p><p>By default, the sentence splitter and POS tagger in the OpenNLP toolkit 1 are used for sentence splitting and POS tagging. A Java version of Penn TreeBank tokenizer 2 is applied in tokenization. JWNL 3 , a Java API for accessing the WordNet <ref type="bibr" target="#b12">(Miller, 1990)</ref> thesaurus, is used to find the lemma form of each token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Feature and Instance Extraction</head><p>After gathering the formatted information in the preprocessing step, we use an instance extractor together with a list of feature extractors to extract the instances and their associated features.</p><p>Previous research has found that combining multiple knowledge sources achieves high WSD accuracy ( <ref type="bibr" target="#b13">Ng and Lee, 1996;</ref><ref type="bibr" target="#b6">Lee and Ng, 2002;</ref><ref type="bibr" target="#b4">Decadt et al., 2004</ref>). In IMS, we follow <ref type="bibr" target="#b6">Lee and Ng (2002)</ref> and combine three knowledge sources for all content word types 4 :</p><p>• POS Tags of Surrounding Words We use the POS tags of three words to the left and three words to the right of the target ambiguous word, and the target word itself. The POS tag feature cannot cross sentence boundary, which means all the associated surrounding words should be in the same sentence as the target word. If a word crosses sentence boundary, the corresponding POS tag value will be assigned as null. • Surrounding Words Surrounding words features include all the individual words in the surrounding context of an ambiguous word w. The surrounding words can be in the current sentence or immediately adjacent sentences.</p><p>However, we remove the words that are in a list of stop words. Words that contain no alphabetic characters, such as punctuation  For example, suppose there is a set of surrounding words features {account, economy, rate, take} in the training data set of the word interest. For a test instance of interest in the sentence "My brother has always taken a keen interest in my work .", the surrounding word feature vector will be &lt;0, 0, 0, 1&gt;.</p><p>• Local Collocations We use 11 local collocations features including:</p><formula xml:id="formula_0">C −2,−2 , C −1,−1 , C 1,1 , C 2,2 , C −2,−1 , C −1,1 , C 1,2 , C −3,−1 , C −2,1 , C −1,2</formula><p>, and C 1,3 , where C i,j refers to an ordered sequence of words in the same sentence of w. Offsets i and j denote the starting and ending positions of the sequence relative to w, where a negative (positive) offset refers to a word to the left (right) of w.</p><p>For example, suppose in the training data set, the word interest has a set of local collocations {"account .", "of all", "in my", "to be"} for C 1,2 . For a test instance of interest in the sentence "My brother has always taken a keen interest in my work .", the value of feature C 1,2 will be "in my".</p><p>As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, we implement one feature extractor for each feature type. The IMS software package is organized in such a way that users can easily specify their own feature set by implementing more feature extractors to exploit new features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Classification</head><p>In IMS, the classifier trains a model for each word type which has training data during the training process. The instances collected in the previous step are converted to the format expected by the machine learning toolkit in use. Thus, the classification step is separate from the feature extraction step. We use LIBLINEAR <ref type="bibr">5 (Fan et al., 2008)</ref> as the default classifier of IMS, with a linear kernel and all the parameters set to their default values. Accordingly, we implement an interface to convert the instances into the LIBLINEAR feature vector format.</p><p>The utilization of other machine learning software can be achieved by implementing the corresponding module interfaces to them. For instance, IMS provides module interfaces to the WEKA machine learning toolkit <ref type="bibr" target="#b19">(Witten and Frank, 2005</ref>), LIBSVM 6 , and MaxEnt 7 .</p><p>The trained classification models will be applied to the test instances of the corresponding word types in the testing process. If a test instance word type is not seen during training, we will output its predefined default sense, i.e., the WordNet first sense, as the answer. Furthermore, if a word type has neither training data nor predefined default sense, we will output "U", which stands for the missing sense, as the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Training Data Set for All-Words Tasks</head><p>Once we have a supervised WSD system, for the users who only need WSD as a component in their applications, it is also important to provide them the classification models. The performance of a supervised WSD system greatly depends on the size of the sense-annotated training data used.</p><p>To overcome the lack of sense-annotated training examples, besides the training instances from the widely used sense-annotated corpus SEMCOR <ref type="bibr" target="#b11">(Miller et al., 1994)</ref> and DSO corpus <ref type="bibr" target="#b13">(Ng and Lee, 1996)</ref>, we also follow the approach described in <ref type="bibr" target="#b1">Chan and Ng (2005)</ref> to extract more training examples from parallel texts. The process of extracting training examples from parallel texts is as follows:</p><p>• Collect a set of sentence-aligned parallel texts. • Perform tokenization on the English texts with the Penn TreeBank tokenizer.</p><p>• Perform Chinese word segmentation on the Chinese texts with the Chinese word segmentation method proposed by <ref type="bibr" target="#b7">Low et al. (2005)</ref>.</p><p>• Perform word alignment on the parallel texts using the GIZA++ software <ref type="bibr" target="#b14">(Och and Ney, 2000</ref>).</p><p>• Assign Chinese translations to each sense of an English word w.</p><p>• Pick the occurrences of w which are aligned to its chosen Chinese translations in the word alignment output of GIZA++.</p><p>• Identify the senses of the selected occurrences of w by referring to their aligned Chinese translations.</p><p>Finally, the English side of these selected occurrences together with their assigned senses are used as training data. We only extract training examples from parallel texts for the top 60% most frequently occurring polysemous content words in Brown Corpus (BC), which includes 730 nouns, 190 verbs, and 326 adjectives. For each of the top 60% nouns and adjectives, we gather a maximum of 1,000 training examples from parallel texts. For each of the top 60% verbs, we extract not more than 500 examples from parallel texts, as well as up to 500 examples from the DSO corpus. We also make use of the sense-annotated examples from SEMCOR as part of our training data for all nouns, verbs, adjectives, and 28 most frequently occurring adverbs in BC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS</head><p>noun verb adj adv # of types 11,445 4,705 5,129 28 <ref type="table">Table 1</ref>: Statistics of the word types which have training data for WordNet 1.7.1 sense inventory</p><p>The frequencies of word types which we have training instances for WordNet sense inventory version 1.7.1 are listed in <ref type="table">Table 1</ref>. We generated classification models with the IMS system for over 21,000 word types which we have training data. On average, each word type has 38 training instances. The total size of the models is about 200 megabytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In our experiments, we evaluate our IMS system on SensEval and SemEval tasks, the benchmark data sets for WSD. The evaluation on both lexicalsample and all-words tasks measures the accuracy of our IMS system as well as the quality of the training data we have collected.  In SensEval English lexical-sample tasks, both the training and test data sets are provided. A common baseline for lexical-sample task is to select the most frequent sense (MFS) in the training data as the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">English Lexical-Sample Tasks</head><p>We evaluate IMS on the SensEval-2 and SensEval-3 English lexical-sample tasks. <ref type="table" target="#tab_3">Table 2</ref> compares the performance of our system to the top two systems that participated in the above tasks ( <ref type="bibr" target="#b20">Yarowsky et al., 2001;</ref><ref type="bibr" target="#b9">Mihalcea and Moldovan, 2001;</ref><ref type="bibr" target="#b10">Mihalcea et al., 2004</ref>). Evaluation results show that IMS achieves significantly better accuracies than the MFS baseline. Comparing to the top participating systems, IMS achieves comparable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">English All-Words Tasks</head><p>In SensEval and SemEval English all-words tasks, no training data are provided. Therefore, the MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in SEMCOR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in all-words tasks.</p><p>Using the training data collected with the method described in Section 2.2, we apply our system on the SensEval-2, SensEval-3, and SemEval-2007 English all-words tasks. Similarly, we also compare the performance of our system to the top two systems that participated in the above tasks <ref type="bibr" target="#b15">(Palmer et al., 2001;</ref><ref type="bibr" target="#b17">Snyder and Palmer, 2004;</ref><ref type="bibr" target="#b16">Pradhan et al., 2007</ref>). The evaluation results are shown in <ref type="table" target="#tab_4">Table 3</ref>. IMS easily beats the WNs1 baseline. It ranks first in SensEval-3 English finegrained all-words task and SemEval-2007 English coarse-grained all-words task, and is also competitive in the remaining tasks. It is worth noting that because of the small test data set in SemEval-2007 English fine-grained all-words task, the differences between IMS and the best participating systems are not statistically significant.</p><p>Overall, IMS achieves good WSD accuracies on both all-words and lexical-sample tasks. The performance of IMS shows that it is a state-of-the-art WSD system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents IMS, an English all-words WSD system. The goal of IMS is to provide a flexible platform for supervised WSD, as well as an all-words WSD component with good performance for other applications.</p><p>The framework of IMS allows us to integrate different preprocessing tools to generate knowledge sources. Users can implement various feature types and different machine learning methods or toolkits according to their requirements. By default, the IMS system implements three kinds of feature types and uses a linear kernel SVM as the classifier. Our evaluation on English lexicalsample tasks proves the strength of our system. With this system, we also provide a large number of classification models trained with the senseannotated training examples from SEMCOR, DSO corpus, and 6 parallel corpora, for all content words. Evaluation on English all-words tasks shows that IMS with these models achieves stateof-the-art WSD accuracies compared to the top participating systems.</p><p>As a Java-based system, IMS is platform independent. The source code of IMS and the classification models can be found on the homepage: http://nlp.comp.nus.edu. sg/software and are available for research, non-commercial use.</p><p>SensEval-2 <ref type="table" target="#tab_3">SensEval-3  SemEval-2007  Fine-grained Fine-grained Fine-grained Coarse-grained  IMS</ref> 68.2% 67.6% 58.3%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>82.6% Rank 1 System</head><p>69.0% 65.2% 59.1% 82.5% Rank 2 System 63.6% 64.6% 58.7% 81.6% WNs1 61.9% 62.4% 51.4% 78.9% </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For example, suppose we want to disam- biguate the word interest in a POS-tagged sentence "My/PRP$ brother/NN has/VBZ always/RB taken/VBN a/DT keen/JJ inter- est/NN in/IN my/PRP$ work/NN ./.". The 7 POS tag features for this instance are &lt;VBN, DT, JJ, NN, IN, PRP$, NN&gt;.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: IMS system architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>WSD accuracies on SensEval lexical-
sample tasks 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>WSD accuracies on SensEval/SemEval all-words tasks 

International Workshop on Evaluating Word Sense 
Disambiguation Systems (SensEval-3), pages 108-
112, Barcelona, Spain. </table></figure>

			<note place="foot" n="1"> http://opennlp.sourceforge.net/ 2 http://www.cis.upenn.edu/ ∼ treebank/ tokenizer.sed 3 http://jwordnet.sourceforge.net/ 4 Syntactic relations are omitted for efficiency reason.</note>

			<note place="foot" n="5"> http://www.bwaldvogel.de/ liblinear-java/ 6 http://www.csie.ntu.edu.tw/ ∼ cjlin/ libsvm/ 7 http://maxent.sourceforge.net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is done for CSIDM Project No. CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority (MDA) of Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation using word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling up word sense disambiguation via parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 20th National Conference on Artificial Intelligence (AAAI)<address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1037" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word sense disambiguation improves statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NUS-PT: Exploiting parallel texts for word sense disambiguation in the English all-words tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval-2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GAMBL, genetic algorithm optimization of memory-based WSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Decadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronique</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third</title>
		<meeting>the Third</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoong Keok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Kiat</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="161" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SenseLearner: Word sense disambiguation for all words in unrestricted text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL) Interactive Poster and Demonstration Sessions</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL) Interactive Poster and Demonstration Sessions<address><addrLine>Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="53" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pattern learning and active feature selection for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2)</title>
		<meeting>the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="127" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The SensEval-3 English lexical sample task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3)</title>
		<meeting>the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using a semantic concordance for sense identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shari</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ARPA Human Language Technology Workshop</title>
		<meeting>ARPA Human Language Technology Workshop<address><addrLine>Morristown, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="240" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordnet: An on-line lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="312" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hian Beng</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 34th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Santa Cruz, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 38th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">English tasks: All-words and verb lexical sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Delfs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2)</title>
		<meeting>the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SemEval-2007 task-17: English lexical sample, SRL and all words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval-2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The English all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3)</title>
		<meeting>the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word sense disambiguation in information retrieval revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Stokoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Oakes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentySixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the TwentySixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siviu</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Schafer</surname></persName>
		</author>
		<title level="m">The Johns Hopkins</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SensEval-2 system description</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2)</title>
		<meeting>the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="163" to="166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
