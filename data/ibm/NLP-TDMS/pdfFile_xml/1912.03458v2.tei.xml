<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Convolution: Attention over Convolution Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
							<email>xidai@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
							<email>dochen@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<email>luyuan@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename></persName>
						</author>
						<title level="a" type="main">Dynamic Convolution: Attention over Convolution Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Light-weight convolutional neural networks (CNNs) suffer performance degradation as their low computational budgets constrain both the depth (number of convolution layers) and the width (number of channels) of CNNs, resulting in limited representation capability. To address this issue, we present Dynamic Convolution, a new design that increases model complexity without increasing the network depth or width. Instead of using a single convolution kernel per layer, dynamic convolution aggregates multiple parallel convolution kernels dynamically based upon their attentions, which are input dependent. Assembling multiple kernels is not only computationally efficient due to the small kernel size, but also has more representation power since these kernels are aggregated in a non-linear way via attention. By simply using dynamic convolution for the state-ofthe-art architecture MobileNetV3-Small, the top-1 accuracy of ImageNet classification is boosted by 2.9% with only 4% additional FLOPs and 2.9 AP gain is achieved on COCO keypoint detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Interest in building light-weight and efficient neural networks has exploded recently. It not only enables new experiences on mobile devices, but also protects user's privacy from sending personal information to the cloud. Recent works (e.g. MobileNet <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11]</ref> and ShuffleNet <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b22">23]</ref>) have shown that both efficient operator design (e.g. depthwise convolution, channel shuffle, squeeze-and-excitation <ref type="bibr" target="#b12">[13]</ref>, asymmetric convolution <ref type="bibr" target="#b4">[5]</ref>) and architecture search ( <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2]</ref>) are important for designing efficient convolutional neural networks.</p><p>However, even the state-of-the-art efficient CNNs (e.g. MobileNetV3 <ref type="bibr" target="#b10">[11]</ref>) suffer significant performance degradation when the computational constraint becomes extremely low. For instance, when the computational cost of Mo-bileNetV3 reduces from 219M to 66M Multi-Adds, the top-1 accuracy of ImageNet classification drops from 75.2% to 67.4%. This is because the extremely low computational  cost severely constrains both the network depth (number of layers) and width (number of channels), which are crucial for the network performance but proportional to the computational cost. This paper proposes a new operator design, named dynamic convolution, to increase the representation capability with negligible extra FLOPs. Dynamic convolution uses a set of K parallel convolution kernels {W k ,b k } instead of using a single convolution kernel per layer (see <ref type="figure" target="#fig_1">Figure  2</ref>). These convolution kernels are aggregated dynamicallỹ W = k π k (x)W k for each individual input x (e.g. im-age) via input dependent attention π k (x). The biases are aggregated using the same attentionb = k π k (x)b k . Dynamic convolution is a non-linear function with more representation power than its static counterpart. Meanwhile, dynamic convolution is computationally efficient. It does not increase the depth or width of the network, as the parallel convolution kernels share the output channels by aggregation. It only introduces extra computational cost to compute attentions {π k (x)} and aggregate kernels , which is negligible compared to convolution. The key insight is that within reasonable cost of model size (as convolution kernels are small), dynamic kernel aggregation provides an efficient way (low extra FLOPs) to boost representation capability.</p><p>Dynamic convolutional neural networks (denoted as DY-CNNs) are more difficult to train, as they require joint optimization of all convolution kernels and the attention across multiple layers. We found two keys for efficient joint optimization: (a) constraining the attention output as k π k (x) = 1 to facilitate the learning of attention model π k (x), and (b) flattening attention (near-uniform) in early training epochs to facilitate the learning of convolution kernels {W k ,b k }. We simply integrate these two keys by using softmax with a large temperature for kernel attention.</p><p>We demonstrate the effectiveness of dynamic convolution on both image classification (ImageNet) and keypoint detection (COCO). Without bells and whistles, simply replacing static convolution with dynamic convolution in Mo-bileNet V2 and V3 achieves solid improvement with only a slight increase (4%) of computational cost (see <ref type="figure" target="#fig_0">Figure 1</ref>). For instance, with 100M Multi-Adds budget, our method gains 4.5% and 2.9% top-1 accuracy on image classification for MobileNetV2 and MobileNetV3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Efficient CNNs: Recently, designing efficient CNN architectures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b22">23]</ref> has been an active research area. SqueezeNet <ref type="bibr" target="#b14">[15]</ref> reduces the number of parameters by using 1 × 1 convolution extensively in the fire module. Mo-bileNetV1 <ref type="bibr" target="#b11">[12]</ref> substantially reduces FLOPs by decomposing a 3 × 3 convolution into a depthwise convolution and a pointwise convolution. Based upon this, MobileNetV2 <ref type="bibr" target="#b26">[27]</ref> introduces inverted residuals and linear bottlenecks. Mo-bileNetV3 <ref type="bibr" target="#b10">[11]</ref> applies squeeze-and-excitation <ref type="bibr" target="#b12">[13]</ref> in the residual layer and employs a platform-aware neural architecture approach <ref type="bibr" target="#b28">[29]</ref> to find the optimal network structures. ShuffleNet further reduces MAdds for 1 × 1 convolution by channel shuffle operations. ShiftNet <ref type="bibr" target="#b32">[33]</ref> replaces expensive spatial convolution by the shift operation and pointwise convolutions. Compared with existing methods, our dynamic convolution can be used to replace any static convolution kernels (e.g. 1 × 1, 3 × 3, depthwise convolution, group convolution) and is complementary to other advanced operators like squeeze-and-excitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Compression and Quantization:</head><p>Model compression <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10]</ref> and quantization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref> approaches are also important for learning efficient neural networks. They are complementary to our work, helping reduce the model size for our dynamic convolution method. Dynamic Deep Neural Networks: Our method is related to recent works of dynamic neural networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14]</ref> that focus on skipping part of an existing model based on input image. D 2 NN <ref type="bibr" target="#b20">[21]</ref>, SkipNet <ref type="bibr" target="#b30">[31]</ref> and Block-Drop <ref type="bibr" target="#b33">[34]</ref> learn an additional controller for skipping decision by using reinforcement learning. MSDNet <ref type="bibr" target="#b13">[14]</ref> allows early-exit based on the current prediction confidence. Slimmable Nets <ref type="bibr" target="#b38">[39]</ref> learns a single neural network executable at different width. Once-for-all <ref type="bibr" target="#b0">[1]</ref> proposes a progressive shrinking algorithm to train one network that supports multiple sub-networks. The accuracy for these subnetworks is the same as independently trained networks. Compared with these works, our method has two major differences. Firstly, our method has dynamic convolution kernels but static network structure, while existing works have static convolution kernels but dynamic network structure. Secondly, our method does not require an additional controller. The attention is embedded in each layer, enabling end-to-end training. Compared to the concurrent work <ref type="bibr" target="#b36">[37]</ref>, our method is more efficient with better performance. Neural Architecture Search: Recent research works in neural architecture search (NAS) are powerful on finding high-accuracy neural network architectures <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref> as well as hardware-aware efficient network architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. The hardware-aware NAS methods incorporate hardware latency into the architecture search process, by making it differentiable. <ref type="bibr" target="#b6">[7]</ref> proposed single path supernet to optimize all architectures in the search space simultaneously, and then perform evolutionary architecture search to handle computational constraints. Based upon NAS, MobileNetV3 <ref type="bibr" target="#b10">[11]</ref> shows significant improvements over human-designed baselines (e.g. MobileNetV2 <ref type="bibr" target="#b26">[27]</ref>). Our dynamic convolution method can be easily used in advanced architectures found by NAS. Later in this paper, we will show that dynamic convolution not only improves the performance for human-designed networks (e.g. Mobiel-NetV2), but also boosts the performance for automatically searched architectures (e.g. MobileNetV3), with low extra FLOPs. In addition, our method provides a new and effective component to enrich the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dynamic Convolutional Neural Networks</head><p>We describe dynamic convolutional neural networks (DY-CNNs) in this section. The goal is to provide better trade-off between network performance and computational burden, within the scope of efficient neural networks. The two most popular strategies to boost the performance are making neural networks "deeper" or "wider". How-ever, they both incur heavy computation cost, thus are not friendly to efficient neural networks.</p><p>We propose dynamic convolution, which does not increase either the depth or the width of the network, but increase the model capability by aggregating multiple convolution kernels via attention. Note that these kernels are assembled differently for different input images, from where dynamic convolution gets its name. In this section, We firstly define the generic dynamic perceptron, and then apply it to convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary: Dynamic Perceptron</head><p>Definition: Let us denote the traditional or static perceptron as y = g(W T x + b), where W and b are weight matrix and bias vector, and g is an activation function (e.g. ReLU <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref>). We define the dynamic perceptron by aggregating multiple (K) linear functions {W T k x +b k } as follows:</p><formula xml:id="formula_0">y = g(W T (x)x +b(x)) W (x) = K k=1 π k (x)W k ,b(x) = K k=1 π k (x)b k s.t. 0 ≤ π k (x) ≤ 1, K k=1 π k (x) = 1,<label>(1)</label></formula><p>where π k is the attention weight for the k th linear functioñ W T k x+b k . Note that the aggregated weightW (x) and bias b(x) are functions of input and share the same attention. Attention: the attention weights {π k (x)} are not fixed, but vary for each input x. They represent the optimal aggregation of linear models {W T k x +b k } for a given input. The aggregated modelW T (x)x+b(x) is a non-linear function. Thus, dynamic perceptron has more representation power than its static counterpart. Computational Constraint: compared with static perceptron, dynamic perceptron has the same number of output channels but bigger model size. It also introduces two additional computations: (a) computing the attention weights {π k (x)}, and (b) aggregating parameters based upon attention k π kWk and k π kbk . The additional computational cost should be significantly less than the cost of computingW T x +b. Mathematically, the computational constraint can be represented as follows:</p><formula xml:id="formula_1">O(W T x +b) O π kWk +O π kbk +O (π(x))<label>(2)</label></formula><p>where O(·) measures the computational cost (e.g. FLOPs). Note that fully connected layer does not satisfy this, while convolution is a proper fit for this constraint. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dynamic Convolution</head><p>In this subsection, we showcase a specific dynamic perceptron, dynamic convolution that satisfies the computational constraint (Eq. 2). Similar to dynamic perceptron, dynamic convolution ( <ref type="figure" target="#fig_2">Figure 3</ref>) has K convolution kernels that share the same kernel size and input/output dimensions. They are aggregated by using the attention weights {π k }. Following the classic design in CNN, we use batch normalization and an activation function (e.g. ReLU) after the aggregated convolution to build a dynamic convolution layer. Attention: we apply squeeze-and-excitation <ref type="bibr" target="#b12">[13]</ref> to compute kernel attentions {π k (x)} (see <ref type="figure" target="#fig_2">Figure 3</ref>). The global spatial information is firstly squeezed by global average pooling. Then we use two fully connected layers (with a ReLU between them) and softmax to generate normalized attention weights for K convolution kernels. The first fully connected layer reduces the dimension by 4. Different from SENet <ref type="bibr" target="#b12">[13]</ref> which computes attentions over output channels, we compute attentions over convolution kernels. The computation cost for the attention is cheap. For an input feature map with dimension H × W × C in , the attention requires O(π(x)) = HW C in +C 2 in /4+C in K/4 Mult-Adds. This is much less than the computational cost of convolu-</p><formula xml:id="formula_2">tion, i.e. O(W T x +b) = HW C in C out D 2</formula><p>k Mult-Adds, where D k is the kernel size, and C out is the number of output channels. Kernel Aggregation: aggregating convolution kernels is computationally efficient due to the small kernel size. Aggregating K convolution kernels with kernel size D k × D k , C in input channels and C out output channels introduces</p><formula xml:id="formula_3">KC in C out D 2 k + KC out extra Multi-Adds. Compared with the computational cost of convolution (HW C in C out D 2 k )</formula><p>, the extra cost is neligible if K HW . <ref type="table" target="#tab_0">Table 1</ref> shows the computational cost of using dynamic convolution in Mo-bileNetV2. For instance, when using MobileNetV2 (×1.0), dynamic convolution with K = 4 kernels only increases the computation cost by 4%. Note that even though dynamic convolution increases the model size, it does not increase the output dimension of each layer. The amount of the increase is acceptable as convolution kernels are small. From CNNs to DY-CNNs: dynamic convolution can be easily used as a drop-in replacement for any convolution (e.g. 1 × 1 conv, 3 × 3 conv, group convolution, depthwise convolution) in any CNN architecture. It is also complementary to other operators (like squeeze-and-excitation <ref type="bibr" target="#b12">[13]</ref>) and activation functions (e.g. ReLU6, h-swish <ref type="bibr" target="#b10">[11]</ref>).</p><p>In the rest of the paper, we use prefix DYfor the networks that use dynamic convolution. For example, DY-MobileNetV2 refers to using dynamic convolution in Mo-bileNetV2. We also use weightW k to denote a convolution kernel and ignore biasb k , for the sake of brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Two Insights of Training Deep DY-CNNs</head><p>Training deep DY-CNNs is challenging, as it requires joint optimization of all convolution kernels {W k } and attention model π k (x) across multiple layers. In this section, we discuss two insights for more efficient joint optimization, which are crucial especially to deep DY-CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Insight 1: Sum the Attention to One</head><p>The first insight is: constraining the attention output can facilitate the learning of attention model π k (x). Specifically, we have the constraint k π k (x) = 1 in Eq. (1) to keep the aggregated kernelW = k π kWk within the convex hull of {W k } in the kernel space. <ref type="figure" target="#fig_3">Figure 4</ref> shows an example with 3 convolution kernels. The constraint 0 ≤ π k (x) ≤ 1 only keeps the aggregated kernel within the two pyramids. The sum-to-one constraint further compresses the kernel space to a triangle. It compresses the red line that comes from the origin into a dot by normalizing the attention sum. This normalization significantly simplifies the learning of π k (x), when it is jointly optimized with {W k } in a deep network. Softmax is a natural choice of k π k (x) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Insight 2: Near-uniform Attention in Early Training Epochs</head><p>The second insight is: near-uniform attention can facilitate the learning of all kernels {W k } during early training epochs. This is because near-uniform attention enables more convolution kernels to be optimized simultaneously.</p><p>Softmax does NOT work well on this due to its near one-hot output. It only allows a small subset of kernels across layers to be optimized. <ref type="figure" target="#fig_4">Figure 5</ref>-(Left) shows that the training converges slowly when using softmax (blue curves) to compute attention. Here, DY-MobileNetV2 with width multiplier ×0.5 is used. The final top-1 accuracy (64.8%) is even worse than its static counterpart (65.4%). This inefficiency is related to the number of dynamic convolution layers. To validate this, we reduce the number of dynamic convolution layers by 3 (only use dynamic convolution for the last 1×1 convolution in each bottleneck residual block) and expect faster convergence in training. The training and validation errors are shown in <ref type="figure" target="#fig_4">Figure 5</ref>-(Right) (blue curves). As we expected, the training converges faster with higher top-1 accuracy (65.9%) at the end.</p><p>We address this inefficiency in training deeper DY-CNNs by using a large temperature in softmax to flatten attention as follows:</p><formula xml:id="formula_4">π k = exp(z k /τ ) j exp(z j /τ ) ,<label>(3)</label></formula><p>where z k is the output of the second FC layer in attention branch (see <ref type="figure" target="#fig_2">Figure 3</ref>), and τ is the temperature. The original softmax is a special case (τ = 1). As τ increases, the output is less sparse. When using a large temperature τ = 30, the training becomes significantly more efficient (see the red curves in <ref type="figure" target="#fig_4">Figure 5</ref>-(Left)). As a result, the top-1 accuracy boosts to 69.4%. The larger temperature is also helpful when stacking fewer dynamic convolution layers (see red curves in <ref type="figure" target="#fig_4">Figure 5</ref>-(Right)). Temperature annealing, i.e. reducing τ from 30 to 1 linearly in the first 10 epochs, can further improve the top-1 accuracy (from 69.4% to 69.9%). These results support that near-uniform attention in early training epochs is crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relation to Concurrent Work</head><p>These two insights are the key differences between our method and the concurrent work (CondConv <ref type="bibr" target="#b36">[37]</ref>), which uses sigmoid to compute kernel attention. Even though sigmoid provides near-uniform attention in early training   epochs, it has significantly larger kernel space (two pyramids in <ref type="figure" target="#fig_3">Figure 4</ref>) than our method (the shaded triangle in <ref type="figure" target="#fig_3">Figure 4</ref>). Thus, learning attention model π k (x) becomes more difficult. As a result, our method has less kernels per layer, smaller model size, less computations but achieves higher accuracy (see <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments: ImageNet Classification</head><p>In this section, we present experimental results of dynamic convolution along with comprehensive ablations on ImageNet <ref type="bibr" target="#b3">[4]</ref> classification. ImageNet has 1000 classes, including 1,281,167 images for training and 50,000 images for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We evaluate dynamic convolution on three architectures (ResNet <ref type="bibr" target="#b8">[9]</ref>, MobileNetV2 <ref type="bibr" target="#b26">[27]</ref>, and MobileNetV3 <ref type="bibr" target="#b10">[11]</ref>), by using dynamic convolution for all convolution layers except the first layer. Each layer has K = 4 convolution kernels. The batch size is 256. We use different training setups for the three architectures as follows:</p><p>Training setup for DY-ResNet: The initial learning rate is 0.1 and drops by 10 at epoch 30, 60 and 90. The weight decay is 1e-4. All models are trained using SGD optimizer with 0.9 momentum for 100 epochs. We use dropout rate 0.1 before the last layer of DY-ResNet-18. Training setup for DY-MobileNetV2: The initial learning rate is 0.05 and is scheduled to arrive at zero within a sin-  gle cosine cycle. The weight decay is 4e-5. All models are trained using SGD optimizer with 0.9 momentum for 300 epochs. To prevent overfitting, we use label smoothing and dropout before the last layer for larger width multipliers (×1.0 and ×0.75). The dropout rate are 0.2 and 0.1 for ×1.0 and ×0.75, respectively. Mixup <ref type="bibr" target="#b40">[41]</ref> is used for ×1.0.</p><p>Training setup for DY-MobileNetV3: The initial learning rate is 0.1 and is scheduled to arrive at zero within a single cosine cycle. The weight decay is 3e-5. We use SGD optimizer with 0.9 momentum for 300 epochs and dropout rate of 0.2 before the last layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Inspecting DY-CNN</head><p>We inspect if DY-CNN is dynamic, using DY-MobileNetV2 ×0.5, which has K = 4 kernels per layer and is trained by using τ = 30. Two properties are expected if it is dynamic: (a) the convolution kernels are diverse per layer, and (b) the attention is input dependent. We examine these two properties by contradiction. Firstly, if the convolution kernels are not diverse, the performances will be stable if different attentions are used. Thus, we vary the kernel aggregation per layer in three different ways: averaging W k /K, choosing the convolution kernel with the maximum attentionW argmax k (π k ) , and random shuffling attention over kernels per image π j (x)W k , j = k. Compared with using the original attention, the performances of these variations are significantly degraded (shown in <ref type="table" target="#tab_4">Table  3</ref>). When choosing the convolution kernel with the maximum attention, the top-1 accuracy (0.1) is as low as randomly choosing a class. The significant instability confirms the diversity of convolution kernels. In addition, we shuffle attentions across images to check if the attention is input dependent. The poor accuracy (27.3%) indicates that it is crucial for each image to use its own attention.</p><p>Furthermore, we inspect the attention across layers and find that attentions are flat at low levels and sparse at high levels. This is helpful to explain why variations in <ref type="table" target="#tab_4">Table 3</ref>   <ref type="figure">Figure 6</ref>. The number of convolution kernels (K) in DY-MobileNetV2 with different depth and width multipliers. Left: depth multiplier is 1.0, Middle: depth multiplier is 0.7, Right: depth multiplier is 0.5. Each curve has four width multipliers ×1.0, ×0.75, ×0.5, and ×0. <ref type="bibr" target="#b34">35</ref>. Dynamic convolution outperforms its static counterpart by a clear margin for all width/depth multipliers. Best viewed in color.  <ref type="table">Table 4</ref>. Inspecting DY-CNN by enabling/disabling attention at different input resolutions. DY-MobileNetV2 ×0.5 is used. Each resolution has two options: indicates enabling attention π k (x)W k for each layer in that resolution, while − indicates disabling attention and using average kernel W k /K for each layer in the corresponding resolutions. The attention is more effective at higher layers with lower resolutions. have poor accuracy. For instance, averaging kernels with sparse attention at high levels or picking one convolution kernel (with the maximum attention) at low levels (where attention is flat) is problematic. <ref type="table">Table 4</ref> shows how attention affects the performance across layers. We group layers by their input resolutions, and switch on/off attention for these groups. If attention is switched off for a resolution, each layer in that resolution aggregates kernels by averaging. When enabling attention at higher levels alone (resolution 14 2 and 7 2 ), the top-1 accuracy is 67.0%, close to the performance (69.4%) of using attention for all layers. If attention is used for lower levels alone (resolution 112 2 , 56 2 and 28 2 ), the top-1 accuracy is poor 42.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>We perform a number of ablations on DY-MobileNetV2 and DY-MobileNetV3. The default setup includes using K = 4 kernels per layer and τ = 30. The number of convolution kernels (K): the hyper-  <ref type="table">Table 5</ref>. Dynamic convolution at different layers in Mo-bileNetV2 ×0.5. C1, C2 and C3 indicate the 1 × 1 convolution that expands output channels, the 3 × 3 depthwise convolution and the 1 × 1 convolution that shrinks output channels per block respectively. C1=1 indicates using static convolution, while C1=4 indicates using dynamic convolution with 4 kernels. The numbers in brackets denote the improvement over the baseline.</p><p>parameter K controls the model complexity. <ref type="figure">Figure 6</ref> shows the classification accuracy and computational cost for dynamic convolution with different K. We compare DY-MobileNetV2 with MobileNetV2 on different depth/width multipliers. Firstly, the dynamic convolution outperforms its static counterpart for all depth/width multipliers, even with small K = 2. This demonstrates the strength of our method. In addition, the accuracy stops increasing once K is larger than 4. This is because as K increases, even though the model has more representation power, it is more difficult to optimize all convolution kernels and attention simultaneously and the network is more prone to over-fitting. Dynamic convolution at different layers: <ref type="table">Table 5</ref> shows the classification accuracy for using dynamic convolution at three different layers (1×1 conv, 3×3 depthwise conv, 1×1 conv) per bottleneck residual block in MobileNetV2 ×0.5.</p><p>The accuracy increases as more dynamic convolution layers are used. Using dynamic convolution for all three layers yields the best accuracy. If only one layer is allowed to use dynamic convolution, using it for the last 1 × 1 convolution yields the best performance.  Softmax Temperature: the temperature τ in softmax controls the sparsity of attention weights. It is important for training DY-CNNs effectively. <ref type="table" target="#tab_7">Table 6</ref> shows the classification accuracy for using different temperatures. τ = 30 has the best performance. Furthermore, temperature annealing (reducing τ from 30 to 1 linearly in the first 10 epochs) provides additional improvement on top-1 accuracy (from 69.4% to 69.9%). Therefore, using large temperature in the early stage of training is important. Dynamic Convolution vs Squeeze-and-Excitation (SE) <ref type="bibr" target="#b12">[13]</ref>: MobileNetV3-Small <ref type="bibr" target="#b10">[11]</ref> is used, in which the locations of SE layers are considered optimal as they are found by network architecture search (NAS). The results are shown in <ref type="table" target="#tab_8">Table 7</ref>. Without using SE, the top-1 accuracy for MobileNetV3-Small drops 2%. However, DY-MobileNetV3-Small without SE outperforms MobileNetV3-Small with SE by 2.2% in top-1 accuracy.</p><p>Combining dynamic convolution and SE gains additional 0.7% improvement. This suggests that attention over kernels and attention over output channels can work together. <ref type="table" target="#tab_10">Table 8</ref> shows the comparison between dynamic convolution and its static counterpart in three CNN architectures (MobileNetV2, MobileNetV3 and ResNet). K = 4 kernels are used in each dynamic convolution layer and temperature annealing is used in the training. Although we focus on efficient CNNs, we evaluate dynamic convolution on two shallow ResNets (ResNet-10 and ResNet-18) to show its   <ref type="table">Table 9</ref>. Light-weight head structures for keypoint detection. We use MobileNetV2's bottleneck residual block <ref type="bibr" target="#b26">[27]</ref> (denoted as bneck). Each row is corresponding to a stage, which starts with a bilinear upsampling operator to scale up the feature map by 2. #out denotes the number of output channels, and n denotes the number of bottleneck residual blocks. effectiveness on 3 × 3 convolution, which is only used for the first layer in MobileNet V2 and V3. Without bells and whistles, dynamic convolution outperforms its static counterpart by a clear margin for all three architectures, with small extra computational cost (∼ 4%). DY-ResNet and DY-MobileNetV2 gains more than 2.3% and 3.2% top-1 accuracy, respectively. DY-MobileNetV3-Small is 2.9% more accurate than the state-of-the-art MobileNetV3-Small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DY-CNNs for Human Pose Estimation</head><p>We use COCO 2017 dataset <ref type="bibr" target="#b18">[19]</ref> to evaluate dynamic convolution on single-person keypoint detection. Our models are trained on train2017, including 57K images and 150K person instances labeled with 17 key-points. We evaluate our method on val2017 containing 5000 images and use the mean average precision (AP) over 10 object key point similarity (OKS) thresholds as the metric. Implementation Details: We implement two types of networks to evaluate dynamic convolution. Type-A follows SimpleBaseline <ref type="bibr" target="#b34">[35]</ref> by using deconvolution in head. We use MobileNetV2 and V3 as a drop-in replacement for the backbone feature extractor and compare static convolution and dynamic convolution in the backbone alone. Type-B still uses MobileNetV2 and V3 as backbone. But it uses upsampling and MobileNetV2's bottleneck residual block in head. We compare dynamic convolution with its static counterpart in both backbone and head. The details of head structure are shown in <ref type="table">Table 9</ref>. For both types, we use K = 4 kernels in each dynamic convolution layer.</p><p>Training setup: We follow the training setup in <ref type="bibr" target="#b27">[28]</ref>. Testing: We follow <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28]</ref> to use two-stage top-down paradigm: detecting person instances using a person detector and then predicting keypoints. We use the same person detectors provided by <ref type="bibr" target="#b34">[35]</ref>. The keypoints are predicted on the average heatmap of the original and flipped images by adjusting the highest heat value location with a quarter offset from the highest response to the second highest response.</p><p>Main Results and Ablations: Firstly we compare dynamic convolution with its static counterpart in the backbone (Type-A). The results are shown in the top half of  <ref type="table" target="#tab_0">Table 11</ref>. Keypoint detection results of using dynamic convolution in backbone and head separately. We use MobileNetV2 ×0.5 as backbone and use the light-weight head structure discussed in <ref type="table">Table 9</ref>. The numbers in brackets denote the performance improvement over the baseline. Dynamic convolution can improve AP at both the backbone and the head.</p><p>We perform an ablation to investigate the effects of dynamic convolution at backbone and head separately <ref type="table" target="#tab_0">(Table  11</ref>). Even though most of improvement comes from the dynamic convolution at the backbone, dynamic convolution at the head is also helpful. This is mainly because the backbone has more convolution layers than the head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we introduce dynamic convolution, which aggregates multiple convolution kernels dynamically based upon their attentions for each input. Compared to its static counterpart (single convolution kernel per layer), it significantly improves the representation capability with negligible extra computation cost, thus is more friendly to efficient CNNs. Our dynamic convolution can be easily integrated into existing CNN architectures. By simply replacing each convolution kernel in MobileNet (V2 and V3) with dynamic convolution, we achieve solid improvement for both image classification and human pose estimation. We hope dynamic convolution becomes a useful component for efficient network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this appendix, we report running time and perform additional analysis for our dynamic convolution method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Inference Running Time</head><p>We report the running time of dynamic MobileNetV2 (DY-MobileNetV2) with four different width multipliers (×1.0, ×0.75, ×0.5, and ×0. <ref type="bibr" target="#b34">35</ref>) and compare with its static counterpart (MobileNetV2 <ref type="bibr" target="#b26">[27]</ref>) in <ref type="table" target="#tab_0">Table 12</ref>. We use a single-threaded core of Intel Xeon CPU E5-2650 v3 (2.30GHz) to measure running time (in milliseconds). The running time is calculated by averaging the inference time of 5,000 images with batch size 1. Both MobileNetV2 and DY-MobileNetV2 are implemented using PyTorch <ref type="bibr" target="#b24">[25]</ref>.</p><p>Compared with its static counterpart, DY-MobileNetV2 consumes about 10% more running time and 4% more Multi-Adds. The overhead of running time is higher than Multi-Adds. We believe this is because the optimizations of global average pooling and small inner-product operations are not as efficient as convolution. With the small additional computational cost, our dynamic convolution method significantly improves the model performance. <ref type="figure" target="#fig_7">Figure 7</ref> shows that the shallower DY-MobileNetV2 (depth ×0.5) has better trade-off between accuracy and computational cost than the deeper MobileNetV2 (depth ×1.0), even though shallower networks (depth ×0.5) have performance degradation for both DY-MobileNetV2 and MobileNetV2. Improvement on shallow networks is useful as they are friendly to parallel computation. Furthermore, dynamic convolution achieves more improvement for thinner and shallower networks with small width/depth multipliers. This is because thinner and shallower networks are  <ref type="bibr" target="#b3">[4]</ref> classification. We use dynamic convolution with K = 4 kernels for all convolution layers in DY-MobileNetV2 except the first layer. CPU: CPU time in milliseconds measured on a single core of Intel Xeon CPU E5-2650 v3 (2.30GHz). The running time is calculated by averaging the inference time of 5,000 images with batch size 1. The numbers in brackets denote the performance improvement over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Dynamic Convolution in Shallower and Thinner Networks</head><p>underfitted due to their limited model size and dynamic convolution significantly improves their capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Example: Learning XOR</head><p>To make the idea of dynamic perceptron more concrete, we use it on a simple task, i.e. learning the XOR function. In this example, we want our network to perform correctly on the four points X = {[0, 0] T , [0, 1] T , [1, 0] T , [1, 1] T }. Compared with the solution using two static perceptron layers <ref type="bibr" target="#b5">[6]</ref> as follows:</p><formula xml:id="formula_5">y = w T max{0, W T x + b} w = 1 −2 , W = 1 1 1 1 b = 0 −1 ,<label>(4)</label></formula><p>dynamic perception only needs a single layer as follows:</p><formula xml:id="formula_6">y = 2 k=1 π k (x)W T k x + π k (x)b k W 1 = −1 0 0 0 ,b 1 = 1 0 ,W 2 = 1 0 0 0 ,b 2 = 0 0 ,<label>(5)</label></formula><p>where the attentions are π 1 (x) = x 2 , π 2 (x) = 1 − x 2 . This example demonstrates that dynamic perceptron has more representation power due to the non-linearity. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The trade-off between computational cost (MAdds) and top-1 accuracy of ImageNet classification. Dynamic convolution significantly boosts the accuracy with a small amount of extra MAdds on MobileNet V2 and V3. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Dynamic perceptron. It aggregates multiple linear functions dynamically based upon their attentions {π k }, which are input dependent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>A dynamic convolution layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of constraint k π k (x) = 1. It compresses the space of aggregated kernel k π kWk from two pyramids (used in CondConv<ref type="bibr" target="#b36">[37]</ref>) to a triangle. A red line is compressed into a dot by normalizing attention sum. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Training and validation errors for using different softmax temperatures. Left: using dynamic convolution for all layers. Right: using dynamic convolution for the last layer in each bottleneck residual block. We use DY-MobileNetV2 with width multiplier ×0.5, and each dynamic convolution layer has K = 4 convolution kernels. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The human detection boxes are cropped from the image and resized to 256×192. The data augmentation includes random rotation ([−45 • , 45 • ]), random scale ([0.65, 1.35]), flipping, and half body data augmentation. All models are trained from scratch for 210 epochs, using Adam optimizer [17]. The initial learning rate is set as 1e-3 and is dropped to 1e-4 and 1e-5 at the 170 th and 200 th epoch, respectively. The temperature of softmax in DY-CNNs is set as τ = 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Shallower DY-MobileNetV2 vs Deeper MobileNetV2. The shallower DY-MobileNetV2 (depth ×0.5) has better trade-off between accuracy and computational cost than the deeper Mo-bileNetV2 (depth ×1.0). To make comparison fair, we also plot the deeper DY-MobileNetV2 and shallower MobileNetV2. For both DY-MobileNetV2 and MobileNetV2, deeper networks have better performance. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Mult-Adds of static convolution and dynamic convolution in MobileNetV2 with four different width multipliers (×1.0, ×0.75, ×0.5, and ×0.35).</figDesc><table><row><cell></cell><cell>×1.0</cell><cell>×0.75</cell><cell>×0.5</cell><cell>×0.35</cell></row><row><cell>static</cell><cell>300.0M</cell><cell>209.0M</cell><cell>97.0M</cell><cell>59.2M</cell></row><row><cell>K=2</cell><cell>309.5M</cell><cell>215.6M</cell><cell>100.5M</cell><cell>61.5M</cell></row><row><cell>K=4</cell><cell>312.9M</cell><cell>217.5M</cell><cell>101.4M</cell><cell>62.0M</cell></row><row><cell>K=6</cell><cell>316.3M</cell><cell>219.5M</cell><cell>102.3M</cell><cell>62.5M</cell></row><row><cell>K=8</cell><cell>319.8M</cell><cell>221.4M</cell><cell>103.2M</cell><cell>62.9M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Comparison between DY-CNNs and the concurrent work (CondConv [37]) on ImageNet classification using MobileNetV2 ×1.0 and ×0.5.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Inspecting DY-CNN using different kernel aggregations. DY-MobileNetV2 ×0.5 is used. The proper aggregation of convolution kernels {W k } using attention π k (x) is shown in the first line. Shuffle per image means shuffling the attention weights for the same image over different kernels. Shuffle across images means using the attention of an image x for another image x . The poor performance for the bottom four aggregations validates that the DY-CNN is dynamic.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Softmax Temperature: large temperature in early training epochs is important. Temperature annealing refers to reducing τ from 30 to 1 linearly in the first 10 epochs. The numbers in brackets denote the performance improvement over the baseline.</figDesc><table><row><cell>Network</cell><cell>Temperature</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>MobileNetV2</cell><cell>-</cell><cell>65.4</cell><cell>86.4</cell></row><row><cell></cell><cell>τ = 1</cell><cell>64.8 (−0.6)</cell><cell>85.5 (−0.9)</cell></row><row><cell></cell><cell>τ = 5</cell><cell>65.7 (+0.3)</cell><cell>85.8 (−0.6)</cell></row><row><cell></cell><cell>τ = 10</cell><cell>67.5 (+2.1)</cell><cell>87.4 (+1.0)</cell></row><row><cell>DY-MobileNetV2</cell><cell>τ = 20</cell><cell>69.4 (+4.0)</cell><cell>88.5 (+2.1)</cell></row><row><cell></cell><cell>τ = 30</cell><cell>69.4 (+4.0)</cell><cell>88.6 (+2.2)</cell></row><row><cell></cell><cell>τ = 40</cell><cell>69.2 (+3.8)</cell><cell>88.4 (+2.0)</cell></row><row><cell></cell><cell>τ annealing</cell><cell>69.9 (+4.5)</cell><cell>89.0 (+2.6)</cell></row><row><cell>Network</cell><cell></cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell cols="2">MobileNetV3-Small</cell><cell>67.4</cell><cell>86.4</cell></row><row><cell cols="2">MobileNetV3-Small w/o SE</cell><cell>65.4 (−2.0)</cell><cell>85.2 (−1.2)</cell></row><row><cell cols="2">DY-MobileNetV3-Small</cell><cell>70.3 (+2.9)</cell><cell>88.7 (+2.3)</cell></row><row><cell cols="2">Dy-MobileNetV3-Small w/o SE</cell><cell>69.6 (+2.2)</cell><cell>88.4 (+2.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Dynamic</figDesc><table /><note>convolution vs Squeeze-and-Excitation (SE [13]) on MobileNetV3-Small. The numbers in brackets denote the performance improvement over the baseline. Compared with static convolution with SE, dynamic convolution without SE gains 2.2% top-1 accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table><row><cell cols="5">ImageNet [4] classification results of DY-CNNs. The</cell></row><row><cell cols="5">numbers in brackets denote the performance improvement over the</cell></row><row><cell>baseline.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>Operator</cell><cell>exp size</cell><cell>#out</cell><cell>n</cell></row><row><cell>16 × 12 × Bout</cell><cell>bneck, 5 × 5</cell><cell>768</cell><cell>256</cell><cell>2</cell></row><row><cell>32 × 24 × 256</cell><cell>bneck, 5 × 5</cell><cell>768</cell><cell>128</cell><cell>1</cell></row><row><cell>64 × 48 × 128</cell><cell>bneck, 5 × 5</cell><cell>384</cell><cell>128</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Keypoint detection results on COCO validation set. All models are trained from scratch. The top half uses dynamic convolution in the backbone and uses deconvolution in the head (Type A). The bottom half use MobileNetV2's bottleneck residual blocks in the head and use dynamic convolution in both the backbone and the head (Type B). Each dynamic convolution layer includes K = 4 kernels. The numbers in brackets denote the performance improvement over the baseline.</figDesc><table><row><cell>Type</cell><cell cols="2">Backbone Networks #Param</cell><cell>MAdds</cell><cell cols="2">Head Operator #Param</cell><cell>MAdds</cell><cell>AP</cell><cell>AP 0.5</cell><cell>AP 0.75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell></row><row><cell>A</cell><cell>ResNet-18 DY-ResNet-18</cell><cell>10.6M 42.2M</cell><cell>1.77G 1.81G</cell><cell>dconv dconv</cell><cell>8.4M 8.4M</cell><cell>5.4G 5.4G</cell><cell>67.0 68.6 (1.6)</cell><cell>87.9 88.4</cell><cell>74.8 76.1</cell><cell>63.6 65.3</cell><cell>73.5 75.1</cell><cell>73.1 74.6</cell></row><row><cell>A</cell><cell>MobileNetV2 ×1.0 DY-MobileNetV2 ×1.0</cell><cell>2.2M 9.8M</cell><cell>292.6M 305.3M</cell><cell>dconv dconv</cell><cell>8.4M 8.4M</cell><cell>5.4G 5.4G</cell><cell>64.7 67.6 (2.9)</cell><cell>87.2 88.1</cell><cell>72.6 75.5</cell><cell>61.3 64.4</cell><cell>71.0 74.1</cell><cell>71.0 73.8</cell></row><row><cell>A</cell><cell>MobileNetV2 ×0.5 DY-MobileNetV2 ×0.5</cell><cell>0.7M 2.7M</cell><cell>93.7M 98.0M</cell><cell>dconv dconv</cell><cell>8.4M 8.4M</cell><cell>5.4G 5.4G</cell><cell>57.0 61.9 (4.9)</cell><cell>83.7 85.8</cell><cell>63.1 69.7</cell><cell>53.9 58.9</cell><cell>63.1 67.9</cell><cell>63.7 68.4</cell></row><row><cell>A</cell><cell>MobileNetV3-Small DY-MobileNetV3-Small</cell><cell>1.1M 2.8M</cell><cell>62.7M 65.1M</cell><cell>dconv dconv</cell><cell>8.4M 8.4M</cell><cell>5.4G 5.4G</cell><cell>57.1 59.3 (2.2)</cell><cell>83.7 84.7</cell><cell>63.8 66.7</cell><cell>54.9 56.9</cell><cell>62.3 64.7</cell><cell>64.1 66.1</cell></row><row><cell>B</cell><cell>MobileNetV2 ×1.0 DY-MobileNetV2 ×1.0</cell><cell>2.2M 9.8M</cell><cell>292.6M 305.3M</cell><cell>bneck bneck</cell><cell>1.2M 6.3M</cell><cell>701.1M 709.4M</cell><cell>64.6 68.2 (3.6)</cell><cell>87.0 88.4</cell><cell>72.4 76.0</cell><cell>61.3 65.0</cell><cell>71.0 74.7</cell><cell>71.0 74.2</cell></row><row><cell>B</cell><cell>MobileNetV2 ×0.5 DY-MobileNetV2 ×0.5</cell><cell>0.7M 2.7M</cell><cell>93.7M 98.0M</cell><cell>bneck bneck</cell><cell>1.2M 6.3M</cell><cell>701.1M 709.4M</cell><cell>59.2 62.8 (3.6)</cell><cell>84.3 86.1</cell><cell>66.4 70.4</cell><cell>56.2 59.9</cell><cell>65.0 68.6</cell><cell>65.6 69.1</cell></row><row><cell>B</cell><cell>MobileNetV3-Small DY-MobileNetV3-Small</cell><cell>1.1M 2.8M</cell><cell>62.7M 65.1M</cell><cell>bneck bneck</cell><cell>1.0M 4.9M</cell><cell>664.2M 671.1M</cell><cell>57.1 60.0 (2.9)</cell><cell>83.8 85.0</cell><cell>63.7 67.8</cell><cell>55.0 57.6</cell><cell>62.2 65.4</cell><cell>64.1 66.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Dynamic convolution gains 1.6, 2.9, 2.2 AP for ResNet-18, MobileNetV2 and MobileNetV3-Small, respectively.Secondly, we replace the heavy deconvolution head with light-weight upsampling and MobileNetV2's bottleneck residual blocks (Type-B) to make the whole network small and efficient. Thus, we can compare dynamic convolution with its static counterpart in both backbone and head. The results are shown in the bottom half ofTable 10. Similar to Type-A, dynamic convolution outperforms its static counterpart by a clear margin. It gains 3.6 and 2.9 AP for MobileNetV2 and MobileNetV3-Small, respectively.</figDesc><table><row><cell>Backbone</cell><cell>Head</cell><cell>AP</cell><cell>AP 0.5</cell><cell>AP 0.75</cell></row><row><cell>static</cell><cell>static</cell><cell>59.2</cell><cell>84.3</cell><cell>66.4</cell></row><row><cell>static</cell><cell>dynamic</cell><cell>60.3 (1.1)</cell><cell>84.9</cell><cell>67.3</cell></row><row><cell>dynamic</cell><cell>static</cell><cell>62.3 (3.1)</cell><cell>85.6</cell><cell>70.0</cell></row><row><cell>dynamic</cell><cell>dynamic</cell><cell>62.8 (3.6)</cell><cell>86.1</cell><cell>70.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 .</head><label>12</label><figDesc>Inference running time of DY-MobileNetV2 [27] on ImageNet</figDesc><table><row><cell>Network</cell><cell>Top-1</cell><cell>MAdds</cell><cell>CPU (ms)</cell></row><row><cell>MobileNetV2 ×1.0</cell><cell>72.0</cell><cell>300.0M</cell><cell>127.9</cell></row><row><cell>DY-MobileNetV2 ×1.0</cell><cell>75.2 (3.2)</cell><cell>312.9M</cell><cell>141.2</cell></row><row><cell>MobileNetV2 ×0.75</cell><cell>69.8</cell><cell>209.0M</cell><cell>99.5</cell></row><row><cell>DY-MobileNetV2 ×0.75</cell><cell>73.7 (3.9)</cell><cell>217.5M</cell><cell>110.5</cell></row><row><cell>MobileNetV2 ×0.5</cell><cell>65.4</cell><cell>97.0M</cell><cell>69.6</cell></row><row><cell>DY-MobileNetV2 ×0.5</cell><cell>69.9 (4.5)</cell><cell>101.4M</cell><cell>77.4</cell></row><row><cell>MobileNetV2 ×0.35</cell><cell>60.3</cell><cell>59.2M</cell><cell>61.1</cell></row><row><cell>DY-MobileNetV2 ×0.35</cell><cell>65.9 (5.6)</cell><cell>62.0M</cell><cell>67.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Once for all: Train one network and specialize it for efficient deployment. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Searching for mobilenetv3. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;1mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2181" to="2191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanlan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Haq: Hardware-aware automated quantization with mixed precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Shift: A zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quantization networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Slimmable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Trained ternary quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenzhuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1611.01578</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
