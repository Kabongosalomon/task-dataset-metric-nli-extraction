<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
							<email>dstamoul@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Neural Architecture Search · Hardware-aware ConvNets</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can we automatically design a Convolutional Network (Con-vNet) with the highest image classification accuracy under the latency constraint of a mobile device? Neural architecture search (NAS) has revolutionized the design of hardware-efficient ConvNets by automating this process. However, the NAS problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours. Our contributions are as follows: 1. Single-path search space: Compared to previous differentiable NAS methods, Single-Path NAS uses one singlepath over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2. Hardware-efficient ImageNet classification: Single-Path NAS achieves 74.96% top-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar inference latency constraints (≤ 80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30 TPUhours), which is up to 5,000× faster compared to prior work. 4. Reproducibility: Unlike all recent mobile-efficient NAS methods which only release pretrained models, we open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>"Is it possible to reduce the considerable search cost of Neural Architecture Search (NAS) down to only few hours? " NAS has revolutionized the design of Convolutional Networks (ConvNets) <ref type="bibr" target="#b26">[27]</ref>, yielding state-of-the-art results in several deep learning applications <ref type="bibr" target="#b15">[16]</ref>. NAS methods already have a profound impact on the design of hardware-efficient ConvNets for computer vision tasks under the constraints (e.g., inference latency) imposed by mobile devices <ref type="bibr" target="#b19">[20]</ref>.</p><p>Despite the recent breakthroughs, NAS remains an intrinsically costly optimization problem. Searching for which convolution operation to use per ConvNet layer, gives rise to a combinatorially large search space: e.g., for a mobileefficient ConvNet with 22 layers, choosing among five candidate operations yields 5 22 ≈ 10 15 possible ConvNet architectures. To traverse this design space, earlier NAS methods guide the exploration via reinforcement learning (RL) <ref type="bibr" target="#b19">[20]</ref>. Nonetheless, training the RL controller poses prohibitive computational challenges, and thousands of candidate ConvNets need to be trained <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure">Fig. 1</ref>. Single-Path NAS directly optimizes for the subset of convolution kernel weights and searches over an over-parameterized "superkernel" in each ConvNet layer (right). This novel view of the design space eliminates the need for maintaining separate paths for each candidate operation, as in previous multi-path approaches (left). Our key insight drastically reduces the NAS search cost by up to 5,000× with state-of-the-art accuracy on ImageNet for the same mobile latency setting, compared to prior work.</p><p>Inefficiencies of multi-path NAS: Recent NAS literature has seen a shift towards one-shot differentiable formulations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> which search over a supernet that encompasses all candidate architectures. Specifically, current NAS methods relax the combinatorial optimization problem of finding the optimal ConvNet architecture to an operation/path selection problem: first, an overparameterized, multi-path supernet is constructed, where, for each layer, every candidate operation is added as a separate trainable path, as illustrated in <ref type="figure">Figure 1 (left)</ref>. Next, NAS formulations solve for the (distributions of) paths of the multi-path supernet that yield the optimal architecture.</p><p>As expected, naively branching out all paths is inefficient due to an intrinsic limitation: the number of trainable parameters that need to be maintained and updated during the search grows linearly with respect to the number of candidate operations per layer <ref type="bibr" target="#b0">[1]</ref>. To tame the memory explosion introduced by the multi-path supernet, current methods employ creative "workaround" solutions: e.g., searching on a proxy dataset (subset of ImageNet <ref type="bibr" target="#b20">[21]</ref>), or employing a memory-wise scheme with only a subset of paths being updated during the search <ref type="bibr" target="#b3">[4]</ref>. Nevertheless, these techniques remain considerably costly, with an overall computational demand of at least 200 GPU-hours.</p><p>In this paper, we propose Single-Path NAS, a novel NAS method for designing hardware-efficient ConvNets in less than 4 hours. Our key insight is illustrated in <ref type="figure">Figure 1</ref> (right). We build upon the observation that different candidate convolutional operations in NAS can be viewed as subsets of a single "superkernel". Without having to choose among different paths/operations as in multi-path methods, we instead solve the NAS problem as finding which subset of kernel weights to use in each ConvNet layer. By sharing the convolutional kernel weights, we encode all candidate NAS operations into a single "superkernel", i.e., with a single path, for each layer of the one-shot NAS supernet. This novel encoding of the design space yields a drastic reduction to the number of trainable parameters/gradients, allowing our NAS method to use batch sizes of 1024, a four-fold increase compared to prior art's search efficiency.</p><p>Our contributions are as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Hardware-efficient ConvNets: While complex ConvNet designs have unlocked unprecedented performance levels in computer vision tasks, the accuracy improvement has come at the cost of higher computational complexity, making the deployment of state-of-the-art ConvNets to mobile devices challenging <ref type="bibr" target="#b18">[19]</ref>. To this end, a significant body of prior work aims to co-optimize for the inference latency of ConvNets. Earlier approaches focus on human expertise to introduce hardware-efficient operations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>. Pruning <ref type="bibr" target="#b4">[5]</ref> and quantization <ref type="bibr" target="#b8">[9]</ref> methods share the same goal to improve the efficiency of ConvNets. Neural Architecture Search (NAS): NAS aims at automating the process of designing ConvNets, giving rise to methods based on reinforcement learning (RL), evolutionary algorithms, or gradient-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Earlier approaches train an agent (e.g., RNN controller) by sampling candidate architectures over a cell-based design space, where the same cell is repeated in all layers and the focus is on searching the cell architecture <ref type="bibr" target="#b26">[27]</ref>. Nonetheless, training the controller over different architectures makes the search costly.</p><p>Hardware-aware NAS: Earlier NAS methods focused on maximizing accuracy under FLOPs constraints <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>, but low FLOP count does not necessarily translate to hardware efficiency <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. More recent methods incorporate hardware terms (e.g., runtime, power) into cell-based NAS formulations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, but cell-based implementations are not hardware friendly <ref type="bibr" target="#b20">[21]</ref>. Breaking away from cell-based assumptions in the search space encoding, recent work employs NAS over a generalized MobileNetV2-based design space introduced in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Hardware-aware Differentiable NAS: Recent NAS literature has seen a shift towards one-shot NAS formulations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>. Gradient-based NAS in particular has gained increased popularity and has achieved state-of-the-art results <ref type="bibr" target="#b1">[2]</ref>. One-shot-based methods use an over-parameterized super-model network, where, for each layer, every candidate operation is added as a separate trainable path. Nonetheless, multi-path search spaces have an intrinsic limitation: the number of trainable parameters that need to be maintained and updated with gradients during the search grows linearly with respect to the number of different convolutional operations per layer, resulting in memory explosion <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>To this end, state-of-the-art approaches employ different novel "workaround" solutions. FBNet <ref type="bibr" target="#b20">[21]</ref> searches on a "proxy" dataset (i.e., subset of the ImageNet dataset). Despite the decreased search cost thanks to the reduced number of training images, these approaches do not address the fact that the entire supermodel needs to be maintained in memory during search, hence the efficiency is limited due to inevitable use of smaller batch sizes. ProxylessNAS <ref type="bibr" target="#b3">[4]</ref> has employed a memory-wise one-shot model scheme, where only a set of paths is updated during the search. However, such implementation-wise improvements do not address a second key suboptimality of one-shot approaches, i.e., the fact that separate gradient steps are needed to update the weights and the architectural decisions interchangeably <ref type="bibr" target="#b13">[14]</ref>. Although the number of trainable parameters, with respect to the memory cost, is kept to the same level at any step, the way that multi-path-based methods traverse the design space remains inefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method: Single-Path NAS</head><p>In this Section, we present our proposed method. First, we discuss our novel single-path view (Subsection 3.1) of the search space. Next, we encode the NAS problem as finding the subset of convolution weights over the over-parameterized "superkernel" (Subsection 3.2), and we discuss how it compares to existing multi-path-based NAS (Subsection 3.3). Last, we formulate the hardware-aware NAS objective function, where we incorporate an accurate inference latency model of ConvNets executing on the Pixel 1 smartphone (Subsection 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mobile ConvNets Search Space: A Novel View</head><p>Background -Mobile ConvNets: State-of-the-art NAS builds upon a fixed "backbone" ConvNet <ref type="bibr" target="#b3">[4]</ref> inspired by the MobileNetV2 design <ref type="bibr" target="#b16">[17]</ref>, illustrated in  <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>, where the goal is to identify the type of mobile inverted bottleneck convolution (MBConv) <ref type="bibr" target="#b16">[17]</ref> per layer. Our one-shot supernet encapsulates all possible NAS architectures in the search space, without the need for appending each candidate operation as a separate path. Single-Path NAS directly searches over the weights of a searchable "superkernel" that encodes all MBConv types. <ref type="figure" target="#fig_0">Figure 2</ref> (top). Specifically, in this fixed macro-architecture, except for the head and stem layers, all ConvNet layers are grouped into blocks based on their filter sizes. The filter numbers per block follow the values in <ref type="bibr" target="#b20">[21]</ref>, i.e., we use seven blocks with up to four layers each. Each layer of these blocks follows a mobile inverted bottleneck convolution MBConv <ref type="bibr" target="#b16">[17]</ref> micro-architecture, which consists of a point-wise (1 × 1) convolution, a k × k depthwise convolution, and a linear 1 × 1 convolution <ref type="figure" target="#fig_0">(Figure 2</ref>, middle). Unless the layer has a stride value of two, a skip path is introduced to provide a residual connection from input to output.</p><p>Each MBConv layer is parameterized by k, i.e., the kernel size of the depthwise convolution, and by expansion ratio e, i.e., the ratio between the output and input of the first 1 × 1 convolution. Based on this parameterization, we denote each MBConv as MBConv-k × k-e. Mobile-efficient NAS aims to choose each MBConv-k × k-e layer, by selecting among different k and e values <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>. In particular, we consider MBConv layers with kernel sizes {3, 5} and expansion ratios {3, 6}. NAS also considers a special skip-op "layer", which "zeroes-out" the kernel and feeds the input directly to the output, i.e., the entire layer is dropped.</p><p>Novel view of design space: Our key insight is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. We build upon the observation that different candidate convolutional operations in NAS can be viewed as subsets of the weights of an over-parameterized single "superkernel" <ref type="figure" target="#fig_0">(Figure 2</ref>, bottom). This observation allows us to view the NAS combinatorial problem as finding which subset of kernel weights to use in each MBConv layer. This observation is important since it allows sharing the kernel parameters across different MBConv architectural options. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we encode all candidate NAS operations to this single "superkernel", i.e., with a single path, for each layer of the one-shot NAS supernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Methodology: Single-Path NAS formulation</head><p>Key idea -Relaxing NAS decisions over an over-parameterized kernel: To simplify notation and to illustrate the key idea, without loss of generality, we show the case of choosing between a 3 × 3 or a 5 × 5 kernel for an MBConv layer. Let us denote the weights of the two candidate kernels as w 3×3 and w 5×5 , respectively. As shown in <ref type="figure" target="#fig_1">Figure 3</ref> (left), we observe that the weights of the 3 × 3 kernel can be viewed as the inner core of the weights of the 5 × 5 kernel, while "zeroing" out the weights of the "outer " shell. We denote this (outer ) subset of weights (that does not contribute to output of the 3 × 3 kernel but only to the 5 × 5 kernel), as w 5×5\3×3 . Hence, the NAS architectural choice of using the 5 × 5 convolution corresponds to using both the inner w 3×3 weights and the outer shell, i.e., w 5×5 = w 3×3 + w 5×5\3×3 <ref type="figure" target="#fig_1">(Figure 3</ref>, left). We can therefore encode the NAS decision directly into the superkernel of an MBConv layer as a function of kernel weights as follows:</p><formula xml:id="formula_0">w k = w 3×3 + 1(use 5 × 5) · w 5×5\3×3 (1) where 1(·) is the indicator function that encodes the architectural NAS choice, i.e., if 1(·) = 1 then w k = w 3×3 + w 5×5\3×3 = w 5×5 , else 1(·) = 0 then w k = w 3×3 .</formula><p>Trainable indicator/condition function: While the indicator function encodes the NAS decision, a critical choice is how to formulate the condition over which the 1(·) is evaluated. Our intuition is that, for an indicator function that represents whether to use the subset of weights, its condition should be directly a function of the subset's weights. Thus, our goal is to define an "importance" signal of the subset weights that intrinsically captures their contribution to the overall ConvNet loss. We draw inspiration from weight-based conditions that have been successfully used for quantization-related decisions <ref type="bibr" target="#b7">[8]</ref> and we use the group Lasso term. Specifically, for the indicator related to the w 5×5\3×3 "outer shell" decision, we write the following condition:</p><formula xml:id="formula_1">w k = w 3×3 + 1( w 5×5\3×3 2 &gt; t k=5 ) · w 5×5\3×3<label>(2)</label></formula><p>where t k=5 is a latent variable that controls the decision (e.g., a threshold value) of selecting kernel 5 × 5. The threshold will be compared to the Lasso term to determine if the outer w 5×5\3×3 weights are used to the overall convolution.</p><p>It is important to notice that, instead of picking the thresholds (e.g., t k=5 ) by hand, we seamlessly treat them as trainable parameters to learn via gradient descent. To compute the gradients for thresholds, we relax the indicator function g(x, t) = 1(x &gt; t) to a sigmoid function, σ(·), when computing gradients, i.e.,</p><formula xml:id="formula_2">g(x, t) = σ(x &gt; t).</formula><p>Searching for expansion ratio and skip-op: Since the result of the kernelbased NAS decision w k (Equation 2) is a convolution kernel itself, we can in turn apply our formulation to also encode NAS decisions for the expansion ratio of the w k kernel. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> (right), the channels of the depthwise convolution in an MBConv-k × k-3 layer with expansion ratio e = 3 can be viewed as using one half of the channels of an MBConv-k × k-6 layer with expansion ratio e = 6, while "zeroing" out the second half of channels {w k,6\3 }. Finally, by "zeroing" out the first half of the output filters as well, the entire superkernel contributes nothing if added to the residual connection of the MBConv layer: i.e., by deciding if e = 3, we can encode the NAS decision of using, or not, only the "skip-op" path. For both decisions over w k kernel, we write:</p><formula xml:id="formula_3">w = 1( w k,3 2 &gt; t e=3 ) · (w k,3 + 1( w k,6\3 2 &gt; t e=6 ) · w k,6\3 )<label>(3)</label></formula><p>Hence, for input x, the output of the i-th MBConv layer of the network is:</p><formula xml:id="formula_4">o i (x) = conv(x, w i |t i k=5 , t i e=6 , t i e=3 )<label>(4)</label></formula><p>Searchable MBConv kernels: Each MBConv uses 1 × 1 convolutions for the point-wise (first) and linear stages, while the kernel-size decisions affect only the (middle) k × k depthwise convolution <ref type="figure" target="#fig_0">(Figure 2</ref>). To this end, we use our searchable k × k depthwise kernel at this middle stage. In terms of number of channels, the depthwise kernel depends on the point-wise 1 × 1 output, which allows us to directly encode the expansion ratio e at the middle stage as well: by setting the point-wise 1 × 1 output to the maximum candidate expansion ratio, we can instead solve for which of them not to "zero" out at the depthwise (middle) state. In other words, we directly use our searchable depthwise convolution superkernel to effectively encode the NAS decision for the expansion ratio. Hence, our single-path, convolution-based formulation can sufficiently capture any MBConv type (e.g., MBConv-3 × 3-6, MBConv-5 × 5-3, etc.) in the MobileNetV2based design space <ref type="figure" target="#fig_0">(Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Single-Path vs. Existing Multi-Path Assumptions</head><p>Comparison with multi-path over-parameterized network: We briefly illustrate how our single-path formulation compares to multi-path NAS approaches. In existing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>, the output of each layer i is a (weighted) sum defined over the output of N different paths, where each path j corresponds to a different candidate kernel w i,j k×k,e . The weight of each path α i,j corresponds to the probability that this path is selected over the parallel paths:</p><formula xml:id="formula_5">o i multi−path (x) = N j=1 α i,j ·o i,j (x) = α i,0 ·conv(x, w i,0 3×3 )+· · ·+α i,N ·conv(x, w i,N 5×5 )<label>(5)</label></formula><p>It is easy to see how our novel single-path view is advantageous, since the output of the convolution at layer i of our search space is directly a function of the weights of our single over-parameterized kernel (Equation <ref type="formula" target="#formula_4">4)</ref>:</p><formula xml:id="formula_6">o i single−path (x) = o i (x) = conv(x, w i |t i k=5 , t i e=6 , t i e=3 )<label>(6)</label></formula><p>Comparison with multi-path NAS optimization: Multi-path NAS methods solve for the optimal architecture parameters α (path weights), such that the weights w α of the corresponding α-architecture have minimal loss L(α, w α ):</p><formula xml:id="formula_7">min α min wα L(α, w α )<label>(7)</label></formula><p>However, solving Equation 7 gives rise to a challenging bi-level optimization problem <ref type="bibr" target="#b13">[14]</ref>. Existing methods interchangeably update the α's while freezing the w's and vice versa, leading to more gradient steps. In contrast, with our single-path formulation, the overall network loss is directly a function of the "superkernel" weights, where the learnable kerneland expansion ratio-related threshold variables, t k and t e , are directly derived as a function (norm) of the kernel weights w. Consequently, Single-Path NAS formulates the NAS problem as solving directly over the weight kernels w of a single-path, compact neural network. Formally, the NAS problem becomes:</p><formula xml:id="formula_8">min w L(w|t k , t e )<label>(8)</label></formula><p>Efficiency of Single-Path NAS : Unlike the bi-level optimization problem in prior work, solving our NAS formulation in Equation 8 is as expensive as training the weights of a single-path, branchless, compact neural network with vanilla gradient descent. Therefore, our formulation eliminates the need for separate gradient steps between the ConvNet weights and the NAS parameters. Moreover, the reduction of the trainable parameters w per se, further leads to a drastic reduction of the search cost down to just a few epochs, as our experimental results show later in Section 4. Our NAS problem formulation allows us to efficiently solve Equation 8 with batch sizes of 1024, a four-fold increase compared to prior art's search efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hardware-Aware NAS with Differentiable Runtime Loss</head><p>To design hardware-efficient ConvNets, the differentiable objective in <ref type="bibr">Equation 8</ref> should reflect both the accuracy of the searched architecture and its inference latency on the target hardware. Hence, we use a latency-aware formulation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>:</p><formula xml:id="formula_9">L(w|t k , t e ) = CE(w|t k , t e ) + λ · log(R(w|t k , t e ))<label>(9)</label></formula><p>The first term CE corresponds to the cross-entropy loss of the single-path model. The hardware-related term R is the runtime in milliseconds (ms) of the searched NAS model on the target mobile platform. Finally, the coefficient λ modulates the trade-off between cross-entropy and runtime. Runtime model over the single-path design space: To preserve the differentiability of the objective, another critical choice is the formulation of the latency term R. Prior art has showed that the total network latency of a mobile ConvNet can be modeled as the sum of each i-th layer's runtime R i , since the runtime of each operator is independent of other operators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref>:</p><formula xml:id="formula_10">R(w|t k , t e ) = i R i (w i |t i k , t i e )<label>(10)</label></formula><p>For our approach, we adapt the per-layer runtime model as a function of the NAS-related decisions t. We profile the target mobile platform (Pixel 1) and we record the runtime for each candidate kernel operation per layer i, i.e., R i 3×3,3 , R i 3×3,6 , R i 5×5,3 , and R i 5×5,6 . We denote the runtime of layer i by following the notation in Equation 3. Specifically, the runtime of layer i is defined first as a function of the expansion ratio decision:</p><formula xml:id="formula_11">R i e = 1( w k,3 2 &gt; t e=3 )·(R i 5×5,3 +1( w k,6\3 2 &gt; t e=6 )·(R i 5×5,6 −R i 5×5,3 )) (11)</formula><p>Next, by incorporating the kernel size decision, the total runtime is:</p><formula xml:id="formula_12">R i = R i 3×3,6 R i 5×5,6 · R i e + R i e · (1 − R i 3×3,6 R i 5×5,6 ) · 1( w 5×5\3×3 2 &gt; t k=5 )<label>(12)</label></formula><p>As in Equation 2, we relax the indicator function to a sigmoid function σ(·) when computing gradients. By using this model, the runtime term in the loss function remains differentiable with respect to layer-wise NAS choices. As we show in our results, the model is accurate, with an average prediction error of 1.76%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset and target application: We use Single-Path NAS to design ConvNets for image classification on ImageNet <ref type="bibr" target="#b6">[7]</ref>. We use Pixel 1 as the target mobile platform. The choice of this experimental setup is important, since it allows for a representative comparison with prior hardware-efficient NAS methods that optimize for the same Pixel 1 device around a target latency of 80ms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. Implementation and deployment: We implement our NAS framework in TensorFlow (TF version 1.12). During both search and training stages, we use TPUs (version 2) <ref type="bibr" target="#b12">[13]</ref>. To this end, we build on top of the TPUEstimator classes following the TPU-related documentation of the MnasNet repository 4 . Last, all models (ours and prior work) are deployed with TensorFlow TFLite to the mobile device. On the device, we profile runtime using the Facebook AI Performance Evaluation Platform (FAI-PEP) 5 that supports profiling for tflite models with detailed per-layer runtime breakdown.</p><p>Implementing the custom "superkernels": We use Keras to implement our trainable "superkernels." Specifically, we define a custom Keras-based depthwise convolution kernel where the output is a function of both the weights and the threshold-based decisions (Equations 2-3). Our custom layer also returns the effective runtime of the layer (Equations 11-12). We document our implementation in our project GitHub repository: https://github.com/dstamoulis/single-path-nas, with detailed steps on how to reproduce the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">State-of-the-art Runtime-Constrained ImageNet Classification</head><p>We apply our method to design ConvNets for the Pixel 1 phone with an overall target latency of 80ms. We train the derived Single-Path NAS model for 350 epochs, following the MnasNet training schedule <ref type="bibr" target="#b19">[20]</ref>. We compare our method with mobile ConvNets designed by human experts and state-of-the-art NAS methods in <ref type="table">Table 1</ref>, in terms of classification accuracy and search cost. In terms of hardware efficiency, prior work has shown that low FLOP count does not necessarily translate to high hardware efficiency <ref type="bibr" target="#b9">[10]</ref>, we therefore evaluate the various NAS methods with respect to the inference runtime on Pixel 1 (≤ 80ms).</p><p>Enabling a representative comparison: While we provide the original values from the respective papers, our goal is to ensure a fair comparison. To this end, we retrain the baseline models following the same schedule (in fact, we find that the MnasNet-based training schedule improves the top1 accuracy compared to what is reported in several previous methods). Similarly, we profile the models on the same Pixel 1 device. For prior work that does not optimize for Pixel 1, we retrain and profile their model closest to the MnasNet baseline (e.g., the FBNet-B and ChamNet-B networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>, since the authors use these ConvNets to compare against the MnasNet model). Finally, to enable a representative comparison of the search cost per method, we directly report the number of epochs reported per method, hence canceling out the effect of different hardware systems (GPU vs TPU hours).</p><p>ImageNet classification: <ref type="table">Table 1</ref> shows that our Single-Path NAS achieves top-1 accuracy of 74.96%, which is the new state-of-the-art ImageNet accuracy among hardware-efficient NAS methods. More specifically, our method <ref type="table">Table 1</ref>. Single-Path NAS achieves state-of-the-art accuracy (%) on ImageNet for similar mobile latency setting compared to previous NAS methods (≤ 80ms on Pixel 1), with up to 5, 000× reduced search cost in terms of number of epochs. *The search cost in epochs is estimated based on the claim <ref type="bibr" target="#b3">[4]</ref> that ProxylessNAS is 200× faster than MnasNet. ‡ChamNet does not detail the model derived under runtime constraints <ref type="bibr" target="#b5">[6]</ref> so we cannot retrain or measure the latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top achieves better top-1 accuracy than ProxylessNAS by +0.31%, while maintaining on par target latency of ≤ 80ms on the same target mobile phone.</p><p>Single-Path NAS outperforms methods in this mobile latency range, i.e., better than MnasNet (+0.35%), FBNet-B (+0.86%), and MobileNetV2 (+1.37%). NAS search cost: Single-Path NAS has orders of magnitude reduced search cost compared to all previous hardware-efficient NAS methods. Specifically, MnasNet reports that the controller uses 8k sampled models, each trained for 5 epochs, for a total of 40k train epochs. In turn, ChamNet trains an accuracy predictor on 240 samples, which assuming an aggressively fast training schedule of five epochs per sample (same as in MnasNet), corresponds to a total search cost of 1.2k epochs. ProxylessNAS reports 200× search cost improvement over MnasNet, hence the overall cost is the TPU-equivalent of 200 epochs. Finally, FBNet reports 90 epochs of training on a proxy dataset (10% of ImageNet). While the number of images per epoch is reduced, we found that a TPU can accommodate a FBNet-like supermodel with maximum batch size of 128, hence the number of steps per FBNet epoch are still 8× more compared to the steps per epoch in our method.</p><p>In comparison, Single-Path NAS has a total cost of eight epochs, which is 5,000× faster than MnasNet, 25× faster than ProxylessNAS, and 11× faster than FBNet. In particular, we use an aggressive training schedule similar to the few-epochs schedule used in MnasNet to train the individual ConvNet samples <ref type="bibr" target="#b19">[20]</ref>. Due to space limitations, we provide implementation details (e.g., label smoothing, learning rates, λ value, etc.) in our project repository. Overall, we visualize the search efficiency of our method in <ref type="figure" target="#fig_2">Figure 4</ref>, where we show the progress of both  CE and R terms of Equation <ref type="bibr" target="#b7">8</ref>. Earlier during our search (first six epochs), we employ dropout across the different subsets of the kernel weights <ref type="figure" target="#fig_2">(Figure 4, right)</ref>. Dropout is a common technique in NAS methods to prevent the supernet from learning as an ensemble. Unlike prior art that employs this technique over the separate paths of the multi-path supernet, we directly drop randomly the subsets of the superkernel in our single-path search space. We search for ∼ 10k steps (8 epochs with a batch size of 1024), which corresponds to total wall-clock time of 3.75 hours on a TPUv2. In particular, given than a TPUv2 has 2 chips with 4 cores each, this corresponds to a total of 30 TPU-hours.</p><p>Visualization of Single-Path NAS ConvNet: Our derived ConvNet architecture is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Moreover, to illustrate how the searchable superkernels effectively capture NAS decisions across subsets of kernel weights, we plot the standard deviation of weight values in <ref type="figure" target="#fig_4">Figure 6</ref> (shown in logscale, with lighter colors indicating smaller values). Specifically, we compute the standard deviation of weights across the channel-dimension for all superkernels. For various layers shown in <ref type="figure" target="#fig_4">Figure 6</ref> (per i-th ConvNet's layer from <ref type="figure" target="#fig_3">Figure 5</ref>), we observe that the outer w 5×5\3×3 "shells" reflect the NAS architectural choices: for layers where the entire w 5×5 is selected, the w 5×5\3×3 values drastically vary across the channels. On the contrary, for all layers where 3 × 3 convolution is selected, the outer shell values do not vary significantly.</p><p>Comparison with random search: We find surprising that mobile-efficient NAS methods lack a comparison against random search. To this end, we randomly sample ten ConvNets based on our design space; we employ sampling by rejection,   where we keep samples with predicted runtime from 75ms to 80ms. The average accuracy and runtime of the random samples are reported in <ref type="table">Table 1</ref>. We observe that, while random search does not outperform NAS methods, the overall accuracy is comparable to MobileNetV2. This highlights that the effectiveness of NAS methods heavily relies upon the properties of the MobileNetV2-based design space. Nonetheless, the search cost of random search is not representative: to avoid training all ten samples, we would follow a selection process similar to MnasNet, by training each sample for few epochs and picking the one with highest accuracy. Hence, the actual search cost for random search is not negligible, and for ≥ 10 samples it is in fact comparable to automated NAS methods.</p><p>Different channel size scaling: Next, we follow a typical analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>, by rescaling the networks using a width multiplier <ref type="bibr" target="#b16">[17]</ref>. As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, we observe that our model consistently outperforms prior methods under varying runtime settings. For instance, Single-Path NAS with 79.48ms is 1.56× faster than the MobileNetV2 scaled model of similar accuracy.</p><p>Runtime model: To train the runtime model, we record the runtime per layer (MBConv operations breakdown) by profiling ConvNets with different  <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. To evaluate the runtime-prediction accuracy of the model, we generate 100 randomly designed ConvNets and we measure their runtime on the device. As illustrated in <ref type="figure" target="#fig_5">Figure 7</ref>, our model can accurately predict the actual runtimes: the Root Mean Squared Error (RMSE) is 1.32ms, which corresponds to an average 1.76% prediction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study: Kernel-based Accuracy-Efficiency Trade-off</head><p>Single-Path NAS searches over subsets of the convolutional kernel weights. Hence, we conduct experiments to highlight how kernel-weight subsets can capture accuracy-efficiency trade-off effectively. To this end, we use the MobileNetV2 macro-architecture as a backbone (we maintain the location of stride-2 layers as default). As two baseline networks, we consider the default MobileNetV2 with MBConv-3 × 3-6 blocks (i.e., w 3×3 kernels for all depthwise convolutions), and a network with MBConv-5 × 5-6 blocks (i.e., w 5×5 kernels).</p><p>Next, to capture the subset-based training of weights during a Single-Path NAS search, we consider a ConvNet with MBConv-5 × 5-6 blocks, where we compute the loss of the model over two subsets, (i) the inner w 3×3 weights, and (ii) by also using the remaining w 5×5\3×3 weights. For each loss computed over these subsets, we accumulate back-propagated gradients and update the respective weights, i.e., gradients are being applied separately to the inner and to the entire kernel per layer. We follow training steps similar to the "switchable" training across channels as in <ref type="bibr" target="#b22">[23]</ref> (for the remaining training hyper-parameters we use the same setup as the default MnasNet). As shown in <ref type="table" target="#tab_2">Table 2</ref>, we observe the final accuracy across the kernel granularity, i.e., with the inner w 3×3 and the entire w 5×5 = w 3×3 + w 5×5\3×3 kernels, follows an accuracy change relative to ConvNets with individually trained kernels.</p><p>Such finding is significant in the context of NAS, since choosing over subsets of kernels can effectively capture the accuracy-runtime trade-offs similar to their individually trained counterparts. We therefore conjecture that our efficient superkernel-based design search can be flexibly adapted and benefit the guided search space exploration in other RL-based NAS methods. Beyond the NAS literature, our finding is closely related to Slimmable networks <ref type="bibr" target="#b22">[23]</ref>. SlimmableNets limit however their analysis across the channel dimension, and our work is the first to study trade-offs across the NAS kernel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed Single-Path NAS, a NAS method that reduces the search cost for designing hardware-efficient ConvNets to less than 4 hours. The key idea is to revisit the one-shot supernet design space with a novel single-path view, by formulating the NAS problem as finding which subset of kernel weights to use in each ConvNet layer. Single-Path NAS achieved 74.96% top-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is state-of-the-art accuracy with latency on-par with previous NAS methods (≤ 80ms). More importantly, we reduced the search cost of hardware-efficient NAS down to only 8 epochs (30 TPU-hours), which is up to 5,000× faster compared to prior work. Impact beyond differentiable NAS: While we used a differentiable NAS formulation, our novel design space encoding can be flexibly incorporated into other NAS methodologies. Hence, Single-Path NAS could enable future work that builds upon the efficiency of our single-path, one-shot design space for RLor evolutionary-based NAS methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Single-path search space: Our method builds upon hierarchical MobileNetV2like search spaces</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Encoding NAS decisions into the superkernel: We formulate all candidate convolution operations (i.e., different kernel size (left) and expansion ratio (right) values) directly into the searchable superkernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Single-Path NAS search progress: Progress of both objective terms, i.e., cross entropy CE (left) and runtime R (right) during NAS search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Hardware-efficient ConvNet found by Single-Path NAS, with top-1 accuracy of 74.96% on ImageNet and inference time of 79.48ms on Pixel 1 phone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of kernel-based architectural contributions. The standard deviation of superkernel values across the kernel channels is shown in log-scale, with lighter colors indicating smaller values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>The runtime model (Equation 10) is accurate, with an average prediction error of 1.76%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Single-Path NAS outperforms MobileNetV2 and MnasNet across various channel size scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Searching across subsets of kernel weights: ConvNets with weight values trained over subsets of the kernels (3 × 3 as subset of 5 × 5) achieve performance (top-1 accuracy) similar to ConvNets with individually trained kernels.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 Acc (%) Top-5 Acc (%)</cell></row><row><cell>Baseline ConvNet -w3×3 kernels</cell><cell>73.59</cell><cell>91.41</cell></row><row><cell>Baseline ConvNet -w5×5 kernels</cell><cell>74.10</cell><cell>91.67</cell></row><row><cell>Single-Path ConvNet -inference w/ w3×3 kernels</cell><cell>73.43</cell><cell>91.42</cell></row><row><cell>Single-Path ConvNet -inference w/ w3×3 + w 5×5\3×3 kernels</cell><cell>73.86</cell><cell>91.72</cell></row><row><cell cols="3">MBConv types, i.e., we obtain the R i 3×3,3 , R i 3×3,6 , R i 5×5,3 , and R i 5×5,6 runtime</cell></row><row><cell>values per MBConv layer i (Equations</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet 5 https://github.com/facebook/FAI-PEP</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by National Science Foundation CSR Grant No. 1815780 and National Science Foundation CCF Grant No. 1815899. Dimitrios Stamoulis also acknowledges support from the Qualcomm Innovation Fellowship (QIF) 2018 and the TensorFlow Research Cloud programs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<title level="m">Smash: one-shot model architecture search through hypernetworks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neuralpower: Predict and deploy energy-efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Layer-compensated pruning for resourceconstrained convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00518</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08934</idno>
		<title level="m">Chamnet: Towards efficient network design through platform-aware model adaptation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flightnns: Lightweight quantized deep neural networks for fast and accurate inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blanton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lightnn: Filling the gap between conventional deep neural networks and binarized networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blanton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Great Lakes Symposium on VLSI 2017</title>
		<meeting>the on Great Lakes Symposium on VLSI 2017</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08198</idno>
		<title level="m">Dpp-net: Deviceaware progressive search for pareto-optimal neural architectures</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10332</idno>
		<title level="m">Monas: Multi-objective neural architecture search using reinforcement learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<title level="m">Efficient neural architecture search via parameter sharing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<title level="m">Regularized evolution for image classifier architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hyperpower: Power-and memoryconstrained hyper-parameter optimization for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Designing adaptive neural networks for energy-constrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W R</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sajja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bognar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design</title>
		<meeting>the International Conference on Computer-Aided Design</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03443</idno>
		<title level="m">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">Snas: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08928</idno>
		<title level="m">Slimmable neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ö</forename><surname>Arık</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07912</idno>
		<title level="m">Resource-efficient neural architect</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
