<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Convolutional LSTM Approach for Tool Tracking in Laparoscopic Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-02-15">15 Feb 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mutter ·</roleName><forename type="first">Jacques</forename><surname>Marescaux</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Chinedu Innocent Nwoye · Nicolas Padoy ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>IHU</postCode>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IRCAD, IHU Strasbourg</orgName>
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Convolutional LSTM Approach for Tool Tracking in Laparoscopic Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-15">15 Feb 2019</date>
						</imprint>
					</monogr>
					<note>Chinedu Innocent Nwoye · Didier Mutter · Jacques Marescaux · Nicolas Padoy 2 Chinedu Innocent Nwoye et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Surgical workflow analysis · tool tracking · weak supervision · spatio-temporal coherence · ConvLSTM · endoscopic videos</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Purpose: Real-time surgical tool tracking is a core component of the future intelligent operating room (OR), because it is highly instrumental to analyze and understand the surgical activities. Current methods for surgical tool tracking in videos need to be trained on data in which the spatial positions of the tools are manually annotated. Generating such training data is difficult and time-consuming. Instead, we propose to use solely binary presence annotations to train a tool tracker for laparoscopic videos. Methods: The proposed approach is composed of a CNN + Convolutional LSTM (ConvLSTM ) neural network trained end-to-end, but weakly supervised on tool binary presence labels only. We use the ConvLSTM to model the temporal dependencies in the motion of the surgical tools and leverage its spatio-temporal ability to smooth the class peak activations in the localization heat maps (Lhmaps). Results: We build a baseline tracker on top of the CNN model and demonstrate that our approach based on the ConvLSTM outperforms the baseline in tool presence detection, spatial localization, and motion tracking by over 5.0%, 13.9%, and 12.6%, respectively. Conclusions: In this paper, we demonstrate that binary presence labels are sufficient for training a deep learning tracking model using our proposed method. We also show that the ConvLSTM can leverage the spatio-temporal coherence of consecutive image frames across a surgical video to improve tool presence detection, spatial localization, and motion tracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The automated analysis of surgical workflow can support many routine surgical activities by providing clinical decision support, report generation, and data annotation. This has sparked active research in the medical computer vision community, particularly on surgical phase recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and tool detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Since surgical activities can now be captured using cameras, large amounts of data become available for their analysis. Surgical tools tracking is a multi-object tracking (MOT) problem that entails the modeling of the trajectories of all surgical tools throughout a surgical video sequence. It is needed to model and analyze tool-tissue interactions. The predominant MOT approach has been tracking-bydetection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, which is an integration of a detection model, a localization model and a tracking algorithm. In this approach, object detectors like <ref type="bibr" target="#b10">[11]</ref> are used for predicting the presence or absence of objects of interest. The bounding box coordinates of the detected objects are then extracted using a localization model as seen in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. Most times, the localization model is regressed over the bounding box annotations in a fully supervised manner. This is usually concluded by a one-to-one assignment of the detected objects to object trajectories using a data association algorithm. Bipartite graph matching <ref type="bibr" target="#b11">[12]</ref> has been widely used in this regard. Most works in the medical computer vision community view this matching as a linear assignment problem learnable by stochastic optimization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. Meanwhile, recent works have also shown that the long short-term memory (LSTM) model has the capability to learn a data association task <ref type="bibr" target="#b9">[10]</ref>, making it easier to build a unified deep learning tracking model. Surgical tool tracking in endoscopic videos is not an easy task. In particular, laparoscopic data presents several challenges, such as the presence of blood stains on the tools, smoke from electric coagulation and cutting, motion blur for fast-moving tools, and the removal and re-insertion of the endoscope during the procedure. Furthermore, most endoscopic datasets are not fully exploited by deep learning methods because only a small fraction of the dataset can be spatially annotated with localization information. The implication is that most intriguing tasks are only explored and tested on a very tiny fraction of the dataset. Creating spatial annotations such as region boundaries and pixel-wise masks is indeed tedious and time-consuming. Since generating binary annotations just indicating the presence of the tools requires less effort, exploiting this information for tracking becomes an interesting research question.</p><p>Previous tool tracking work in the medical computer vision community relies on spatially annotated data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. In this paper, we propose a new deep learning object tracking method that circumvents the lack of spatially annotated surgical data with weak supervision on binary presence labels. Weak supervision is here motivated by the idea that when a convolutional neural network (CNN) is trained in a fully convolutional manner for a classification task, some of the convolution layers before the dense layer learn a general notion about the detected object. The activations in these inner layers can therefore be exploited for other tasks than the ones they were originally trained for. Based on this observation, weak supervision has been employed for cancerous region detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, surgical tool center localization <ref type="bibr" target="#b4">[5]</ref> and object instance segmentation <ref type="bibr" target="#b15">[16]</ref>.</p><p>Following the same trend, we propose a weakly-supervised approach for surgical tool tracking. First, we train a surgical tool detector on image-level labels. From a class peak response, we learn the whole region boundaries of the surgical tools in the laparoscopic videos. Then, we employ a Convolutional LSTM (ConvLSTM) to learn the spatio-temporal coherence across the surgical video frames. Without any spatial appearance and motion cue, the ConvLSTM is naturally able to learn the tools' spatio-temporal positions for tracking. To the best of our knowledge, this is the first study that builds a complete deep learning tracking model for endoscopic surgery using weak supervision and also the first study that evaluates surgical tool tracking performance on MOT metrics. Finally, we evaluate our approach on the largest public endoscopic video dataset to date, Cholec80, which is fully annotated with binary presence information for 7 tools and of which 5 videos have been annotated with bounding box information for testing.</p><p>The remaining of this paper presents a review of related literature (sect. 2), our proposed methods (sect. 3) and implementation details (sect. 4), followed by a comparative discussion of our results (sect. 5) and a conclusion (sect. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the past, while many works have focused on surgical tools detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, less have explored their localization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref> and tracking <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref> from video data only. This may be because most localization tasks, and tracking by extension, have been traditionally approached with fully supervised methods that require spatially annotated datasets <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surgical Tools Detection, Localization and Tracking:</head><p>We review some of the endoscopic tool detection, localization and tracking approaches from the literature, which are mostly concentrated in retinal microsurgery <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, and laparoscopic surgery <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>. In most cases, the localization and/or tracking models rely on a fully supervised object detector <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4]</ref>. Sometimes, a unified detector-tracker framework is used <ref type="bibr" target="#b5">[6]</ref>. Whereas some tracking models use an optical flow tracker <ref type="bibr" target="#b19">[20]</ref>, others have casted tracking as an energy minimization function using a gradient-based tracker <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, density estimation <ref type="bibr" target="#b5">[6]</ref>, or an image similarity measure based on weighted mutual information <ref type="bibr" target="#b2">[3]</ref>. From another perspective, works in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> model the tool articulation parts and estimate the instruments locations by either a non-maximum suppression technique <ref type="bibr" target="#b3">[4]</ref> or by template tracking <ref type="bibr" target="#b16">[17]</ref>. In <ref type="bibr" target="#b7">[8]</ref>, a fully supervised region-based convolutional network is employed to detect and localize surgical tools in laparoscopic videos. While the model is able to detect tool presence and localize beyond the tool tips, it requires bounding box annotations for training. Also, the approach does not take into account the temporal consistency over time. The experiments are carried out on selected images from surgical videos in the m2cai-tool-locations dataset. In all the above-reviewed literature, the object detection and localization models, and, by extension, the trackers, are fully supervised on a spatially annotated dataset for position estimation.</p><p>Weak Supervision: Considering the difficulty to annotate datasets spatially, <ref type="bibr" target="#b4">[5]</ref> localized surgical tools on a whole laparoscopic video sequence using a weaklysupervised Fully Convolutional Networks (FCN) model. The localization is limited to the center pixels of the tools. Other interesting applications of weak supervision in medical imaging are seen in the segmentation of cancerous regions in histopathological images <ref type="bibr" target="#b14">[15]</ref> and in the detection of the region of interest (ROI) in chest X-rays and mammograms <ref type="bibr" target="#b13">[14]</ref>. The aforementioned weakly-supervised approaches do not exploit the temporal coherence of a video sequence and do not perform tracking.</p><p>Temporal Coherence: An effort to utilize the temporal interconnection of video frames in deep learning approaches is presented in <ref type="bibr" target="#b20">[21]</ref>, where 3D object detection and motion forecasting are integrated to track moving objects. The core idea of this approach is the modeling of temporal coherence using early and late fusion in CNNs. However, the decisions on the birth/death of an object track are hard-coded by the aggregation of past, current and future predictions. A unified approach for processing temporal streams of images is also presented in <ref type="bibr" target="#b21">[22]</ref>, where ConvLSTM are injected in between convolution layers to refine feature map and propagate frame-level information across time. While these models exploit temporal-coherence of a video sequence, the approaches are all fully supervised. Temporal coherence has also been used to improve binary tool presence detection by adding an LSTM to the output of ResNet-50 <ref type="bibr" target="#b12">[13]</ref> and to the output of ensembled CNN architectures <ref type="bibr" target="#b6">[7]</ref>. These approaches are however not constructed for localization and tracking.</p><p>Constructing upon <ref type="bibr" target="#b4">[5]</ref>, we implement a weakly-supervised approach to train a ConvLSTM for surgical tool tracking. The model is trained on image-level binary labels only. Like in <ref type="bibr" target="#b7">[8]</ref>, our model localizes the whole region boundaries of the tools beyond their center points. We leverage the ConvLSTM's spatio-temporal ability to learn the surgical tool trajectories across the frames without requiring more than the image-level class labels. The ConvLSTM does not only improve presence detection, but also refines and propagates localization feature map across time and helps to track over occlusions. Its internal gating mechanism enables it to naturally handle the birth, propagation, and death of tool tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>Our models are built on the ResNet-18 architecture <ref type="bibr" target="#b10">[11]</ref>, which is popular for its excellent performance on object detection. We present below the architectures used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">FCN Baseline</head><p>Detector: To build a tracker for surgical tools, we first reproduce the FCN model in <ref type="bibr" target="#b4">[5]</ref> (illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>) with similar accuracy on surgical tool presence detection and spatial localization. The general configuration of the FCN baseline model is R + C, where R represents a modified ResNet-18 network and C a convolution layer. C is a 1x1 7-channel convolution layer that acts as the localization heat map (Lh-map). It replaces the FC-layer of R. The strides of the last two blocks of ResNet-18 are adjusted from 2 to 1 pixel to obtain an Lh-map with higher resolution. The FCN model is trained only on tool presence binary labels. Taking an RGB input image, the R layer extracts spatial feature maps and the C layer uses 7 convolution filters to convolve these maps into a 7-channel Lh-map. Each channel is by design constrained to learn and localize a distinct tool type out of the 7 tools present in the considered laparoscopic procedure. With wildcat spatial pooling <ref type="bibr" target="#b22">[23]</ref>, we transform the Lh-map into a 1 × 7 vector of class-wise confidence values indicating the probability of a tool being present or absent. The positive classes are selected by a threshold of 0.5.</p><p>Apart from the single-channel map (single-map) model (R + C M 1 ) discussed above, a multiple channel map (multi-map <ref type="bibr" target="#b22">[23]</ref>) variant (R + C M 4 ) is built by using a convolution layer with m × 7 channels followed by an average pooling over each consecutive group of m channels to give the final 7 channels. We retain m = 4 as used in <ref type="bibr" target="#b4">[5]</ref>. Both variants are trained on the normal images and on patch masked images (R + C M 1 mask , R + C M 4 mask ). During patch masking <ref type="bibr" target="#b8">[9]</ref>, random patches are created on the original images and their pixel values replaced with the mean pixel value of the entire training dataset. According to <ref type="bibr" target="#b8">[9]</ref>, this enables the network to learn meticulously the necessary details of the object of interest. We now have a total of four variants of the FCN models by pairing the two models based on single-or multi-map with the two models based on maskedor unmasked-input.</p><p>Tracker: We leverage the separation of the tool type in the 7-channel Lh-map from the FCN detector to build a baseline model for tool tracking. For localization, the raw Lh-map is resized to the original input image size by bilinear interpolation. Then, with a disc structuring element of size 12, we perform a morphological closing on the resized map to fill small holes in the image. On each channel of the Lh-map, a segmentation mask is extracted from the connected component around the pixel with maximum value using Otsu automatic thresholding <ref type="bibr" target="#b23">[24]</ref>. A bounding box is then drawn over the mask to extract the tool location coordinates.</p><p>For tracking, the Intersection over Union (IoU) of the bounding boxes between the current frame F t and the previous frame F t−1 is computed for each detected tool. Tools detected at time t are included in the previous trajectories if the IoU with previous detections at time t−1 is at least 0.5. In the case of multiple instances of the same tool, the closest tool instance compared to the detections in F t−1 is selected. Unmatched tools are discarded as false detections, while untracked tools are discarded as dead tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">ConvLSTM Tracker</head><p>The aforementioned FCN baseline tracker is trained on images and does not utilize the temporal cues of video data. This may be a problem when a tool's motion becomes irregular beyond what an IoU of 0.5 with the previous frame can capture, since the tracking algorithm is hard-coded. Knowing that object motion is encoded in temporal information <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, we propose to integrate a temporal model in the previous FCN framework, in a manner that still allows for weakly-supervised training. This results in an elegant end-to-end tracking method that can model the spatio-temporal motion of the tools and also adapt to the various types of motion appearing in a video.</p><p>As temporal model, we propose to use a recurrent neural network (RNN), with the aim to determine the current position of each tool from the input feature map along with information from prior images captured in RNN's state. In designing this architecture, it is necessary to ensure that the overall network can still retain spatio-temporal information for each tool when being trained in a weakly-supervised manner on binary presence data, namely that the localization information per tool is not lost but remains the key information used for predicting the binary presence.</p><p>Using a fully convolutional architecture is key in this regard. We therefore employ a ConvLSTM unit for its ability to learn the spatio-temporal dependencies of the localization heat maps. The ConvLSTM achieve this by using a convolution kernel whose receptive field considers temporal information. Compared to stacking a regular LSTM, the spatial relationships are maintained. And unlike using a simple convolution layer, the ConvLSTM takes into account the features from the previous frames, thereby enforcing consistency across time. At the level of the ConvLSTM, the localization heat maps from each tool remain independent: in this final part of the network, information is indeed not shared across maps to retain the spatial information for each tool. Our ConvLSTM Tracker is constructed by adding a ConvLSTM unit to the FCN baseline detector, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We have explored several variants of the architecture, described further below. By naturally smoothing out the class peak activations using temporal information, the ConvLSTM replaces the IoU-based selection from the baseline tracker and naturally handles the birth and death of tracks for each tool.</p><p>In practice, we construct the ConvLSTM trackers using the baseline model R + C M 1 mask , which has the best performance across the 3 tasks (as shown in Tables 1-3). We select the single-map architecture, as the multi-map architecture is more complex and shows no better performance both in <ref type="bibr" target="#b4">[5]</ref> and in our baseline spatial experiments (as shown in <ref type="table" target="#tab_1">Tables 2 &amp; 3)</ref>. Like in ResNet (R), which contains skip connections between its layers, we include skip connections in the C and CL layers for their efficiency in training large networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>To perform weakly supervised training on image-level labels y, we transform the Lh-maps (see <ref type="figure" target="#fig_1">Figure 2</ref>) into class-wise probabilitiesŷ using wildcat pooling <ref type="bibr" target="#b22">[23]</ref>. We then learn a weighted cross-entropy loss function L for multi-label classification:</p><formula xml:id="formula_0">L ←− C c=1 −1 N [W c y c log(σ(ŷ c )) + (1 − y c ) log(1 − σ(ŷ c ))] ,<label>(1)</label></formula><p>where y c andŷ c are respectively the ground truth and predicted tool presence for class c, σ is the sigmoid function, and W c the weight for class c. The effect of the class weights W c in this loss function is that W c &gt; 1 decreases false negatives (FN) while W c &lt; 1 decreases false positives (FP). With this, we counteract the polarizing effect of class imbalance by reducing FN for less frequent tools and reducing FP for dominant tools. The W c is calculated as in Equation 2, where m is the median frequency of all tools in the train set and F c is the frequency of the tools in class c:</p><formula xml:id="formula_1">W c ←− m F c .<label>(2)</label></formula><p>We propose three different configurations with similar architectures:</p><formula xml:id="formula_2">1. R + C + CL 2. R + CL + C 3. R + CL</formula><p>where R, C and CL are ResNet, Convolution and ConvLSTM respectively.</p><p>R + C + CL Configuration: In this configuration, illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the ConvLSTM receives spatial input features from the C layer, refines them with temporal information and outputs spatio-temporal Lh-maps. The motivation for adding the ConvLSTM unit immediately after the baseline FCN (R + C) is to refine the spatial Lh-maps with spatio-temporal information. This helps to smooth the class peak activations as well as the shape and size of the tools segmentation masks. It is important to note that the localization process is performed on the spatio-temporal Lh-maps.</p><p>R + CL + C Configuration: With the ConvLSTM unit added before the last Convolution layer of the baseline FCN, it refines the R spatial features with spatiotemporal information before localization by C. This guides the model in choosing relevant features based on temporal information across the video frames. By doing so, the receptive fields of C become aware of the temporal information. It is also important to note that the localization is on the C layer, which receives a spatiotemporal feature map and outputs a spatial Lh-map. This model is expected to be more robust to occlusion and noise.</p><p>R + CL Configuration: The last variant replaces the C layer of the FCN baseline detector with a ConvLSTM (CL) layer. Owing to its internal convolution process, the CL layer takes over the task of localization from the C layer as well as the refinement of the feature map with temporal information. This results in a less complex architecture with the localization process on the CL layer that produces spatio-temporal Lh-maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The dataset used in this experiment is Cholec80 <ref type="bibr" target="#b0">[1]</ref>. It consists of 80 videos of cholecystectomy surgeries aimed at removing the gallbladder laparoscopically, monitored through an endoscope. The videos are recorded at the frame rate of 25f ps and downsampled to 1f ps at which the tool presence binary annotations are generated. While most of the videos are recorded at a resolution of 854 × 480 pixels, a few are 1920 × 1080 pixels with the same aspect ratio. For uniformity, all frames are resized to 854 × 480 pixels in our experiment. For the tool detection task, the dataset is split into 40, 10 and 30 videos for training, validation, and testing respectively. For localization and tracking evaluation, we use 5 videos from the test set annotated with tool centers and bounding boxes around the tool tips. The tool shafts are excluded, following common practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>All the models presented in this paper are trained by transfer learning. The FCN baseline models are trained for 160 epochs with stepwise decaying learning rates starting at the initial values of 1e −1 and 1e −3 for the R and the C layers respectively. We use the different learning rates to strike a learning balance for R that has been pretrained on ImageNet and C that is trained from scratch. The ConvLSTM and the baseline models have the same backbone feature extractor which converges after 160 epochs. For spatial-temporal refinement, C and CL of the ConvLSTM models are trained up to 120 epochs with an initial learning rate of 1e −3 that decays exponentially. During this period, the R layer is frozen for fair comparison with the baseline. The training input images are masked by 16 × 16 patches selected randomly at a probability of 0.5. This patch masking, together with rotation and horizontal flipping of the images, are the 3 data augmentation styles employed in training the R + CL model. In finetuning the ConvLSTM layer, the dataset augmentation is limited to image patch masking to reduce the training time, since the video dataset already contains lots of variability in the images.</p><p>All the models are trained for multi-label classification. The optimized loss function L is the weighted cross-entropy with logits presented in Equation 1. An L 2 norm with a weight decay constant of 1e −4 for the baseline FCN and 1e −5 for the ConvLSTM models is applied to regularize the optimization. The models are trained with the momentum optimizer (initial momentum µ = 0.9) and using truncated back-propagation. Owing to our GPU memory constraints and large input dimension, the network is trained with a maximum batch size of 16 and the ConvLSTM models are unrolled for 16 timesteps. We also propagate the ConvLSTM states between batches. To maintain continuity in a video, we initialize the ConvLSTM input states of every batch with the output states of the immediate previous batch. States propagation is performed during testing as well. Our model network is implemented in Tensorflow using TFRecords to build the dataset input pipeline and trained on GeForce GTX 1080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Presence Detection Results</head><p>To quantify the tool presence detection results, we use average precision (AP), which is defined as the area under the precision-recall curve. Comparing the AP of our model with the baseline (as presented in <ref type="table" target="#tab_0">Table 1</ref>) shows that temporal information is helpful in improving the tool presence detection by over 5.0%. The performance improvement can also be seen across the tools. This suggests that the temporal information helps the detection of tools under occlusion and noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Spatial Localization Results</head><p>To quantify the network's ability to localize the distinct tools in various frames, we compute the bounding box IoUs between the detected tools and the groundtruths. This performance measure does not take into account the temporal consistency of the tools across the frames. However, a localization is only considered to be correct if and only if the IoU ≥ 0.5. Note that this is stricter than the center-in-bounding box localization metric in <ref type="bibr" target="#b4">[5]</ref>, which does not takes the IoU into consideration. The localization results compared with our baseline model is presented in <ref type="table" target="#tab_1">Table 2</ref>. From this result, our model improved the spatial localization of five out of the seven surgical tools: grasper, bipolar, hook, scissors and specimen bag. For the irrigator and the clipper, for which the ConvLSTM models do not have the best performance, the performance is comparable. Generally, the ConvLSTM shows a good performance on this metric by improving the mean accuracy by 13.9%, illustrating the benefits of using temporal information during training. Also, all the ConvLSTM models outperform all the baseline models on mean spatial localization accuracy. This shows that the temporal data modeling can help in understanding the full spatial boundaries of moving objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Motion Tracking Results</head><p>For the tracking performance evaluation, we adopted the widely used CLEAR MOT metrics <ref type="bibr" target="#b25">[26]</ref>: multiple objects tracking precision (MOTP) and multiple objects tracking accuracy (MOTA). MOTP, a measure of the localization precision, gives the average overlap between all the correctly matched hypotheses and their corresponding targets for a given IoU threshold (Θ).</p><formula xml:id="formula_3">M OT P = t,i D t,i t C t ,<label>(3)</label></formula><p>where D t,i is the bounding box IoU of the tracked target i with the groundtruth, C t is the number of matches in frame t. The value typically ranges between [Θ%, 100]. On the other hand, MOTA shows the tracker's ability at keeping consistent trajectories. It evaluates the effectiveness of the tracker from three errors, namely FP, FN and identity switches (IDSW) in respect the number of groundtruth objects (GT) as in equation 4:</p><formula xml:id="formula_4">M OT A = 1 − t F P t + F N t + IDSW t t GT t .<label>(4)</label></formula><p>The score, which usually ranges between (-∞, 100], can be negative in cases where the number of errors made by the tracker exceeds the number of all objects in the scene. Refer to <ref type="bibr" target="#b25">[26]</ref> for more details on the MOT metrics. The tracking results across varying Θ in comparison with our baseline models are presented in <ref type="table" target="#tab_2">Table 3</ref>. Our approach improved the baseline performance significantly. The results show that with comparable MOTP, ConvLSTM can improve the MOTA baseline by 11.7% at Θ = 0.3, 19.8% at Θ = 0.5 and 3.7% at a strict Θ = 0.7. Generally, the ConvLSTM shows its ability to learn a smoother trajectory by outperforming all the baseline in both mean MOTP and mean MOTA significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Results</head><p>The qualitative results in <ref type="figure" target="#fig_2">Figure 3</ref> show visually how the ConvLSTM is able to leverage the temporal coherence for tracking and localization for the 7 tools. From the positioning of the bounding boxes around the tools, it can be seen that the ConvLSTM model learns the region boundaries better than the baseline. The Lhmaps show that the ConvLSTM helps to smooth the localization and approximates the shape and size of the tools in each image. The overlay shows that it satisfactorily learns a trajectory close to the ground truth. A supplementary video that further demonstrates the qualitative performance of our approach can be found here: https://youtu.be/vnMwlS5tvHE. Our experiments also show that the ConvLSTM model trained on videos at 1fps can generalize to unlabelled videos at 25fps, making it unconstrained by the fps, as can be seen here: https://youtu.be/SNhd1yzOe50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion</head><p>The evaluation presented in this paper shows the positive contribution of the ConvLSTMs in modeling temporal data during weakly-supervised training for surgical tool tracking in laparoscopic videos. The most notable improvement is seen in the R + CL + C variant, which has the best results both in localization and in tracking. We believe that this is due to the fact that in this configuration, CL refines the feature map from R with temporal considerations before they are localized separately by C. This is more robust than in R + C + CL and R + CL, where the temporal refinement at the end of the pipeline may dilute the localization information and output a map with a slightly different semantic. In the R + CL + C variant, the temporal information across the video frames guides the model in choosing relevant features for the Lh-maps.</p><p>In the qualitative results, we observe failure cases in different situations. First, due to the nature of the model, tools are missed when multiple instances of the same class are present. It would be interesting to see if the low activations in the Lh-maps could be exploited in order to estimate the number of instances for each class. The qualitative results also show that the models fail to detect a tool when less than <ref type="bibr">1 5</ref> th of its tip is visible. We also observe that our models only localize the tool's tip, not its shaft, likely because shafts are similar for all tools and cannot be easily captured by a weakly-supervised approach relying on binary presence.</p><p>From the qualitative results, we however notice that the Lh-maps produce a weak segmentation of the tool tips, suggesting that this approach could be extended to segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper aims at tracking tools in laparoscopic surgical videos without using any spatial annotation during training. A weakly-supervised Convolutional LSTM approach that relies solely on binary tool presence information is proposed. First, we build a baseline tracker by performing a one-to-one data association on the localization results generated by the FCN proposed in <ref type="bibr" target="#b4">[5]</ref>. Then, we propose a fully convolutional spatio-temporal model for end-to-end tracking that is suitable for weakly-supervised training. It relies on a ConvLSTM that leverages the temporal information present in the video to smooth the class peak activations and better detect the presence of tools, optimize their spatial localization and smooth their trajectory over time. This approach is evaluated on the Cholec80 dataset and yields 12.6% overall improvement on MOTA, 13.9% improvement on localization mean accuracy and 5% improvement on tool presence detection mAP. The quantitative and qualitative results also suggest that the proposed approach could be integrated into a surgical video labeling software to initialize the tool annotations, such as their bounding boxes and segmentation masks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Architecture of FCN baseline model (R + C M 1 mask variant).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>The ConvLSTM tracker architecture with the R + C + CL configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Qualitative results showing the localization and tracking performance of the baseline and ConvLSTM models for the 7 tools. For each tool, we present a comparison of the detected bounding box (cyan in colour) with the ground truth (dotted yellow box), the Lh-map, and the overlay of the segmented mask with the original image (best seen in colour).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Tool presence detection average precision (AP) for the evaluated models.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Grasper</cell><cell>Bipolar</cell><cell>Hook</cell><cell>Scissor</cell><cell>Clipper</cell><cell>Irrigator</cell><cell>Specimen</cell><cell>Bag</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell>R + C M 1 R + C M 1 mask R + C M 4 R + C M 4 mask</cell><cell>96.7 99.8 95.9 99.6</cell><cell>91.9 92.6 89.4 90.9</cell><cell>99.4 99.8 99.5 99.8</cell><cell>50.6 85.1 69.3 48.5</cell><cell>80.3 96.9 85.4 88.5</cell><cell>85.2 60.9 89.5 66.2</cell><cell cols="2">88.3 78.6 87.1 91.0</cell><cell>84.6 87.7 87.9 83.6</cell></row><row><cell>Ours</cell><cell>R + C + CL R + CL + C R + CL</cell><cell cols="5">99.7 99.8 95.6 99.9 95.6 99.8 99.5 93.8 99.9 90.3 97.5 86.9 97.5 76.1 97.1</cell><cell>74.7 77.4 65.1</cell><cell cols="2">96.1 93.9 74.0</cell><cell>92.9 91.4 88.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Localization accuracy of tools detected at IoU ≥ 0.5 for the evaluated models.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Grasper</cell><cell>Bipolar</cell><cell>Hook</cell><cell>Scissor</cell><cell>Clipper</cell><cell>Irrigator</cell><cell>Specimen</cell><cell>Bag</cell><cell>Mean</cell></row><row><cell>Baseline</cell><cell>R + C M 1 R + C M 1 mask R + C M 4 R + C M 4 mask</cell><cell>05.9 15.5 05.0 08.7</cell><cell>20.5 10.1 11.5 0.01</cell><cell>34.7 27.8 15.5 25.6</cell><cell>03.5 20.0 25.1 20.0</cell><cell>06.4 13.3 8.7 20.0</cell><cell>55.1 53.7 42.5 49.0</cell><cell cols="2">44.4 06.4 14.8 02.2</cell><cell>24.3 21.0 17.6 17.9</cell></row><row><cell>Ours</cell><cell>R + C + CL R + CL + C R + CL</cell><cell>33.8 54.5 42.5</cell><cell>20.8 14.6 08.0</cell><cell>41.9 50.0 44.4</cell><cell>21.1 23.2 25.3</cell><cell>12.6 11.8 14.0</cell><cell>52.1 53.6 53.5</cell><cell cols="2">23.8 60.1 41.7</cell><cell>29.3 38.2 32.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Tracking performance of the evaluated models.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Θ = 0.3</cell><cell cols="2">Θ = 0.5</cell><cell cols="2">Θ = 0.7</cell><cell cols="2">Mean</cell></row><row><cell></cell><cell>Model</cell><cell cols="8">MOTP MOTA MOTP MOTA MOTP MOTA MOTP MOTA</cell></row><row><cell>Baseline</cell><cell cols="2">R + C M 1 R + C M 1 mask 49.9 58.1 R + C M 4 46.6 R + C M 4 mask 48.3</cell><cell>29.8 47.9 29.6 40.4</cell><cell>66.6 61.2 60.4 61.0</cell><cell>19.3 21.2 09.6 15.3</cell><cell>77.3 75.3 75.4 75.8</cell><cell>05.3 02.7 -00.3 01.9</cell><cell>67.3 62.1 60.8 61.7</cell><cell>18.1 23.9 13.1 19.2</cell></row><row><cell></cell><cell>R + C + CL</cell><cell>58.0</cell><cell>46.4</cell><cell>65.9</cell><cell>29.4</cell><cell>77.4</cell><cell>03.2</cell><cell>67.1</cell><cell>26.3</cell></row><row><cell>Ours</cell><cell>R + CL + C R + CL</cell><cell>59.0 54.4</cell><cell>59.6 47.7</cell><cell>65.9 63.3</cell><cell>41.0 26.1</cell><cell>77.3 76.7</cell><cell>09.0 00.3</cell><cell>67.4 64.8</cell><cell>36.5 24.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by French state funds managed within the Investissements d'Avenir program by BPI France (project CONDOR) and by the ANR (references ANR-11-LABX-0004 and ANR-10-IAHU-02).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endonet: A deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deepphase: Surgical phase recognition in cataracts videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giataganas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Visual tracking of surgical tools for proximity detection in retinal surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IPCAI</publisher>
			<biblScope unit="page" from="55" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast part-based classification for instrument detection in minimally invasive surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="692" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Weakly-supervised learning for tool localization in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05573</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unified detection and tracking in retinal microsurgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehlbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jedynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monitoring tool usage in surgery videos using boosted convolutional and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="203" to="218" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tool detection and operative skill assessment in surgical videos using region-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jopling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Azagury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
			<biblScope unit="page" from="691" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hide-and-seek: Forcing a network to be meticulous for weaklysupervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online multi-target tracking using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning latent temporal connectionism of deep residual visual abstractions for identifying surgical tools in laparoscopy procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sathish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-transfer learning for weakly supervised lesion localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Constrained deep weak supervision for histopathology image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2376" to="2388" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Weakly supervised instance segmentation using class peak response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00880</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time localization of articulated surgical instruments in retinal microsurgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Di San Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alsheakhali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="82" to="100" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Data-driven visual tracking in retinal microsurgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="568" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vision-based and marker-less surgical tool detection and tracking: a review of the literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="633" to="654" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Episode classification for the analysis of tissue/instrument interaction with multiple visual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Darzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3569" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mobile video object detection with temporally-aware feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Residual convolutional lstm for tweet count prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the The Web Conference 2018 on The Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1309" to="1316" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
