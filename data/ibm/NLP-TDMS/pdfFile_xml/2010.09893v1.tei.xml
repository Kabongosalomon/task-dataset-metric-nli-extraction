<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LT-GAN: Self-Supervised GAN with Latent Transformation Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parth</forename><surname>Patel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Birla Institute of Technology &amp; Science</orgName>
								<address>
									<country>Pilani India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
							<email>nupkumar@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media and Data Science Research Lab</orgName>
								<address>
									<country>Adobe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
							<email>msingh@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media and Data Science Research Lab</orgName>
								<address>
									<country>Adobe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Media and Data Science Research Lab</orgName>
								<address>
									<country>Adobe</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LT-GAN: Self-Supervised GAN with Latent Transformation Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) coupled with self-supervised tasks, have shown promising results in unconditional and semi-supervised image generation. We propose a self-supervised approach (LT-GAN) to improve the generation quality and diversity of images by estimating the GAN-induced transformation (i.e. transformation induced in the generated images by perturbing the latent space of generator). Specifically, given two pairs of images where each pair comprises of a generated image and its transformed version, the self-supervision task aims to identify whether the latent transformation applied in the given pair is same to that of the other pair. Hence, this auxiliary loss encourages the generator to produce images that are distinguishable by the auxiliary network, which in turn promotes the synthesis of semantically consistent images with respect to latent transformations. We show the efficacy of this pretext task by improving the image generation quality in terms of FID on state-of-the-art models for both conditional and unconditional settings on CIFAR-10, CelebA-HQ and ImageNet datasets. Moreover, we empirically show that LT-GAN helps in improving controlled image editing for CelebA-HQ and ImageNet over baseline models. We experimentally demonstrate that our proposed LT self-supervision task can be effectively combined with other state-of-the-art training techniques for added benefits. Consequently, we show that our approach achieves the new state-of-the-art FID score of 9.8 on conditional CIFAR-10 image generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) have become a popular class of generative models as they have shown im-* Authors contributed equally † Work done during Adobe MDSR internship pressive capacity in modelling complex data distributions, such as images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref> and audio <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. GANs consist of a generator and a discriminator network with competing goals: the generator's objective is to generate realistic samples to fool the discriminator and the discriminator's objective is to distinguish between the real samples and the fake ones synthesized by the generator. The generator learns a mapping from a latent distribution to the data distribution via adversarial training with the discriminator. Despite the significant progress of GANs, there lacks enough understanding of how different semantics are encoded in the latent space of generator. It has been observed that in a well trained GAN, semantics encoded in the latent space are disentangled to some extent which makes it possible to perform controlled image editing <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Another class of unsupervised learning called selfsupervision has demonstrated promising results on a diverse set of computer vision tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b13">14]</ref>. This training paradigm usually involves designing an auxiliary task with pseudo-labels derived from the structural information of the data. Self-supervised learning has also been used in collaboration with adversarial training in GANs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> to improve training stability and unconditional/semisupervised image generation quality <ref type="bibr" target="#b26">[27]</ref>. Generally, the role of self-supervised methods in GANs is to regularize the discriminator which in turn guides the generator to produce images with more informed geometric or global structure. For example, SS-GAN <ref type="bibr" target="#b3">[4]</ref> introduced an auxiliary task of predicting the degree of rotation in the input image to the discriminator during GAN training.</p><p>The authors of <ref type="bibr" target="#b50">[51]</ref> propose to learn the unsupervised feature representation by encoding the input data transformation rather than data itself <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18]</ref>. Specifically, in AET <ref type="bibr" target="#b50">[51]</ref>, image transformation operators are sampled, and the objective is to estimate the transformation given the feature representation of the original and the transformed images. This framework of unsupervised feature learning en-courages encoding of the essential visual structure of the transformation so that the transformation can be predicted. <ref type="bibr" target="#b50">[51]</ref> has shown promising results on various standard unsupervised visual downstream tasks. Inspired by this approach <ref type="bibr" target="#b50">[51]</ref>, in the domain of GAN, we propose a binary self-supervised task that aims to detect if the GAN-induced transformation (as described in <ref type="bibr" target="#b50">[51]</ref>) applied on two generated images is same. The key idea in this transformation prediction task is to promote useful representation learning as the features would have to encode sufficient information about visual structures of both the original and transformed images for auxiliary training. Note that this is in contrast with the previous line of works <ref type="bibr" target="#b49">[50]</ref> that augments the input data to the discriminator using a fixed set of static transformations and penalizes the sensitivity of discriminatory features to these transformations.</p><p>In this work, we propose a self-supervised task called Latent Transformation (LT) for improving the quality of image synthesis and latent space semantics. Previous methods make use of limited predetermined augmentation or transformations (such as rotation, translation) to define the selfsupervised loss. However, we utilize GAN-induced transformations as described in <ref type="bibr" target="#b50">[51]</ref> to define our pretext task. In contrast to earlier works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b18">19]</ref> that adds a self-supervised loss to the discriminator, our auxiliary task promotes the generator to synthesize images such that the GAN-induced transformations are distinguishable at the feature representation level.</p><p>Our main contributions in this paper are the following :</p><p>• We propose a novel self-supervised task of identifying GAN induced latent transformations to optimize the generator in collaboration with the adversarial training of GANs.</p><p>• We demonstrate the efficacy of our proposed LT-GAN approach on several standard datasets with various architectures by improving the conditional and unconditional state-of-the-art image generation performance on Fréchet inception distance (FID) <ref type="bibr" target="#b16">[17]</ref> metric.</p><p>• We empirically show that our LT-GAN model improves controlled image editing using existing semantic editing frameworks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37]</ref> over baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Self-Supervised learning is an unsupervised learning framework that seeks to leverage supervisory signals from the structural information present in the data by defining auxiliary pretext tasks. Self-supervision techniques have shown a huge potential in a diverse set of research tasks, ranging from robotics to computer vision <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref>. In visual domain, a pretext task is designed with labels derived from the images themselves that help in learn-ing rich feature representation useful for downstream tasks <ref type="bibr" target="#b13">[14]</ref>. Some of the earliest effort <ref type="bibr" target="#b7">[8]</ref> in this area utilize relative position prediction of image patches. Inspired by this task's relation to prediction of context in images, the authors of <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> use a pretext task of predicting the permutation in a image with shuffled patches. <ref type="bibr" target="#b10">[11]</ref> used the surrogate objective of predicting the angle of rotation for unlabelled image. The task of in-painting <ref type="bibr" target="#b35">[36]</ref> and image colorization <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> have also been used as auxiliary tasks in the self-supervised learning framework. In contrast with utilizing the geometric and structural in-variances, <ref type="bibr" target="#b2">[3]</ref> uses the task of predicting the cluster assignment in feature space as pseudo labels for unlabeled data. Also, <ref type="bibr" target="#b31">[32]</ref> obtains supervision signal by counting the visual primitives present in the patches of images. Along the lines of transformation prediction task like <ref type="bibr" target="#b10">[11]</ref>, AET <ref type="bibr" target="#b50">[51]</ref> introduces a surrogate task of reconstruction of input data transformations to learn unsupervised feature representations. Inspired by this work, our approach LT-GAN proposes the auxiliary task of estimating GAN-induced transformations. We hypothesize that it would encourage the generator to synthesize semantically consistent image transformations with respect to similar latent space perturbations.</p><p>GANs with self-supervised auxiliary tasks Recently, self-supervised learning has been coupled with adversarial training to improve the training stability and image quality of GANs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref>. The motivation behind adding self-supervised loss to GAN training is to equip the feature representations to recognize the global structures present in real data through the pretext tasks. SS-GAN <ref type="bibr" target="#b3">[4]</ref> uses the auxiliary task of image rotation degree classification based on the discriminator features. The authors of <ref type="bibr" target="#b18">[19]</ref> propose to use the pretext task of distinguishing between real images and corrupted real images with GAN training. These corrupted images are created by randomly exchanging pairs of patches in an image's convolutional feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Space Manipulation for semantic editing in</head><p>GANs Conventional approaches of finding interpretable manipulations in GAN latent space compute linear directions corresponding to attribute change by using annotated attributes tags of the images <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref>. <ref type="bibr" target="#b46">[47]</ref> showed this to be even true for pre-trained classifier where interpolation in latent feature space of target and source images leads to interpretable transfer of visual properties from a source image to a target image. To assume control over the image generation process in GANs, work by <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref> propose modifications in architecture and training approach. <ref type="bibr" target="#b33">[34]</ref> allows the generation of images belonging to a certain class and therefore requires access to labels for training the model. <ref type="bibr" target="#b4">[5]</ref> learns disentangled representations by maximizing the mutual information between a subset of the latent variables and the ob-servation which enables the process of finding a posteriori semantic direction. However, work by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b11">12]</ref> shows that the latent space directions corresponding to transformations (such as zoom, scale, shift, brightness) can be computed using the respective augmentation of images on pretrained GAN models. These approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref> lighten the requirement of attribute tagged images for some general image editing tasks and can also serves as a measure for generalization capacity of generative models. The performance of these latent self-supervised trajectories are limited by biases in the training dataset and the model's generalization performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>. Recent advances in GANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref> in generating photo-realistic images have unlocked the potential for content creation and fine tuning modifications <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b0">1]</ref>. <ref type="bibr" target="#b41">[42]</ref> performs semantic face editing (for changing attributes such as age, expression, etc.) on a fixed pre-trained GAN model by using linear subspace projection techniques and thus demonstrating disentanglement of the latent space of pre-trained GANs. We show that our approach LT-GAN improves controlled image editing over baseline models by using existing semantic editing frameworks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first present the standard GAN formulation and terminologies used in the paper. We then introduce our training methodology for LT-GAN that leverages a self-supervised task defined on the latent space of generator to better organize the semantics encoded in the latent space and promote diverse image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generative Adversarial Networks</head><p>Generative adversarial network (GAN) consists of a generator G : z → x and a discriminator D : x → R. G learns a mapping from the latent code z ∈ R d sampled from a prior distribution p(z) to an observation x ∈ R n e.g. a natural image manifold. The role of discriminator D is to differentiate between samples from real data distribution p(x) and the ones generated from G. The standard training of GAN involves minimizing the following loss function in an alternating fashion:</p><formula xml:id="formula_0">L D : −E x∼p(x) [log(D(x))] − E x∼p(z) [1 − log(D(G(z)))] L G : −E z∼p(z) [log(D(G(z)))]</formula><p>(1) This loss function is commonly known as non-saturating loss and was originally proposed in <ref type="bibr" target="#b12">[13]</ref>. A notable modification of the loss for improved training is the hinge loss <ref type="bibr" target="#b42">[43]</ref>:</p><formula xml:id="formula_1">L D : E x∼p(x) [1 − D(x)] + + E x∼p(z) [1 + D(G(z))] + L G : −E z∼p(z) [D(G(z))]</formula><p>where [y] + = max(0, y)</p><p>(2) Here, latent code z is usually sampled from a normal distribution and for each step of generator update, discriminator is updated for d step times. A common issue with GANs is its instability during training that generally requires stabilization techniques <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>. In this work, we use the widely accepted practice of spectral normalization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref> for stable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Transformation GAN (LT-GAN)</head><p>One of the attractive use-cases of GAN which has recently received significant attention is controlled image synthesis by latent space manipulation. We introduce a selfsupervision task on the generator for improved steerability in latent space by leveraging GAN-induced transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN-induced Transformation</head><p>Give a latent code z sampled from a prior distribution p(z) and the corresponding generated image I = G(z), we define GAN-induced transformation as:</p><formula xml:id="formula_2">T (G(z)) = G(z + ) : ∼ p( )<label>(3)</label></formula><p>For a fixed generator, the transformation T is parametrized by , a perturbation of small magnitude, sampled from a distribution p( ). Applying T to the image I generated from latent code z generates a transformed version of the image T (I). In our self-supervision task, we aim to enforce that when a transformation T parametrized by an is applied to latent codes, the change in original and transformed images are semantically consistent across all generated images (e.g translation, background change).</p><p>LT Self Supervision Let, E : x → E(x) be an encoder network to extract the features of an image. Given a transformation T , the feature representation corresponding to the change between original and transformed image can be written as:</p><formula xml:id="formula_3">f (z, z + ) = E(G(z)) − E((T (G(z))<label>(4)</label></formula><p>Where f captures the change in the image feature and its GAN-induced transformation. We choose to implement it simply as the subtraction of encoder features, though other operations like concatenation are valid choices to explore. In LT self supervision, given f 1 = f (z 1 , z 1 + 1 ) and parameterized by same or different. Specifically, the selfsupervision loss is defined as:</p><formula xml:id="formula_4">f 2 = f (z 2 , z 2 + 2 ) where z 1 , z 2 ∼ p(z),</formula><formula xml:id="formula_5">L A = E z1,z2∼p(z) 1 , 2∼p( ) L A [f (z 1 , z 1 + 1 ), f (z 2 , z 2 + 2 )] , y ss y ss = ( 1 == 2 ) (5)</formula><p>where L is standard binary cross entropy loss and label y ss is 1 if 1 is equal to 2 , otherwise 0. During training with the above self-supervision loss, generator G and the auxiliary network A are updated simultaneously alternating with discriminator updates. Thus, the training objective of LT-GAN is:</p><formula xml:id="formula_6">L G : − E z∼p(z) ∼p( ) [D(G(z)) + D(T (G(z)))] + λ.L A L D : E x∼p(x) [1 − D(x)] + + E z∼p(z) ∼p( ) ([1 + D(G(z))] + + [1 + D(T (G(z)))] + )<label>(6)</label></formula><p>Here, λ denotes the weightage of self-supervision loss in generator. We choose p(z) and p( ) both to be a normal distribution with standard deviation σ z and σ respectively, where σ &lt; σ z . The function f in Eq. 4 is implemented as difference of encoded features and E(G(z)) features are chosen as the intermediate layer activation of the discriminator. Furthermore, in order to balance the min-max training between the generator and the discriminator, we also train the discriminator to predict fake on GAN-induced transformations. An overview of the generator training in LT-GAN is shown in <ref type="figure" target="#fig_0">Fig. 1</ref> and pseudo-code of LT-GAN training is explained in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Latent Transformation GAN (LT-GAN)</head><p>begin Input: Generator, Discriminator and Auxiliary network parameters θ G , θ D and θ A . Batch size 2b, weight of self-supervision loss λ, standard deviation σ of normal distribution p( ), discriminator update steps dstep for each generator update, Adam hyperparemters α, β 1 , β 2 . for number of training iterations do for t :</p><formula xml:id="formula_7">1...dstep do Sample batch x ∼ p data (x) Sample {z (i) , (i) } b i=1 ∼ p(z), p( ) z = {z (i) } b i=1 ∪ {z (i) + (i) } b i=1 L D = [1 − D(x)] + + [1 + D(G(z))] + Update θ D ← Adam(L D , α, β 1 , β 2 ) end Sample z = {z (i) } 2b i=1 ∼ p(z) , 1 , 2 ∼ p( ) = [ 1 , 2 ].repeat(b) repeat along batch dimension Generate images G(z) Generate GAN-induced transformation G(z + ) f (z, z + ) = E(G(z)) − E(G(z + )) shuf f le() = permutation(2b) L A = L A([f (z, z + ), f (z, z + ).shuf f le()]), yss yss = ( == .shuf f le()) L G = −D(G(z)) − D(G(z + )) Update θ A ← Adam(L A , α, β 1 , β 2 ) Update θ G ← Adam((L G + λ.L A ), α, β 1 , β 2 ) end end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>Datasets We validate our proposed self-supervised task on CIFAR-10 <ref type="bibr" target="#b23">[24]</ref>, STL-10 <ref type="bibr" target="#b5">[6]</ref>, CelebA-HQ-128 <ref type="bibr" target="#b21">[22]</ref> and ImageNet-2012 <ref type="bibr" target="#b24">[25]</ref> datasets. CIFAR-10 consists of 60K 32×32 images, belonging to 10 different classes: 50K images for training and 10K for testing. In STL-10, we use 100K unlabelled images for training (resized to 48 × 48) and 8K images for testing. CelebA-HQ-128 (CelebA-HQ) consists of 30K 128×128 face images. We randomly sample 3K images for testing and the rest for training. ImageNet-2012 consists of approximately 1.2 million images which we downsample to 128-128 resolution for our experiments. We use the 50K validation set images of Ima-geNet for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN Architectures and Evaluation</head><p>We use the GAN architecture of BigGAN <ref type="bibr" target="#b1">[2]</ref>, StyleGAN <ref type="bibr" target="#b22">[23]</ref>, SNDC-GAN <ref type="bibr" target="#b28">[29]</ref> with their proposed training techniques as the baseline. We also compare against state-of-the-art training technique CR-GAN <ref type="bibr" target="#b49">[50]</ref>. In the conditional setting, we perform experiments on CIFAR-10 and ImageNet-2012 with BigGAN architecture. In the unconditional setting, we perform experiments on CelebA-HQ-128 with StyleGAN and SNDCGAN, on CIFAR-10 with SNDCGAN and STL-10 with ResNet architecture <ref type="bibr" target="#b28">[29]</ref>.</p><p>We use Fréchet Inception Distance (FID) <ref type="bibr" target="#b16">[17]</ref> as the primary metric for evaluating image quality and diversity. FID has been shown to be more consistent with human evaluation of image quality and also helps in detecting intra-class mode collapse <ref type="bibr" target="#b16">[17]</ref>. We calculate FID between test set images and equal number of generated images for all datasets and report the best FID obtained across 3 runs. We found our methodology to be stable and we show the variance analysis of FID in the supplementary section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training and Implementation Details</head><p>The architecture of the auxiliary network A used for the self supervision task consists of a two-layer fully connected network with ReLU activation at the hidden layer and sigmoid activation at the output. Let the features E(G(z)) extracted from the discriminator network be of shape C ×H × W . The input layer of A is of 2 × C × H × W dimension (since we flatten and concatenate the features) and the hidden layer is of C dimension. The self-supervised task is introduced after n warmup iterations of training using the standard GAN loss to ensure that the generated images and its transformations are closer to natural image manifold. Furthermore, we only experimented with sampling two distinct and repeating along the batch dimension for calculating GAN-induced transformation. We leave exploring the effect of varying the above number and relaxing the strict equality between while calculating self-supervision loss in eq. 5 for the future work. Across all model architectures and datasets, we observe the optimal value of σ to lie in the range of  state-of-the-art GANs based on FID. * denotes our best reproduced result using the implementation of 1 , which is different from the score reported in <ref type="bibr" target="#b49">[50]</ref>. # denotes BigGAN Imagenet implementation of 2 plementary. We use Adam optimizer in all our experiments and spectral normalization (SN) <ref type="bibr" target="#b28">[29]</ref> in the discriminator (except in the case of StyleGAN). Hinge loss is used by default for training (except in case of StyleGAN, which uses nonsaturating loss with R 1 regularization <ref type="bibr" target="#b25">[26]</ref>). We follow the default configuration for all architectures and hence we train till 200k generator steps for CIFAR-10 and STL-10, 100k generator steps for CelebA-HQ on SNDCGAN, 525K generator steps on StyleGAN. For ImageNet, we train the model for 250K steps unless the training collapses.</p><p>In the following sections, we show that our proposed self-supervision task helps in improving FID scores over the baseline models and can be effectively combined with other regularization techniques in GANs, e.g. CR-GAN <ref type="bibr" target="#b49">[50]</ref>, across datasets and model architectures. We empirically show that LT-GAN results in a more steerable and disentangled latent space by performing latent space manipulation on CelebA-HQ and ImageNet datasets. We also compare our approach with the recently proposed self-supervised SS-GAN, which uses a rotation-based auxiliary task <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Unconditional GANs In the unconditional setting, we perform experiments on CIFAR-10 with SNDCGAN <ref type="bibr" target="#b28">[29]</ref> architecture and CelebA-HQ with SNDCGAN and Style-GAN [23] architectures 1 . From <ref type="table">Table 1</ref>, we can see that LT-GAN results in improved FID scores compared to the baseline models. Moreover, we also combine our self-supervision task with the current state of the art training methodology CR-GAN <ref type="bibr" target="#b49">[50]</ref>. Using CR+LT-GAN results in further improvement of FID score on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional GANs</head><p>In the conditional setting, both the generator and the discriminator are provided with the underlying class labels information. We perform experiments on CIFAR-10 and ImageNet datasets 2 using the recent stateof-the-art BigGAN [2] model. We observe that for both datasets our self-supervision task improves the FID score as shown in <ref type="table">Table 1</ref>. We also present experimental results of combining our self-supervision technique with the current state-of-the-art CR-GAN <ref type="bibr" target="#b49">[50]</ref> on CIFAR-10 that further improves the FID score over CR-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Steerability in Latent Space</head><p>In this section, we empirically demonstrate that our proposed self-supervision task helps to learn a more steerable latent space. We analyse models trained on CelebA-HQ dataset using the framework of InterfaceGAN <ref type="bibr" target="#b41">[42]</ref>. On Im-ageNet dataset, we use the methodology proposed in <ref type="bibr" target="#b36">[37]</ref> to show that BigGAN model <ref type="bibr" target="#b1">[2]</ref> trained with our approach LT-GAN helps in finding better edit directions in the latent space corresponding to image transformations like translation, brightness and scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet Dataset</head><p>We analyze the latent space structure of generator trained on ImageNet by finding interpretable directions corresponding to parametrizable continuous factors of variation like translation, zoom and brightness using the framework of <ref type="bibr" target="#b36">[37]</ref>. To this end, authors in <ref type="bibr" target="#b36">[37]</ref> propose a novel reconstruction loss between randomly generated images and the transformed version of these images with varying intensity level (e.g. zoom at various scales) to first determine the latent code corresponding to transformed images. Using this training set of pair of latent codes of original and transformed images, the direction in latent space corresponding to that particular transformation is learned as explained in <ref type="bibr" target="#b36">[37]</ref>. We use this methodology to discover latent space trajectories corresponding to the image transformations: brightness, scale, horizontal shift and vertical shift for BigGAN <ref type="bibr" target="#b1">[2]</ref> conditional model trained on ImageNet dataset. <ref type="figure" target="#fig_2">Fig.2</ref> shows the qualitative comparison of brightness and vertical shift direction vectors between baseline BigGAN and LT-BigGAN. We can see in the figure that our approach results in smoother and more meaningful transformations in the image space while preserving the content of the image and avoiding distortions at the extremes. More qualitative comparison on latent space steerability including horizontal shift and zoom is shown in the supplementary.</p><p>CelebA-HQ Dataset InterfaceGAN <ref type="bibr" target="#b41">[42]</ref> provides a framework to find the interpretable semantic directions encoded in the latent space of face synthesis GAN models. Using it, we discover directions in the latent space to smoothly vary facial attributes, namely age, gender, smile expression and eyeglasses. We use the following procedure as proposed in <ref type="bibr" target="#b41">[42]</ref> to discover facial attribute boundaries for StyleGAN and SNDCGAN architectures:</p><p>• Randomly generate 500K images and use a ResNet50 facial attribute detector to predict the value of each binary facial attribute for the generated images. For each binary attribute, sort the list of 500k images based on the predicted value of the attribute and collect top 10k and bottom 10k images. Out of these 20k images, randomly sample 14k images to use as the training set.</p><p>• For each attribute, train a linear SVM using the above collected 14k images to identify the value of the attribute (i.e. 0/1) given the latent code that was used to generate the image. The trained SVM represents a hyperplane that serves as a boundary in the latent space separating the two (-ve/+ve) classes of the binary attribute.</p><p>We report the accuracy of each of the trained SVMs on remaining set of images (i.e. 480k images). A higher SVM accuracy indicates a more steerable latent space. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our self-supervision task improves upon the baseline accuracy on all four facial attributes (i.e. age, eyeglasses, gender and expression) for both SNDCGAN and StyleGAN architectures. We also compare LT-GAN against SSGAN, which is another self-supervision based GAN. LT-GAN achieves better accuracy on all four attributes compared to SSGAN <ref type="bibr" target="#b3">[4]</ref>. <ref type="figure" target="#fig_3">Fig.3</ref> shows some example images of attribute manipulation by moving the latent code in the direction normal to attribute boundary. We show more qualitative samples and its comparison with the baseline model in the supplementary section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Ablation Studies</head><p>StyleGAN Latent Space Disentanglement To demonstrate that our proposed self-supervision task helps in achieving a more disentangled latent space, we adopt the  InterFaceGAN framework <ref type="bibr" target="#b41">[42]</ref> to measure the correlation between synthesized facial attributes distributions of Style-GAN trained on CelebA-HQ dataset. We synthesize 500K images by randomly sampling the latent space. Using a pre-trained ResNet50 facial attribute detector, we assign attribute scores to all 500K images for all four facial attributes (age, eyeglasses, gender, and smile). Treating each attribute score as a random variable, we can compute the correlation between two attributes using their distribution observed over the 500K generated images. The formula to compute correlation between two attributes X and Y is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNDCGAN StyleGAN Baseline SS-GAN LT-GAN Baseline LT-GAN</head><formula xml:id="formula_8">ρ XY = Cov(X, Y ) σ X σ Y ,</formula><p>where Cov(·, ·) denotes covariance and σ denotes standard deviation. Correlation values closer to zero indicate a more disentangled latent space. <ref type="table" target="#tab_4">Table  3</ref> shows the correlation values between attributes for both baseline StyleGAN and LT-StyleGAN. It can be observed that the correlation between attributes is more closer to 0 for LT-StyleGAN as compared to baseline StyleGAN. Similar to <ref type="bibr" target="#b22">[23]</ref>, we also compute the perceptual path length for both latent spaces Z and W of StyleGAN. The idea is that a more disentangled latent space will result in perceptually smoother transitions in the image space as we interpolate in the latent space, and thus give lower perceptual path length. For the Z space, perceptual path length is 242. <ref type="bibr" target="#b32">33</ref>    Choice of hyper-parameter σ and λ. Hyper-parameter σ controls the difficulty of self-supervision task. A large value of σ makes the self-supervision task trivial (since it is easier to distinguish between latent space perturbations that are far apart). In contrast, a smaller value of σ makes the pretext task too difficult and may cripple training. Hyperparameter λ controls the ratio of weight assigned to selfsupervision loss and adversarial loss in generator objective function. To study the effect of these hyper-parameters on model performance (i.e. FID), we perform ablation experiments by varying one hyper-parameter and fixing the other to its optimal value. We conduct this experiment on SNDC-GAN architecture with CelebA-HQ dataset and the results are as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. It can be observed that minimum FID is achieved at the optimal values of σ = 0.5 and λ = 1.0. FID increases as we move away from the optimal values and the graphs show a U-shaped trend.</p><p>Auxiliary network accuracy on generative transformations We validate the efficacy of learned auxiliary network A in SNDCGAN CelebA-HQ setting with σ = 0.5.</p><p>We vary the σ of p( ) and test the ability of the auxiliary network to distinguish between these GAN-induced transformations. In <ref type="figure" target="#fig_5">Fig. 5</ref>, we report the binary classification accuracy on random 25K generated samples and its transformation from the trained generator. It can be observed that the auxiliary model classifies relatively well for transformations with σ in the neighbourhood of 0.5 on which it was trained, but the performance decreases as σ diverges from 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>CelebA-HQ CIFAR-10 STL-10   Comparison with SS-GAN We also compare LT-GAN with SS-GAN <ref type="bibr" target="#b3">[4]</ref> which is a recently proposed technique to train GAN's with rotation self-supervision. In contrast to SS-GAN our self-supervision task is only defined with respect to generator and considers generative transformations instead of rotation transformation. We compare across 3 datasets: CIFAR-10 and CelebA-HQ on SNDCGAN architecture and STL-10 on resent architecture <ref type="bibr" target="#b28">[29]</ref>. The results are shown in <ref type="table" target="#tab_6">Table 4</ref>. We observe that LT-GAN performs better on CELBA-HQ and STL-10 and is comparable to SS-GAN on CIFAR-10. Since rotation transformation is less informative for datasets with single domain images like faces, SS-GAN performs worse than baseline on CelebA-HQ dataset. However in comparison to SS-GAN, LT-GAN improves the FID score for all datasets.</p><p>Classification Accuracy Score (CAS) CAS <ref type="bibr" target="#b38">[39]</ref> was recently proposed as an additional metric for evaluating conditional generative models on the downstream task of image classification. A standard image classification network is trained using images generated from the model as a training set. The trained model is used to predict labels on the testing set of real images and the obtained test accuracy is the CAS metric (higher the better). It was shown that neither FID <ref type="bibr" target="#b16">[17]</ref> nor IS <ref type="bibr" target="#b39">[40]</ref> scores are predictive of CAS, and thus it serves as another independent evaluation metric. We compare the CAS score of LT-BigGAN and baseline Big-GAN model on Imagenet and CIFAR-10 dataset. For Ima-geNet dataset, we trained a ResNet-50 <ref type="bibr" target="#b15">[16]</ref> classifier similar to <ref type="bibr" target="#b38">[39]</ref> using the generated samples and evaluated its performance on its validation set. LT-GAN achieves the top-5 accuracy of 49.24 as compared to 44.15 of the baseline model, outperforming it by over 5%. Furthermore, on closer ex-amination of class level classification accuracy, we found that many classes in the baseline model suffer from severe mode collapse, which is alleviated to a large extent in LT-GAN. Sample images from those classes for both baseline and LT-GAN are shown in supplementary section <ref type="figure" target="#fig_8">Fig. 7</ref>. On CIFAR-10 dataset, a ResNet-56 model (as used in <ref type="bibr" target="#b38">[39]</ref>) trained via samples generated from LT-BigGAN achieved the test accuracy of 79.93% whereas the baseline model achieved 70.57%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we present LT-GAN, a novel selfsupervised technique for improving the diversity and quality of GAN's image generation. The pretext task of identifying GAN-induced transformation helps the generator blocks of GANs to learn steerable latent feature representation and synthesise high-fidelity images. The experimental results demonstrate that when combined along with strong GAN baselines <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>, our model LT-GAN improves the quality and diversity of generated image on several standard datasets. The performance on FID metric and controlled image editing highlights the effectiveness of LT-GAN in unconditional and class conditional GAN settings. We hope that this approach of leveraging latent transformation as a pretext task can be extended to other generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Additional Results and Ablation Study</head><p>Variance analysis of FID We show the FID <ref type="bibr" target="#b16">[17]</ref> variance box-plot of our approach LT-GAN on SNDCGAN <ref type="bibr" target="#b28">[29]</ref> and BigGAN <ref type="bibr" target="#b1">[2]</ref> architectures for CIFAR-10 and CelebA-HQ datasets in <ref type="figure" target="#fig_6">Fig 6.</ref> To provide a fair study of FID evaluation on our approach, for each configuration, we compute the FID 3 times with different random initial seeds.</p><p>Inception Score We evaluate our approach of LT-GAN using another GAN evaluation metric named Inception Score (IS) <ref type="bibr" target="#b39">[40]</ref>. Here, we report the Inception Score of models trained on CIFAR-10 dataset. As shown in <ref type="table" target="#tab_8">Table  5</ref>, LT-GAN improves IS over baseline, while CR+LT approach achieves the best IS results, on both SNDCGAN and BigGAN architectures.    Choice of Architecture of Auxiliary Network A On SNDCGAN CelebA-HQ with the optimal setting of σ = 0.5, we experimented with different architectures for the auxiliary network A:</p><p>• Linear Network: A linear network (with a single fullyconnected layer) capacity was not sufficient to distinguish between generative transformations resulting from different 's and hence the auxiliary task training failed.</p><p>• Non-linear Network: A non-linear network (with two fully-connected layers and ReLU activation at the hidden layer) achieved a FID score of 19.63.</p><p>• Convolutional Network: A convolutional network (with one convolutional layer, batch normalisation layer and fully-connected layer) achieved a FID score of 21.27.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Qualitative Analysis of Generated Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">LT-BigGAN ImageNet</head><p>Steerability of latent space We show more qualitative samples of varying the zoom, brightness, vertical position and horizontal position in generated images of classes same as <ref type="bibr" target="#b36">[37]</ref> through latent space manipulation as discussed in Section 4.3 in the paper. <ref type="figure" target="#fig_10">Fig. 8</ref> shows sample images generated from LT-BigGAN and BigGAN model on perturbing the latent code in the positive and negative direction of brightness and zoom vector. Similarly, <ref type="figure">Fig. 9</ref> shows latent space steerability on horizontal and vertical shift. We can observe that the baseline model generates distorted images at the extremes and fails to control brightness factor and semantic content for all categories. In contrast, LT-BigGAN generates smooth variations of images while preserving the content and is able to generalize the brightness even for categories that usually are not available in a dark environment e.g cheeseburger class.</p><p>Mode Collapse In the conditional image generation setting on ImageNet dataset using our proposed selfsupervision approach, we observe that it not only improves the FID score but also helps in alleviating the issue of mode collapse. In <ref type="figure" target="#fig_8">Fig. 7</ref> we show example images of classes which suffer from mode collapse in a baseline BigGAN model trained on ImageNet and its corresponding samples generated from LT-BigGAN. We can see that images generated from LT-BigGAN are more diverse as compared to the baseline model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Image Editing on LT-StyleGAN CelebA-HQ</head><p>In <ref type="figure" target="#fig_0">Fig. 10, we</ref> show more examples of the manipulation of facial attributes namely age, gender, smile expression and eyeglasses by using the InterfaceGAN framework <ref type="bibr" target="#b41">[42]</ref> on LT-StyleGAN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Hyper-parameter Details</head><p>This section mentions the choice of hyper-parameters for training LT-GAN over different datasets and architectures. For experiments in CR-GAN, as mentioned in <ref type="bibr" target="#b49">[50]</ref>, the augmentation used for consistency regularization is a combination of randomly shifting the image by a few pixels and random horiozontal flipping. The shift size is 4 pixels for both CIFAR-10 and CelebA-HQ datasets and rest all hyperparameters remain same as baseline. For our LT-GAN approach, we used twice the batch size for G and kept the batch size of D same as that of the baseline, because this modification achieved better results. Note that for fair comparison, we also tried doubling the batch size of G for baseline models, however the FID performance deteriorated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">SNDCGAN CIFAR-10</head><p>Following the hyper-parameter choices of <ref type="bibr" target="#b49">[50]</ref>, we use d step = 1 and set the dimensionality of the latent space to be 128. Adam optimizer with α = 0.0002, β 1 = 0.5 and β 2 = 0.999 is used for both G and D. We use a batch size of 64 for both G and D for Baseline models.</p><p>LT-GAN: σ is chosen to be 0.6 and Adam optimizer with default values of α = 0.001, β 1 = 0.9 and β 2 = 0.999 is chosen for the auxiliary network A. λ is set to 1.0. The encoder features E(G(z)) corresponding to generated images G(z) are taken from the fifth layer of the discriminator <ref type="bibr" target="#b2">3</ref> . The features E(G(z)) are passed through an average pool 2D layer with kernel size 2, stride 2 and zero padding, and then flattened before being passed to the auxiliary network. The number of warmup iterations n before introducing the self supervision task is 2000.</p><p>CR+LT-GAN: σ is chosen to be 0.55. Rest all hyperparameters are same as those mentioned for LT-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">BigGAN CIFAR-10</head><p>We use the standard value <ref type="bibr" target="#b1">[2]</ref> of d steps = 4, dimensionality of z as 128 and batch size of 64. Adam optimizer with α = 0.0002, β 1 = 0.0 and β 2 = 0.999 is used for both G and D.</p><p>3 SNDCGAN discriminator consists of 7 convolutional layers followed by a linear layer at the end. Each convolutional layer is followed by a ReLU activation. We treat each convolutional layer followed by its ReLU activation as a single layer. Thus, SNDCGAN discriminator consists of 8 layers.  LT-GAN: σ is chosen to be 0.6. Adam optimizer with default values of α = 0.001, β 1 = 0.9 and β 2 = 0.999 is chosen for the auxiliary network A. λ is set to 1.0. The encoder features E(G(z)) corresponding to generated images G(z) are taken from the last layer of the discriminator just before sum pooling. The number of warmup iterations n before introducing the self supervision task is 2000.</p><p>CR+LT-GAN: All hyper-parameters are same as that of LT-GAN with default CR-GAN configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3.">SNDCGAN CelebA-HQ</head><p>Following the hyper-parameter choices of <ref type="bibr" target="#b49">[50]</ref>, we use d steps = 1 and set the dimensionality of the latent space to be 128. Adam optimizer with α = 0.0002, β 1 = 0.5 and β 2 = 0.999 is used for both G and D. We use a batch size of 64 for both G and D for baseline model.</p><p>LT-GAN: σ is chosen to be 0.5. Adam optimizer with default values of α = 0.001, β 1 = 0.9 and β 2 = 0.999 is chosen for the auxiliary network A. λ is set to 1.0. The encoder features E(G(z)) corresponding to generated images G(z) are taken from the seventh layer of the discriminator.</p><p>The features E(G(z)) are passed through an average pool 2D layer with kernel size 4, stride 4 and zero padding, and then flattened before being passed to the auxiliary network. The number of warmup iterations n before introducing the self supervision task is chosen to be 1500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CR+LT-GAN:</head><p>The number of warmup iterations n before introducing the self supervision task is chosen to be 5000. Rest all hyper-parameters are same as those mentioned for LT-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.">StyleGAN CelebA-HQ</head><p>StyleGAN adopts progressive growing of both the generator and the discriminator networks. In LT-StyleGAN, <ref type="figure">Figure 9</ref>: Qualitative comparison on geometric transformation horizontal and vertical shift between LT-BigGAN (left) and Baseline BigGAN (right) in five categories of ImageNet through latent space manipulation method of <ref type="bibr" target="#b36">[37]</ref> we introduce the self supervision task after the layer corresponding to 128 resolution has completely faded into the network architecture. The per-pixel noise added after after each convolution block in generator is kept same while generating images corresponding to latent codes z and z + . For incorporating mixing regularization in LT-StyleGAN,the GAN-induced transformation of G(z 1 , z 2 ) is generated as G(z 1 + 1 , z 2 + 2 ), where 1 and + 2 are distinct.</p><p>Following the hyper-parameter choices of <ref type="bibr" target="#b22">[23]</ref>, we set the dimensionality of both the latent spaces Z and W to be 512. The mapping network from Z to W is a 8 layer MLP. While training using progressive growing, we start from 8 × 8 resolution, fade in a new layer during the next 600K images and then let the network stabilize for next 600K images before introducing a new layer. For 128 × 128 resolution, we use Adam optimizer with α = 0.0015, β 1 = 0.0 and β 2 = 0.99 for both G's synthesis network and D. We reduce the learning rate by two orders of magnitude for G's mapping network (i.e. a learning rate of α = 0.000015), as specified in <ref type="bibr" target="#b22">[23]</ref>. We use d steps = 1. We use a batch size of 32 for both G and D with mixing probability to 0.9 for baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LT-GAN:</head><p>We choose σ to be 0.5 with a mixing probability of 0.5. Adam optimizer with default values of α = 0.001, β 1 = 0.9 and β 2 = 0.999 is set for the auxiliary network A. We use λ value of 0.5. We take the encoder features E(G(z)) corresponding to generated images G(z) from the layer of the discriminator corresponding to 16×16 resolution. The features E(G(z)) pass through an average pool 2D layer with kernel size 2, stride 2 and zero padding, and then we flatten it before passing it to the auxiliary network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>we introduce an auxiliary network A to classify whether the above pair of features {f 1 , f 2 } are corresponding to transformations Overview of our proposed LT-GAN self-supervision task for generator training. Generated images G(z) and its GAN-induced transformation G(z + ) are used for defining the self-supervision loss (bce loss). Given intermediate discriminator features of above generated images i.e. E(G(z)) and E(G(z + )), the feature representation of the GAN-induced transformation is f (z, z + ). Auxiliary network A and generator G are trained simultaneously on pretext task to predict if f (z 1 , z 1 + 1 ) and f (z 2 , z 2 + 2 ) features are generated from same perturbation in the latent space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[0.4, 0.6]. For hyper-parameter λ, we found the value of 1.0 to work well except for BigGAN on ImageNet and StyleGAN on CelebA-HQ where we use the value of 0.5. More details about training hyper-parameters for each dataset and architecture are mentioned in the sup-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative comparison of Brightness (left) and Vertical shift (right) using latent space manipulation on randomly generated images of ImageNet for Baseline BigGAN and LT-BigGAN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Manipulation of Age(top left), Gender(top right), Smile(bottom left)and Eyeglasses(bottom right) attributes by navigating the latent space of LT-StyleGAN using InterfaceGAN [42] framework. Original images are in the centre and the left and right images are generated by moving the latent code in negative and positive directions respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>FID on varying σ and λ for LT-SNDCGAN on CelebA-HQ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy (%) of our binary self-supervision task on varying σ for LT-SNDCGAN on CelebA-HQ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>FID Variance box plot of LT-GAN approach on different architectures for CIFAR-10 and CelebA-HQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>SNDCGAN</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Samples of generated images from categories with mode collapse in Baseline BigGAN and its corresponding images generated from LT-BigGAN model. The 6 blocks of images corresponds to ImageNet classes: (a) digital clock, (b) parachute, (c) nematode, (d) anemone fish ,(e) theater curtain, (f) daisy. In each block (that comprises of 4 rows of images), the top part (1st and 2nd row) corresponds to images generated using Baseline (BigGAN) model and the bottom part (3rd and 4th row corresponds to images produced using our approach LT-BigGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative comparison for varying brightness and zoom between LT-BigGAN (left) and Baseline BigGAN (right) in five categories of ImageNet through latent space manipulation method of<ref type="bibr" target="#b36">[37]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Classification accuracy (%) on separation boundaries in latent space with respect to different attributes of CelebA-HQ. Attributes are A: Age, E: Eyeglasses, G: Gender, and S: Smiling.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>for baseline StyleGAN and 133.11 for LT-StyleGAN. For the W space, perceptual path length is 77.48 for baseline StyleGAN and 72.71 for LT-StyleGAN.</figDesc><table><row><cell></cell><cell>A</cell><cell>E</cell><cell>G</cell><cell>S</cell></row><row><cell cols="5">A 1./1. 0.373/0.326 0.466/0.462 -0.128/-0.111</cell></row><row><cell>E</cell><cell>-</cell><cell>1./1.</cell><cell cols="2">0.292/0.262 -0.096/-0.088</cell></row><row><cell>G</cell><cell>-</cell><cell>-</cell><cell>1./1.</cell><cell>-0.297/-0.293</cell></row><row><cell>S</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1./1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Correlation matrix of synthesized attribute distributions of</figDesc><table><row><cell>StyleGAN on CelebA-HQ. In each cell, the first value corresponds to</cell></row><row><cell>baseline StyleGAN and the second value (following /) corresponds to LT-</cell></row><row><cell>StyleGAN. Attributes are A: Age, E: Eyeglasses, G: Gender, and S: Smil-</cell></row><row><cell>ing.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>FID comparison of LT-GAN with SS-GAN on different datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Inception Score for SNDCGAN and BigGAN architectures trained using different approaches on CIFAR-10.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the open source implementation of https://github. com/google/compare_gan for SNDCGAN and https: //github.com/rosinality/style-based-gan-pytorch for StyleGAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Experiments on BigGAN use the implementation of https:// github.com/ajbrock/BigGAN-PyTorch. For ImageNet dstep is 1 instead of the more optimal setting of 2 [2] because of less computation requirements of the former.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) LT-BigGAN (b) BigGAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the pretrained model of https://github.com/ ajbrock/BigGAN-PyTorch as baseline and use the provided checkpoint at 100K for fine-tuning with LT-GAN</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5.">BigGAN ImageNet</head><p>We use the default configuration 4 of d step = 1, dimensionality of z as 120, batch size of 8 * 256. We select Adam optimizer with α = 0.0001 and 0.0004 for G for D respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LT-GAN:</head><p>We choose σ to be 0.5. We set Adam optimizer with default values of α = 0.001, β 1 = 0.9 and β 2 = 0.999 for the auxiliary network A. λ is set to 0.5. We take the encoder features E(G(z)) corresponding to gener-ated images G(z) from the seventh layer of the discriminator. The features E(G(z)) pass through an average pool 2D layer with kernel size 2, stride 2 and zero padding, and then we flatten before passing to the auxiliary network. The number of warmup iterations n before introducing the self supervision task is set to 100K 4 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gan dissection: Visualizing and understanding generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1606.03657</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting bias with generative counterfactual face attribute augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Fairness Accountability Transparency and Ethics in Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miller</forename><surname>Puckette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04208</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Kumar Krishna Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gansynth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08710</idno>
		<title level="m">Adversarial neural audio synthesis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ganalyze: Toward visual definitions of cognitive image properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lore</forename><surname>Goetschalckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fx-gan: Self-supervised gan learning via feature exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenju</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng-Yok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3183" to="3191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the &quot;steerability&quot; of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grasp2vec: Learning object representations from self-supervised grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Which training methods for gans do actually converge? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>abs/1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-fidelity image generation with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Controlling generative models with continuous factors of variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Plumerault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Hervé Le Borgne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hudelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross and learn: Cross-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nawid</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="228" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08896</idno>
		<title level="m">Deep and hierarchical implicit models</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">An improved self-supervised gan via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Bao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1905.05469</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Self-supervised gan: Analysis and improvement with multi-class minimax game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Bao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep feature interpolation for image content changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by autoencoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Splitbrain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
