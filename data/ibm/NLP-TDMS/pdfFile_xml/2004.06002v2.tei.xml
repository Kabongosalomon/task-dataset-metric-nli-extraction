<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhang</surname></persName>
							<email>hongkai.zhang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>3 TuSimple</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
							<email>changhong@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>3 TuSimple</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
							<email>bpma@ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>3 TuSimple</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>3 TuSimple</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>dynamic training, high quality object detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although two-stage object detectors have continuously advanced the state-of-the-art performance in recent years, the training process itself is far from crystal. In this work, we first point out the inconsistency problem between the fixed network settings and the dynamic training procedure, which greatly affects the performance. For example, the fixed label assignment strategy and regression loss function cannot fit the distribution change of proposals and thus are harmful to training high quality detectors. Consequently, we propose Dynamic R-CNN to adjust the label assignment criteria (IoU threshold) and the shape of regression loss function (parameters of SmoothL1 Loss) automatically based on the statistics of proposals during training. This dynamic design makes better use of the training samples and pushes the detector to fit more high quality samples. Specifically, our method improves upon ResNet-50-FPN baseline with 1.9% AP and 5.5% AP90 on the MS COCO dataset with no extra overhead. Codes and models are available at https://github.com/hkzhang95/DynamicRCNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Benefiting from the advances in deep convolutional neural networks (CNNs) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14]</ref>, object detection has made remarkable progress in recent years. Modern detection frameworks can be divided into two major categories of onestage detectors <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29]</ref> and two-stage detectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>. And various improvements have been made in recent studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref>. In the training procedure of both kinds of pipelines, a classifier and a regressor are adopted respectively to solve the recognition and localization tasks. Therefore, an effective training process plays a crucial role in achieving high quality object detection 1 .  Different from the image classification task, the annotations for the classification task in object detection are the ground-truth boxes in the image. So it is not clear how to assign positive and negative labels for the proposals in classifier training since their separation may be ambiguous. The most widely used strategy is to set a threshold for the IoU of the proposal and corresponding ground-truth. As mentioned in Cascade R-CNN <ref type="bibr" target="#b2">[3]</ref>, training with a certain IoU threshold will lead to a classifier that degrades the performance at other IoUs. However, we cannot directly set a high IoU from the beginning of the training due to the scarcity of positive samples. The solution that Cascade R-CNN provides is to gradually refine the proposals by several stages, which are effective yet time-consuming. As for regressor, the problem is similar. During training, the quality of proposals is improved, however the parameter in SmoothL1 Loss is fixed. Thus it leads to insufficient training for the high quality proposals.</p><p>To solve this issue, we first examine an overlooked fact that the quality of proposals is indeed improved during training as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. We can find that even under different IoU thresholds, the number of positives still increases significantly. Inspired by the illuminating observations, we propose Dynamic R-CNN, a simple yet effective method to better exploit the dynamic quality of proposals for object detection. It consists of two components: Dynamic Label Assignment and Dynamic SmoothL1 Loss, which are designed for classification and regression branches, respectively. First, to train a better classifier that is discriminative for high IoU proposals, we gradually adjust the IoU threshold for positive/negative samples based on the proposals distribution in the training procedure. Specifically, we set the threshold as the IoU of the proposal at a certain percentage since it can reflect the quality of the overall distribution. For regression, we choose to change the shape of the regression loss function to adaptively fit the distribution change of regression label and ensure the contribution of high quality samples to training. In particular, we adjust the β in SmoothL1 Loss based on the regression label distribution, since β actually controls the magnitude of the gradient of small errors (shown in <ref type="figure" target="#fig_4">Figure 4</ref>).</p><p>By this dynamic scheme, we can not only alleviate the data scarcity issue at the beginning of the training, but also harvest the benefit of high IoU training. These two modules explore different parts of the detector, thus could work collaboratively towards high quality object detection. Furthermore, despite the simplicity of our proposed method, Dynamic R-CNN could bring consistent performance gains on MS COCO <ref type="bibr" target="#b29">[30]</ref> with almost no extra computational complexity in training. And during the inference phase, our method does not introduce any additional overhead. Moreover, extensive experiments verify the proposed method could generalize to other baselines with stronger performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Region-based object detectors. The general practice of region-based object detectors is converting the object detection task into a bounding box classification and a regression problem. In recent years, region-based approaches have been the leading paradigm with top performance. For example, R-CNN <ref type="bibr" target="#b11">[12]</ref>, Fast R-CNN <ref type="bibr" target="#b10">[11]</ref> and Faster R-CNN <ref type="bibr" target="#b37">[38]</ref> first generate some candidate region proposals, then randomly sample a small batch with certain foreground-background ratio from all the proposals. These proposals will be fed into a second stage to classify the categories and refine the locations at the same time. Later, some works extended Faster R-CNN to address different problems. R-FCN <ref type="bibr" target="#b6">[7]</ref> makes the whole network fully convolutional to improve the speed; and FPN <ref type="bibr" target="#b27">[28]</ref> proposes a top-down pathway to combine multi-scale features. Besides, various improvements have been witnessed in recent studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52]</ref>. Classification in object detection. Recent researches focus on improving object classifier from various perspectives <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>. The classification scores in detection not only determine the semantic category for each proposal, but also imply the localization accuracy, since Non-Maximum Suppression (NMS) suppresses less confident boxes using more reliable ones. It ranks the resultant boxes first using the classification scores. However, as mentioned in IoU-Net <ref type="bibr" target="#b18">[19]</ref>, the classification score has low correlation with localization accuracy, which leads to noisy ranking and limited performance. Therefore, IoU-Net <ref type="bibr" target="#b18">[19]</ref> adopts an extra branch for predicting IoU scores and refining the classification confidence. Softer NMS <ref type="bibr" target="#b16">[17]</ref> devises an KL loss to model the variance of bounding box regression directly, and uses that for voting in NMS. Another direction to improve is to raise the IoU threshold for training high quality classifiers, since training with different IoU thresholds will lead to classifiers with corresponding quality. However, as mentioned in Cascade R-CNN <ref type="bibr" target="#b2">[3]</ref>, directly raising the IoU threshold is impractical due to the vanishing positive samples. Therefore, to produce high quality training samples, some approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">48]</ref> adopt sequential stages which are effective yet time-consuming. Essentially, it should be noted that these methods ignore the inherent dynamic property in training procedure which is useful for training high quality classifiers. Bounding box regression. It has been proved that the performance of models is dependent on the relative weight between losses in multi-task learning <ref type="bibr" target="#b20">[21]</ref>. Cascade R-CNN <ref type="bibr" target="#b2">[3]</ref> also adopt different regression normalization factors to adjust the aptitude of regression term in different stages. Besides, Libra R-CNN <ref type="bibr" target="#b33">[34]</ref> proposes to promote the regression gradients from the accurate samples; and SABL <ref type="bibr" target="#b44">[45]</ref> localizes each side of the bounding box with a lightweight two step bucketing scheme for precise localization. However, they mainly focus on a fixed scheme ignoring the dynamic distribution of learning targets during training. Dynamic training. There are various researches following the idea of dynamic training. A widely used example is adjusting the learning rate based on the training iterations <ref type="bibr" target="#b32">[33]</ref>. Besides, Curriculum Learning <ref type="bibr" target="#b0">[1]</ref> and Self-paced Learning <ref type="bibr" target="#b22">[23]</ref> focus on improving the training order of the examples. Moreover, for object detection, hard mining methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> can also be regarded as a dynamic way. However, they don't handle the core issues in object detection such as constant label assignment strategy. Our method is complementary to theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic Quality in the Training Procedure</head><p>Generally speaking, Object detection is complex since it needs to solve two main tasks: recognition and localization. Recognition task needs to distinguish foreground objects from backgrounds and determine the semantic category for them. Besides, the localization task needs to find accurate bounding boxes for different objects. To achieve high-quality object detection, we need to further explore the training process of both two tasks as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proposal Classification</head><p>How to assign labels is an interesting question for the classifier in object detection. It is unique to other classification problems since the annotations are the ground-truth boxes in the image. Obviously, a proposal should be negative if it does not overlap with any ground-truth, and a proposal should be positive if its overlap with a ground-truth is 100%. However, it is a dilemma to define whether a proposal with IoU 0.5 should be labeled as positive or negative.</p><p>In Faster R-CNN <ref type="bibr" target="#b37">[38]</ref>, labels are assigned by comparing the box's highest IoU with ground-truths using a pre-defined IoU threshold. Formally, the paradigm can be formulated as follows (we take a binary classification loss for simplicity): ∆ distribution at different iterations and IoU thresholds (we randomly select some points for simplicity). Column 1&amp;2: under the same IoU threshold, the regression labels are more concentrated as the training goes. Column 2&amp;3: at the same iteration, raising the IoU threshold will significantly change the distribution.</p><formula xml:id="formula_0">label =      1, if max IoU (b, G) ≥ T + 0, if max IoU (b, G) &lt; T − −1, otherwise.</formula><p>positives, negatives and ignored samples, respectively. As for the second stage of Faster R-CNN, T + and T − are set to 0.5 by default <ref type="bibr" target="#b12">[13]</ref>. So the definition of positives and negatives is essentially hand-crafted.</p><p>Since the goal of classifier is to distinguish the positives and negatives, training with different IoU thresholds will lead to classifiers with corresponding quality <ref type="bibr" target="#b2">[3]</ref>.Therefore, to achieve high quality object detection, we need to train the classifier with a high IoU threshold. However, as mentioned in Cascade R-CNN, directly raising the IoU threshold is impractical due to the vanishing positive samples. Cascade R-CNN uses several sequential stages to lift the IoU of the proposals, which are effective yet time-consuming.</p><p>So is there a way to get the best of two worlds? As mentioned above, the quality of proposals actually improves along the training. This observation inspires us to take a progressive approach in training: At the beginning, the proposal network is not capable to produce enough high quality proposals, so we use a lower IoU threshold to better accommodate these imperfect proposals in second stage training. As training goes, the quality of proposals improves, we gradually have enough high quality proposals. As a result, we may increase the threshold to better utilize them to train a high quality detector that is more discriminative at higher IoU. We will formulate this process in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bounding Box Regression</head><p>The task of bounding box regression is to regress the positive candidate bounding box b to a target ground-truth g. This is learned under the supervision of the regression loss function L reg . To encourage the regression label invariant to scale and location, L reg operates on the offset ∆ = (δ x , δ y , δ w , δ h ) defined by</p><formula xml:id="formula_1">δ x = (g x − b x )/b w , δ y = (g y − b y )/b h δ w = log(g w /b w ), δ h = log(g h /b h ).<label>(2)</label></formula><p>Since the bounding box regression performs on the offsets, the absolute values of Equation <ref type="formula" target="#formula_1">(2)</ref> can be very small. To balance the different terms in multitask learning, ∆ is usually normalized by pre-defined mean and stdev (standard deviation) as widely used in many work <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>However, we discover that the distribution of regression labels are shifting during training. As shown in <ref type="figure">Figure 2</ref>, we calculate the statistics of the regression labels under different iterations and IoU thresholds. First, from the first two columns, we find that under the same IoU threshold for positives, the mean and stdev are decreasing as the training goes due to the improved quality of proposals. With the same normalization factors, the contributions of those high quality samples will be reduced based on the definition of SmoothL1 Loss function, which is harmful to the training of high quality regressors. Moreover, with a higher IoU threshold, the quality of positive samples is further enhanced, thus their contributions are reduced even more, which will greatly limit the overall performance. Therefore, to achieve high quality object detection, we need to fit the distribution change and adjust the shape of regression loss function to compensate for the increasing of high quality proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dynamic R-CNN</head><p>To better exploit the dynamic property of the training procedure, we propose Dynamic R-CNN which is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Our key insight is adjusting the second stage classifier and regressor to fit the distribution change of proposals. The two components designed for the classification and localization branch will be elaborated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dynamic Label Assignment</head><p>The Dynamic Label Assignment (DLA) process is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref> (a). Based on the common practice of label assignment in Equation <ref type="formula">(1)</ref> in object detection, the DLA module can be formulated as follows:</p><formula xml:id="formula_2">label = 1, if max IoU (b, G) ≥ T now 0, if max IoU (b, G) &lt; T now ,<label>(3)</label></formula><p>where T now stands for the current IoU threshold. Considering the dynamic property in training, the distribution of proposals is changing over time. Our DLA updates the T now automatically based on the statistics of proposals to fit this distribution change. Specifically, we first calculate the IoUs I between proposals and their target ground-truths, and then select the K I -th largest value from I as the threshold T now . As the training goes, T now will increase gradually which reflects the improved quality of proposals. In practice, we first calculate the K Ith largest value in each batch, and then update T now every C iterations using the mean of them to enhance the robustness of the training. It should be noted that the calculation of IoUs is already done by the original method, so there is almost no additional complexity in our method. The resultant IoU thresholds used in training are illustrated in <ref type="figure" target="#fig_3">Figure 3</ref> (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dynamic SmoothL1 Loss</head><p>The localization task for object detection is supervised by the commonly used SmoothL1 Loss, which can be formulated as follows:</p><formula xml:id="formula_3">SmoothL1(x, β) = 0.5|x| 2 /β, if |x| &lt; β, |x| − 0.5β, otherwise.<label>(4)</label></formula><p>Here the x stands for the regression label. β is a hyper-parameter controlling in which range we should use a softer loss function like l 1 loss instead of the original l 2 loss. Considering the robustness of training, β is set default as 1.0 to prevent the exploding loss due to the poor trained network in the early stages. We also illustrate the impact of β in <ref type="figure" target="#fig_4">Figure 4</ref>, in which changing β leads to different curves of loss and gradient. It is easy to find that a smaller β actually accelerate 0.00 0.25 0.50 0.75 1.00 1. the saturation of the magnitude of gradient, thus it makes more accurate sample contributes more to the network training.</p><p>As analyzed in Section 3.2, we need to fit the distribution change and adjust the regression loss function to compensate for the high quality samples. So we propose Dynamic SmoothL1 Loss (DSL) to change the shape of loss function to gradually focus on high quality samples as follows:</p><formula xml:id="formula_4">DSL(x, β now ) = 0.5|x| 2 /β now , if |x| &lt; β now , |x| − 0.5β now , otherwise.<label>(5)</label></formula><p>Similar to DLA, DSL will change the value of β now according to the statistics of regression labels which can reflect the localization accuracy. To be more specific, we first obtain the regression labels E between proposals and their target ground-truths, then select the K β -th smallest value from E to update the β now in Equation <ref type="formula" target="#formula_3">(4)</ref>. Similarly, we also update the β now every C iterations using the median of the K β -th smallest label in each batch. We choose median instead of mean as in the classification because we find more outliers in regression labels. Through this dynamic way, appropriate β now will be adopted automatically as shown in <ref type="figure" target="#fig_3">Figure 3</ref> (b), which will better exploit the training samples and lead to a high quality regressor.</p><p>To summarize the whole method, we describe the proposed Dynamic R-CNN in Algorithm 1. Besides the proposals P and ground-truths G, Dynamic R-CNN has three hyperparamters: IoU threshold top-k K I , β top-k K β and update iteration count C. Note that compared with baseline, we only introduce one additional hyperparameter. And we will show soon the results are actually quite robust to the choice of these hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Dynamic R-CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Proposal set P , ground-truth set G. IoU threshold top-k KI , β top-k K β , update iteration count C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>Trained object detector D. 1: Initialize IoU threshold and SmoothL1 β as Tnow, βnow 2: Build two empty sets SI , SE for recording the IoUs and regression labels 3: for i = 0 to max iter do 4:</p><p>Obtain matched IoUs I and regression labels E between P and G 5:</p><p>Select thresholds I k , E k based on the KI , K β 6:</p><p>Record corresponding values, add I k to SI and E k to SE 7:</p><p>if i % C == 0 then 8:</p><p>Update IoU threshold: Tnow = Mean(SI ) 9:</p><p>Update SmoothL1 β: βnow = Median(SE) 10: SI = Ø, SE = Ø 11:</p><p>Train the network with Tnow, βnow 12: return Improved object detector D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Evaluation Metrics</head><p>Experimental results are mainly evaluated on the bounding box detection track of the challenging MS COCO <ref type="bibr" target="#b29">[30]</ref> 2017 dataset. Following the common practice <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b14">15]</ref>, we use the COCO train split (∼118k images) for training and report the ablation studies on the val split (5k images). We also submit our main results to the evaluation server for the final performance on the test-dev split, which has no disclosed labels. The COCO-style Average Precision (AP) is chosen as the main evaluation metric which averages AP across IoU thresholds from 0.5 to 0.95 with an interval of 0.05. We also include other metrics to better understand the behavior of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>For fair comparisons, all experiments are implemented on PyTorch <ref type="bibr" target="#b34">[35]</ref> and follow the settings in maskrcnn-benchmark 2 and SimpleDet <ref type="bibr" target="#b3">[4]</ref>. We adopt FPNbased Faster R-CNN <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b27">28]</ref> with ResNet-50 <ref type="bibr" target="#b15">[16]</ref> model pre-trained on Ima-geNet <ref type="bibr" target="#b8">[9]</ref> as our baseline. All models are trained on the COCO 2017 train set and tested on val set with image short size at 800 pixels unless noted. Due to the scarcity of positives in the training procedure, we set the NMS threshold of RPN to 0.85 instead of 0.7 for all the experiments. <ref type="table">Table 1</ref>. Comparisons with different baselines (our re-implementations) on COCO test-dev set. "MST" and "*" stand for multi-scale training and testing respectively. "2×" and "3×" are training schedules which extend the iterations by 2/3 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone AP AP50 AP75 APS APM APL </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Main Results</head><p>We compare Dynamic R-CNN with corresponding baselines on COCO test-dev set in <ref type="table">Table 1</ref>. For fair comparisons, We report our re-implemented results.</p><p>First, we prove that our method can work on different backbones. Dynamic R-CNN achieves 39.1% AP with ResNet-50 <ref type="bibr" target="#b15">[16]</ref>, which is 1.8 points higher than the FPN-based Faster R-CNN baseline. With a stronger backbone like ResNet-101, Dynamic R-CNN can also achieve consistent gains (+1.9 points).</p><p>Then, our dynamic design is also compatible with other training and testing skills. The results are consistently improved by progressively adding in 2× longer training schedule, multi-scale training (extra 1.5× longer training schedule), multi-scale testing and deformable convolution <ref type="bibr" target="#b51">[52]</ref>. With the best combination, out Dynamic R-CNN achieves 49.2% AP, which is still 2.3 points higher than the Faster R-CNN baseline.</p><p>These results show the effectiveness and robustness of our method since it can work together with different backbones and multiple training and testing skills. It should also be noted that the performance gains are almost free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Experiments</head><p>To show the effectiveness of each proposed component, we report the overall ablation studies in <ref type="table" target="#tab_1">Table 2</ref>. 1) Dynamic Label Assignment (DLA). DLA brings 1.2 points higher box AP than the ResNet-50-FPN baseline. To be more specific, results in higher IoU  metrics are consistently improved, especially for the 2.9 points gains in AP 90 . It proves the effectiveness of our method for pushing the classifier to be more discriminative at higher IoU thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Dynamic SmoothL1 Loss (DSL)</head><p>. DSL improves the box AP from 37.0 to 38.0. Results in higher IoU metrics like AP 80 and AP 90 are hugely improved, which validates the effectiveness of changing the loss function to compensate for the high quality samples during training. Moreover, as analyzed in Section 3.2, with DLA the quality of positives is further improved thus their contributions are reduced even more. So applying DSL on DLA will also bring reasonable gains especially on high quality metrics. To sum up, Dynamic R-CNN improves the baseline by 1.9 points AP and 5.5 points AP 90 .</p><p>3) Illustration of dynamic training. To further illustrate the dynamics in the training procedure, we show the trends of IoU threshold and SmoothL1 β under different settings based on our method in <ref type="figure" target="#fig_5">Figure 5</ref>. Here we clip the values of IoU threshold and β to 0.4 and 1.0 respectively at the beginning of training. Regardless of the specific values of K I and K β , the overall trend of IoU threshold is increasing while that for SmoothL1 β is decreasing during training. These results again verify the proposed method work as expected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Studies on the effect of hyperparameters</head><p>Ablation study on K I in DLA. Experimental results on different K I are shown in <ref type="table">Table 3</ref>. Compared to the baseline, DLA can achieve consistent gains in AP regardless of the choice of K I . These results prove the universality of K I . Moreover, the performance on various metrics are changed under different K I . Choosing K I as 64/75/100 means that nearly 12.5%/15%/20% of the whole batch are selected as positives. Generally speaking, setting a smaller K I will increase the quality of selected samples, which will lead to better accuracy under higher metrics like AP 90 . On the contrary, adopting a larger K I will be more helpful for the metrics at lower IoU. Finally, we find that setting K I as 75 achieves the best trade-off and use it as the default value for further experiments. All these ablations prove the effectiveness and robustness of the DLA part. Ablation study on K β in DSL. As shown in <ref type="table">Table 5</ref>, we first try different β on Faster R-CNN and empirically find that a smaller β leads to better performance. Then, experiments under different K β are provided to show the effects of K β . Regardless of the certain value of K β , DSL can achieve consistent improvements compared with various fine-tuned baselines. Specifically, with our best setting, DSL can bring 1.0 point higher AP than the baseline, and the improvement mainly lies in the high quality metrics like AP 90 (+3.5 points). These experimental results prove that our DSL is effective in compensating for high quality samples and can lead to a better regressor due to the advanced dynamic design. Ablation study on iteration count C. Due to the concern of robustness, we update T now and β now every C iterations using the statistics in the last interval. To show the effects of different iteration count C, we try different values of C on the proposed method. As shown in <ref type="table">Table 4</ref>, setting C as 20, 100 and 500 leads to very similar results, which proves the robustness to this hyperparameter. Complexity and speed. As shown in Algorithm 1, the main computational complexity of our method lies in the calculations of IoUs and regression labels, which are already done by the original method. Thus the additional overhead only lies in calculating the mean or median of a short vector, which basically does not increase the training time. Moreover, since our method only changes the training procedure, obviously the inference speed will not be slowed down.</p><p>Our advantage compared to other high quality detectors like Cascade R-CNN is the efficiency. Cascade R-CNN increases the training time and slows down the inference speed while our method does not. Specifically, as shown in <ref type="table">Table 6</ref>, Dynamic R-CNN achieves 13.9 FPS, which is ∼1.25 times faster than Cascade R-CNN (11.2 FPS) under ResNet-50-FPN backbone. Moreover, with larger heads, the cascade manner will further slow down the speed. Dynamic Mask R-CNN runs ∼1.5 times faster than Cascade Mask R-CNN. Note that the difference will be more apparent as the backbone gets smaller (∼1.74 times faster, 13.6 FPS vs 7.8 FPS under ResNet-18 backbone with mask head), since the main overhead of Cascade R-CNN is the two additional headers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Universality</head><p>Since the viewpoint of dynamic training is a general concept, we believe that it can be adopted in different methods. To validate the universality, we further apply the dynamic design on Mask R-CNN with different backbones. As shown in <ref type="table" target="#tab_3">Table 7</ref>, adopting the dynamic design can not only bring ∼2.0 points higher box AP but also improve the instance segmentation results regardless of backbones. Note that we only adopt the DLA and DSL which are designed for object detection, so these results further demonstrate the universality and effectiveness of our dynamic design on improving training procedure for current detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Comparison with State-of-the-Arts</head><p>We compare Dynamic R-CNN with the state-of-the-art object detectors on COCO test-dev set in <ref type="table" target="#tab_4">Table 8</ref>. Considering that various backbones and training/testing settings are adopted by different detectors (including deformable  <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52]</ref>, image pyramid scheme <ref type="bibr" target="#b40">[41]</ref>, large-batch Batch Normalization <ref type="bibr" target="#b35">[36]</ref> and Soft-NMS <ref type="bibr" target="#b1">[2]</ref>), we report the results of our method with two types.</p><p>Dynamic R-CNN applies our method on FPN-based Faster R-CNN with ResNet-101 as backbone, and it can achieve 42.0% AP without bells and whistles. Dynamic R-CNN* adopts image pyramid scheme (multi-scale training and testing), deformable convolutions and Soft-NMS. It further improves the results to 50.1% AP, outperforming all the previous detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we take a thorough analysis of the training process of detectors and find that the fixed scheme limits the overall performance. Based on the advanced dynamic viewpoint, we propose Dynamic R-CNN to better exploit the training procedure. With the help of the simple but effective components like Dynamic Label Assignment and Dynamic SmoothL1 Loss, Dynamic R-CNN brings significant improvements on the challenging COCO dataset with no extra cost. Extensive experiments with various detectors and backbones validate the universality and effectiveness of Dynamic R-CNN. We hope that this dynamic viewpoint can inspire further researches in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Effectiveness on One-stage Detectors</head><p>Considering that dynamic training is a general viewpoint, we also try to apply the dynamic design on one-stage object detectors. Specifically, we choose a representative one-stage detector RetinaNet <ref type="bibr" target="#b28">[29]</ref> to validate the effectiveness of our method. As shown in <ref type="table" target="#tab_5">Table 9</ref>, our dynamic design brings 0.7 points higher box AP than the RetinaNet baseline. It should be noted that RetinaNet has already changed the β of SmoothL1 Loss to a small value (0.11), so in our experiment, adjusting β to a slightly smaller value (0.05 with DSL) has little effect. Moreover, since the input of RetinaNet is the pre-defined anchors, the distribution of input is relatively fixed. So the impact of DLA can be regarded as using a more reasonable IoU threshold (e.g. 0.55) for training. We believe our method can be applied to other one-stage detectors if their inputs are more dynamic, like Guided Anchoring <ref type="bibr" target="#b43">[44]</ref>. To further demonstrate the effectiveness of our method, we conduct experiments on PASCAL VOC <ref type="bibr" target="#b9">[10]</ref> dataset with the same hyperparameters as on MS COCO dataset. We use the union of VOC2007 and VOC2012 trainval as the training set and report the results on the VOC2007 test set. As shown in Table 10, Dynamic R-CNN improves the Faster R-CNN baseline by 1.5 points AP and 3.5 points AP 90 . Thus we reach similar conclusions as from the results on COCO dataset, which validates the effectiveness and universality of Dynamic R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experimental Results on PASCAL VOC dataset</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) The number of positive proposals under different IoU thresholds during the training process. The curve shows the numbers of positives vary significantly during training, with corresponding changes in regression labels distribution (σx and σw stands for the standard deviation for x and w respectively). (b) The IoU and regression label of the 75th and 10th most accurate proposals respectively in the training procedure. These curves further show the improved quality of proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>Here b stands for a bounding box, G represents for the set of ground-truths, T + and T − are the positive and negative threshold for IoU. 1, 0, −1 stand for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The overall pipeline of the proposed Dynamic R-CNN. Considering the dynamic property of the training process, Dynamic R-CNN consists of two main components (a) Dynamic Label Assignment (DLA) process and (b) Dynamic SmoothL1 Loss (DSL) from different perspectives.From the left part of (a) we can find that there are more high quality proposals as the training goes. With the improved quality of proposals, DLA will automatically raise the IoU threshold based on the proposal distribution. Then positive (green) and negative (red) labels are assigned for the proposals by DLA which are shown in the right part of (a). Meanwhile, to fit the distribution change and compensate for the increasing of high quality proposals, the shape of regression loss function is also adjusted correspondingly in (b). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>We show curves for (a) loss and (b) gradient of SmoothL1 Loss with different β here. β is set default as 1.0 in the R-CNN part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Trends of (a) IoU threshold and (b) SmoothL1 β under different settings based on our method. Obviously the distribution has changed a lot during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of each component in Dynamic R-CNN on COCO val set.</figDesc><table><row><cell>Backbone</cell><cell cols="4">DLA DSL AP ∆AP AP50 AP60 AP70 AP80 AP90</cell></row><row><cell>ResNet-50-FPN</cell><cell>37.0</cell><cell>-</cell><cell>58.0 53.5 46.0 32.6</cell><cell>9.7</cell></row><row><cell>ResNet-50-FPN</cell><cell cols="4">38.0 +1.0 57.6 53.5 46.7 34.4 13.2</cell></row><row><cell>ResNet-50-FPN</cell><cell cols="4">38.2 +1.2 57.5 53.6 47.1 35.2 12.6</cell></row><row><cell>ResNet-50-FPN</cell><cell cols="4">38.9 +1.9 57.3 53.6 47.4 36.3 15.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .Table 5 .</head><label>345</label><figDesc>Ablation study on KI .K I AP AP 50 AP 60 AP 70 AP 80 AP 90 Ablation study on C.C AP AP 50 AP 60 AP 70 AP 80 AP 90 Ablation study on K β .Setting AP AP 50 AP 60 AP 70 AP 80 AP 90</figDesc><table><row><cell>-37.0 58.0 53.5 46.0 32.6 9.7</cell><cell></cell><cell cols="2">-37.0 58.0 53.5 46.0 32.6 9.7</cell></row><row><cell>64 38.1 57.2 53.3 46.8 35.1 12.8</cell><cell cols="3">20 38.0 57.4 53.5 47.0 35.0 12.5</cell></row><row><cell>75 38.2 57.5 53.6 47.1 35.2 12.6</cell><cell cols="3">100 38.2 57.5 53.6 47.1 35.2 12.6</cell></row><row><cell>100 37.9 57.9 53.8 46.9 34.2 11.6</cell><cell cols="3">500 38.1 57.6 53.5 47.2 34.8 12.6</cell></row><row><cell></cell><cell></cell><cell cols="2">Table 6. Inference speed compar-</cell></row><row><cell></cell><cell></cell><cell cols="2">isons using ResNet-50-FPN back-</cell></row><row><cell cols="2">β = 1.0 37.0 58.0 53.5 46.0 32.6 9.7 β = 2.0 35.9 57.7 53.2 45.1 30.1 8.3</cell><cell>bone on RTX 2080TI GPU. Method</cell><cell>FPS</cell></row><row><cell cols="2">β = 0.5 37.5 57.6 53.3 46.4 33.5 11.3</cell><cell>Dynamic R-CNN</cell><cell>13.9</cell></row><row><cell cols="2">K β = 15 37.6 57.3 53.1 46.0 33.9 12.5 K β = 10 38.0 57.6 53.5 46.7 34.4 13.2 K β = 8 37.6 57.5 53.3 45.9 33.9 12.4</cell><cell cols="2">Cascade R-CNN Dynamic Mask R-CNN 11.3 11.2 Cascade Mask R-CNN 7.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>The universality of Dynamic R-CNN. We apply the idea of dynamic training on Mask R-CNN under different backbones. "bbox" and "segm" stand for object detection and instance segmentation results on COCO val set, respectively.</figDesc><table><row><cell>Backbone</cell><cell cols="2">+Dynamic AP bbox AP bbox 50</cell><cell>AP bbox 75</cell><cell cols="2">AP segm AP segm 50</cell><cell>AP segm 75</cell></row><row><cell>ResNet-50-FPN</cell><cell>37.5 39.4</cell><cell>58.0 57.6</cell><cell>40.7 43.3</cell><cell>33.8 34.8</cell><cell>54.6 55.0</cell><cell>36.0 37.5</cell></row><row><cell>ResNet-101-FPN</cell><cell>39.7 41.8</cell><cell>60.7 60.4</cell><cell>43.2 45.8</cell><cell>35.6 36.7</cell><cell>56.9 57.5</cell><cell>37.7 39.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Comparisons of single-model results on COCO test-dev set.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">AP AP50 AP75 APS APM APL</cell></row><row><cell>RetinaNet [29]</cell><cell>ResNet-101</cell><cell cols="3">39.1 59.1 42.3 21.8 42.7 50.2</cell></row><row><cell>CornerNet [24]</cell><cell cols="4">Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9</cell></row><row><cell>FCOS [43]</cell><cell>ResNet-101</cell><cell cols="3">41.0 60.7 44.1 24.0 44.1 51.0</cell></row><row><cell>FreeAnchor [50]</cell><cell>ResNet-101</cell><cell cols="3">41.8 61.1 44.9 22.6 44.7 53.9</cell></row><row><cell>RepPoints [47]</cell><cell cols="4">ResNet-101-DCN 45.0 66.1 49.0 26.6 48.6 57.5</cell></row><row><cell>CenterNet [51]</cell><cell cols="4">Hourglass-104 45.1 63.9 49.3 26.6 47.1 57.7</cell></row><row><cell>ATSS [49]</cell><cell cols="4">ResNet-101-DCN 46.3 64.7 50.4 27.7 49.8 58.4</cell></row><row><cell>Faster R-CNN [28]</cell><cell>ResNet-101</cell><cell cols="3">36.2 59.1 39.0 18.2 39.0 48.2</cell></row><row><cell>Mask R-CNN [15]</cell><cell>ResNet-101</cell><cell cols="3">38.2 60.3 41.7 20.1 41.1 50.2</cell></row><row><cell>Regionlets [46]</cell><cell>ResNet-101</cell><cell>39.3 59.8</cell><cell>-</cell><cell>21.7 43.7 50.9</cell></row><row><cell>Libra R-CNN [34]</cell><cell>ResNet-101</cell><cell cols="3">41.1 62.1 44.7 23.4 43.7 52.5</cell></row><row><cell>Cascade R-CNN [3]</cell><cell>ResNet-101</cell><cell cols="3">42.8 62.1 46.3 23.7 45.5 55.2</cell></row><row><cell>SNIP [41]</cell><cell cols="4">ResNet-101-DCN 44.4 66.2 49.9 27.3 47.4 56.9</cell></row><row><cell>DCNv2 [52]</cell><cell cols="4">ResNet-101-DCN 46.0 67.9 50.8 27.8 49.1 59.5</cell></row><row><cell>TridentNet [26]</cell><cell cols="4">ResNet-101-DCN 48.4 69.7 53.5 31.8 51.3 60.3</cell></row><row><cell>Dynamic R-CNN</cell><cell>ResNet-101</cell><cell cols="3">42.0 60.7 45.9 22.7 44.3 54.3</cell></row><row><cell cols="5">Dynamic R-CNN* ResNet-101-DCN 50.1 68.3 55.6 32.8 53.0 61.2</cell></row><row><cell>convolutions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Effectiveness of the dynamic design on RetinaNet with ResNet-50-FPN as backbone on COCO val set. 55.4 38.4 20.3 39.5 46.5 Dynamic RetinaNet 36.3 55.5 38.8 20.7 39.9 47.5</figDesc><table><row><cell>Method</cell><cell>AP AP50 AP75 APS APM APL</cell></row><row><cell>RetinaNet</cell><cell>35.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 .</head><label>10</label><figDesc>Experimental results using ResNet-50-FPN backbone on PASCAL VOC2007 test set. CNN 48.7 76.9 71.5 60.9 42.1 11.6</figDesc><table><row><cell>Method</cell><cell cols="2">AP AP50 AP60 AP70 AP80 AP90</cell></row><row><cell>Faster R-CNN</cell><cell>47.2 76.9 71.0 59.9 39.2</cell><cell>8.1</cell></row><row><cell>Dynamic R-</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/maskrcnn-benchmark</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Soft-NMS -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SimpleDet: A simple and versatile distributed framework for object detection and instance recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">156</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01570</idno>
		<title level="m">Revisiting feature alignment for one-stage object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Revisiting RCNN: On awakening the classification power of faster RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Fast R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Appearance-preserving 3d convolution for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning where to focus for efficient video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">DetNet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Libra R-CNN: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">MegDet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An analysis of scale invariance in object detection -SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning to rank proposals for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Side-aware boundary localization for more precise object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep regionlets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">RepPoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cascade RetinaNet: Maintaining consistency for single-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bridging the gap between anchorbased and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">FreeAnchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
