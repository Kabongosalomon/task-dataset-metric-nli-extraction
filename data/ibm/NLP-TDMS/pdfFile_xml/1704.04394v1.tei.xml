<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a Deep Stochastic IOC 1 RNN Encoderdecoder framework, DESIRE, for the task of future predictions of multiple interacting agents in dynamic scenes. DESIRE effectively predicts future locations of objects in multiple scenes by 1) accounting for the multi-modal nature of the future prediction (i.e., given the same context, future may vary), 2) foreseeing the potential future outcomes and make a strategic prediction based on that, and 3) reasoning not only from the past motion history, but also from the scene context as well as the interactions among the agents. DESIRE achieves these in a single end-to-end trainable neural network model, while being computationally efficient. The model first obtains a diverse set of hypothetical future prediction samples employing a conditional variational autoencoder, which are ranked and refined by the following RNN scoring-regression module. Samples are scored by accounting for accumulated future rewards, which enables better long-term strategic decisions similar to IOC frameworks. An RNN scene context fusion module jointly captures past motion histories, the semantic scene context and interactions among multiple agents. A feedback mechanism iterates over the ranking and refinement to further boost the prediction accuracy. We evaluate our model on two publicly available datasets: KITTI and Stanford Drone Dataset. Our experiments show that the proposed model significantly improves the prediction accuracy compared to other baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is far better to foresee even without certainty than not to foresee at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Henri Poincaré (Foundations of Science)</head><p>Considering the future as a consequence of a series of past events, a prediction entails reasoning about probable 1 IOC: Abbreviation for inverse optimal control, which will be more explained throughout the paper.  outcomes based on past observations. But predicting the future in many computer vision tasks is inherently riddled with uncertainty (see <ref type="figure" target="#fig_1">Fig. 1</ref>). Imagine a busy traffic intersection, where such ambiguity is exacerbated by diverse interactions of automobiles, pedestrians and cyclists with each other, as well as with semantic elements such as lanes, crosswalks and traffic lights. Despite tremendous recent interest in future prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>, existing state-of-the-art produces outcomes that are either deterministic, or do not fully account for interactions, semantic context or long-term future rewards.</p><p>In contrast, we present DESIRE, a Deep Stochastic IOC RNN Encoder-decoder framework, to overcome those limitations. The key traits of DESIRE are its ability to simultaneously: (a) generate diverse hypotheses to reflect a distribution over plausible futures, (b) reason about interactions between multiple dynamic objects and the scene context, (c) rank and refine hypotheses with consideration of long-term future rewards (see <ref type="figure" target="#fig_1">Fig. 1</ref>). These objectives are cast within a deep learning framework.</p><p>We model the scene as composed of semantic elements (such as roads and crosswalks) and dynamic participants or agents (such as cars and pedestrians). A static or moving observer is also considered as an instance of an agent. We formulate future prediction as determining the locations of agents at various instants in the future, relying solely on observations of the past states of the scene, in the form of agent trajectories and scene context derived from image-based features or other sensory data if available. The problem is posed in an optimization framework that maximizes the potential future reward of the prediction. Specifically, we propose the following novel mechanisms to realize the above advantages, also illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>:</p><p>• Diverse Sample Generation: Sec. 3.1 presents a conditional variational auto-encoder (CVAE) framework <ref type="bibr" target="#b40">[41]</ref> to learn a sampling model that, given observations of past trajectories, produces a diverse set of prediction hypotheses to capture the multimodality of the space of plausible futures. The CVAE introduces a latent variable to account for the ambiguity of the future, which is combined with a recurrent neural network (RNN) encoding of past trajectories, to generate hypotheses using another RNN. • IOC-based Ranking and Refinement: In Sec. 3.2, we propose a ranking module that determines the most likely hypotheses, while incorporating scene context and interactions. Since an optimal policy is hard to determine where multiple agents make strategic inter-dependent choices, the ranking objective is formulated to account for potential future rewards similar to inverse optimal control (IOC). This also ensures generalization to new situations further in the future, given limited training data. The module is trained in a multitask framework with a regression-based refinement of the predicted samples. In the testing phase, we iterate the above multiple times to obtain more accurate refinements of the future prediction. • Scene Context Fusion: Sec. 3.3 presents the Scene Context Fusion (SCF) layer that aggregates interactions between agents and the scene context encoded by a convolutional neural network (CNN). The fused embedding is channeled to the aforementioned RNN scoring module and allows to produce the rewards based on the contextual information.</p><p>While DESIRE is a general framework that is applicable to any future prediction task, we demonstrate its utility in two applications -traffic scene understanding for autonomous driving and behavior prediction in aerial surveillance. Sec. 4 demonstrates outstanding accuracy for predicting the future locations of traffic participants in the KITTI raw dataset and pedestrians in the Stanford Drone dataset.</p><p>To summarize, this paper presents DESIRE, which is a deep learning based stochastic framework for time-profiled distant future prediction, with several attractive properties:</p><p>• Scalability: The use of deep learning rather than handcrafted features enables end-to-end training and easy incor-poration of multiple cues arising from past motions, scene context and interactions between multiple agents. • Diversity: The stochastic output of a deep generative model (CVAE) is combined with an RNN encoding of past observations to generate multiple prediction hypotheses that hallucinate ambiguities and multimodalities inherent in future prediction. • Accuracy: The IOC-based framework accumulates longterm future rewards for sampled trajectories and the regression-based refinement module learns to estimate a deformation of the trajectory, enabling more accurate predictions further into the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Classical methods Path prediction problems have been studied extensively with different approaches such as Kalman filters <ref type="bibr" target="#b17">[18]</ref>, linear regressions <ref type="bibr" target="#b28">[29]</ref> to non-linear Gaussian Process regression models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>, autoregressive models <ref type="bibr" target="#b1">[2]</ref> and time-series analysis <ref type="bibr" target="#b31">[32]</ref>. Such predictions suffice for scenarios with few interactions between the agent and the scene or other agents (like a flight monitoring system). In contrast, we propose methods for more complex environments such as surveillance for a crowd of pedestrians or traffic intersections, where the locomotion of individual agents is severely influenced by the scene context (e.g., drivable road or building) and the other agents (e.g., people or cars try to avoid colliding with the other). IOC for path prediction Kitani et al. recover human preferences (i.e., reward function) to forecast plausible paths for a pedestrian in <ref type="bibr" target="#b22">[23]</ref> using inverse optimal control (IOC), or inverse reinforcement learning (IRL) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b51">52]</ref>, while <ref type="bibr" target="#b25">[26]</ref> adapt IOC and propose a dynamic reward function to address changes in environments for sequential path predictions. Combined with a deep neural network, deep IOC/IRL has been proposed to learn non-linear reward functions and showed promising results in robot control <ref type="bibr" target="#b10">[11]</ref> and driving <ref type="bibr" target="#b49">[50]</ref> tasks. However, one critical assumption made in IOC frameworks, which makes them hard to be applied to general path prediction tasks, is that the goal state or the destination of agent should be given a priori, whereby feasible paths must be found to the given destination from the planning or control point of view. A few approaches relaxed this assumption with so-called goal set <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10]</ref>, but these goals are still limited to a target task space. Furthermore, a recovered cost function using IOC is inherently static, thus it is not suitable for time-profiled prediction tasks. Finally, past approaches do not incorporate interaction between agents, which is often a key constraint to the motion of multiple agents. In contrast, our methods are designed for more natural scenarios where agent goals are open-ended, unknown or time-varying and where agents interact with each other while dynamically adapting in anticipation of future behaviors. Future prediction Walker et al. <ref type="bibr" target="#b46">[47]</ref> propose a visual pre-diction framework with a data-driven unsupervised approach, but only on a static scene, while <ref type="bibr" target="#b4">[5]</ref> learn scene-specific motion patterns and apply to novel scenes for motion prediction as a knowledge transfer. A method for future localization from egocentric perspective is also addressed successfully in <ref type="bibr" target="#b29">[30]</ref>. But unlike our method, none of those can provide time-profiled predictions. Recently, a large dataset is collected in <ref type="bibr" target="#b35">[36]</ref> to propose the concept of social sensitivity to improve forecasting models and the multi-target tracking task. However, their social force <ref type="bibr" target="#b13">[14]</ref> based model has limited navigation styles represented merely using parameters of distance-based Gaussians.</p><p>Interactions When modeling the behavior of an agent, it should also be taken into account that the dynamics of an agent not only depend on its own, but also on the behavior of others. Predicting the dynamics of multiple objects is also studied in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref>, to name a few. Recently, a novel pooling layer is presented by <ref type="bibr" target="#b2">[3]</ref>, where the hidden state of neighboring pedestrians are shared together to joinly reason across multiple people. Nonetheless, these models lack predictive capacity as they do not take into account scene context. In <ref type="bibr" target="#b23">[24]</ref>, a dynamic Bayesian network to capture situational awareness is proposed as a context cue for pedestrian path prediction, but the model is limited to orientations and distances of pedestrians to vehicles and the curbside. A large body of work in reinforcement learning, especially game theoretical generalizations of Markov Decision Processes (MDPs), addresses multi-agent cases such as minmax-Q learning <ref type="bibr" target="#b26">[27]</ref> and Nash-Q learning <ref type="bibr" target="#b15">[16]</ref>. However, as noted in <ref type="bibr" target="#b37">[38]</ref>, typically learning in multi-agent setting is inherently more complex than single agent setting <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>RNNs for sequence prediction Recurrent neural networks (RNNs) are natural generalizations of feedforward neural networks to sequences <ref type="bibr" target="#b41">[42]</ref> and have achieved remarkable results in speech recognition <ref type="bibr" target="#b12">[13]</ref>, machine translation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7]</ref> and image captioning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b8">9]</ref>. The power of RNNs for sequence-to-sequence modeling thus makes them a reasonable model of choice to learn to generate sequential future prediction outputs. Our approach is similar to <ref type="bibr" target="#b6">[7]</ref> in making use of the encoder-decoder structure to embed a hidden representation for encoding and decoding variable length inputs and outputs. We choose to use gated recurrent units (GRUs) over long short-term memory units (LSTMs) <ref type="bibr" target="#b14">[15]</ref> since the former is found to be simpler yet yields no degraded performance <ref type="bibr" target="#b7">[8]</ref>. Despite the promise inherent in RNNs, however, only a few works have applied RNNs to behavior prediction tasks. Multiple LSTMs are used in <ref type="bibr" target="#b2">[3]</ref> to jointly predict human trajectories, but their model is limited to producing fixed-length trajectories, whereas our model can produce variable-length ones. A Fusion-RNN that combines information from sensory streams to anticipate a driver's maneuver is proposed in <ref type="bibr" target="#b16">[17]</ref>, but again their model outputs deterministic and fixed-length predictions.</p><p>Deep generative models Our work is also related to deep generative models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>, as we have a sample generation process that is built on a variational auto-encoder (VAE) <ref type="bibr" target="#b21">[22]</ref> within the framework. Since our prediction model essentially performs posterior-based probabilistic inference where candidate samples are generated based on conditioning variables (i.e., past motions besides latent variables), we naturally extend our method to exploit a conditional variational auto-encoder (CVAE) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref> during the sample generation process. Dense trajectories of pixels are predicted from a single image using CVAE in <ref type="bibr" target="#b45">[46]</ref>, while we focus on predicting long-term behaviors of multiple interacting agents in dynamic scenes. Unlike our framework, all aforementioned approaches lack either consideration of scene context, modeling of interaction with other agents or capabilities in producing continuous, time-profiled and long-term accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We formulate the future prediction problem as an optimization process, where the objective is to learn the posterior distribution P (Y|X, I) of multiple agents' future trajecto-</p><formula xml:id="formula_0">ries Y = {Y 1 , Y 2 , .</formula><p>., Y n } given their past trajectories X = {X 1 , X 2 , .., X n } and sensory input I where n is the number of agents. The future trajectory of an agent i is defined as</p><formula xml:id="formula_1">Y i = {y i,t+1 , y i,t+2 , .</formula><p>., y i,t+δ }, and the past trajectory is defined similarly as</p><formula xml:id="formula_2">X i = {x i,t−ι+1 , x i,t−ι+2 , .., x i,t }.</formula><p>Here, each element of a trajectory (e.g., y i,t ) is a vector in R 2 (or R 3 ) representing the coordinates of agent i at time t, and δ and ι refer to the maximum length of time steps for future and past respectively. Since direct optimization of continuous and high dimensional Y is not feasible, we design our method to first sample a diverse set of future predictions and assign a probabilistic score to each of the samples to approximate P (Y|X, I). In this section, we describe the details of DESIRE <ref type="figure">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Diverse Sample Generation with CVAE</head><p>Future prediction can be inherently ambiguous and has uncertainties as multiple plausible scenarios can be explained under the same past situation (e.g., a vehicle heading toward an intersection can make different turns as seen in <ref type="figure" target="#fig_1">Fig. 1</ref>). Thus, learning a deterministic function f that directly maps {X, I} to Y will under-represent potential prediction space and easily over-fit to training data. Moreover, a naively trained network with a simple loss will produce predictions that average out all possible outcomes.</p><p>In order to tackle the uncertainty, we adopt a deep generative model, conditional variational auto-encoder (CVAE) <ref type="bibr" target="#b40">[41]</ref>, inside of DESIRE framework. CVAE is a generative model that can learn the distribution P (Y i |X i ) of the output Y i conditioned on the input X i by introducing a stochastic latent variable z i 2 . It is composed of multiple neural networks, such as recognition network</p><formula xml:id="formula_3">Q φ (z i |Y i , X i ), (conditional) prior network P ν (z i |X i ), and generation net- work P θ (Y i |X i , z i ).</formula><p>Here, θ, φ, ν denote the parameters of corresponding networks. The prior of the latent variables z i is modulated by the input X i , however, this can be relaxed to make the latent variables statistically independent of input variables, i.e., P ν (z i |X i ) = P ν (z i ) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>. Essentially, a CVAE introduces stochastic latent variables z i that are learned to encode a diverse set of predictions Y i given input X i , making it suitable for modeling one-to-many mapping. During training, Q φ (z i |Y i , X i ) is learned such that it gives higher probability to z i that is likely to produce a reconstruc-tionŶ i close to actual prediction given the full context X i and Y i . At test time z i is sampled randomly from the prior distribution and decoded through the decoder network to produce a prediction hypothesis. This enables probabilistic inference which serves to handle multi-modalities in the prediction space. Train phase: Firstly, the past and future trajectories of an agent i, X i and Y i respectively, are encoded through two RNN encoders with separate set of parameters (i.e., RNN En-coder1 and RNN Encoder2 in <ref type="figure" target="#fig_2">Fig. 2</ref>). The resulting two encodings, H Xi and H Yi , are concatenated and passed through one fully connected (f c) layer with a non-linear activation (e.g., relu). Two side-by-side f c layers are followed to produce both the mean µ zi and the standard deviation σ zi over z i . The distribution of z i is modeled as a Gaussian distribution (i.e., z i ∼ Q φ (z i |X i , Y i ) = N (µ zi , σ zi )) and is regularized by the KL divergence against a prior distribution P ν (z i ) := N (0, I) during the training. Upon successful training, the target distribution is learned in the latent vari- <ref type="bibr" target="#b1">2</ref> Notice that we learn the distribution independently over different agents in this step. Interaction between agents is considered in Sec. <ref type="bibr">3.2.</ref> able z i , which allows one to draw a random sample z i from a Gaussian distribution to reconstruct Y i at test time. Since back-propagation is not possible through random sampling, we adopt the standard reparameterization trick <ref type="bibr" target="#b21">[22]</ref> to make it differentiable.</p><p>In order to model P θ (Y i |X i , z i ), z i is combined with X i as follows. The sampled latent variable z i is passed to one f c layer to match the dimension of H Xi that is followed by a sof tmax layer, producing β(z i ). Then that is combined with the encodings of past trajectories H Xi through a masking operation (i.e., element-wise multiplication). One can interpret this as a guided drop out where the guidance β is derived from the full context of individual trajectory during the training phase, while it is randomly drawn from X i , Y i agnostic prior distribution z (k) i ∼ P ν (z i ) in the testing phase. Finally, the following RNN decoder (i.e., RNN Decoder1 in <ref type="figure" target="#fig_2">Fig. 2</ref>) takes the output of the previous step, H Xi β(z (k) i ), and generates K number of future prediction samples, i.e.,</p><formula xml:id="formula_4">Y i (1) ,Ŷ i (2) , ..,Ŷ i (K)</formula><p>. There are two loss terms in training the CVAE-based RNN encoder-decoder.</p><p>• Reconstruction Loss:</p><formula xml:id="formula_5">Recon = 1 K k Y i −Ŷ i (k)</formula><p>. This loss measures how far the generated samples are from the actual ground truth.  <ref type="figure" target="#fig_2">Fig. 2)</ref> to generate a diverse set of prediction hypotheses. Further details: For both train and test phases, we pass trajectories through a temporal convolution layer before encoding to encourage the network to learn the concept of velocity from adjacent frames before getting passed into RNN encoders. Also, RNNs are implemented using gated recurrent units (GRU) <ref type="bibr" target="#b6">[7]</ref> to learn long-term dependencies, yet they can be easily replaced with other popular RNNs like long short-term memory units (LSTM) <ref type="bibr" target="#b14">[15]</ref>. In summary, this sample generation module produces a set of diverse hypotheses critical to capturing the multimodality of the prediction task, through a effective combination of CVAE and RNN encoder-decoder. Unlike <ref type="bibr" target="#b45">[46]</ref>, where CVAE is used to predict for short-term visual motion from a single image, our CVAE module generates diverse set of future trajectories based on a past trajectory.</p><formula xml:id="formula_6">• KLD Loss: KLD = D KL (Q φ (z i |Y i , X i ) P ν (z i )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">IOC-based Ranking and Refinement</head><p>Predicting a distant future can be far more challenging than predicting one close by. In order to tackle this, we adopt the concept of decision-making process in reinforcement learning (RL) where an agent is trained to choose its actions that maximizes long-term rewards to achieve its goal <ref type="bibr" target="#b42">[43]</ref>. Instead of designing a reward function manually, however, IOC <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b10">11]</ref> learns an unknown reward function. Inspired by this, we design an RNN model that assigns rewards to each prediction hypothesisŶ i (k) and measures their goodness s (k) i based on the accumulated long-term rewards. Thereafter, we also directly refine prediction hypotheses by learning dis-</p><formula xml:id="formula_7">placements Ŷ i (k)</formula><p>to the actual prediction through another f c layer. Lastly, the module receives iterative feedbacks from regressed predictions and keeps adjusting so that it produces precise predictions at the end. The model is illustrated in the right side of <ref type="figure" target="#fig_2">Fig. 2</ref>. During the process, we combine 1) past motion history through the embedding vector H X , 2) semantic scene context through a CNN with parameters ρ, and 3) interaction among multiple agents by using interaction features (Sec. 3.3). Notice that unlike typical robotics applications <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b10">11]</ref>, we do not assume that the goal (final destination) is known or the dynamics of the agents are given. Our model learns the agents dynamics as well as the scene context in a coherent framework. Learning to score: For an agent i, there are K number of samples (i.e.,Ŷ  </p><p>whereŶ <ref type="bibr">(∀)</ref> j\i is the prediction samples of other agents (i.e., ∀j, where j = i),ŷ τ &lt;t is all the prediction samples until a time-step t, T is the maximum prediction length, and ψ is the reward function that assigns a reward value at each time-step. ψ is implemented as an f c layer that is connected to the hidden vector of RNN cell at each time step. We share the parameters of the f c layer over all the time steps (each RNN cell outputs the hidden state of the same dimension). Therefore, the score s is accumulated rewards over time, accounting for the entire future rewards being assigned to each hypothesis. This enables our model to make a strategic decision by allowing us to rank samples as in other samplingbased IOC frameworks <ref type="bibr" target="#b10">[11]</ref>. In addition, the reward function ψ incorporates both scene context I as well as the interaction between agents (see Sec. 3.3). Learning to refine: Alongside the scores, our model also estimates a regression vector Ŷ (k) i that refines each prediction sampleŶ (k) i . The regression vector for each agent i is obtained with the regression function η defined as follows,</p><formula xml:id="formula_9">Ŷ (k) i = η(Ŷ (k) i ; I, X,Ŷ (∀) j\i ).<label>(2)</label></formula><p>Represented as parameters of a neural network, the regression function η accumulates both scene contexts and all other agents dynamics from the past to entire future frames, and estimates the best displacement vector Ŷ (k) i over entire time-horizon T . Similarly to the score s, it accounts for what happens in the future both in terms of scene context and interactions among dynamic agents to produce the output. We implement η as another f c layer that is connected to the last hidden vector of the RNN which outputs M ×T dimensional vector. M = 2 (or 3) is the dimension of the location state. Iterative feedback: Using the displacement vector Ŷ (k) i , we iteratively refine the prediction hypothesisŶ , and fed into the IOC module. This process is similar to the gradient descent optimization ofŶ i over the score function s, but it does not require to compute the gradient over RNN which can be very unstable due to the recurrent structure (i.e., vanishing or exploding gradient). We observe that iterative refinement indeed improves the quality of prediction samples in the experiments (see <ref type="figure">Fig. 4</ref> and <ref type="figure">Fig. 5</ref>). Losses: There are two loss terms in training the IOC ranking and refinement module.</p><p>• Cross-entropy Loss: CE = H(p, q) of which the target distribution q is obtained by sof tmax</p><formula xml:id="formula_10">(−d(Y i ,Ŷ (k) i )), where d(Y i ,Ŷ (k) i ) = max Ŷ (k) i − Y i . • Regression Loss: Reg = 1 K k Y i −Ŷ i (k) − Ŷ i (k)</formula><p>Finally, the total loss of the entire network is defined as a multi-task loss as follows, where N is the number of agents in one batch. <ref type="figure">Figure 3</ref>. Details of Scene Context Fusion unit (SCF) in RNN Decoder2 in <ref type="figure" target="#fig_2">Fig. 2</ref>. Note that the input to the GRU cell at each time-step, xt, integrates multiple cues (i.e., the dynamics of agents, scene context and interaction between agents).</p><formula xml:id="formula_11">T otal = 1 N i∈N Recon + KLD + CE + Reg (3) SCF Feature Pooling RNN Decoder2 Velocity fc ReLU (yi,t) ∧ • x t p (y i,t; ρ(I)) ∧ hY j\i ∧ r (y i,t; yj\i,t,hY j\i ) ∧ ∧ ∧ GRU ⊞ GRU x t-1 y j\i,t ∧ y i,t ∧ ρ(I) GRU x t+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scene Context Fusion</head><p>As discussed in the previous section, our ranking and refinement module relies on the hidden representation of the shared RNN module. Thus, it is important that the RNN must contain the information about 1) individual past motion context, 2) semantic scene context and 3) the interaction between multiple agents, in order to provide proper hidden representations that can score and refine a predictionŶ (k) i . We achieve the goal by having an RNN that takes following input x t at each time step:</p><formula xml:id="formula_12">x t = γ(v i,t ), p(ŷ i,t ; ρ(I)), r(ŷ i,t ;ŷ j\i,t , hŶ j\i )<label>(4)</label></formula><p>wherev i,t is a velocity ofŶ (k) i at t, γ is a f c layer with a ReLU activation that maps the velocity to a high dimensional representation space, p(ŷ i,t ; ρ(I)) is a pooling operation that pools the CNN feature ρ(I) at the locationŷ i,t , r(ŷ i,t ;ŷ j\i,t , hŶ j\i ) is the interaction feature computed by a fusion layer that spatially aggregates other agents hidden vectors, similar to SocialPooling (SP) layer <ref type="bibr" target="#b2">[3]</ref>. The embedding vector H Xi (the output of the RNN Encoder1 in <ref type="figure" target="#fig_2">Fig. 2)</ref> is shared as the initial hidden state of the RNN, in order to provide the individual past motion context. We share this embedding with the CVAE module since both require the same information to be embedded in the vector. Interaction Feature: We implement a spatial grid based pooling layer similar to SP layer <ref type="bibr" target="#b2">[3]</ref>. For each sample k of an agent i at t, we define spatial grid cells centered atŷ  j,t ∈ g. Instead of using the max pooling operation with rectangular grids, we adopt log-polar grids with an average pooling. Combined with CNN features, the SCF module provides the RNN decoder with both static and dynamic scene information. It learns consistency between semantics of agents and scenes for reliable prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Characteristics of DESIRE</head><p>This section highlights particularly distinctive features of DESIRE that naturally enable higher accuracy and reliability.</p><p>• The framework is based on deep neural network and is trainable end-to-end, rather than relying on hand-crafted parametric representation and interactions terms. Trajectories of each agent are represented using RNN encoders and are combined together through a fusion layer within the architecture. Scene context is represented through CNN and is not solely restricted to images (i.e., can handle nonvisual sensors too). Overall, the algorithm is scalable and flexible. • CVAE is combined with RNN encodings to generate stochastic prediction hypothesis, which handles ambiguities and multimodalities inherent in future prediction. • A novel RNN module coherently integrates multiple cues that have critical influence on behavior prediction such as dynamics of all neighboring agents and scene semantics. • An IOC framework is used to train the trajectory ranking objective by measuring potential long-term future rewards. This makes the model less reactive, and enables more accurate predictions further into the future. • A regression vector is learned to refine trajectories and an iterative feedback mechanism sequentially adjusts the predicted behavior, resulting in more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>KITTI Raw Data <ref type="bibr" target="#b11">[12]</ref>: The dataset provides images of driving scenes and Velodyne 3D laser scan along with calibration information between cameras and sensors. To prepare data examples (i.e., X, Y, I), we performed the following: As the dataset does not provide semantic labels for 3D points (which we need for scene context), we first perform semantic segmentations of images and project Velodyne laser scans onto the image plane using the provided camera matrix to label 3D points. The semantically labeled 3D points are then registered into the world coordinates using GPS-IMU tags. Finally we create top-down view feature maps I of size H × W × C (H, W : size of crop and C: number of classes for scene elements, e.g., road, sidewalk, and vegitation shown as red, blue and green color in <ref type="figure" target="#fig_11">Fig. 6.)</ref>. I is cropped with respect to the view point of the camera to simulate actual driving scenario (H, W = 80m and the size of pixel is 0.5m. The camera is located at the left-center.). Since laser scans on dynamic objects generate traces during registration, we remove moving objects and only use static scene elements. The trajectories X, Y are generated by extracting the center locations of the 3D tracklets and registering them in the world coordinates. We use all annotated videos from Road and City scenes for our experiments and generate approximately 2,500 training examples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics and Baselines</head><p>The following metrics are used to measure the performance of future prediction task in various aspects: (i) L2 distance between the prediction and ground truth at multiple time steps, (ii) miss-rate with a threshold in terms of L2 distance at multiple time steps, (iii) maximum L2 distance over entire time frames, (iv) maximum miss-rate over entire time frames, and (v) oracle error over top K number of samples (i.e., E oracle = min ∀k∈K E(Ŷ (k) i − Y i )) to account for the uncertainty in the future prediction (similar to MEE in <ref type="bibr" target="#b45">[46]</ref>). We set K to be 50 throughout the main experiments.</p><p>We compare our method with the following baselines: • Linear: A linear regressor that estimates linear parameters by minimizing the least square error. • RNN ED: An RNN encoder-decoder model that directly regresses the prediction only using the past trajectories. • RNN ED-SI: An RNN ED augmented with our SCF unit into the decoder similar to <ref type="bibr" target="#b16">[17]</ref>. The model combines the scene and interaction features while making prediction and uses the same information as ours, but makes a prediction at t + 1 solely based on the past information up to t. • DESIRE: The proposed method. We denote our model with only semantic scene context in SCF module as DESIRE-S and our model with both scene context and interaction as DESIRE-SI. We also evaluate DESIRE-X-IT{N}, where N is the number of iterative feedbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning Details</head><p>We train the model with Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with the initial learning rate of 0.004. The learning rate is decreased by half at every quarter of total epochs, albeit we do not observe clear improvement with this. All the models including Encoder-decoder baselines are trained for 600 epochs for KITTI and 8 epochs for SDD (about 50K iterations with a batch size 32). The full details on the architecture are discussed in the supplementary materials. In order to avoid Iteration 0 Iteration 1 Iteration 3 <ref type="figure">Figure 5</ref>. Improved DESIRE-SI prediction samples (red) over iterations. Iterative regression refines the predictions closer to the ground truth future trajectory (blue) matching with scene context. exploding gradient in RNNs, we apply gradient clipping with L2 norm of 1.0. During the training procedure, we randomly rotate the scene and trajectories to augment data and reduce over-fitting. For all experiments, we run randomized 5 fold cross validation without overlapping videos in different splits. All models observe maximum of 2 seconds for past trajectories and make a prediction up to 4 seconds into the future. All models are implemented using Tensor-Flow and trained end-to-end with a NVIDIA Tesla K80 GPU. Training takes approximately one to two days per model. <ref type="table" target="#tab_3">Table 1</ref> and <ref type="figure">Fig. 4</ref> compare the oracle prediction errors 3 of various methods. We present L2 distance error for both datasets and miss-rate with 1m threshold for KITTI only, as trajectories in SDD are defined in image pixel space. Note that Linear, RNN ED, and RNN ED-SI output a single prediction, thus their results are shown as horizontal lines. CVAE samples are sorted randomly without confidence values. Baselines: RNN ED performs significantly better than Linear since it can learn non-linear motion. We observe that RNN ED-SI performs worse than RNN ED on the KITTI since the model learns to behave reactive (see <ref type="figure" target="#fig_11">Fig. 6</ref>). This might be due to the small size of the dataset, which makes it hard to learn predictive CNN/interaction features (i.e., features need to have high capacity to encode long-term information). On the contrary, RNN ED-SI significantly outperforms RNN ED on SDD dataset since SDD is much bigger and has a large number of interactions among agents. Proposed models: With a single random sample (CVAE 1 in <ref type="table" target="#tab_3">Table 1</ref>), CVAE performs worse than RNN ED since RNN ED directly optimizes for L2 distance during training. Given more than few samples (e.g., CVAE 10% in <ref type="table" target="#tab_3">Table 1)</ref>, CVAE outperforms RNN ED quickly on both datasets, which confirms the multi-modal nature of the prediction problem. DESIRE-X-IT0 without iterative regression properly ranks the random CVAE samples achieving lower error with few samples. Note that DESIRE-X-IT0 only ranks the samples without regression, thus achieves the same error as used all samples, i.e., at Top K ratio of 1.0 in <ref type="figure">Fig. 4</ref>. As we iterate over, the outputs get refined and achieve smaller oracle error (i.e., DESIRE-X10%-IT0 vs. DESIRE-X10%-IT4). <ref type="figure">Fig. 5</ref> shows an example of the iterative feedback. Finally, we observe that considering the interaction between agents further helps to achieve lower error. The difference between  The row 4 shows the multi-modal nature of the prediction problem. While the cyclist is making a right turn, it is also possible that he turns around the round-about (denoted with arrow). DESIRE-SI predicts such equally possible future as the top prediction, while covering the ground truth future within top 10 predictions. The row 5&amp;6 also show that DESIRE-SI provides superior predictions by reasoning about both static and dynamic scene contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>DESIRE-S and DESIRE-SI is smaller in KITTI experiment, since KITTI has only few interactions between cars. However, we observe clear improvement on the SDD dataset since there are rich set of scenes with interactions between agents. Although our model with top 1 sample (DESIRE Best) achieves higher error compared to the direct regression baselines, using a few more samples yields much better prediction accuracy (i.e., DESIRE 10%). Note that direct regression models with lower error are not necessarily better if averaging various futures (e.g., going straight). We believe that in some applications, probabilistic prediction over a variety of outcomes is more desirable than a single MAP prediction. For both datasets, DESIRE achieves error on par with best baselines using as little as top 2 samples of DESIRE-SI-IT4 predictions (see <ref type="figure">Fig. 4</ref>). Qualitative results are presented in <ref type="figure" target="#fig_11">Fig. 6</ref> and in the supplementary material.</p><p>Ablative study: We conduct further experiments for varying K and past length to supplement the main experiments and report the results in <ref type="table" target="#tab_4">Table 2 and Table 3</ref>. <ref type="bibr">Method</ref> 1.0 (sec) 2.0 (sec) 3.0 (sec) 4.0 (sec) KITTI (error in meters / miss-rate with 1 m threshold)   <ref type="table">Table 3</ref>. Prediction errors of DESIRE-S-IT4 on KITTI at 4s for varying time length for past trajectory. The model trained with 1s past slightly worse than ours (2s), showing that 2 second past contains enough cues to encode motion context. Note also that prior works adopt similar past lengths (2.8s in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce a novel framework DESIRE for distant future prediction of multiple agents in complex scene. The model incorporates both static and dynamic scene contexts with a deep IOC framework and produces stochastic, continuous, and time-profiled long-term predictions that can effectively account for the uncertainty in the future prediction task. Our empirical evaluations on driving and surveillance scenarios demonstrate clear improvement over other baselines. For future work, we believe that our model can be further improved on larger datasets and be applied to various robotics applications with a direct use of perspective images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>(a) A driving scenario: The white van may steer into left or right while trying to avoid a collision to other dynamic agents. DESIRE produces accurate future predictions (shown as blue paths) by tackling multi-modaility of future prediction while accounting for a rich set of both static and dynamic scene contexts. (b) DESIRE generates a diverse set of hypothetical prediction samples, and then ranks and refines them through a deep IOC network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 )</head><label>2</label><figDesc>in the following structure: Sample Generation Module (Sec. 3.1), Ranking and Refinement Module (Sec. 3.2), and Scene Context Fusion (Sec. 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>The overview of proposed prediction framework DESIRE. First, DESIRE generates multiple plausible prediction samplesŶ via a CVAE-based RNN encoder-decoder (Sample Generation Module). Then the following module assigns a reward to the prediction samples at each time-step sequentially as IOC frameworks and learns displacements vector ∆Ŷ to regress the prediction hypotheses (Ranking and Refinement Module). The regressed prediction samples are refined by iterative feedback. The final prediction is the sample with the maximum accumulated future reward. Note that the flow via aquamarine-colored paths is only available during the training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>generated by our CVAE sampler. Let the score s of individual prediction hypothesisŶ (k) i for the agent i be defined as follows,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(k) i,t is the k th prediction sample of an agent i at time t,Ŷ (∀)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>. Over each grid cell g, we pool the hidden representation of all the other agents' samples that are within the spatial cell, ∀j = i, ∀k,ŷ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .</head><label>6</label><figDesc>KITTI results (top 3 rows): The row 1&amp;2 in (b) show highly reactive nature of RNN ED-SI (i.e., prediction turns after it hits near non-drivable area). On the contrary, DESIRE shows its long-term prediction capability by considering potential future rewards. DESIRE-SI also produces more convincing predictions in the presence of other vehicles. SDD results (bottom 3 rows):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This regularization loss measures how close the sampling distribution at test time is to the distribution of latent variable that we learn during training.</figDesc><table><row><cell>(k) i</cell><cell>drawn from the prior z</cell><cell>(k)</cell></row><row><cell></cell><cell>(k)</cell><cell></cell></row></table><note>Test phase: At test time, the encodings of future trajectories H Yi are not available, thus the encodings of past trajectories H Xi are combined with multiple random samples of latent variable zi ∼ P ν (z i ). Similar to the training phase, H Xi β(zi ) is passed to the following RNN decoder (i.e., RNN Decoder1 in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 4. Oracle prediction errors over the number of samples on the KITTI dataset. X axis represents the ratio of top samples used in the oracle error evaluation (Y axis). Best viewed in color.</figDesc><table><row><cell>Top-K Oracle Miss-rate</cell></row><row><cell>Linear RNN ED RNN ED-SI CVAE DESIRE-S-IT0 DESIRE-S-IT1 DESIRE-S-IT4 DESIRE-SI-IT0 DESIRE-SI-IT1 DESIRE-SI-IT4</cell></row><row><cell>Stanford Drone Dataset [36]: The dataset contains a large</cell></row><row><cell>volume of aerial videos captured in a university campus</cell></row><row><cell>using a drone. There are various classes of dynamic objects</cell></row><row><cell>interacting with each other, often in the form of high density</cell></row><row><cell>crowds. Except for less stabilized cameras and lost labels,</cell></row><row><cell>we used all videos to create examples to train/test our model,</cell></row><row><cell>yielding approximately 16, 000 examples. Note that we</cell></row><row><cell>directly use raw images to extract visual features, rather than</cell></row><row><cell>semantically labeled feature maps. We resize the images by</cell></row><row><cell>1/5 in following experiments to avoid memory overhead.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Prediction errors over future time steps on KITTI and SDD datasets. Our method, DESIRE-IT4, achieves by far the lowest top 10% error, addressing the multimodal nature of the task effectively.</figDesc><table><row><cell>Linear</cell><cell cols="2">0.89 / 0.31</cell><cell>2.07 / 0.49</cell><cell cols="2">3.67 / 0.59</cell><cell>5.62 / 0.64</cell></row><row><cell>RNN ED</cell><cell cols="2">0.45 / 0.13</cell><cell>1.21 / 0.39</cell><cell cols="2">2.35 / 0.54</cell><cell>3.86 / 0.62</cell></row><row><cell>RNN ED-SI</cell><cell cols="2">0.56 / 0.16</cell><cell>1.40 / 0.44</cell><cell cols="2">2.65 / 0.58</cell><cell>4.29 / 0.65</cell></row><row><cell>CVAE 1</cell><cell cols="2">0.61 / 0.22</cell><cell>1.81 / 0.50</cell><cell cols="2">3.68 / 0.60</cell><cell>6.16 / 0.65</cell></row><row><cell>CVAE 10%</cell><cell cols="2">0.35 / 0.06</cell><cell>0.93 / 0.30</cell><cell cols="2">1.81 / 0.49</cell><cell>3.07 / 0.59</cell></row><row><cell>DESIRE-S-IT0 Best</cell><cell cols="2">0.53 / 0.17</cell><cell>1.52 / 0.45</cell><cell cols="2">3.02 / 0.58</cell><cell>4.98 / 0.64</cell></row><row><cell>DESIRE-S-IT0 10%</cell><cell cols="2">0.32 / 0.05</cell><cell>0.84 / 0.26</cell><cell cols="2">1.67 / 0.43</cell><cell>2.82 / 0.54</cell></row><row><cell>DESIRE-S-IT4 Best</cell><cell cols="2">0.51 / 0.15</cell><cell>1.46 / 0.42</cell><cell cols="2">2.89 / 0.56</cell><cell>4.71 / 0.63</cell></row><row><cell>DESIRE-S-IT4 10%</cell><cell cols="2">0.27 / 0.04</cell><cell>0.64 / 0.18</cell><cell cols="2">1.21 / 0.30</cell><cell>2.07 / 0.42</cell></row><row><cell>DESIRE-SI-IT0 Best</cell><cell cols="2">0.52 / 0.16</cell><cell>1.50 / 0.44</cell><cell cols="2">2.95 / 0.57</cell><cell>4.80 / 0.63</cell></row><row><cell>DESIRE-SI-IT0 10%</cell><cell cols="2">0.33 / 0.06</cell><cell>0.86 / 0.25</cell><cell cols="2">1.66 / 0.42</cell><cell>2.72 / 0.53</cell></row><row><cell>DESIRE-SI-IT4 Best</cell><cell cols="2">0.51 / 0.15</cell><cell>1.44 / 0.42</cell><cell cols="2">2.76 / 0.54</cell><cell>4.45 / 0.62</cell></row><row><cell>DESIRE-SI-IT4 10%</cell><cell cols="2">0.28 / 0.04</cell><cell>0.67 / 0.17</cell><cell cols="2">1.22 / 0.29</cell><cell>2.06 / 0.41</cell></row><row><cell></cell><cell cols="4">SDD (pixel error at 1/5 resolution)</cell></row><row><cell>Linear</cell><cell>2.58</cell><cell></cell><cell>5.37</cell><cell>8.74</cell><cell>12.54</cell></row><row><cell>RNN ED</cell><cell>1.53</cell><cell></cell><cell>3.74</cell><cell>6.47</cell><cell>9.54</cell></row><row><cell>RNN ED-SI</cell><cell>1.51</cell><cell></cell><cell>3.56</cell><cell>6.04</cell><cell>8.80</cell></row><row><cell>CVAE 1</cell><cell>2.51</cell><cell></cell><cell>6.01</cell><cell cols="2">10.28</cell><cell>14.82</cell></row><row><cell>CVAE 10%</cell><cell>1.84</cell><cell></cell><cell>3.93</cell><cell>6.47</cell><cell>9.65</cell></row><row><cell>DESIRE-S-IT0 Best</cell><cell>2.02</cell><cell></cell><cell>4.47</cell><cell>7.25</cell><cell>10.29</cell></row><row><cell>DESIRE-S-IT0 10%</cell><cell>1.59</cell><cell></cell><cell>3.31</cell><cell>5.27</cell><cell>7.75</cell></row><row><cell>DESIRE-S-IT4 Best</cell><cell>2.11</cell><cell></cell><cell>4.69</cell><cell>7.58</cell><cell>10.66</cell></row><row><cell>DESIRE-S-IT4 10%</cell><cell>1.30</cell><cell></cell><cell>2.41</cell><cell>3.67</cell><cell>5.62</cell></row><row><cell>DESIRE-SI-IT0 Best</cell><cell>2.00</cell><cell></cell><cell>4.41</cell><cell>7.18</cell><cell>10.23</cell></row><row><cell>DESIRE-SI-IT0 10%</cell><cell>1.55</cell><cell></cell><cell>3.24</cell><cell>5.18</cell><cell>7.61</cell></row><row><cell>DESIRE-SI-IT4 Best</cell><cell>2.12</cell><cell></cell><cell>4.69</cell><cell>7.55</cell><cell>10.65</cell></row><row><cell>DESIRE-SI-IT4 10%</cell><cell>1.29</cell><cell></cell><cell>2.35</cell><cell>3.47</cell><cell>5.33</cell></row><row><cell>Method</cell><cell></cell><cell cols="4">K (the number of prediction samples)</cell></row><row><cell></cell><cell></cell><cell>25</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell cols="2">DESIRE-S-IT4 Best</cell><cell>4.87</cell><cell>4.71</cell><cell>4.81</cell><cell>4.70</cell></row><row><cell cols="2">DESIRE-S-IT4 top20</cell><cell>2.03</cell><cell>2.04</cell><cell>1.99</cell><cell>1.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Prediction errors of DESIRE-S-IT4 on KITTI at 4s for varying K. The best sample errors remain similar, while top 20 oracle errors decrease slightly as K increases.</figDesc><table><row><cell>Method</cell><cell cols="3">Time length for past (sec)</cell></row><row><cell></cell><cell>1.0</cell><cell>2.0</cell><cell>4.0</cell></row><row><cell>DESIRE-S-IT4 Best</cell><cell>4.94</cell><cell>4.71</cell><cell>4.78</cell></row><row><cell>DESIRE-S-IT4 10%</cell><cell>2.11</cell><cell>2.07</cell><cell>2.05</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The maximum error inTable 1might be different fromFig. 4due to the test examples without ground truth labels at 4 seconds in the future.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was part of N. Lee's summer internship at NEC Labs America and also supported by the EPSRC, ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fitting autoregressive models for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annals of the institute of Statistical Mathematics</title>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="243" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Knowledge transfer for scene-specific motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06987</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, And Cybernetics-Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Manipulation planning with goal sets using constrained trajectory optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="4582" to="4588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Guided cost learning: Deep inverse optimal control via policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00448</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nash q-learning for general-sum stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for driver activity anticipation via sensory-fusion architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-based pedestrian path prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F P</forename><surname>Kooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="618" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to predict trajectories of cooperatively navigating agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4015" to="4020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting wide receiver trajectories in american football</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Goal set inverse optimal control and iterative re-planning for predicting human reaching motions in shared workspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mainprice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02111</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generalized linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>CRC press</publisher>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Egocentric future localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Spectral analysis and time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Priestley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A unifying view of sparse approximate gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1939" to="1959" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Long-term planning by short-term prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ben-Zrihem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01580</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multiagent systems: Algorithmic, game-theoretic, and logical foundations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">If multi-agent learning is the answer, what is the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep generative stochastic networks trainable by backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thibodeau-Laufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Patch to the future: Unsupervised visual prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3302" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prediction with gaussian processes: From linear regression to linear prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="599" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Watch this: Scalable cost-function learning for path planning in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02329</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
