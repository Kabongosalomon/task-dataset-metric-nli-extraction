<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Makes Training Multi-modal Classification Networks Hard?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
							<email>weiyaowang@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
							<email>trandu@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">What Makes Training Multi-modal Classification Networks Hard?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consider end-to-end training of a multi-modal vs. a unimodal network on a task with multiple input modalities: the multi-modal network receives more information, so it should match or outperform its uni-modal counterpart. In our experiments, however, we observe the opposite: the best uni-modal network often outperforms the multi-modal network. This observation is consistent across different combinations of modalities and on different tasks and benchmarks for video classification.</p><p>This paper identifies two main causes for this performance drop: first, multi-modal networks are often prone to overfitting due to their increased capacity. Second, different modalities overfit and generalize at different rates, so training them jointly with a single optimization strategy is sub-optimal. We address these two problems with a technique we call Gradient-Blending, which computes an optimal blending of modalities based on their overfitting behaviors. We demonstrate that Gradient Blending outperforms widely-used baselines for avoiding overfitting and achieves state-of-the-art accuracy on various tasks including human action recognition, ego-centric action recognition, and acoustic event detection. <ref type="figure">Figure 1</ref>: Standard regularizers do not provide a good improvement over the best Uni-modal network. Best uni-modal network (RGB) vs standard approaches on a multi-modal network (RGB+Audio) on Kinetics. Various methods to avoid overfitting (orange: Pre-training, Early-stopping, and Dropout) do not solve the issue. Different fusion architectures (red: Midconcatenation fusion, SE-gate, and NL-gate) also do not help. Dropout and Mid-concatenation fusion approaches provide small improvements (+0.3% and +0.2%), while other methods degrade accuracy. multi-modal networks have higher train accuracy and lower validation accuracy. Late fusion audio-visual (A+RGB) network has nearly two times the parameters of a visual network, and one may suspect that the overfitting is caused by the increased number of parameters.</p><p>There are two ways to approach this problem. First, one can consider solutions such as dropout <ref type="bibr" target="#b42">[43]</ref>, pre-training, or early stopping to reduce overfitting. On the other hand, one may speculate that this is an architectural deficiency. We experiment with mid-level fusion by concatenation <ref type="bibr" target="#b36">[37]</ref> and fusion by gating [31], trying both Squeeze-and-Excitation (SE) <ref type="bibr" target="#b25">[26]</ref> gates and Non-Local (NL) [51] gates.</p><p>Remarkably, none of these provide an effective solu-1 arXiv:1905.12681v5 [cs.CV] 3 Apr 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider a late-fusion multi-modal network, trained endto-end to solve a task. Uni-modal solutions are a strict subset of the solutions available to the multi-modal network; a well-optimized multi-modal model should, in theory, always outperform the best uni-modal model. However, we show here that current techniques do not always achieve this. In fact, what we observe is contrary to common sense: the best uni-modal model often outperforms the joint model, across different modalities <ref type="table" target="#tab_0">(Table 1</ref>) and datasets (details in section 3). Anecdotally, the performance drop with multiple input streams appears to be common and was noted in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref>. This (surprising) phenomenon warrants investigation and solution.</p><p>Upon inspection, the problem appears to be overfitting: tion. For each method, we record the best audio-visual results on Kinetics in <ref type="figure" target="#fig_1">Figure 1</ref>. Pre-training fails to offer improvements, and early stopping tends to under-fit the RGB stream. Mid-concat and dropout provide only modest improvements over RGB model. We note that dropout and mid-concat (with 37% fewer parameters compared to late-concat) make 1.5% and 1.4% improvements over lateconcat, confirming the overfitting problem with late-concat. We refer to supplementary materials for details. How do we reconcile these experiments with previous multi-modal successes? Multi-modal networks have successfully been trained jointly on tasks including sound localization <ref type="bibr" target="#b58">[59]</ref>, image-audio alignment <ref type="bibr" target="#b4">[5]</ref>, and audiovisual synchronization <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b33">34]</ref>. However, these tasks cannot be performed with a single modality, so there is no unimodal baseline and the performance drop found in this paper does not apply. In other work, joint training is avoided entirely by using pre-trained uni-modal features. Good examples include two-stream networks for video classification <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref> and image+text classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>. These methods do not train multiple modalities jointly, so they are again not comparable, and their accuracy may likely be sub-optimal due to independent training.</p><p>Our contributions in this paper include:</p><p>• We empirically demonstrate the significance of overfitting in joint training of multi-modal networks, and we identify two causes for the problem. We show the problem is architecture agnostic: different fusion techniques can also suffer the same overfitting problem. • We propose a metric to understand the problem quantitatively: the overfitting-to-generalization ratio (OGR), with both theoretical and empirical justification. • We propose a new training scheme which minimizes OGR via an optimal blend (in a sense we make precise below) of multiple supervision signals. This Gradient-Blending (G-Blend) method gives significant gains in ablations and achieves state-of-the-art (SoTA) accuracy on benchmarks including Kinetics, EPIC-Kitchen, and AudioSet by combining audio and visual signals. We note that G-Blend is task-agnostic, architecture-agnostic and applicable to other scenarios (e.g. used in <ref type="bibr" target="#b38">[39]</ref> to combine point cloud with RGB for 3D object detection)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Video classification. Video understanding has been one of the most active research areas in computer vision recently. There are two unique features with respect to videos: temporal information and multi-modality. Previous works have made significant progress in understanding temporal information <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b16">17]</ref>. However, videos are also rich in multiple modalities: RGB frames, motion vectors (optical flow), and audio. Previous works that exploit the multi-modal natures primarily focus on RGB+Optical Flow, with the creation of two-stream fusion networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b11">12]</ref>, typically using pre-trained features and focusing on the fusion <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref> or aggregation architectures <ref type="bibr" target="#b56">[57]</ref>. In contrast, we focus on joint training of the entire network. Instead of focusing on the architectural problem, we study model optimization: how to jointly learn and optimally blend multi-modal signals. With proper optimization, we show audio is useful for video classification. Multi-modal networks. Our work is related to previous research on multi-modal networks <ref type="bibr" target="#b6">[7]</ref> for classifications <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>, which primarily uses pre-training in contrast to our joint training. On the other hand, our work is related to cross-modal tasks <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9]</ref> and cross-modal self-supervised learning <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34]</ref>. These tasks either take one modality as input and make prediction on the other modality (e.g. Visual-Q&amp;A <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b23">24]</ref>, image captioning <ref type="bibr" target="#b8">[9]</ref>, sound localization <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b58">59]</ref> in videos) or uses cross-modality correspondences as self-supervision (e.g. image-audio correspondence <ref type="bibr" target="#b4">[5]</ref>, video-audio synchronization <ref type="bibr" target="#b33">[34]</ref>). Instead, we try to address the problem of joint training of multi-modal networks for classification. Multi-task learning. Our proposed Gradient-Blending training scheme is related to previous works in multi-task learning in using auxiliary loss <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b12">13]</ref>. These methods either use uniform/manually tuned weights, or learn the weights as parameters during training (no notion of overfitting prior used), while our work re-calibrates supervision signals using a prior OGR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multi-modal training via Gradient-Blending</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Background</head><p>Uni-modal network. Given train set T = {X 1...n , y 1...n }, where X i is the i-th training example and y i is its true label, training on a single modality m (e.g. RGB frames, audio, or optical flows) means minimizing an empirical loss:</p><formula xml:id="formula_0">L (C (ϕ m (X)) , y)<label>(1)</label></formula><p>where ϕ m is normally a deep network with parameter Θ m , and C is a classifier, typically one or more fully-connected (FC) layers with parameter Θ c . For classification problems considered here, L is the cross entropy loss. Minimizing Eq. 1 gives a solution Θ * m and Θ * c . <ref type="figure" target="#fig_0">Fig. 2a</ref> shows independent training of two modalities m 1 and m 2 . Multi-modal network. We train a late-fusion model on M different modalities ({m i } k 1 ). Each modality is processed by a different deep network ϕ mi with parameter Θ mi , and their features are fused and passed to a classifier C. Formally, training is done by minimizing the loss:</p><formula xml:id="formula_1">L multi = L (C (ϕ m 1 ⊕ ϕ m 2 ⊕ · · · ⊕ ϕ m k ) , y) (2)</formula><p>where ⊕ denotes a fusion operation (e.g. concatenation). <ref type="figure" target="#fig_0">Fig. 2b</ref> shows an example of a joint training of two modalities m 1 and m 2 . The multi-modal network in Eq. 2 is a  <ref type="figure">Figure 3</ref>: Overfitting-to-Generalization Ratio. Between any two training checkpoints, we can measure the change in overfitting and generalization. When ∆O ∆V is small, the network is learning well and not overfitting much.</p><formula xml:id="formula_2">L multi L multi L 1 L 2 L 1 L 2 a) b) c) w 1 w 2 w multi</formula><p>super-set of the uni-model network in Eq. 1: for any solution to Eq. 1 on any modality m i , one can construct an equally-good solution to Eq. 2 by choosing parameters Θ c that mute all modalities other than m i . In practice, this solution is not found, and we next explain why.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generalizing vs. Overfitting</head><p>Overfitting is typically understood as learning patterns in a train set that do not generalize to the target distribution. Given model parameters at epoch N , let L T N be the model's average loss over the fixed train set, and L * N be the "true" loss w.r.t the hypothetical target distribution. (In what follows, L * is approximated by a held-out validation loss L V .) We define overfitting at epoch N as the gap between L T N and L * N (approximated by O N in <ref type="figure">fig. 3</ref>). The quality of training between two model checkpoints can be measured by the change in overfitting and generalization (∆O, ∆G in <ref type="figure">fig. 3</ref>). Between checkpoints N and N + n, we can define the overfitting-to-generalization-ratio (OGR):</p><formula xml:id="formula_3">OGR ≡ ∆O N,n ∆G N,n = O N +n − O N L * N − L * N +n<label>(3)</label></formula><p>OGR between checkpoints measures the quality of learned information (with cross-entropy loss, it is the ratio of bits not generalizable to bits which do generalize). We propose minimizing OGR during training. However, optimizing OGR globally would be very expensive (e.g. variational methods over the whole optimization trajectory). In addition, very underfit models, for example, may still score quite well (difference of train loss and validation loss is very small for underfitting models; in other words, O is small).</p><p>Therefore, we propose to solve an infinitesimal problem: given several estimates of the gradient, blend them to minimize an infinitesimal OGR 2 . We apply this blend to our optimization process (e.g. SGD with momentum). Each gradient step now increases generalization error as little as possible per unit gain on the validation loss, minimizing overfitting. In a multi-modal setting, this means we combine gradient estimates from multiple modalities and minimize OGR 2 to ensure each gradient step now produces a gain no worse than that of the single best modality. As we will see in this paper, this L 2 problem admits a simple, closed-form solution, is easy to implement, and works well in practice.</p><p>Consider a single parameter update step with estimateĝ for the gradient. As the distance between two checkpoints is small (in the neighborhood in which a gradient step is guaranteed to decrease the train loss), we use the first-order approximations: ∆G ≈ ∇L * ,ĝ and ∆O ≈ ∇L T − L * ,ĝ . Thus, OGR 2 for a single vectorĝ is</p><formula xml:id="formula_4">OGR 2 = ∇L T − ∇L * ,ĝ ∇L * ,ĝ 2<label>(4)</label></formula><p>See supplementary materials for details on OGR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Blending of Multiple Supervision Signals by OGR Minimization</head><p>We can obtain multiple estimates of gradient by attaching classifiers to each modality's features and to the fused features (see <ref type="figure" target="#fig_0">fig 2c)</ref>. Per-modality gradient {ĝ i } k i=1 are obtained by back-propagating through each loss separately (so per-modality gradients contain many zeros in other parts of the network). Our next result allows us to blend them all into a single vector with better generalization behavior.</p><p>Proposition 1 (Optimal Gradient Blend). Let {v k } M 0 be a set of estimates for ∇L * whose overfitting satisfies E ∇L T − ∇L * , v k ∇L T − ∇L * , v j = 0 for j = k. Given the constraint k w k = 1 the optimal weights w k ∈ R for the problem</p><formula xml:id="formula_5">w * = arg min w E ∇L T − ∇L * , k w k v k ∇L * , k w k v k 2<label>(5)</label></formula><p>are given by</p><formula xml:id="formula_6">w * k = 1 Z ∇L * , v k σ 2 k (6) where σ 2 k ≡ E[ ∇L T − ∇L * , v k 2 ] and Z = k ∇L * ,v k 2σ 2 k is a normalizing constant.</formula><p>Assumption E ∇L T − ∇L * , v k ∇L T − ∇L * , v j = 0 will be false when two models' overfitting is very correlated. However, if this is the case then very little can be gained by blending their gradients. In informal experiments we have indeed observed that these cross terms are often small relative to the E ∇L T − ∇L * , v k 2 . This is likely due to complementary information across modalities, and we speculate that this happens naturally as joint training tries to learn complementary features across neurons. Please see supplementary materials for proof of Proposition 1, including formulas for the correlated case.</p><p>Proposition 1 may be compared with well-known results for blending multiple estimators; e.g. for the mean, a minimum-variance estimator is obtained by blending uncorrelated estimators with weights inversely proportional to the individual variances (see e.g. <ref type="bibr" target="#b0">[1]</ref>). Proposition 1 is similar, where variance is replaced by O 2 and weights are inversely proportional to the individual O 2 (now with a numerator G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Use of OGR and Gradient-Blending in practice</head><p>We adapt a multi-task architecture to construct an approximate solution to the optimization above ( <ref type="figure" target="#fig_0">fig 2c)</ref>.</p><p>Optimal blending by loss re-weighting At each backpropagation step, the per-modality gradient for m i is ∇L i , and the gradient from the fused loss is given by Eq. 2 (denote as ∇L k+1 ). Taking the gradient of the blended loss</p><formula xml:id="formula_7">L blend = k+1 i=1 w i L i<label>(7)</label></formula><p>thus produces the blended gradient k+1 i=1 w i ∇L i . For appropriate choices of w i this yields a convenient way to implement gradient blending. Intuitively, loss reweighting recalibrates the learning schedule to balance the generalization/overfitting rate of different modalities.</p><p>Measuring OGR in practice. In practice, ∇L * is not available. To measure OGR, we hold out a subset V of the training set to approximate the true distribution (i.e. L V ≈ L * ). We find it is equally effective to replace the loss measure by an accuracy metric to compute G and O and estimate optimal weights from Gradient-Blending. To reduce computation costs, we note that weights estimation can be done on a small subset of data, without perturbing the weights too much (see supplementary materials).</p><p>Gradient-Blending algorithms take inputs of training data T , validation set V, k input modalities {m i } k i=1 and a joint head m k+1 <ref type="figure" target="#fig_0">(Fig. 2c</ref>). In practice we can use a subset of training set T to measure train loss/ accuracy. To compute the Gradient-Blending weights when training from N for n epochs, we provide a Gradient-Blending weight estimation in Algorithm 1. We propose two versions of gradientblending: 1. Offline Gradient-Blending is a simple version of gradient-blending. We compute weights only once, and use a fixed set of weights to train entire epoch. This is very easy to implement. See Algorithm 2. 2. Online Gradient-Blending is the full version. We re-compute weights regularly (e.g. every n epochscalled a super-epoch), and train the model with new weights for a super-epoch. See Algorithm 3. Empirically, offline performs remarkably well. We compare the two in section 3, with online giving additional gains. </p><formula xml:id="formula_8">} k+1 i=1 = 1 Z G i O i2 ; Algorithm 2: Offline Gradient-Blending input: ϕ 0 , Initialized model N , # of epochs Result: Trained multi-head model ϕ N Compute per-modality weights {w i } k i=1 = GB Estimate(ϕ 0 , N ) ; Train ϕ 0 with {w i } k i=1</formula><p>for N epochs to get ϕ N ;</p><formula xml:id="formula_9">Algorithm 3: Online Gradient-Blending input: ϕ 0 , Initialized model N , # of epochs n, super-epoch length for i = 0, ..., N n − 1 do Current epoch N i = i * n ; Compute per-modality weights {w i } k i=1 = GB Estimate(ϕ Ni , N i + n) ; Train ϕ Ni with {w i } k i=1</formula><p>for n epochs to ϕ Ni+n ; end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Ablation Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental setup</head><p>Datasets. We use three video datasets for ablations: Kinetics, mini-Sports, and mini-AudioSet. Kinetics is a standard benchmark for action recognition with 260k videos <ref type="bibr" target="#b27">[28]</ref> of 400 human action classes. We use the train split (240k) for training and the validation split (20k) for testing. Mini-Sports is a subset of Sports-1M <ref type="bibr" target="#b26">[27]</ref>, a large-scale classification dataset with 1.1M videos of 487 different finegrained sports. We uniformly sampled 240k videos from train split and 20k videos from the test split. Mini-AudioSet is a subset of AudioSet <ref type="bibr" target="#b21">[22]</ref>, a multi-label dataset consisting of 2M videos labeled by 527 acoustic events. Au-dioSet is very class-unbalanced, so we remove tiny classes and subsample the rest (see supplementary). The balanced mini-AudioSet has 418 classes with 243k videos. Input preprocessing &amp; augmentation. We consider three modalities: RGB, optical flow and audio. For RGB and flow, we use input clips of 16×224×224 as input. We follow <ref type="bibr" target="#b45">[46]</ref> for visual pre-processing and augmentation. For audio, we use log-Mel with 100 temporal frames by 40 Mel filters. Audio and visual are temporally aligned. Backbone architecture. We use ResNet3D <ref type="bibr" target="#b46">[47]</ref> as our visual backbone for RGB and flow and ResNet <ref type="bibr" target="#b24">[25]</ref> as our audio model, both with 50 layers. For fusion, we use a two-FC-layer network on concatenated features from visual and audio backbones, followed by one prediction layer. Training and testing.</p><p>We train our models with synchronous distributed SGD on GPU clusters using Caffe2 <ref type="bibr" target="#b10">[11]</ref>, with setup as <ref type="bibr" target="#b46">[47]</ref>. We hold out a small portion of training data for weight estimate (8% for Kinetics and mini-Sports, 13% for mini-AudioSet). The final video prediction is made by using center crops of 10 uniformlysampled clips and averaging the 10 predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overfitting Problems in Naive Joint Training</head><p>We first compare naive audio-RGB joint training with unimodal audio-only and RGB-only training. <ref type="figure" target="#fig_5">Fig. 4</ref> plots the training curves on Kinetics (left) and mini-Sports (right). On both datasets, the audio model overfits the most and video overfits least. The naive joint audio-RGB model has lower training error and higher validation error compared with the video-only model; i.e. naive audio-RGB joint training increases overfitting, explaining the accuracy drop compared to video alone.</p><p>We extend the analysis and confirm severe overfitting on other multi-modal problems. We consider all 4 possible combinations of the three modalities (audio, RGB, and optical flow). In every case, the validation accuracy of naive joint training is significantly worse than the best single stream model <ref type="table" target="#tab_0">(Table 1)</ref>, and training accuracy is almost always higher (see supplementary materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Clip <ref type="formula" target="#formula_0">V@1</ref>   <ref type="table">Table 2</ref>: Both offline and online Gradient-Blending outperform Naive late fusion and RGB only. Offline G-Blend is lightly less accurate compared with the online version, but much simpler to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gradient-Blending is an effective regularizer</head><p>In this ablation, we first compare the performance of online and offline versions of G-Blend. Then we show that G-Blend works with different types of optimizers, including ones with adaptive learning rates. Next, we show G-Blend improves the performance on different multi-modal problems (different combinations of modalities), different model architectures and different tasks. Online G-Blend Works. We begin with the complete version of G-Blend, online G-Blend. We use an initial superepoch size of 10 (for warmup), and a super-epoch size of 5 thereafter. On Kinetics with RGB-audio setting, online Gradient-Blending surpasses both uni-modal and naive multi-modal baselines, by 3.2% and 4.1% respectively. The weights for online are in <ref type="figure">fig. 5a</ref>. In general, weights tend to be stable at first with slightly more focused on visual; then we see a transition at epoch 15 where the model does "pre-training" on visual trunk; at epoch 20 A/V trunk got all weights to sync the learning from visual trunk. After that, weights gradually stabilize again with a strong focus on visual learning. We believe that, in general, patterns learned by neural network are different at different stage of training (e.g. <ref type="bibr" target="#b35">[36]</ref>), thus the overfitting / generalization behavior also changes during training; this leads to different weights at different stages of the training.</p><p>Moreover, we observe that G-Blend always outperforms naive training in the online setting ( <ref type="figure">Fig. 5b</ref>  to the extra weight computations. As we will now see, Offline G-Blend can be easily adopted and works remarkably well in practice. On the same audio-RGB setting on Kinetics, offline G-Blend also outperforms uni-modal baseline and naive joint training by a large margin, 2.1% and 3.0% respectively ( <ref type="table">Table 2)</ref>, and is only slightly worse than online (-1.1%). Based on such observation, we opt to use offline G-Blend in the rest of the ablations, demonstrating its performance across different scenarios. We speculate the online version will be particularly useful for some cases not covered here, for example a fast-learning low-capacity model (perhaps using some frozen pre-trained features), paired with a high-capacity model trained from scratch. Adaptive Optimizers. Section 2.2 introduced G-Blend in an infinitesimal setting: blending different gradient estimation at a single optimization step and assumes same learning rate for each gradient estimator. This is true for many popular SGD-based algorithms, such as SGD with Momentum. However, the assumption may not be rigorous with adaptive optimization methods that dynamically adjust learning rate for each parameter, such as Adam <ref type="bibr" target="#b31">[32]</ref> and AdaGrad <ref type="bibr" target="#b14">[15]</ref>. We empirically show that offline Gradient-Blending (Algorithm 2) also works with different optimizers. Since SGD gives the best accuracy among the three optimizers, we opt to use SGD for all of our other experiments. Different Modalities. On Kinetics, we study all combinations of three modalities: RGB, optical flow, and audio.   <ref type="table">Table 4</ref> presents comparison of our method with naive joint training and best single stream model. We observe significant gains of G-Blend compared to both baselines on all multi-modal problems. It is worth noting that G-Blend is generic enough to work for more than two modalities. Different Architectures. We conduct experiments on midfusion strategy <ref type="bibr" target="#b36">[37]</ref>, which suffers less overfitting and outperforms visual baseline <ref type="figure" target="#fig_1">(Figure 1</ref>). On audio-visual setting, Gradient-Blending gives 0.8% improvement (top-1 from 72.8% to 73.6%). On a different fusion architecture with Low-Rank Multi-Modal Fusion (LMF) <ref type="bibr" target="#b34">[35]</ref>, Gradient-Blending gives 4.2% improvement (top-1 from 69.3% to 73.5%). This suggests Gradiend-Blending can be adopted to other fusion strategies besides late-fusion and other fusion architectures besides concatenation. Different Tasks/Benchmarks. We pick the problem of joint audio-RGB model training, and go deeper to compare Gradient-Blending with other regularization methods on different tasks and benchmarks: action recognition (Kinetics), sport classification (mini-Sports), and acoustic event detection (mini-AudioSet). We include three baselines: adding dropout at concatenation layer <ref type="bibr" target="#b42">[43]</ref>, pre-training single stream backbones then finetuning the fusion model, and blending the supervision signals with equal weights (which is equivalent to naive training with two auxiliary losses). Auxiliary losses are popularly used in multi-task learning, and we extend it as a baseline for multi-modal training.</p><p>As presented in <ref type="table" target="#tab_7">Table 5</ref>, Gradient-Blending outperforms all baselines by significant margins on both Kinetics and mini-Sports. On mini-AudioSet, G-Blend improves all baselines on mAP, and is slightly worse on mAUC com-  <ref type="table">Table 4</ref>: Gradient-Blending (G-Blend) works on different multi-modal problems. Comparison between G-Blend with naive late fusion and single best modality on Kinetics. On all 4 combinations of different modalities, G-Blend outperforms both naive late fusion network and best uni-modal network by large margins, and it also works for cases with more than two modalities. G-Blend results are averaged over three runs with different initialization. Variances are small and are provided in supplementary pared to auxiliary loss baseline. The reason is that the weights learned by Gradient-Blending are very similar to equal weights. The failures of auxiliary loss on Kinetics and mini-Sports demonstrates that the weights used in G-Blend are indeed important. We note that for mini-AudioSet, even though the naively trained multi-modal baseline is better than uni-modal baseline, Gradient-Blending still improves by finding more generalized information. We also experiment with other less obvious multi-task techniques such as treating the weights as learnable parameters <ref type="bibr" target="#b29">[30]</ref>. However, this approach converges to a similar result as naive joint training. This happens because it lacks of overfitting prior, and thus the learnable weights were biased towards the head that has the lowest training loss which is audio-RGB.  <ref type="figure" target="#fig_3">Fig. 6</ref> presents top and bottom 10 classes on Kinetics where G-Blend makes the most and least improvements compared with RGB-only. We observe that improved classes usually have a strong audio-correlation, such as clapping and laughing. For texting, although audio-only has nearly 0 accuracy, when combined with RGB using G-Blend, there are still significant improvements. On bottom-10 classes, we indeed find that audio does not seem to be very semantically relevant (e.g. unloading truck). See supplementary materials for more qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparison with State-of-the-Art</head><p>In this section, we train our multi-modal networks with deeper backbone architectures using offline Gradient-Blending and compare them with state-of-the-art methods on Kinetics, EPIC-Kitchen <ref type="bibr" target="#b13">[14]</ref>, and AudioSet. EPIC-Kitchen is a multi-class egocentric dataset with 28K training videos associated with 352 noun and 125 verb classes. For ablations, following <ref type="bibr" target="#b7">[8]</ref>, we construct a validation set of unseen kitchen environments. G-Blend is trained with RGB and audio input. For Kinetics and EPIC-Kitchen, we use ip-CSN <ref type="bibr" target="#b45">[46]</ref> for visual backbone with 32 frames and ResNet for audio backbone, both with 152 layers. For AudioSet, we use R(2+1)D for visual <ref type="bibr" target="#b46">[47]</ref> with 16 frames and ResNet for audio, both with 101 layers. We use the same training setup in section 3. For EPIC-Kitchen, we follow the same audio feature extractions as <ref type="bibr" target="#b28">[29]</ref>; the visual backbone is pretrained on IG-65M <ref type="bibr" target="#b22">[23]</ref>. We use the same evaluation setup as section 3 for AudioSet and EPIC-Kitchen. For Kinetics, we follow the 30-crop evaluation setup as <ref type="bibr" target="#b50">[51]</ref>. Our main purposes in these experiments are: 1) to confirm the benefit of Gradient-Blending on high-capacity models; and 2) to compare G-Blend with state-of-the-art methods on different large-scale benchmarks. Results. <ref type="table" target="#tab_8">Table 6</ref> presents results of G-Blend and compares them with current state-of-the-art methods on Kinetics. First, G-Blend provides an 1.3% improvement over RGB model (the best uni-modal network) with the same backbone architecture ip-CSN-152 <ref type="bibr" target="#b45">[46]</ref> when both models are trained from scratch. This confirms that the benefits of G-Blend still hold with high capacity model. Second, G-Blend outperforms state-of-the-arts multi-modal baseline Shift-Attention Network [10] by 1.4% while using less modalities (not using optical flow) and no pre-training. It is on-par with SlowFast <ref type="bibr" target="#b16">[17]</ref> while being 2x faster. G-Blend, when fine-tuned from Sports-1M on visual and AudioSet on audio, outperforms SlowFast Network and SlowFast augmented by Non-Local [51] by 1.5% and 0.6% respectively, while being 2x faster than both. Using weakly-supervised pre-training by IG-65M [23] on visual, G-Blend gives unparalleled 83.3% top-1 accuracy and 96.0% top-5 accuracy.</p><p>We also note that there are many competitive methods reporting results on Kinetics, due to the space limit, we select only a few representative methods for comparison including Shift-Attention <ref type="bibr" target="#b9">[10]</ref>, SlowFast <ref type="bibr" target="#b16">[17]</ref>, and ip-CSN <ref type="bibr" target="#b45">[46]</ref>. Shift-Attention and SlowFast are the methods with the best published accuracy using multi-modal and uni-modal input, respectively. ip-CSN is used as the visual backbone of G-    Blend thus serves as a direct baseline. <ref type="table" target="#tab_9">Table 7</ref> presents G-Blend results on AudioSet. Since Au-dioSet is very large (2M), we use mini-AudioSet to estimate weights. G-Blend outperforms two state-of-the-art Multilevel Attention Network <ref type="bibr" target="#b55">[56]</ref> and TAL-Net[53] by 5.8% and 5.5 % on mAP respectively, although the first one uses strong features (pre-trained on YouTube100M) and the second uses 100 clips per video, while G-Blend uses only 10.  place on seen kitchen. Comparing to published results, G-Blend uses less modalities (not using optical flow as TBN Ensemble <ref type="bibr" target="#b28">[29]</ref>), less backbones (Baidu-UTS <ref type="bibr" target="#b51">[52]</ref> uses three 3D-CNNs plus two detection models), and a single model (TBN Ensemble <ref type="bibr" target="#b28">[29]</ref> uses ensemble of five models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In uni-modal networks, diagnosing and correcting overfitting typically involves manual inspection of learning curves. Here we have shown that for multi-modal networks it is essential to measure and correct overfitting in a principled way, and we put forth a useful and practical measure of overfitting. Our proposed method, Gradient-Blending, uses this measure to obtain significant improvements over baselines, and either outperforms or is comparable with stateof-the-art methods on multiple tasks and benchmarks. The method potentially applies broadly to end-to-end training of ensemble models, and we look forward to extending G-Blend to other fields where calibrating multiple losses is needed, such as multi-task.  <ref type="figure">Figure 7</ref>: Weight Estimations on Subsets of Data. We used a small subset of Kinetics dataset to estimate the weights. The weights are quite robust as we decrease the volume of dataset. This suggests feasibility to use subsets to reduce the costs for Gradient-Blending.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Estimating Weights on Subsets of Data</head><p>We show that weight estimations by Gradient-Blending is robust on small subsets of data. We sampled 25%, 50% and 75% of Kinetics dataset and use these subsets as train sets in Alg. 2 in main paper. As shown in <ref type="figure">Fig. 7</ref>, the estimated weights are stable on small subsets of data. This suggests that the computational cost of the algorithm can be reduced by using a small subset of data for weight estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Understanding OGR</head><p>Overfitting is typically understood as learning patterns in a training set that do not generalize to the target distribution. We quantify this as follows. Given model parameters Θ (N ) , where N indicates the training epoch, let L T (Θ (N ) ) be the model's average loss over the fixed training set, and L * (Θ (N ) ) be the "true" loss w.r.t the hypothetical target distribution. (In practice, L * is approximated by the test and validation losses.) For either loss, the quantity L(Θ (0) ) − L(Θ (N ) ) is a measure of the information gained during training. We define overfitting as the gap between the gain on the training set and the target distribution:</p><formula xml:id="formula_10">O N ≡ L T (Θ (0) ) − L T (Θ (N ) ) − L * (Θ (0) ) − L * (Θ (N ) )</formula><p>and generalization to be the amount we learn (from training) about the target distribution:</p><formula xml:id="formula_11">G N ≡ L * (Θ (0) ) − L * (Θ (N ) )</formula><p>The overfitting-to-generalization ratio is a measure of information quality for the training process of N epochs:</p><formula xml:id="formula_12">OGR = (L T (Θ (0) )−L T (Θ (N ) ))−(L * (Θ (0) )−L * (Θ (N ) )) L * (Θ (0) )−L * (Θ (N ) )<label>(8)</label></formula><p>We can also define the amount of overfitting and generalization for an intermediate step from epoch N to epoch</p><formula xml:id="formula_13">N + n, where ∆O N,n ≡ (O N +n − O N ) and ∆G N,n ≡ (G N +n − G N )</formula><p>Together, this gives OGR between any two checkpoints:</p><formula xml:id="formula_14">OGR ≡ ∆O N,n ∆G N,n</formula><p>However, it does not make sense to optimize this as-is. Very underfit models, for example, may still score quite well (difference of train loss and validation loss is very small for underfitting models). What does make sense, however, is to solve an infinitesimal problem: given several estimates of the gradient, blend them to minimize an infinitesimal OGR (or equivalently OGR 2 ). We can then apply this blend to our optimization process by stochastic gradients (eg. SGD with momentum). In a multi-modal setting, this means we can combine gradient estimates from multiple modalities and minimize OGR to ensure each gradient step now produces a gain no worse than that of the single best modality.</p><p>Consider this in an infinitesimal setting (or a single parameter update step). Given parameter Θ, the full-batch gradient with respect to the training set is ∇L T (Θ), and the groundtruth gradient is ∇L * (Θ). We decompose ∇L T into the true gradient and a remainder:</p><formula xml:id="formula_15">∇L T (Θ) = ∇L * (Θ) +<label>(9)</label></formula><p>In particular, = ∇L T (Θ) − ∇L * (Θ) is exactly the infinitesimal overfitting. Given an estimateĝ with learning rate η, we can measure its contribution to the losses via Taylor's theorem:</p><formula xml:id="formula_16">L T (Θ + ηĝ) ≈ L T (Θ) + η ∇L T ,ĝ L * (Θ + ηĝ) ≈ L * (Θ) + η ∇L * ,ĝ</formula><p>which impliesĝ's contribution to overfitting is given by ∇L T − ∇L * ,ĝ . If we train for N steps with gradients {ĝ i } N 0 , and η i is the learning rate at i-th step, the final OGR can be aggregated as:</p><formula xml:id="formula_17">OGR = N i=0 η i ∇L T (Θ (i) ) − ∇L * (Θ (i) ),ĝ i N i=0 η i ∇L * (Θ (ni ),ĝ i<label>(10)</label></formula><p>and OGR 2 for a single vectorĝ i is</p><formula xml:id="formula_18">OGR 2 = ∇L T (Θ (i) ) − ∇L * (Θ (i) ),ĝ i ∇L * (Θ (i) ),ĝ i 2<label>(11)</label></formula><p>Next we will compute the optimal blend to minimize singlestep OGR 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Proposition 1</head><p>Proof of Proposition 1. Without loss of generality, we solve the problem with a different normalization:</p><formula xml:id="formula_19">∇L * , k w k v k = 1<label>(12)</label></formula><p>(Note that one can pass between normalizations simply by uniformly rescaling the weights.) With this constraint, the problem simplifies to:</p><formula xml:id="formula_20">w * = arg min w E[( ∇L T − ∇L * , k w k v k ) 2 ]<label>(13)</label></formula><p>We first compute the expectation:</p><formula xml:id="formula_21">E[( ∇L T − ∇L * , k w k v k ) 2 ] = E[( k w k ∇L T − ∇L * , v k ) 2 ] = E[ k,j w k w j ∇L T − ∇L * , v k ∇L T − ∇L * , v j ] = k,j w k w j E ∇L T − ∇L * , v k ∇L T − ∇L * , v j = k w 2 k σ 2 k<label>(14)</label></formula><p>where σ 2 k = E[ ∇L T − ∇L * , v k 2 ] and the cross terms vanish by assumption.</p><p>We apply Lagrange multipliers on our objective function <ref type="bibr" target="#b13">(14)</ref> and constraint <ref type="bibr" target="#b11">(12)</ref>:</p><formula xml:id="formula_22">L = k w 2 k σ 2 k − λ k w k ∇L * , v k − 1<label>(15)</label></formula><p>The partials with respect to w k are given by</p><formula xml:id="formula_23">∂L ∂w k = 2w k σ 2 k − λ ∇L * , v k<label>(16)</label></formula><p>Setting the partials to zero, we obtain the weights:</p><formula xml:id="formula_24">w k = λ ∇L * , v k 2σ 2 k<label>(17)</label></formula><p>The only remaining task is obtaining the normalizing constant. Applying the constraint gives:</p><formula xml:id="formula_25">1 = k w k ∇L * , v k = λ k ∇L * , v k 2 2σ 2 k<label>(18)</label></formula><p>In other words,</p><formula xml:id="formula_26">λ = 2 k ∇L * ,v k 2 σ 2 k<label>(19)</label></formula><formula xml:id="formula_27">Setting Z = 1/λ we obtain w * k = 1 Z ∇L * ,v k 2 2σ 2 k .</formula><p>Dividing by the sum of the weights yields the original normalization.</p><p>Note: if we relax the assumption that E[ ∇L T − ∇L * , v k ∇L T − ∇L * , v j ] = 0 for k = j, the proof proceeds similarly, although from <ref type="bibr" target="#b13">(14)</ref> it becomes more convenient to proceed in matrix notation. Define a matrix Σ with entries given by</p><formula xml:id="formula_28">Σ kj = E[ ∇L T − ∇L * , v k ∇L T − ∇L * , v j ]</formula><p>Then one finds that</p><formula xml:id="formula_29">w * k = 1 Z j Σ −1 kj ∇L * , v k Z = 1 2 k,j Σ −1 kj ∇L * , v k 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Variances of G-Blend Runs</head><p>The variances of the performances on the datasets used by the paper are typically small, and previous works provide results on a single run. To verify that G-Blend results are reproducible, we conducted multiple runs for G-Blend results in <ref type="table" target="#tab_4">Table 3</ref> of the main paper. We found that the variance is consistent across different modalities for G-Blend results ( <ref type="table">Table 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Sub-sampling and Balancing Multi-label Dataset</head><p>For a single-label dataset, one can subsample and balance at a per-class level such that each class may have the same volume of data. Unlike single-label dataset, classes in multi-label dataset can be correlated. As a result, sampling a single data may add volume for more than one class. This makes the naive per-class subsampling approach difficult.</p><p>To uniformly sub-sample and balance AudioSet to get mini-AudioSet, we propose the following algorithm:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Details on Model Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Late Fusion By Concatenation</head><p>In late fusion by concatenation strategy, we concatenate the output features from each individual network (i.e. k modalities' 1-D vectors with n dimensions). If needed, we add dropout after the feature concatenations.</p><p>The fusion network is composed of two FC layers, with each followed by an ReLU layer, and a linear classifier. The first FC maps kn dimensions to n dimensions, and the second one maps n to n. The classifier maps n to c, where c is the number of classes.</p><p>As sanity check, we experimented using less or more F C layers on Kinetics: • 0 FC. We only add a classifier that maps kn dimensions to c dimensions. • 1 FC. We add one FC layer that maps kn dimensions to n dimension, followed by an ReLU layer and classifier to map n dimension to c dimensions. • 4 FC. We add one FC layer that maps kn dimensions to n dimension, followed by an ReLU layer. Then we add 3 FC-ReLU pairs that preserve the dimensions. Then we add an a classifier to map n dimension to c dimensions.</p><p>We noticed that the results of all these approaches are sub-optimal. We speculate that less layers may fail to fully learn the relations of the features, while deeper fusion network overfits more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Mid Fusion By concatenation</head><p>Inspired by <ref type="bibr" target="#b36">[37]</ref>, we also concatenate the features from each stream at an early stage rather than late fusion. The problem with mid fusion is that features from individual streams can have different dimensions. For example, audio features are 2-D (time-frequency) while visual features are 3-D (time-height-width).</p><p>We propose three ways to match the dimension, depending on the output dimension of the concatenated features:</p><p>• 1-D Concat. We downsample the audio features to 1-D by average pooling on the frequency dimension. We downsample the visual features to 1-D by average pooling over the two spatial dimensions.  <ref type="table">Table 9</ref>: Last row of <ref type="table" target="#tab_4">Table 3</ref> in main papers with variance. Results are averaged over three runs with random initialization, and ± indicates variances.</p><p>• 2-D Concat. We keep the audio features the same and match the visual features to audio features. We downsample the visual features to 1-D by average pooling over the two spatial dimensions. Then we tile the 1-D visual features on frequency dimension to make 2-D visual features. • 3-D Concat. We keep the visual features fixed and match the audio features to visual features. We downsample the audio features to 1-D by average pooling over the frequency dimension. Then we tile the 1-D visual features on two spatial dimensions to make 3-D features.</p><p>The temporal dimension may also be mismatched between the streams: audio stream is usually longer than visual streams. We add convolution layers with stride of 2 to downsample audio stream if we are performing 2-D concat. Otherwise, we upsample visual stream by replicating features on the temporal dimension.</p><p>There are five blocks in the backbones of our ablation experiments (section 4), and we fuse the features using all three strategies after block 2, block 3, and block 4. Due to memory issue, fusion using 3-D concat after block 2 is unfeasible. On Kinetics, we found 3-D concat after block 3 works the best, and it's reported in <ref type="figure" target="#fig_1">Fig. 1</ref> in the main paper. In addition, we found 2-D concat works the best on AudioSet and uses less GFLOPs than 3-D concat. We speculate that the method for dimension matching is taskdependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. SE Gate</head><p>Squeeze-and-Excitement network introduced in <ref type="bibr" target="#b25">[26]</ref> applies a self-gating mechanism to produce a collection of per-channel weights. Similar strategies can be applied in a multi-modal network to take inputs from one stream and produce channel weights for the other stream.</p><p>Specifically, we perform global average pooling on one stream and use the same architectures in <ref type="bibr" target="#b25">[26]</ref> to produce a set of weights for the other channel. Then we scale the channels of the other stream using the weights learned. We either do a ResNet-style skip connection to add the new features or directly replace the features with the scaled features. The gate can be applied from one direction to another, or on both directions. The gate can also be added at different levels for multiple times. We found that on Kinetics, it works the best when applied after block 3 and on both directions.</p><p>We note that we can also first concatenate the features </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4. NL Gate</head><p>Although lightweight, SE-gate fails to offer any spatialtemporal or frequency-temporal level attention. One alternative way is to apply an attention-based gate. We are inspired by the Query-Key-Value formulation of gates in <ref type="bibr" target="#b47">[48]</ref>. For example, if we are gating from audio stream to visual stream, then visual stream is Query and audio stream is Key and Value. The output has the same spatial-temporal dimension as Query.</p><p>Specifically, we use Non-Local gate in <ref type="bibr" target="#b50">[51]</ref> as the implementation for Query-Key-Value attention mechanism. Details of the design are illustrated in <ref type="figure" target="#fig_6">fig. 8</ref>. Similar to SE-gate, NL-Gate can be added with multiple directions and at multiple positions. We found that it works the best when added after block 4, with a 2-D concat of audio and RGB features as Key-Value and visual features as Query to gate the visual stream.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Ablation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. A strong oracle baseline</head><p>In section 3.3, we presented the results on Gradient-Blending as an effective regularizer to train multi-modal networks. Here, we consider an additional strong baseline for the Kinetics, audio-RGB case.</p><p>Suppose we have an oracle to choose the best modality (from audio, RGB and naive A/V) for each class. For example, for "whistling" video, the oracle chooses naive A/V model as it performs the best among the three on "whistling" in validation set. With this oracle, Top-1 video accuracy is 74.1%, or 0.6% lower than the offline G-Blend result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Training Accuracy</head><p>In section 3.2, we introduced the overfitting problem of joint training of multi-modal networks. Here we include both validation accuracy and train accuracy of the multi-modal problems ( <ref type="table" target="#tab_0">Table 10</ref>). We demonstrate that in all cases, the multi-modal networks are performing worse than their single best counterparts, while almost all of their train accuracy are higher (with the sole exception of OF+A, whose train accuracy is similar to audio network's train accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Early Stopping</head><p>In early stopping, we experimented with three different stopping schedules: using 25%, 50% and 75% of iterations per epoch. We found that although overfitting becomes less of a problem, the model tends to under-fit. In practice, we still found that the 75% iterations scheduling works the best among the three, though it's performance is worse than full training schedule that suffers from overfitting. We summarize their learning curves in <ref type="figure" target="#fig_7">fig. 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Additional Qualitative Analysis</head><p>In section 3.3 we presented the qualitative analysis of G-Blend's performance compared with RGB model performance ( <ref type="figure" target="#fig_3">fig.6</ref>). We expand the analysis and provide more details in this section.</p><p>We first expand the analysis to compare the top-20 and bottom-20 improved classes of G-Blend versus RGB model ( <ref type="figure" target="#fig_1">fig. 10</ref>). This is a direct extension of <ref type="figure" target="#fig_3">fig.6</ref>. It further confirms that classes that dropped are indeed not very semantically relevant in audio, and in many of those classes, the audio model's performance is almost 0.</p><p>We further extends the analysis to compare naively trained audio-visual model with RGB-only model ( <ref type="figure" target="#fig_1">fig. 11</ref>). We note that the improvement for top-20 classes is smaller than that of G-B and for bot-20 classes the drop is mroe significant. Moreover, we note that in some bot-20 classes like snorkeling or feeding bird, where the sound of breathing and birds is indeed relevant, naively trained A/V model is not performing well. For these classes, audio model achieves decent performance. We further note that interestingly, for laughing, although naive A/V model outperforms RGB model, it is worse than audio-only model. And only with G-Blend, it benefits from both visual and audio signals, performing better than both.</p><p>Finally, we compare the top-20 and bot-20 classes where G-Blend has the most improvement/ drop with naively trained A/V model. We note that the gains in improved classes are much larger than the decrease in dropped classes.  Top 20 Dropped Class Accuracy <ref type="figure" target="#fig_1">Figure 11</ref>: Top-Bottom 20 classes based on improvement of naively trained audio-visual to RGB model. The improvement tends to be smaller than that of G-B counterpart and the drop is more significant. More interesting, in some classes, the naively trained A/V model performs worse than audio signal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>Top 20 Improved Class Accuracy <ref type="figure" target="#fig_0">Figure 12</ref>: Top-Bottom 20 classes based on improvement of G-Blend to Naive audio-visual model. We note that the gain is much more significant than drop.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Uni-vs. multi-modal joint training. a) Uni-modal training of two different modalities. b) Naive joint training of two modalities by late fusion. c) Joint training of two modalities with weighted blending of supervision signals. Different deep network encoders (white trapezoids) produce features (blue or pink rectangles) which are concatenated and passed to a classifier (yellow rounded rectangles).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>G-B Weight Estimation: GB Estimate input: ϕ N , Model checkpoint at epoch N n, # of epochs Result: A set of optimal weights with for k + 1 losses. for i = 1, ..., k + 1 do Initialize uni-modal/ naive multi-modal network ϕ N mi from corresponding parameters in ϕ N ; Train ϕ N mi for n epochs on T , resulting model ϕ N +n mi ; Compute amount of overfitting O i = O N,n , generalization G i = G N,n according to Eq.3 using V and T for modality m i ; end Compute a set of loss {w * i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Severe overfitting of naive audio-video models on Kinetics and mini-Sports. The learning curves (error-rate) of audio model (A), video model (V), and the naive joint audio-video (AV) model on Kinetics (left) and mini-Sports (right). Solid lines plot validation error while dashed lines show train error. The audio-video model overfits more than visual model and is inferior to the video-only model on validation loss. Online G-Blend. (a) Online G-Blend weights for each head. (b) Online G-Blend outperforms naive training on each super-epoch. For each super-epoch (5 epochs), we use the same snapshot of the model learned by G-Blend, and compare the performance of the models trained by G-Blend and naive at the next 5 epochs. G-Blend always outperforms naive training. This proves that G-Blend always learn more generalize information at a per-step level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Top-Bottom 10 classes based on improvement of G-Blend to RGB model. The improved classes are indeed audio-relevant, while those have performance drop are not very audio semantically-related.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 4 :</head><label>4</label><figDesc>Sub-sampling and Balancing Multi-label Dataset Data: Original Multi-Class Dataset D, Minimum Class Threshold M , Target Class Volume N Result: Balanced Sub-sampled Multi-label Dataset D Initialize empty dataset D ; Remove labels from D such that label volume is less than M ; Randomly shuffle entries in D; for Data Entry d ∈ D do Choose class c of d such that the volume of c is the smallest in D ; Let the volume of c be V c in D ; Let the volume of c be V c in D ; Generate random number r to be an integer between 0 and V c − V c ; if r &lt; N − V c then Select d to D ; else Skip d and continue ; end end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>NL-Gate Implementation. Figure of the implementation of NL-Gate on visual stream. Visual features are the Query. The 2D Mid-Concatenation of visual and audio features is the Key and Value. and use features from both streams to learn the per-channel weights. The results are similar to learning the weights with a single stream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Early stopping avoids overfitting but tends to under-fit. Learning curves for three early stopping schedules we experiment. When we train the model with less number of iterations, the model does not overfit, but the undesirable performance indicates an under-fitting problem instead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Top-Bottom 20 classes based on improvement of G-Blend to RGB model. The improved classes are indeed audio-relevant, while those have performance drop are not very audio semantically-related.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Uni-modal networks consistently outperform multimodal networks. Best uni-modal networks vs late fusion multimodal networks on Kinetics using video top-1 validation accuracy. Single stream modalities include video clips (RGB), Optical Flow (OF), and Audio (A). Multi-modal networks use the same architectures as uni-modal, with late fusion by concatenation at the last layer before prediction.</figDesc><table><row><cell>Dataset</cell><cell>Multi-modal</cell><cell cols="4">V@1 Best Uni V@1 Drop</cell></row><row><cell></cell><cell>A + RGB</cell><cell>71.4</cell><cell>RGB</cell><cell>72.6</cell><cell>-1.2</cell></row><row><cell>Kinetics</cell><cell>RGB + OF A + OF</cell><cell>71.3 58.3</cell><cell>RGB OF</cell><cell>72.6 62.1</cell><cell>-1.3 -3.8</cell></row><row><cell></cell><cell>A + RGB + OF</cell><cell>70.0</cell><cell>RGB</cell><cell>72.6</cell><cell>-2.6</cell></row><row><cell>RGB</cell><cell></cell><cell></cell><cell></cell><cell>72.6</cell><cell></cell></row><row><cell>late-concat</cell><cell>71.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pre-train</cell><cell></cell><cell>71.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>early-stop</cell><cell>71.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dropout</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72.9</cell></row><row><cell>mid-concat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72.8</cell></row><row><cell>SE-gate</cell><cell>71.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NL-gate</cell><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell></row><row><cell>71</cell><cell>71.5</cell><cell cols="2">72</cell><cell>72.5</cell><cell>73</cell></row><row><cell></cell><cell></cell><cell cols="3">Top-1 Accuracy on Kinetics</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). With the same initialization (model snapshots at epoch 0,10,15,...,<ref type="bibr" target="#b39">40)</ref>, we compare the performance of G-Blend model and naive training after a super-epoch (at epoch 10,15,20,...,<ref type="bibr" target="#b44">45)</ref>, and G-Blend models always outperform naive training. This shows that G-Blend always provides more generalizable training information, empirically proving proposition 1. Additionally, it shows the relevance of minimizing OGR, as using weights that minimize OGR improves performance of the model. For fair comparison, we fix the main trunk and finetune the classifier for both Naive A/V and G-Blend as we want to evaluate the quality of their backbones. At epoch 25, the gain is small since G-Blend puts almost all weights on A/V head, making it virtually indistinguishable from naive training for that super-epoch. Offline G-Blend Also Works. Although online G-Blend gives significant gains and addresses overfitting well, it is more complicated to implement, and somewhat slower due</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Kinetics Learning Curve</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Mini-Sports Learning Curve</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AV-Val A-Train A-Val V-Train V-Val AV-Train</cell><cell>Error (%)</cell><cell>0.4 0.5 0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Error (%)</cell><cell>0.5 0.8 0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epoch</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>G-Blend on different optimizers. We compare G-Blend with Visual only and Naive AV on two additional optimizers: AdaGrad, and Adam. G-Blend consistently outperforms Visual-Only and Naive AV base- lines on all three optimizers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Join]=[0.630,0.014,0.356] [RGB,OF,Join]=[0.309,0.495,0.196] [OF,A,Join]=[0.827,0.011,0.162] [RGB,OF,A,Join]=[0.33,0.53,0.01,0.13]</figDesc><table><row><cell>Modal</cell><cell></cell><cell>RGB + A</cell><cell></cell><cell></cell><cell>RGB + OF</cell><cell></cell><cell></cell><cell>OF + A</cell><cell></cell><cell></cell><cell cols="2">RGB + OF + A</cell></row><row><cell cols="2">Weights [RGB,A,Metric Clip</cell><cell>V@1</cell><cell>V@5</cell><cell>Clip</cell><cell>V@1</cell><cell>V@5</cell><cell>Clip</cell><cell>V@1</cell><cell>V@5</cell><cell>Clip</cell><cell>V@1</cell><cell>V@5</cell></row><row><cell>Uni</cell><cell>63.5</cell><cell>72.6</cell><cell>90.1</cell><cell>63.5</cell><cell>72.6</cell><cell>90.1</cell><cell>49.2</cell><cell>62.1</cell><cell>82.6</cell><cell>63.5</cell><cell>72.6</cell><cell>90.1</cell></row><row><cell>Naive</cell><cell>61.8</cell><cell>71.4</cell><cell>89.3</cell><cell>62.2</cell><cell>71.3</cell><cell>89.6</cell><cell>46.2</cell><cell>58.3</cell><cell>79.9</cell><cell>61.0</cell><cell>70.0</cell><cell>88.7</cell></row><row><cell>G-Blend</cell><cell>65.9</cell><cell>74.7</cell><cell>91.5</cell><cell>64.3</cell><cell>73.1</cell><cell>90.8</cell><cell>54.4</cell><cell>66.3</cell><cell>86.0</cell><cell>66.1</cell><cell>74.9</cell><cell>91.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>G-Blend outperforms all baseline methods on different benchmarks and tasks. Comparison of G-blend with different regularization baselines as well as uni-modal networks on Kinetics, mini-Sports, and mini-AudioSet. G-Blend consistently outperforms other methods, except for being comparable with using auxiliary loss on mini-AudioSet due to the similarity of learned weights of G-Blend and equal weights.</figDesc><table><row><cell>Backbone</cell><cell>Pre-train</cell><cell cols="2">V@1 V@5</cell><cell>GFLOPs</cell></row><row><cell>Shift-Attn Net [10]</cell><cell>ImageNet</cell><cell>77.7</cell><cell>93.2</cell><cell>NA</cell></row><row><cell>SlowFast [17]</cell><cell>None</cell><cell>78.9</cell><cell>93.5</cell><cell>213×30</cell></row><row><cell>SlowFast+NL [17]</cell><cell>None</cell><cell>79.8</cell><cell>93.9</cell><cell>234×30</cell></row><row><cell>ip-CSN-152 [46]</cell><cell>None</cell><cell>77.8</cell><cell>92.8</cell><cell>108.8×30</cell></row><row><cell>G-Blend(ours)</cell><cell>None</cell><cell>79.1</cell><cell>93.9</cell><cell>110.1×30</cell></row><row><cell>ip-CSN-152 [46]</cell><cell>Sports1M</cell><cell>79.2</cell><cell>93.8</cell><cell>108.8×30</cell></row><row><cell>G-Blend(ours)</cell><cell>Sports1M</cell><cell>80.4</cell><cell>94.8</cell><cell>110.1×30</cell></row><row><cell>ip-CSN-152 [46]</cell><cell>IG-65M</cell><cell>82.5</cell><cell>95.3</cell><cell>108.8×30</cell></row><row><cell>G-Blend(ours)</cell><cell>IG-65M</cell><cell>83.3</cell><cell>96.0</cell><cell>110.1×30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">mAP mAUC</cell></row><row><cell cols="2">Multi-level Attn. [56] 0.360</cell><cell>0.970</cell></row><row><cell>TAL-Net [53]</cell><cell>0.362</cell><cell>0.965</cell></row><row><cell>Audio:R2D-101</cell><cell>0.324</cell><cell>0.961</cell></row><row><cell>Visual:R(2+1)D-101</cell><cell>0.188</cell><cell>0.918</cell></row><row><cell>Naive A/V:101</cell><cell>0.402</cell><cell>0.973</cell></row><row><cell>G-Blend (ours):101</cell><cell>0.418</cell><cell>0.975</cell></row></table><note>Comparison with state-of-the-art methods on Kinetics. G- Blend used audio and RGB as input modalities; for pre-trained models on Sports1M and IG-65M, G-Blend initializes audio network by pre-training on AudioSet. G-Blend outperforms current state-of-the-art multi-modal method (Shift-Attention Network) despite the fact that it uses fewer modal- ities (G-Blend does not use Optical Flow). G-Blend also gives a good im- provement over RGB model (the best uni-modal network) when using the same backbone, and it achieves the state-of-the-arts.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison with state-of-the-art methods on AudioSet. G-Blend outperforms the state-of-the-art methods by a large margin.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 presents</head><label>8</label><figDesc>G-Blend results and compare with published SoTA results and leaderboard on the EPIC-Kitchens Action Recognition challenge. On validation set, G-Blend outperforms naive A/V baseline on noun, verb and action; it is on par with visual baseline on noun and outperforms visual baseline on verb and action. Currently, G-Blend ranks the 2nd place on unseen kitchen in the challenge and 4th</figDesc><table><row><cell>method</cell><cell cols="2">noun</cell><cell cols="2">verb</cell><cell cols="2">action</cell></row><row><cell></cell><cell>V@1</cell><cell>V@5</cell><cell>V@1</cell><cell>V@5</cell><cell>V@1</cell><cell>V@5</cell></row><row><cell></cell><cell cols="3">Validation Set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Visual:ip-CSN-152 [46]</cell><cell>36.4</cell><cell>58.9</cell><cell>56.6</cell><cell>84.1</cell><cell>24.9</cell><cell>42.5</cell></row><row><cell>Naive A/V:152</cell><cell>34.8</cell><cell>56.7</cell><cell>57.4</cell><cell>83.3</cell><cell>23.7</cell><cell>41.2</cell></row><row><cell>G-Blend(ours)</cell><cell>36.1</cell><cell>58.5</cell><cell>59.2</cell><cell>84.5</cell><cell>25.6</cell><cell>43.5</cell></row><row><cell></cell><cell cols="3">Test Unseen Kitchen (S2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Leaderboard [2]</cell><cell>38.1</cell><cell>63.8</cell><cell>60.0</cell><cell>82.0</cell><cell>27.4</cell><cell>45.2</cell></row><row><cell>Baidu-UTS [52]</cell><cell>34.1</cell><cell>62.4</cell><cell>59.7</cell><cell>82.7</cell><cell>25.1</cell><cell>46.0</cell></row><row><cell>TBN Single [29]</cell><cell>27.9</cell><cell>53.8</cell><cell>52.7</cell><cell>79.9</cell><cell>19.1</cell><cell>36.5</cell></row><row><cell>TBN Ensemble [29]</cell><cell>30.4</cell><cell>55.7</cell><cell>54.5</cell><cell>81.2</cell><cell>21.0</cell><cell>39.4</cell></row><row><cell>Visual:ip-CSN-152</cell><cell>35.8</cell><cell>59.6</cell><cell>56.2</cell><cell>80.9</cell><cell>25.1</cell><cell>41.2</cell></row><row><cell>G-Blend(ours)</cell><cell>36.7</cell><cell>60.3</cell><cell>58.3</cell><cell>81.3</cell><cell>26.6</cell><cell>43.6</cell></row><row><cell></cell><cell cols="3">Test Seen Kitchen (S1)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baidu-UTS(leaderboard)</cell><cell>52.3</cell><cell>76.7</cell><cell>69.8</cell><cell>91.0</cell><cell>41.4</cell><cell>63.6</cell></row><row><cell>TBN Single</cell><cell>46.0</cell><cell>71.3</cell><cell>64.8</cell><cell>90.7</cell><cell>34.8</cell><cell>56.7</cell></row><row><cell>TBN Ensemble</cell><cell>47.9</cell><cell>72.8</cell><cell>66.1</cell><cell>91.2</cell><cell>36.7</cell><cell>58.6</cell></row><row><cell>Visual:ip-CSN-152</cell><cell>45.1</cell><cell>68.4</cell><cell>64.5</cell><cell>88.1</cell><cell>34.4</cell><cell>52.7</cell></row><row><cell>G-Blend(ours)</cell><cell>48.5</cell><cell>71.4</cell><cell>66.7</cell><cell>88.9</cell><cell>37.1</cell><cell>56.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Comparison with state-of-the-art methods on EPIC-Kitchen. G-Blend achieves 2nd place on seen kitchen challenge and 4th place on unseen, despite using fewer modalities, fewer backbones, and single model in contrast to model ensembles compared to published results on leaderboard.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Multi-modal networks have lower validation accuracy but higher train accuracy.Table of Top-1 accuracy of single stream models and naive late fusion models.</figDesc><table><row><cell>Single stream modalities include RGB, Optical Flow (OF),</cell></row><row><cell>and Audio Signal (A). Its higher train accuracy and lower</cell></row><row><cell>validation accuracy signal severe overfitting.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Combining correlated unbiased estimators of the mean of a normal distribution</title>
		<ptr target="https" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Epic-kitchens action recognition</title>
		<ptr target="https://competitions.codalab.org/competitions/20115" />
		<imprint>
			<biblScope unit="page" from="2019" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio-visual scene-aware dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gonzlez. Gated multimodal units for information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Gmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltruvsaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic description generation from images: A survey of models, datasets, and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muscat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="409" to="442" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-the-shelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1708.03805</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Caffe2: A new lightweight, modular, and scalable deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe2-Team</surname></persName>
		</author>
		<ptr target="https://caffe2.ai/.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP 2017</title>
		<meeting>IEEE ICASSP 2017<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Zisserman. The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient large-scale multi-modal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page" from="12" to="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ubernet: Training a &apos;universal&apos; convolutional neural network for low-, mid-, and highlevel vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from selfsupervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01" />
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sgd on neural networks learns functions of increasing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kaplun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalimeris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hypothesis only baselines in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shifting the baseline: Single modality performance on visual navigation &amp; qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Actions˜trans-formations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Baidu-uts submission to the epic-kitchens action recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09383</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09050</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence -Volume Volume Three, IJCAI&apos;11</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence -Volume Volume Three, IJCAI&apos;11</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Multi-level attention model for weakly supervised audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Barsim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02353</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Yin and Yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The sound of pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
