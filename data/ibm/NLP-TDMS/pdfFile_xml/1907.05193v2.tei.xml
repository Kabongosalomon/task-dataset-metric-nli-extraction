<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 1 Cross-Domain Complementary Learning Using Pose for Multi-Person Part Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Life Fellow, IEEE</roleName><forename type="first">Ming-Ting</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 1 Cross-Domain Complementary Learning Using Pose for Multi-Person Part Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human parsing</term>
					<term>learning from synthetic data</term>
					<term>human pose estimation</term>
					<term>domain adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised deep learning with pixel-wise training labels has great successes on multi-person part segmentation. However, data labeling at pixel-level is very expensive. To solve the problem, people have been exploring to use synthetic data to avoid the data labeling. Although it is easy to generate labels for synthetic data, the results are much worse compared to those using real data and manual labeling. The degradation of the performance is mainly due to the domain gap, i.e., the discrepancy of the pixel value statistics between real and synthetic data. In this paper, we observe that real and synthetic humans both have a skeleton (pose) representation. We found that the skeletons can effectively bridge the synthetic and real domains during the training. Our proposed approach takes advantage of the rich and realistic variations of the real data and the easily obtainable labels of the synthetic data to learn multi-person part segmentation on real images without any human-annotated labels. Through experiments, we show that without any human labeling, our method performs comparably to several state-ofthe-art approaches which require human labeling on Pascal-Person-Parts and COCO-DensePose datasets. On the other hand, if part labels are also available in the real-images during training, our method outperforms the supervised state-of-the-art methods by a large margin. We further demonstrate the generalizability of our method on predicting novel keypoints in real images where no real data labels are available for the novel keypoints detection. Code and pre-trained models are available at https: //github.com/kevinlin311tw/CDCL-human-part-segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H UMAN body part segmentation <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> aims at partitioning persons in the image to multiple semantically consistent regions (e.g., head, arms, legs), which is important to many human-centric analysis applications <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. Supervised training with deep Convolutional Neural Networks (CNNs) significantly improves the performance of various visual recognition tasks including the human body part segmentation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. However, it requires large amount of training data. Data labeling, especially at pixel level, is labor intensive and the acquisition of such annotations in large scale is prohibitively expensive.</p><p>A promising solution to address this problem is to take advantage of the graphics simulator to generate synthetic <ref type="bibr">Manuscript</ref>  images with ground truths automatically <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. For example, previous study <ref type="bibr" target="#b14">[15]</ref> proposed to learn single-person part segmentation by directly training the neural networks using synthetic images. However, their method usually produces false alarms in the real-world background, and it does not work well for real-world images consisting of multiple person with interactions and occlusions. Also, recent studies <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> show that the discrepancy of the pixel value statistics between real and synthetic data, so called the domain gap, makes it challenging to transfer knowledge from synthetic domain to real domain. In addition to the pixel value statistics, the discrepancy of the content distributions (e.g., the background scenes and objects) between the two domains makes knowledge transfer even more difficult.</p><p>To address the discrepancies of the content distributions and the pixel value statistics between the two domains, recent studies <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> proposed to train the neural networks using adversarial training for matching the feature distributions of the real and synthetic data. They proposed to train a discriminator for distinguishing the real and synthetic images, and a generator for extracting the domain-invariant features that can fool the discriminator. However, the adversarial training may not converge due to the fact that it is difficult to maintain a balanced training between the generator and the discriminator. Previous approaches also suffer from the issue of mode collapse, where the generator may only capture a part of the real data distribution. Thus, the performances of previous approaches are much worse than the supervised training on real data with pixel-wise manual labeling.</p><p>In this paper, we observe that real and synthetic humans both have a skeleton (pose) representation and show that the skeletons can effectively bridge the synthetic and real domains during the training. With our proposed approach, we can take advantage of the complementary nature of the real and synthetic data, i.e., rich and realistic variations of the real data and the easily obtainable labels of the synthetic data, effectively. Our technique learns multi-person part segmentation on real images without any human-annotated labels and achieves performance comparable to several state-of-the-art approaches which require human labeling. On the other hand, if part labels are also available in the real-images during training, our method outperforms the supervised state-of-the-art methods by a large margin. As shown in <ref type="figure">Figure 1</ref>, we have part segmentation labels from synthetic data, but do not have part segmentation labels from real data. It should be noted that our synthetic images have an extremely simple background with white walls, while the real images have complex backgrounds with a variety of non-human objects. Given such discrepancies between the two domains, we observe that real and synthetic humans both have a common skeleton representation. By learning the skeleton representation of the real and synthetic humans, our proposed model learns a shared feature space for both real and synthetic domains. Different from previous works that try to minimize the discrepancy of the pixel value statistics between the domains, we propose to perform human pose estimation to extract skeletons from the real and synthetic images, and minimize the discrepancy of the feature spaces between the two domains by learning the domain-invariant human skeleton representation. The automatically extracted skeletons capture the structural body information and can effectively bridge the real and synthetic data domains, so that both real and synthetic data can be used in the training effectively without needing the expensive manual human part labeling for the real images. It is worth noting that the learning of human pose estimation requires training labels. However, the pose labels are readily available on several public large-scale datasets like COCO Keypoint dataset <ref type="bibr" target="#b21">[22]</ref> and are easy to obtain than part segmentation labels. Thus, the proposed method has the advantage of saving labeling efforts in practice.</p><p>We also show that our method can be generalized to predict a new set of keypoints for real images. For example, to predict keypoints on hands and feet, we just need to generate synthetic images with hands and feet labels, and the knowledge will transfer from the synthetic domain to the real domain using our proposed approach.</p><p>In summary, the main contributions of this paper include:</p><p>• We discover that human pose is very effective to bridge the real and synthetic domains for human-centric analysis applications. • We introduce an effective framework, called cross-domain complementary learning with pose, to leverage information in both real and the synthetic images for multi-person part segmentation.</p><p>• Through experiments, we show that without any humanannotated part segmentation label, our method performs comparably with several state-of-the-art approaches which require human labeling on Pascal-PersonParts and COCO-DensePose datasets. On the other hand, if parts labels are also available in real images during training, our method outperforms the supervised state-of-the-art methods by a large margin. • We show that our method can be generalized to predict new keypoints such as those on hands and feet in real images without human labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Synthetic data for computer vision tasks</head><p>There has been a long-standing history of exploring the use of 3D synthetic data for computer vision problems <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. Recent studies use 3D CAD models for visual recognition tasks, such as 3D model repository <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, object recognition <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, human analysis applications <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and semantic segmentation for urban scenes <ref type="bibr" target="#b13">[14]</ref>. Among the literature, Varol et al. <ref type="bibr" target="#b14">[15]</ref> proposed to render a single-person avatar on top of a static background image, and generate ground truths for training deep CNNs. However their method only works for the well-controlled environment and the single-person scenario in an image. This is because it is difficult and expensive to render photorealistic images with rich coverage of avatars, background scenes, and objects.</p><p>In this work, we address a more challenging and general scenario, where multiple people with interactions and occlusions are considered. Different from training the deep CNNs using synthetic data only <ref type="bibr" target="#b14">[15]</ref>, we propose to leverage the complementary natural of the real and synthetic data with human pose estimation. In the experiments, we show that our method, which learns to bridge the reality gap, performs more favorably against those proposed in previous studies <ref type="bibr" target="#b14">[15]</ref>. In addition, as demonstrated in the experiments, our technique reduces the requirement on the photorealism of the synthetic data generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Domain adaptation</head><p>Domain adaptation is a special case of transfer learning <ref type="bibr" target="#b29">[30]</ref> that aims to learn a single task from a source domain, so that it performs well on a target domain. Many approaches have been proposed to address the visual dataset bias <ref type="bibr" target="#b30">[31]</ref> for domain adaptation, including active learning with human-inthe-loop <ref type="bibr" target="#b17">[18]</ref>, training deep CNNs with reverse gradient <ref type="bibr" target="#b31">[32]</ref>, learning with auxiliary tasks to reduce domain variations <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, and matching feature distributions of two domains by adversarial training <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b39">[40]</ref>. In particular, Chen et al. <ref type="bibr" target="#b36">[37]</ref> proposed an image-level adaptation approach which tries to make the appearance of synthetic images similar to real images. One key assumption of <ref type="bibr" target="#b36">[37]</ref> is that the content distribution of the synthetic data is similar to the content distribution of the real data. It is not suitable to our research problem because all of our synthetic images have an extremely simple background (empty room with white walls) while real images have complex backgrounds with a variety of objects. The discriminators can easily distinguish our synthetic images from real images thus making the adversarial learning scheme ineffective. Instead of image-level adaptation, Ren and Lee <ref type="bibr" target="#b18">[19]</ref> proposed a feature-level adaptation approach to learn image classifiers and object detectors using synthetic images with adversarial training. A recent study <ref type="bibr" target="#b40">[41]</ref> proposed to learn human pose estimation with synthetic data using adversarial teacher-student network. Tsai et al. <ref type="bibr" target="#b35">[36]</ref> further proposed to enhance the adversarial learning with patch-level alignment. However, existing domain adaptation approaches do not work as well as the fully supervised training approaches. Instead of adversarial training, our approach uses an auxiliary task of human pose estimation to bridge synthetic and real domains, which is shown to be more effective from our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-task learning</head><p>Prior works <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b46">[47]</ref> have shown that multi-task learning is effective for many vision problems. Given multiple different tasks, where a subset of these tasks are related, multitask learning aims to improve the learning of the original task by using knowledge from all or some of the other tasks <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. However, many previous studies assume that, for all the tasks, the labeled data have to be available for training <ref type="bibr" target="#b29">[30]</ref>. Different from the previous works, our method learns without human-annotated segmentation labels in a cross-domain scenario, and learns to bridge the domain gap between real and synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Supervised and semi-supervised part segmentation</head><p>Recent studies <ref type="bibr" target="#b49">[50]</ref>- <ref type="bibr" target="#b54">[55]</ref> proposed to jointly train human part segmentation and human pose estimation for improving Camera Avatar Synthetic Environment Avatar's moving path the performance of part segmentation. However, the successes of the previous studies are mainly attributed to the supervised training with the pixel-wised manual labeling. Different from the fully supervised approaches, we propose to remove the manual labeling requirement by learning with synthetic data. On the other hand, Fang et al. <ref type="bibr" target="#b55">[56]</ref> proposed a semi-supervised approach that aims to augment training samples by transferring the human-labeled part segmentation from an existing dataset to another unlabeled dataset. Our method differs from theirs in that our method does not require any human-labeled part segmentation dataset at all.</p><p>Bearman et al. <ref type="bibr" target="#b56">[57]</ref> proposed a point-level supervision which is related to our work. The key insight of their method is to use the foreground masks (called objectness prior in <ref type="bibr" target="#b56">[57]</ref>) to help find the foregrounds. Their method is effective for extracting the foreground regions, but it remains challenging to find the boundaries between different foreground objects such as the object parts in our problem. For example, when a person's arm is in front of the torso, the arm region is overlapped with the torso region making it difficult to find the boundary of the arm. Instead of relying on objectness prior, we leverage synthetic data to learn the boundaries between different parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYNTHETIC DATA</head><p>It is a common belief that high-quality synthetic data should be created as similar as possible to the real-world scenarios. For example, in generating single-person synthetic data <ref type="bibr" target="#b14">[15]</ref>, the authors composed their synthetically generated human images with a variety of real world background images. An advantage of our technique is that we reduce the requirement on the photorealism of the synthetic data generation. In particular, we use a simple empty room as the background for all of our synthetic data. The reason why our technique works well even with such a simple synthetic background is that our technique learns about the background from the real data.</p><p>We have 20 3D human models with different body shapes and clothing. These avatars are randomly placed at different positions in the virtual room, and they are animated to perform a variety of actions such as walking, jumping, crawling, etc. To create realistic human motions, we retarget the motion capture data from CMU MoCap database [58] to the avatars. We use a ray-tracing based rendering engine <ref type="bibr" target="#b57">[59]</ref>, <ref type="bibr" target="#b58">[60]</ref> to render the scene.</p><p>Multiple virtual cameras are set up at different positions in the environment to capture the scene from a variety of viewpoints. <ref type="figure" target="#fig_0">Figure 2</ref> shows the layout of our simulation environment. The virtual camera model we used is a pinhole camera with a 90 degree FoV. The exposure of the camera is 1/30-th of a second. The focal length is 35 mm. <ref type="figure" target="#fig_1">Figure 3</ref> shows the examples of our synthetic data and the ground truths. Our graphics simulator generates different types of per-pixel ground truth labels for the animations. Following the common definitions of body parts and human pose <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we generate 14 categories of body part ground truth labels, and 17 types of keypoint ground truth labels. It is worth noting that the labels for the synthetic data can be freely extended depending on user preferences, and are more flexible than those in the conventional real datasets. For example, as shown in Section V-F, we generate a new set of keypoints including hands and feet from synthetic data thus allowing our model to predict new keypoints.</p><p>Another advantage of the graphics simulation is that we can easily generate large amount of data. In this work, we generate a total of 17, 211 frames and their corresponding ground truths for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD</head><p>Given a set of synthetic data with human part segmentation labels, we would like to learn a function that performs human part segmentation on real world data. If we directly train a neural network with synthetic data labels, it does not generalize well to real data due to the reality gap. Unlike existing methods <ref type="bibr" target="#b18">[19]</ref> that try to transform the synthetic data to real data domain to make them look similar to each other, we use a complementary learning strategy that effectively leverages the rich variation of the real data and the part segmentation   <ref type="figure">Fig. 4</ref>: An overview of the proposed framework. Our framework consists of two main components. The first is the synthetic input training to learn body parts and human poses in the synthetic domain. In the second component for real input training, we share the network parameters of the backbone, keypoint map head, and part affinity field head with the first component. During learning, we train our network using two modules within a mini-batch, and optimize the network using back-propagation.</p><p>labels of the synthetic data. To make sure the synthetic data and real data are aligned in a common latent space, we use an auxiliary task, pose estimation, to bridge the two domains. In summary, our training data consist of part segmentation labels and pose labels from synthetic data, and pose labels from real data. We learn a part segmentation function without any part segmentation labels from real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning objective</head><p>Given a real dataset with pose labels D pose r , a synthetic dataset with pose labels D pose s , and a synthetic dataset with part segmentation labels D part s , we formulate the cross-domain complementary learning (CDCL) as the following optimization problem:</p><formula xml:id="formula_0">L = αL pose (D pose r ) + βL pose (D pose s ) + γL part (D part s ), (1)</formula><p>where L pose is the loss function for pose estimation, and L part is the loss function for part segmentation. The first two terms together form the objective function for learning the auxiliary task of pose estimation from both real and synthetic data. The third term learns part segmentation from synthetic data. Note that α, β, γ are the hyperparameters for balancing the losses among the three terms.</p><p>Human pose estimation aims at detecting human skeletons in a given image. Previous study <ref type="bibr" target="#b59">[61]</ref> proposed to detect the joint locations (i.e. keypoints) and the associations between the joints (i.e. Part Affinity Fields). After that, human skeletons are reconstructed with a greedy algorithm. Following the common definition of pose labels <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b59">[61]</ref>, we use the annotations of keypoints and Part Affinity Fields (PAFs) <ref type="bibr" target="#b59">[61]</ref> for learning pose estimation. In particular, let D pose</p><formula xml:id="formula_1">r = {I i r , K i r , P i r } M i=1</formula><p>, where M is the total number of real images, I r ∈ R w×h×3 denotes a real RGB image, K r ∈ R w×h×J denotes a real keypoint ground truth, which has J different maps, one per keypoint, P r ∈ R w×h×C denotes a real part affinity ground truth, which has C affinity vector fields. Also, we have a synthetic dataset with pose labels D pose</p><formula xml:id="formula_2">s = {I i s , K i s , P i s } N i=1</formula><p>, where N is the total number of images in the synthetic data. Furthermore, we have a synthetic dataset with part segmentation labels D part</p><formula xml:id="formula_3">s = {I i s , B i s } N i=1</formula><p>, where B s ∈ R w×h×Z is the synthetic body part segmentation ground truth and Z is the total number of body part categories. Note that it is convenient to assume D pose s and D part s share the same set of images. In this work, we use COCO Keypoint dataset <ref type="bibr" target="#b21">[22]</ref> as D pose r .</p><p>In the following, we omit the subscript r and s and use D pose to represent either real or synthetic data. The loss function we use for learning pose estimation is L pose (D pose ) = L kpts (I, K,K) + L paf (I, P,P ) where L kpts (·) and L paf (·) are the Euclidean loss functions minimizing the differences between the predictions and the ground truths, and they are defined below:</p><formula xml:id="formula_4">L kpts (I, K,K) = J j=1 θ M(θ)||K(θ) −K(θ)|| 2 2 ,<label>(2)</label></formula><formula xml:id="formula_5">L paf (I, P,P ) = C c=1 θ M(θ)||P (θ) −P (θ)|| 2 2 ,<label>(3)</label></formula><p>whereK andP denote the predicted keypoint confidence map and the predicted part affinity field, respectively, and K and P denote the ground truths. M is a binary mask, where M(θ) = 0 if the ground truth is missing at the location θ of the image. The mask is used to avoid penalizing the correct predictions as discussed in <ref type="bibr" target="#b59">[61]</ref>. The loss function of learning part segmentation is denoted as L part (D part ) = L part <ref type="figure">(I, B,B)</ref>, which is defined as the categorical cross entropy loss for classifying pixels to different human parts, that is:</p><formula xml:id="formula_6">L part (I, B,B) = − Z z=1 θ M(θ)B(θ) log(B(θ)),<label>(4)</label></formula><p>whereB denotes the predicted body part maps, B denotes the synthetic part segmentation ground truths.</p><p>In summary, the overall objective function is</p><formula xml:id="formula_7">L = α L kpts (I r , K r ,K) + L paf (I r , P r ,P ) + β L kpts (I s , K s ,K) + L paf (I s , P s ,P ) + γL part (I s , B s ,B).<label>(5)</label></formula><p>B. Network architecture <ref type="figure">Figure 4</ref> illustrates the proposed network. Our network takes an image of arbitrary size as input, and predicts three different outputs including (1) a set of body part segmentation mapsB, (2) a set of confidence keypoint mapsK, and (3) a set of Part Affinity Fields (PAFs)P <ref type="bibr" target="#b59">[61]</ref>. For clarity, we describe our network in two components: backbone and head networks.</p><p>1) Backbone network: In this paper, all the results are obtained by using ResNet101 <ref type="bibr" target="#b60">[62]</ref> with pyramid connections <ref type="bibr" target="#b61">[63]</ref>, <ref type="bibr" target="#b62">[64]</ref> as our backbone network. We denote the output feature maps of the residual blocks in ResNet101 as {C 1 , C 2 , C 3 , C 4 , C 5 } for conv1, conv2, conv3, conv4, and conv5, respectively. Following <ref type="bibr" target="#b62">[64]</ref>, we normalize the size of the feature maps {C 1 − C 5 } to a fixed size {C 1 −C 5 } as the input of the subsequent convolution layers. We denote f as our backbone network, and the output of our backbone is F = f (I), where I is an input image.</p><p>2) Head network: We detect multi-person body parts and human poses in a bottom-up strategy, which is in spirit similar to OpenPose <ref type="bibr" target="#b59">[61]</ref>. Our network predicts three target outputs in parallel, which areB,K, andP . Each head network is a fully convolutional network consisting of 8 convolution layers with 3 × 3 filters. Note that this is different from prior studies <ref type="bibr" target="#b59">[61]</ref>, <ref type="bibr" target="#b63">[65]</ref> that have a cascaded multi-stage head architecture. Our head networks do not have such a cascaded design, and can be seen as a single-stage network compared to prior works. Finally, we denote the three head networks as H B , H K , and H P , respectively. The body part segmentation mapsB are computed byB = H B (F ), where F is the output of our backbone. The confidence keypoint mapsK are computed bŷ K = H K (F ), and the Part Affinity Fields <ref type="bibr" target="#b59">[61]</ref>P are computed byP = H P (F ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>We initialized the backbone network using the pre-trained weights on ImageNet <ref type="bibr" target="#b8">[9]</ref>. The head networks are randomly initialized. During training, we randomly pick an equal number of real and synthetic images to form a mini-batch, and feed it to the network. Then, we compute the loss using Eq <ref type="bibr" target="#b4">(5)</ref>, and update the network parameters via Adam optimizer with an initial learning rate 0.001. The training batch size is set to 10. Following the literature <ref type="bibr" target="#b54">[55]</ref>, we set α = 1.0, β = 1.0, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Inference</head><p>During testing, we only predict the part segmentation. Our model predicts 14 body part score maps and one background score map. Following DeepLab <ref type="bibr" target="#b1">[2]</ref>, we run multi-scale inference and perform max-pooling to obtain the final part score maps. The part segmentation is derived by using the argmax value from the final part score maps. Given a fixed image size 368x654, the average inference processing time is about 16 frames per second using a PC with a single Titan XP GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We train our model with COCO Keypoint dataset <ref type="bibr" target="#b21">[22]</ref> and our synthetic dataset. We then evaluated the performance of the resulting model on two public benchmarks, the Pascal-Person-Parts <ref type="bibr" target="#b64">[66]</ref>, and the COCO-DensePose <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation benchmarks</head><p>Pascal-Person-Parts <ref type="bibr" target="#b64">[66]</ref> is a challenging dataset for multiperson body part segmentation. It consists of 1, 716 training and 1, 817 test images, where the human body is split into 6 different parts including head, torso, upper and lower arms, as well as upper and lower legs.</p><p>COCO-DensePose <ref type="bibr" target="#b5">[6]</ref> is a manually annotated dataset with the body part annotations. We evaluate multi-person body part segmentation on its body part annotations. The dataset contains 26, 151 training images, and the minival has 1, 508 validation images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Main results</head><p>We compare our technique with several state-of-the-art supervised approaches, including HAZN <ref type="bibr" target="#b3">[4]</ref>, Attention <ref type="bibr" target="#b2">[3]</ref>, LG-LSTM <ref type="bibr" target="#b65">[67]</ref>, LIP <ref type="bibr" target="#b50">[51]</ref>, Graph LSTM <ref type="bibr" target="#b66">[68]</ref>, DeepLab <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and WSHP <ref type="bibr" target="#b55">[56]</ref>. Note that all these approaches use Pascal-Person-Parts dataset including the part segmentation labels as the training data while our network does not need to use any of the data from Pascal-Person-Parts at all. Following the settings of Pascal-Person-Parts <ref type="bibr" target="#b64">[66]</ref>, we predict 6 body parts and measure the prediction results using the mean Intersection of Union (mIOU) <ref type="bibr" target="#b67">[69]</ref>. <ref type="table" target="#tab_3">Table I</ref> shows the performance comparison with different state-of-the-art methods, and <ref type="figure" target="#fig_3">Figure 5</ref> visualizes our prediction results. Without the segmentation training data provided by Pascal-Person-Parts, the proposed method CDCL achieves 65.02% mIOU, which is comparable to or better than several state-of-the-art supervised approaches, such as DeepLab v2 <ref type="bibr" target="#b1">[2]</ref>   <ref type="bibr" target="#b64">[66]</ref>. Note that the symbol "+" indicates using additional real dataset with human-annotated segmentation labels.   and Graph LSTM <ref type="bibr" target="#b66">[68]</ref>. It is worth noting that the proposed CDCL has better or similar performance compared to the other fully supervised methods, except for the head region. The main reason is that the head definition in our synthetic dataset does not match the head definition in Pascal-Person-Parts dataset. In our synthetic dataset, the head definition consistently includes the head and the neck. But in Pascal-Person-Parts, some of the ground truth head regions do not include the neck.</p><p>We further compare our method with the state-of-the-art approach <ref type="bibr" target="#b55">[56]</ref> on COCO-DensePose. For a fair comparison, we follow the body part settings of WSHP <ref type="bibr" target="#b55">[56]</ref>, and measure mIOU for the 6 different body parts and background. As shown on the second row CDCL of <ref type="table" target="#tab_3">Table II</ref>, our result is slightly better than WSHP <ref type="bibr" target="#b55">[56]</ref> which used real segmentation training data from both Pascal-Person-Parts and AIC <ref type="bibr" target="#b68">[70]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adding real data with part segmentation labels</head><p>To obtain the performance upper bound of our technique, we evaluate our method when real data with part segmentation labels are used during training. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, we share the parameters of all the modules and train the network using real and synthetic datasets. The bottom row CDCL+Pascal of <ref type="table" target="#tab_3">Table I</ref> shows the result on Pascal-Person-Parts where Pascal-Person-Parts training data is used. Our method outperforms WSHP by a large margin.</p><p>The same model is evaluated on the COCO-DensePose test data and the result is shown on the third row CDCL+Pascal of <ref type="table" target="#tab_3">Table II</ref>. Again it outperforms WSHP by a large margin.</p><p>If we use COCO-DensePose training data instead, and evaluate on COCO-DensePose test data, we obtain an additional gain and the result is shown on the fourth row CDCL+COCO of <ref type="table" target="#tab_3">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with adversarial learning</head><p>Recent studies <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b38">[39]</ref> used adversarial training to align the feature spaces of the synthetic and real images. Thus, we compare the performance of our method with the adversarial training strategy. Since the model presented in <ref type="bibr" target="#b18">[19]</ref> cannot be directly used for part segmentation, we implemented our own network similar to <ref type="bibr" target="#b18">[19]</ref>. Our network has a backbone (ResNet101) and two head networks, one for the part segmentation head and the other for the discriminator. <ref type="table" target="#tab_3">Table III</ref> shows the performance comparison on two datasets. We can see that adversarial training (ADV) achieves better performance than that of training with synthetic data only without adversarial training (SYN), but it does not perform as well as our complementary learning technique. Input image SYN NO-SP CDCL <ref type="figure">Fig. 7</ref>: Qualitative comparison of the proposed method with different training strategies.</p><p>E. Ablation study 1) Synthetic pose labels: Since our approach uses both synthetic poses and real poses, one interesting question is whether the synthetic pose is useful. To answer this question, we have trained our network without the synthetic poses (i.e. with synthetic parts and real poses). This configuration is denoted as NO-SP, and the results on Pascal-Person-Parts and COCO-DensePose are shown in <ref type="table" target="#tab_3">Table IV</ref>. For completeness, we also show the results of SYN (synthetic parts + synthetic poses), and CDCL (synthetic parts + synthetic poses + real poses). We can see that NO-SP outperforms SYN by a large margin thanks to the knowledge learned from the real data, and adding synthetic poses further boosts the performance. <ref type="figure">Figure 7</ref> shows a qualitative comparison of the three configurations. SYN has trouble handling the background, NO-SP performs much better, and CDCL further improves upon NO-SP.</p><p>2) Fully supervised baseline: We study a fully supervised baseline by removing the synthetic training data, and train a model using real part segmentation labels only. This configuration is denoted as Fully-supervised, and the results are shown in <ref type="table" target="#tab_8">Table V</ref>. We see that CDCL performs comparably to Fullysupervised because CDCL effectively reduces the domain gap.</p><p>3) Feature space visualization: We visualize the features of two different models (SYN and CDCL) from the real and synthetic images using the t-SNE visualization technique <ref type="bibr" target="#b69">[71]</ref>.   In <ref type="figure" target="#fig_5">Figure 8</ref>, the left column shows the features extracted with the model SYN (trained with synthetic data only), and the right column are from the model CDCL. The first row shows the features extracted at the left elbow position, and the second row shows the features extracted at the right knee position. In each plot, the red dots indicate the real data while the purple dots indicate the synthetic data. We can see that the red and purple dots in the right column are aligned very well, but they do not align well in the left column. This indicates that our complementary learning technique is effective at aligning the feature space of the real data with that of the synthetic data. 4) Synthetic training data analysis: Since our method learns part segmentation from synthetic data, one may wonder what elements of the synthetic data are essential to be rendered. To answer the question, we ablate our synthetic training data by gradually removing the background, colors, and the human texture, and train our model with these configurations, respectively. <ref type="figure">Figure 9</ref> shows the examples of different configurations of the synthetic training data, and <ref type="table" target="#tab_3">Table VI</ref> shows the performance comparison on Pascal-Person-Parts and COCO-DensePose datasets. Firstly, we observe that removing the background from the synthetic data causes only a small drop on the segmentation performance. This is an indication that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>No Background Gray-scale Binary Mask <ref type="figure">Fig. 9</ref>: Different configurations of the synthetic data.   our framework is learning the background from the real data. Secondly, after we further remove the color of the synthetic data (Gray-scale), we again only see a small drop on the performance. Finally, when we degrade our synthetic data to the extreme by just using binary masks, our framework still works reasonably well. These studies indicate that our framework mainly requires the pose variations in the foreground data and the rendering quality is not as critical compared to the conventional approach of directly training from synthetic data. 5) Influence of the synthetic human geometry: Since our synthetic humans each have their own geometry of body shape and clothing, we study the influence of the model geometry by training with different number of synthetic human models. <ref type="table" target="#tab_3">Table VII</ref> shows such results on Pascal-Person-Parts and COCO-DensePose datasets. We see that as we increase the number of synthetic human models for training, it improves the performance for part segmentation. However, the impact becomes less prominent if we use more than 10 synthetic human models for training. 6) Compositing synthetic humans with different backgrounds: Since our synthetic dataset uses a single background of empty room, one may wonder what if we use more variety of backgrounds for training. Because it is time consuming to create a large variety of synthetic 3D background models, we composite the synthetic humans with a variety of real-world scenery images. We randomly select 1000 scenery images from the Holidays dataset <ref type="bibr" target="#b70">[72]</ref> for data generation. <ref type="figure" target="#fig_6">Figure 10</ref> shows a few examples of the composited images and Table VIII shows the results on Pascal-Person-Parts and COCO-DensePose datasets when we composite synthetic humans with different number of backgrounds. We can see that increasing the number of backgrounds improves the performance of part segmentation, but it does not work as well as using a simple background such as an empty room or a blank background. This is probably because the synthetic humans are not placed in realistic positions in the scene and there are lighting inconsistency between the synthetic humans and background.</p><p>7) Influence of different losses: We study the influence of the three terms in our learning objective (in <ref type="figure">Eq.(1)</ref>). The first two terms learn pose estimation from real and synthetic data, respectively. The third term learns part segmentation from synthetic data. We study the influence of the three terms  ) . Thus, we can omit the scaling factor by setting α = 1.0 and vary β and γ. As shown in <ref type="figure" target="#fig_7">Figure 11</ref>, our method performs more favorably when γ 0.5 × β. Our method achieves the best performance when α, β, γ are set to 1.0, 1.0 and 0.5, respectively. This indicates that the first two terms are equally important. We also observed that part segmentation loss is greater than pose estimation loss, thus the losses are better balanced when γ is smaller than β. It is worth noting that the hyperparameters (i.e., α, β, γ) are used to control the quality of the learning process. Our design principle of the hyperparameters is to ensure the three losses should have a similar scale, so that the three loss terms can be balanced and contribute to the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Novel keypoint detection</head><p>Since our approach can easily create arbitrary annotations on synthetic data and transfer the knowledge to real domain, our method is highly scalable and flexible to users needs. For example, suppose we want to predict a new set of keypoints including hands and feet, it would be difficult to re-label the entire COCO dataset. With our technique, we can simply generate new labels on the synthetic data. We have performed an experiment to demonstrate this capability.</p><p>We create 30 novel keypoints for each avatar in the graphics simulator, and use the proposed method to learn the new set of keypoints. <ref type="figure" target="#fig_1">Figure 13</ref> shows the definition of the novel keypoints. To enable our existing network to learn such a new task, we add two additional head networks in our framework to learn the newly created 30 keypoints and their Part Affinity Fields, resulting in a total of 5 head networks in our network  architecture. <ref type="figure" target="#fig_0">Figure 12</ref> shows the qualitative results of our novel keypoint detection. With small modifications of the existing network, our method learns the novel skeleton representations from the synthetic data and transfers the knowledge to the real domain. It eliminates the needs of ground truth labeling of the additional joints on the real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. QUALITATIVE COMPARISON</head><p>Recent study <ref type="bibr" target="#b14">[15]</ref> proposed to estimate body part segmentation by learning with synthetic data, which is closely related to our method. Since MPII dataset <ref type="bibr" target="#b72">[74]</ref> does not have part segmentation labels for quantitative evaluation, Varol et al. <ref type="bibr" target="#b14">[15]</ref> showed qualitative results on selected images from MPII. Given a test image, Varol et al. <ref type="bibr" target="#b14">[15]</ref> used additional preprocessing to normalize the input. From their results on MPII dataset with multiple people, it appears that they cropped each image centered at a specific person before feeding to their network. In contrast, our method does not require such preprocessing. Furthermore, our method produces better results as shown in <ref type="figure">Figure 14</ref>. For each example, we show the original image from MPII dataset <ref type="bibr" target="#b72">[74]</ref>, our part segmentation result on the original image, the cropped version which was used as the network input in <ref type="bibr" target="#b14">[15]</ref>, and the part segmentation result of <ref type="bibr" target="#b14">[15]</ref>.</p><p>We further conduct qualitative comparison with the adversarial training approach on a challenging video <ref type="bibr" target="#b71">[73]</ref>. We compare our method with the adversarial network models presented in Sec V-D, and <ref type="figure" target="#fig_3">Figure 15</ref> shows the results. We can see that our method performs consistently better than the previous baseline approaches on the tested frames. The results validate the effectiveness of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We presented a cross-domain complementary learning framework for multi-person part segmentation. Without using any real data part segmentation labels, our method is able to achieve a comparable or better performance than several stateof-the-art techniques that use real part segmentation data for training. We further demonstrated that our technique can also be used to learn novel keypoint detection from synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Test Image Our Result</head><p>Center-cropped Test Image <ref type="bibr" target="#b14">[15]</ref> Result from <ref type="bibr" target="#b14">[15]</ref> Original Test Image Our Result</p><p>Center-cropped Test Image <ref type="bibr" target="#b14">[15]</ref> Result from <ref type="bibr" target="#b14">[15]</ref> (a) (b) <ref type="figure">Fig. 14:</ref> Qualitative comparison with the state-of-the-art approach <ref type="bibr" target="#b14">[15]</ref>. Previous method required additional preprocessing to normalize the input image, and failed to predict part segmentation for all the people due to occlusions. In contrast, our method does not require any preprocessing, and generated correct part segmentation for all the people even though some of them are heavily occluded by others.</p><p>Input SYN ADV Ours <ref type="figure" target="#fig_3">Fig. 15</ref>: Qualitative comparison on a challenging video <ref type="bibr" target="#b71">[73]</ref>. The left column shows the input images. The second column from the left shows the results of training using synthetic data only (SYN). The third column from the left shows the results of adversarial training (ADV). The right column shows our results. We can see that SYN failed completely for the real-world images due to domain gap. ADV produces many false alarms in the background. In contrast, our approach performs better than the previous approaches on the tested frames. More video results can be found at https://youtu.be/8QaGfdHwH48</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The layout of our synthetic environment. We render multiple avatars performing different actions in a 3D room, and capture the animations from multiple different viewpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Samples of our synthetic data. Our synthetic data contain multiple persons performing various actions in a 3D room. Top row: the synthetic RGB images. Middle row: the synthetic pose labels. Bottom row: the synthetic part labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Backbone</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results of the proposed method CDCL on Pascal-Person-Parts and COCO validation images. γ = 0.5 to balance pose estimation loss and part segmentation loss. We refer the readers to Sec.V-E7 for further details on the hyperparameter analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>An extension of our training framework when real part segmentation labels are available for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>t-SNE visualization<ref type="bibr" target="#b69">[71]</ref> of the feature spaces of the real and synthetic body parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Examples of compositing synthetic humans with a variety of real-world scenery images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Performance comparison (mIOU, %) of CDCL with different combinations of hyperparameters on Pascal-Person-Parts dataset. by fixing the first weight α = 1.0 and iterating different combinations of β and γ. The reason we set α = 1.0 is that we can rewrite Eq.(1) as L = α L pose (D pose r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Novel keypoint detection results on COCO validation images. Without any human labeling effort, our method learns to predict a new set of keypoints including those on the hands and feet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>The definition of our created novel keypoints. There is a total of 30 keypoints and 29 part associations for constructing fine-grained human skeleton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>received Sep. 30, 2019; revised Feb. 1, 2020 and Apr. 7, 2020; accepted May 3, 2020. K. Lin and M.-T. Sun are with the Department of Electrical and Computer Engineering, University of Washington, Seattle, WA, 98195. E-mail: {kvlin, mts}@uw.edu L. Wang, K. Luo, Y. Chen, and Z. Liu are with Microsoft Azure+AI, Redmond, WA, 98052. E-mail: {lijuanw, kun.luo, yiche, zliu}@microsoft.com</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Performance comparison of human body part segmentation (mIOU, %) on Pascal-Person-Parts dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Performance comparison of human body part segmentation (mIOU, %) on COCO-DensePose human body masks<ref type="bibr" target="#b5">[6]</ref>. Note that the symbol "+" indicates using additional real dataset with human-annotated segmentation labels.</figDesc><table><row><cell>Method</cell><cell cols="2">Real Seg. GT</cell><cell>Syn Seg. GT</cell><cell>Head</cell><cell>Torso</cell><cell>U-arms</cell><cell>L-arms</cell><cell>U-legs</cell><cell>L-legs</cell><cell>Bkg</cell><cell>Avg</cell></row><row><cell>WSHP [56]</cell><cell cols="2">+</cell><cell></cell><cell>67.33</cell><cell>62.22</cell><cell>51.50</cell><cell>55.66</cell><cell>54.22</cell><cell>53.11</cell><cell>76.81</cell><cell>60.12</cell></row><row><cell>CDCL</cell><cell></cell><cell></cell><cell></cell><cell>68.45</cell><cell>66.21</cell><cell>59.96</cell><cell>51.72</cell><cell>50.71</cell><cell>50.57</cell><cell>75.55</cell><cell>60.45</cell></row><row><cell cols="2">CDCL+Pascal</cell><cell></cell><cell></cell><cell>66.16</cell><cell>64.80</cell><cell>60.33</cell><cell>61.19</cell><cell>55.97</cell><cell>54.96</cell><cell>92.03</cell><cell>65.06</cell></row><row><cell cols="2">CDCL+COCO</cell><cell></cell><cell></cell><cell>73.15</cell><cell>68.74</cell><cell>63.79</cell><cell>67.66</cell><cell>63.39</cell><cell>60.62</cell><cell>93.55</cell><cell>70.13</cell></row><row><cell>Module 1</cell><cell></cell><cell>Head networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Body Part Maps</cell><cell></cell><cell>Body Part</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Keypoint Maps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Synthetic Inputs</cell><cell>(ResNet101) Backbone</cell><cell></cell><cell>Part Affinity Fields</cell><cell></cell><cell>Skeletons</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Module 2</cell><cell>Parameter Sharing</cell><cell></cell><cell>Body Part Maps</cell><cell></cell><cell>Body Part</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Keypoint Maps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Part Affinity</cell><cell></cell><cell>Skeletons</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Real Inputs</cell><cell>Backbone (ResNet101)</cell><cell>Head networks</cell><cell>Fields</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Performance comparison of human body part segmentation (mIOU, %) of different methods.</figDesc><table><row><cell cols="3">Method Pascal-Person-Parts COCO-DensePose</cell></row><row><cell>SYN</cell><cell>10.18</cell><cell>10.12</cell></row><row><cell>ADV</cell><cell>16.42</cell><cell>19.24</cell></row><row><cell>CDCL</cell><cell>65.02</cell><cell>60.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Ablations of training with different types of data.</figDesc><table><row><cell>Method</cell><cell>Syn.</cell><cell>Syn.</cell><cell>Real</cell><cell>Pascal</cell><cell>COCO</cell></row><row><cell></cell><cell cols="4">Parts Poses Poses mIOU</cell><cell>mIOU</cell></row><row><cell>SYN</cell><cell></cell><cell></cell><cell></cell><cell>10.18</cell><cell>10.12</cell></row><row><cell>NO-SP</cell><cell></cell><cell></cell><cell></cell><cell>49.71</cell><cell>50.66</cell></row><row><cell>CDCL</cell><cell></cell><cell></cell><cell></cell><cell>65.02</cell><cell>60.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Performance comparison (mIOU, %) with the fully supervised baseline.</figDesc><table><row><cell>Method</cell><cell cols="2">Pascal-Person-Parts COCO-DensePose</cell></row><row><cell>CDCL</cell><cell>65.02</cell><cell>60.45</cell></row><row><cell>Fully-supervised</cell><cell>65.40</cell><cell>61.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="5">: Performance comparison (mIOU, %) of our</cell></row><row><cell cols="4">method using different synthetic training data.</cell><cell></cell></row><row><cell></cell><cell cols="4">Original No Background Gray-scale Binary Mask</cell></row><row><cell>Pascal-Person-Parts</cell><cell>65.02</cell><cell>63.96</cell><cell>62.78</cell><cell>43.38</cell></row><row><cell>COCO-DensePose</cell><cell>60.45</cell><cell>59.39</cell><cell>58.34</cell><cell>40.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Ablation study (mIOU, %) of our method using different number of synthetic human models for training.</figDesc><table><row><cell>Number of Human Models</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell></row><row><cell>Pascal-Person-Parts</cell><cell>25.20</cell><cell>52.12</cell><cell>64.91</cell><cell cols="2">64.78 65.02</cell></row><row><cell>COCO-DensePose</cell><cell cols="2">22.12 51.65</cell><cell>60.22</cell><cell cols="2">60.41 60.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VIII</head><label>VIII</label><figDesc></figDesc><table><row><cell cols="4">: Ablation study (mIOU, %) of our method</cell></row><row><cell cols="4">when compositing synthetic humans with different number of</cell></row><row><cell>backgrounds for training.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of Backgrounds</cell><cell>1</cell><cell>100</cell><cell>1000</cell></row><row><cell>Pascal-Person-Parts</cell><cell cols="3">16.23 49.81 50.19</cell></row><row><cell>COCO-DensePose</cell><cell cols="3">14.50 46.33 48.65</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank Alvin Chia, Jon Hanzelka, and Pedro Urbina for their help with the synthetic data generation. We would like to thank Jamie Shotton for his support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">--</forename><surname>Deeplab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Upper body human detection and segmentation in low contrast video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1502" to="1509" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks,&quot; in Proc. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning appearance in virtual scenarios for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gerónimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IROS</title>
		<meeting>IROS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training deep networks with synthetic data: Bridging the reality gap by domain randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brophy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cameracci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boochoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Virtual and real world adaptation for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="797" to="809" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-view object class detection with a 3d geometric model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Description and recognition of curved objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Binford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="98" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The princeton shape benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shilane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Shape Modeling Applications</title>
		<meeting>Shape Modeling Applications</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An informationrich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep object detectors from 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Crdoco: Pixellevel domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to train with synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GCPR</title>
		<meeting>GCPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rpan: An end-to-end recurrent poseattention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">R-cnns for pose estimation and action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5212</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop</title>
		<meeting>CVPR Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A survey on multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards unified human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint multi-person pose estimation and semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pose-guided human parsing by an and/or graph using pose-context features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Weakly and semi supervised human body part parsing via pose-guided knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Whats the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Arnold</surname></persName>
		</author>
		<ptr target="https://www.arnoldrenderer.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Autodesk software</title>
		<ptr target="https://www.autodesk.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Ai Challenger</surname></persName>
		</author>
		<ptr target="https://challenger.ai/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Crazy Uptown Funk flashmob in Sydney</title>
		<ptr target="https://www.youtube.com/watch?v=2DiQUX11YaY" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">He is currently working toward the Ph.D. degree in electrical engineering at the University of Washington. During his study, he was a research intern at Microsoft Research. His research interests include computer vision, machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&apos;10) received the M.S. degree from the Graduate Institute of Networking and Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>National Taiwan University</orgName>
		</respStmt>
	</monogr>
	<note>and natural language processing</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">She joined Microsoft Research in 2006, where she is currently a principle research manager. She has been the key contributor in developing technologies on computer vision, printed and handwritten text recognition, avatar animation, and speech synthesis and recognition, which have been shipped into various Microsoft products. Current research interests include vision-language pre-training, image captioning, human pose estimation and part segmentation, and machine learning. She was part of the team that developed Azure Kinect Body Tracking SDK</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Lijaun Wang received the B.E. degree from Huazhong University of Science and Technology, and the Ph.D. degree from Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>She has published more than 40 papers on the top conferences and journals, and she has been awarded more than 15 US patents. She is a senior member of IEEE</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">He is currently a Research Software Development Engineer at Microsoft. He mainly works on building systems solving computer vision problems via deep learning methods</title>
		<imprint/>
		<respStmt>
			<orgName>Kun Luo received his M.S. degree in computer science from University of California San Diego</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<title level="m">respectively. He is currently a researcher at Microsoft. His current research interests include computer vision and multimedia</title>
		<meeting><address><addrLine>Tempe; Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Yinpeng Chen received the Ph.D. degree in electrical engineering from Arizona State University</orgName>
		</respStmt>
	</monogr>
	<note>2009. He received the B.S. and M.S. degree in electrical engineering from Tsinghua University</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">He is a fellow of IEEE. Ming-Ting Sun (S&apos;79-M&apos;81-SM&apos;89-F&apos;96-LF&apos;20) received the B.S. degree from National Taiwan University and the Ph.D. degree from University of California, Los Angeles, all in electrical engineering. Dr. Sun joined the University of Washington in 1996 where he is a Professor. Previously, he was the Director of Video Signal Processing Research at Bellcore. He has been a chaired/visiting professor at several universities. His main research interest is video and multimedia signal processing. Dr. Sun holds 13 patents and has published about 300 technical papers, including 17 book chapters in the area of video and multimedia technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He received an IEEE CASS Golden Jubilee Medal in 2000. He served as a General Co-Chair of ICME (International Conference on Multimedia and Expo) 2016, an Honorary Chair of VCIP (Visual Communication and Image Processing) 2015, a General Co-Chair chair of Visual Communications and Image Processing</title>
		<meeting><address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="2015" to="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note>He served as the Chair of the VSPC (Visual Signal Processing and Communications) Technical Committee of IEEE CAS. to 1994. He received the TCSVT Best Paper Award in 1993. From 1988 to 1991, he was the chairman of the IEEE CAS Standards Committee and established the IEEE Inverse Discrete Cosine Transform Standard. He is a Life Fellow of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
