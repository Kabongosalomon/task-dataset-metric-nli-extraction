<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
							<email>yilixuan@stu.xjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
							<email>sanpingzhou@stu.xjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
							<email>zbxu@mail.xjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
							<email>dymeng@mail.xjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>DNNs have recently obtained impressive good performance on various applications due to their powerful capacity for modeling complex input patterns. However, DNNs can easily overfit to biased training data 1 , like those containing corrupted labels <ref type="bibr" target="#b1">[2]</ref> or with class imbalance <ref type="bibr" target="#b2">[3]</ref>, leading to  their poor performance in generalization in such cases. This robust deep learning issue has been theoretically illustrated in multiple literatures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In practice, however, such biased training data are commonly encountered. For instance, practically collected training samples always contain corrupted labels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. A typical example is a dataset roughly collected from a crowdsourcing system <ref type="bibr" target="#b17">[18]</ref> or search engines <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, which would possibly yield a large amount of noisy labels. Another popular type of biased training data is those with class imbalance. Real-world datasets are usually depicted as skewed distributions, with a long-tailed configuration. A few classes account for most of the data, while most classes are under-represented. Effective learning with these biased training data, which is regarded to be biased from evaluation/test ones, is thus an important while challenging issue in machine learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Sample reweighting approach is a commonly used strategy against this robust learning issue. The main methodology is to design a weighting function mapping from training loss to sample weight (with hyper-parameters), and then iterates between calculating weights from current training loss values and minimizing weighted training loss for classifier updating. There exist two entirely contradictive ideas for constructing such a loss-weight mapping. One makes the function monotonically increasing as depicted in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, i.e., enforce the learning to more emphasize samples with larger loss values since they are more like to be uncertain hard samples located on the classification boundary. Typical methods of this category include AdaBoost <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, hard negative mining <ref type="bibr" target="#b23">[24]</ref> and focal loss <ref type="bibr" target="#b24">[25]</ref>. This sample weighting manner is known to be necessary for class imbalance problems, since it can prioritize the minority class with relatively higher training losses.</p><p>On the contrary, the other methodology sets the weighting function as monotonically decreasing, as shown in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>, to take samples with smaller loss values as more important ones. The rationality lies on that these samples are more likely to be high-confident ones with clean labels. Typical methods include self-paced learning(SPL) <ref type="bibr" target="#b25">[26]</ref>, iterative reweighting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref> and multiple variants <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. This weighting strategy has been especially used in noisy label cases, since it inclines to suppress the effects of samples with extremely large loss values, possibly with corrupted incorrect labels.</p><p>Although these sample reweighting methods help improve the robustness of a learning algorithm on biased training samples, they still have evident deficiencies in practice. On the one hand, current methods need to manually set a specific form of weighting function based on certain assumptions on training data. This, however, tends to be infeasible when we know little knowledge underlying data or the label conditions are too complicated, like the case that the training set is both imbalanced and noisy. On the other hand, even when we specify certain weighting schemes, like focal loss <ref type="bibr" target="#b24">[25]</ref> or SPL <ref type="bibr" target="#b25">[26]</ref>, they inevitably involve hyper-parameters, like focusing parameter in the former and age parameter in the latter, to be manually preset or tuned by cross-validation. This tends to further raise their application difficulty and reduce their performance stability in real problems.</p><p>To alleviate the aforementioned issue, this paper presents an adaptive sample weighting strategy to automatically learn an explicit weighting function from data. The main idea is to parameterize the weighting function as an MLP (multilayer perceptron) network with only one hidden layer (as shown in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>), called Meta-Weight-Net, which is theoretically a universal approximator for almost any continuous function <ref type="bibr" target="#b30">[31]</ref>, and then use a small unbiased validation set (meta-data) to guide the training of all its parameters. The explicit form of the weighting function can be finally attained specifically suitable to the learning task.</p><p>In summary, this paper makes the following three-fold contributions: 1) We propose to automatically learn an explicit loss-weight function, parameterized by an MLP from data in a meta-learning manner. Due to the universal approximation capability of this weight net, it can finely fit a wide range of weighting functions including those used in conventional research.</p><p>2) Experiments verify that the weighting functions learned by our method highly comply with manually preset weighting manners used in tradition in different training data biases, like class imbalance and noisy label cases as shown in <ref type="figure" target="#fig_1">Fig. 1(d) and 1(e)</ref>), respectively. This shows that the weighting scheme learned by the proposed method inclines to help reveal deeper understanding for data bias insights, especially in complicated bias cases where the extracted weighting function is with complex tendencies (as shown in <ref type="figure" target="#fig_1">Fig. 1(f)</ref>).</p><p>3) The insights of why the proposed method works can be well interpreted. Particularly, the updating equation for Meta-Weight-Net parameters can be explained by that the sample weights of those samples better complying with the meta-data knowledge will be improved, while those violating such meta-knowledge will be suppressed. This tallies with our common sense on the problem: we should reduce the influence of those highly biased ones, while emphasize those unbiased ones.</p><p>The paper is organized as follows. Section 2 presents the proposed meta-learning method as well as the detailed algorithm and analysis of its convergence property. Section 3 discusses related work. Section 4 demonstrates experimental results and the conclusion is finally made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Meta-Weight-Net Learning Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Meta-learning Objective</head><p>Consider a classification problem with the training set {x i , y i } N i=1 , where x i denotes the i-th sample, y i ∈ {0, 1} c is the label vector over c classes, and N is the number of the entire training data. f (x, w) denotes the classifier, and w denotes its parameters. In current applications, f (x, w) is always set as a DNN. We thus also adopt DNN, and call it the classifier network for convenience in the following.</p><p>Generally, the optimal classifier parameter w * can be extracted by minimizing the loss 1</p><formula xml:id="formula_0">N N i=1</formula><p>(y i , f (x i , w)) calculated on the training set. For notation convenience, we denote that L train i (w) = (y i , f (x i , w)). In the presence of biased training data, sample re-weighting methods enhance the robustness of training by imposing weight V(L train i (w); Θ) on the i-th sample loss, where V( ; Θ) denotes the weight net, and Θ represents the parameters contained in it. The optimal parameter w is calculated by minimizing the following weighted loss:</p><formula xml:id="formula_1">w * (Θ) = arg min w L train (w; Θ) 1 N N i=1 V(L train i (w); Θ)L train i (w).<label>(1)</label></formula><p>Meta-Weight-Net: Our method aims to automatically learn the hyper-parameters Θ in a metalearning manner. To this aim, we formulate V(L i (w); Θ) as a MLP network with only one hidden layer containing 100 nodes, as shown in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>. We call this weight net as Meta-Weight-Net or MW-Net for easy reference. Each hidden node is with ReLU activation function, and the output is with the Sigmoid activation function, to guarantee the output located in the interval of [0, 1]. Albeit simple, this net is known as a universal approximator for almost any continuous function <ref type="bibr" target="#b30">[31]</ref>, and thus can fit a wide range of weighting functions including those used in conventional research.</p><p>Meta learning process. The parameters contained in MW-Net can be optimized by using the meta learning idea <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Specifically, assume that we have a small amount unbiased meta-data set (i.e., with clean labels and balanced data distribution) {x</p><formula xml:id="formula_2">(meta) i , y (meta) i } M i=1</formula><p>, representing the meta-knowledge of ground-truth sample-label distribution, where M is the number of meta-samples and M N . The optimal parameter Θ * can be obtained by minimizing the following meta-loss:</p><p>Step 5</p><p>Step 6</p><p>Step 7</p><p>Meta-Weight-Net Classifier network ... <ref type="figure">Figure 2</ref>: Main flowchart of the proposed MW-Net Learning algorithm (steps 5-7 in Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Weight</head><formula xml:id="formula_3">Θ * = arg min Θ L meta (w * (Θ)) 1 M M i=1 L meta i (w * (Θ)),<label>(2)</label></formula><p>where L meta</p><formula xml:id="formula_4">i (w) = y (meta) i , f (x (meta) i , w)</formula><p>is calculated on meta-data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Meta-Weight-Net Learning Method</head><p>Calculating the optimal Θ * and w * require two nested loops of optimization. Here we adopt an online strategy to update Θ and w through a single optimization loop, respectively, to guarantee the efficiency of the algorithm.</p><p>Formulating learning manner of classifier network. As general network training tricks, we employ SGD to optimize the training loss <ref type="bibr" target="#b0">(1)</ref>. Specifically, in each iteration of training, a mini-batch of training samples {(x i , y i ), 1 ≤ i ≤ n} is sampled, where n is the mini-batch size. Then the updating equation of the classifier network parameter can be formulated by moving the current w (t) along the descent direction of the objective loss in Eq. (1) on a mini-batch training data:</p><formula xml:id="formula_5">w (t) (Θ) = w (t) − α 1 n × n i=1 V(L train i (w (t) ); Θ)∇ w L train i (w) w (t) ,<label>(3)</label></formula><p>where α is the step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The MW-Net Learning Algorithm</head><p>Input: Training data D, meta-data set D, batch size n, m, max iterations T . Output: Classifier network parameter w (T ) 1: Initialize classifier network parameter w (0) and Meta-Weight-Net parameter Θ (0) . 2: for t = 0 to T − 1 do 3: {x, y} ← SampleMiniBatch(D, n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>{x (meta) , y (meta) } ← SampleMiniBatch( D, m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Formulate the classifier learning functionŵ (t) (Θ) by Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update Θ (t+1) by Eq. (9). 7:</p><p>Update w (t+1) by Eq. (5). 8: end for Updating parameters of Meta-Weight-Net: After receiving the feedback of the classifier network parameter updating formulationŵ (t) (Θ) 2 from the Eq .(3), the parameter Θ of the Meta-Weight-Net can then be readily updated guided by Eq. (2), i.e., moving the current parameter Θ (t) along the objective gradient of Eq. (2) calculated on the meta-data:</p><formula xml:id="formula_6">Θ (t+1) = Θ (t) − β 1 m m i=1 ∇ Θ L meta i (ŵ (t) (Θ)) Θ (t) ,<label>(4)</label></formula><p>where β is the step size.</p><p>Updating parameters of classifier network: Then, the updated Θ (t+1) is employed to ameliorate the parameter w of the classifier network, i.e.,</p><formula xml:id="formula_7">w (t+1) = w (t) − α 1 n × n i=1 V(L train i (w (t) ); Θ (t+1) )∇ w L train i (w) w (t) .<label>(5)</label></formula><p>The MW-Net Learning algorithm can then be summarized in Algorithm 1, and <ref type="figure">Fig. 2</ref> illustrates its main implementation process (steps 5-7). All computations of gradients can be efficiently implemented by automatic differentiation techniques and generalized to any deep learning architectures of classifier network. The algorithm can be easily implemented using popular deep learning frameworks like PyTorch <ref type="bibr" target="#b35">[36]</ref>. It is easy to see that both the classifier network and the MW-Net gradually ameliorate their parameters during the learning process based on their values calculated in the last step, and the weights can thus be updated in a stable manner, as clearly shown in <ref type="figure" target="#fig_4">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Analysis on the Weighting Scheme of Meta-Weight-Net</head><p>The computation of Eq. (9) by backpropagation can be rewritten as <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_8">Θ (t+1) = Θ (t) + αβ n n j=1 1 m m i=1 G ij ∂V(L train j (w (t) ); Θ) ∂Θ Θ (t) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">G ij = ∂L meta i (ŵ) ∂ŵ T w (t) ∂L train j (w) ∂w w (t) . Neglecting the coefficient 1 m m i=1 G ij ,</formula><p>it is easy to see that each term in the sum orients to the ascend gradient of the weight function V(L train j (w (t) ); Θ).</p><p>1 m m i=1 G ij , the coefficient imposed on the j-th gradient term, represents the similarity between the gradient of the j-th training sample computed on training loss and the average gradient of the mini-batch meta data calculated on meta loss. That means if the learning gradient of a training sample is similar to that of the meta samples, then it will be considered as beneficial for getting right results and its weight tends to be more possibly increased. Conversely, the weight of the sample inclines to be suppressed. This understanding is consistent with why well-known MAML works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Convergence of the MW-Net Learning algorithm</head><p>Our algorithm involves optimization of two-level objectives, and therefore we show theoretically that our method converges to the critical points of both the meta and training loss function under some mild conditions in Theorem 1 and 2, respectively. The proof is listed in the supplementary material. Theorem 1. Suppose the loss function is Lipschitz smooth with constant L, and V(·) is differential with a δ-bounded gradient and twice differential with its Hessian bounded by B, and the loss function have ρ-bounded gradients with respect to training/meta data. Let the learning rate α t satisfies α t = min{1, k T }, for some k &gt; 0, such that k T &lt; 1, and β t , 1 ≤ t ≤ N is a monotone descent sequence,</p><formula xml:id="formula_10">β t = min{ 1 L , c σ √ T } for some c &gt; 0, such that σ √ T c ≥ L and ∞ t=1 β t ≤ ∞, ∞ t=1 β 2 t ≤ ∞. Then the proposed algorithm can achieve E[ ∇G(Θ (t) ) 2 2 ] ≤ in O(1/ 2 ) steps. More specifically, min 0≤t≤T E[ ∇L meta (Θ (t) ) 2 2 ] ≤ O( C √ T ),<label>(7)</label></formula><p>where C is some constant independent of the convergence process, and σ is the variance of drawing uniformly mini-batch sample at random. Theorem 2. The condions in Theorem 1 hold, then we have:</p><formula xml:id="formula_11">lim t→∞ E[ ∇L train (w (t) ; Θ (t+1) ) 2 2 ] = 0.<label>(8)</label></formula><p>3 Related Work Sample Weighting Methods. The idea of reweighting examples can be dated back to dataset resampling <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> or instance re-weight <ref type="bibr" target="#b41">[42]</ref>, which pre-evaluates the sample weights as a preprocessing step by using certain prior knowledge on the task or data. To make the sample weights fit data more flexibly, more recent researchers focused on pre-designing a weighting function mapping from training loss to sample weight, and dynamically ameliorate weights during training process <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. There are mainly two manners to design the weighting function. One is to make it monotonically increasing, specifically effective in class imbalance case. Typical methods include the boosting algorithm (like AdaBoost <ref type="bibr" target="#b21">[22]</ref>) and multiple of its variations <ref type="bibr" target="#b44">[45]</ref>, hard example mining <ref type="bibr" target="#b23">[24]</ref> and focal loss <ref type="bibr" target="#b24">[25]</ref>, which impose larger weights to ones with larger loss values. On the contrary, another series of methods specify the weighting function as monotonically decreasing, especially used in noisy label cases. For example, SPL <ref type="bibr" target="#b25">[26]</ref> and its extensions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, iterative reweighting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref> and other recent work <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b29">30]</ref>, pay more focus on easy samples with smaller losses. The limitation of these methods are that they all need to manually pre-specify the form of weighting function as well as their hyper-parameters, raising their difficulty to be readily used in real applications.</p><p>Meta Learning Methods. Inspired by meta-learning developments <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>, recently some methods were proposed to learn an adaptive weighting scheme from data to make the learning more automatic and reliable. Typical methods along this line include FWL <ref type="bibr" target="#b50">[51]</ref>, learning to teach <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b31">32]</ref> and MentorNet <ref type="bibr" target="#b20">[21]</ref> methods, whose weight functions are designed as a Bayesian function approximator, a DNN with attention mechanism, a bidirectional LSTM network, respectively. Instead of only taking loss values as inputs as classical methods, the weighting functions they used (i.e., the meta-learner), however, are with much more complex forms and required to input complicated information (like sample features). This makes them not only hard to succeed good properties possessed by traditional methods, but also to be easily reproduced by general users.</p><p>A closely related method, called L2RW <ref type="bibr" target="#b0">[1]</ref>, adopts a similar meta-learning mechanism compared with ours. The major difference is that the weights are implicitly learned there, without an explicit weighting function. This, however, might lead to unstable weighting behavior during training and unavailability for generalization. In contrast, with the explicit yet simple Meta-Weight-Net, our method can learn the weight in a more stable way, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, and can be easily generalized from a certain task to related other ones (see in the supplementary material).</p><p>Other Methods for Class Imbalance. Other methods for handling data imbalance include: <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> tries to transfer the knowledge learned from major classes to minor classes. The metric learning based methods have also been developed to effectively exploit the tailed data to improve the generalization ability, e.g., triple-header loss <ref type="bibr" target="#b54">[55]</ref> and range loss <ref type="bibr" target="#b55">[56]</ref>.</p><p>Other Methods for Corrupted Labels. For handling noisy label issue, multiple methods have been designed by correcting noisy labels to their true ones via a supplemental clean label inference step <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15]</ref>. For example, GLC <ref type="bibr" target="#b14">[15]</ref> proposed a loss correction approach to mitigate the effects of label noise on DNN classifiers. Other methods along this line include the Reed <ref type="bibr" target="#b57">[58]</ref>, Co-training <ref type="bibr" target="#b15">[16]</ref>, D2L <ref type="bibr" target="#b58">[59]</ref> and S-Model <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>To evaluate the capability of the proposed algorithm, we implement experiments on data sets with class imbalance and noisy label issues, and real-world dataset with more complicated data bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Class Imbalance Experiments</head><p>We use Long-Tailed CIFAR dataset <ref type="bibr" target="#b59">[60]</ref>, that reduces the number of training samples per class according to an exponential function n = n i µ i , where i is the class index, n i is the original number of training images and µ ∈ (0, 1). The imbalance factor of a dataset is defined as the number of training samples in the largest class divided by the smallest. We trained ResNet-32 <ref type="bibr" target="#b60">[61]</ref> with softmax cross-entropy loss by SGD with a momentum 0.9, a weight decay 5×10 −4 , an initial learning rate 0.1. The learning rate of ResNet-32 is divided by 10 after 80 and 90 epoch (for a total 100 epochs), and the learning rate of WN-Net is fixed as 10 −5 . We randomly selected 10 images per class in validation set as the meta-data set. The compared methods include: 1) BaseModel, which uses a softmax cross-entropy loss to train ResNet-32 on the training set; 2) Focal loss <ref type="bibr" target="#b24">[25]</ref> and Class-Balanced <ref type="bibr" target="#b59">[60]</ref> represent the state-of-the-arts of the predefined sample reweighting techniques; 3) Fine-tuning, fine-tune the result of BaseModel on the meta-data set; 4) L2RW <ref type="bibr" target="#b0">[1]</ref>, which leverages an additional meta-dataset to adaptively assign weights on training samples.   <ref type="table" target="#tab_0">Table 1</ref> shows the classification accuracy of ResNet-32 on the test set and confusion matrices are displayed in <ref type="figure">Fig. 3</ref> (more details are listed in the supplementary material). It can be observed that: 1) Our algorithm evidently outperforms other competing methods on datasets with class imbalance, showing its robustness in such data bias case; 2) When imbalance factor is 1, i.e., all classes are with same numbers of samples, fine-tuning runs best, and our method still attains a comparable performance; 3) When imbalance factor is 200 on long-tailed CIFAR-100, the smallest class has only two samples. An extra fine-tuning achieves performance gain, while our method still perform well in such extreme data bias.</p><p>To understand the weighing scheme of MW-Net, we depict the tendency curve of weight with respect to loss by the learned MW-Net in <ref type="figure" target="#fig_1">Fig. 1(d)</ref>, which complies with the classical optimal weighting manner to such data bias. i.e., larger weights should be imposed on samples with relatively large losses, which are more likely to be minority class sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Corrupted Label Experiment</head><p>We study two settings of corrupted labels on the training set: 1) Uniform noise. The label of each sample is independently changed to a random class with probability p following the same setting in <ref type="bibr" target="#b1">[2]</ref>. 2) Flip noise. The label of each sample is independently flipped to similar classes with total probability p. In our experiments, we randomly select two classes as similar classes with equal probability. Two benchmark datasets are employed: CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b61">[62]</ref>. Both are popularly used for evaluation of noisy labels <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b15">16]</ref>.1000 images with clean labels in validation set are randomly selected as the meta-data set. We adopt a Wide ResNet-28-10 (WRN-28-10) <ref type="bibr" target="#b62">[63]</ref> for uniform noise and ResNet-32 <ref type="bibr" target="#b60">[61]</ref> for flip noise as our classifier network models 4 .</p><p>The comparison methods include: BaseModel, referring to the similar classifier network utilized in our method, while directly trained on the biased training data; the robust learning methods Reed <ref type="bibr" target="#b57">[58]</ref>, S-Model <ref type="bibr" target="#b11">[12]</ref> , SPL <ref type="bibr" target="#b25">[26]</ref>, Focal Loss <ref type="bibr" target="#b24">[25]</ref>, Co-teaching <ref type="bibr" target="#b15">[16]</ref>, D2L <ref type="bibr" target="#b58">[59]</ref>; Fine-tuning, fine-tuning the result of BaseModel on the meta-data with clean labels to further enhance its performance; typical meta-learning methods MentorNet <ref type="bibr" target="#b20">[21]</ref>, L2RW <ref type="bibr" target="#b0">[1]</ref>, GLC <ref type="bibr" target="#b14">[15]</ref>. We also trained the baseline network only on 1000 meta-images. The performance are evidently worse than the proposed method due to the neglecting of the knowledge underlying large amount of training samples. We thus have not involved its results in comparison.</p><p>All the baseline networks were trained using SGD with a momentum 0.9, a weight decay 5 × 10 −4 and an initial learning rate 0.   total of 60 epoches) in flip noise. The learning rate of WN-Net is fixed as 10 −3 . We repeated the experiments 5 times with different random seeds for network initialization and label noise generation.</p><p>We report the accuracy averaged over 5 repetitions for each series of experiments and each competing method in <ref type="table" target="#tab_1">Tables 2 and 3</ref>. It can be observed that our method gets the best performance across almost all datasets and all noise rates, except the second for 40% Flip noise. At 0% noise cases (unbiased ones), our method performs only slightly worse than the BaseModel. For other corrupted label cases, the superiority of our method is evident. Besides, it can be seen that the performance gaps between ours and all other competing methods increase as the noise rate is increased from 40% to 60% under uniform noise. Even with 60% label noise, our method can still obtain a relatively high classification accuracy, and attains more than 15% accuracy gain compared with the second best result for CIFAR100 dataset, which indicates the robustness of our methods in such cases. <ref type="figure">Fig. 4</ref> shows the performance comparison between WRN-28-10 and ResNet32 under fixed flip noise setting. We can observe that the performance gains for our method and BaseModel between two networks takes the almost same value. It implies that the performance improvement of our method is not dependent on the selection of the classifier network architectures.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>(e), the shape of the learned weight function depicts as monotonic decreasing, complying with the traditional optimal setting to this bias condition, i.e., imposing smaller weights on samples with relatively large losses to suppress the effect of corrupted labels. Furthermore, we plot the weight distribution of clean and noisy training samples in <ref type="figure" target="#fig_3">Fig. 5</ref>. It can be seen that almost all large weights belongs to clean samples, and the noisy samples's weights are smaller than that of clean samples, which implies that the trained Meta-Weight-Net can distinguish clean and noisy images. <ref type="figure" target="#fig_4">Fig. 6</ref> plots the weight variation along with training epoches under 40% noise on CIFAR10 dataset of our method and L2RW. y-axis denotes the differences of weights calculated between adjacent epoches, and x-axis denotes the number of epoches. Ten noisy samples are randomly chosen to compute their mean curve, surrounded by the region illustrating the standard deviations calculated on these samples in the corresponding epoch. It is seen that the weight by our method is continuously changed, gradually stable along iterations, and finally converges. As a comparison, the weight during the learning process of L2RW fluctuates relatively more wildly. This could explain the consistently better performance of our method as compared with this competing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Clothing1M</head><p>To verify the effectiveness of the proposed method on real-world data, we conduct experiments on the Clothing1M dataset <ref type="bibr" target="#b63">[64]</ref>, containing 1 million images of clothing obtained from online shopping   websites that are with 14 categories, e.g., T-shirt, Shirt, Knitwear. The labels are generated by using surrounding texts of the images provided by the sellers, and therefore contain many errors. We use the 7k clean data as the meta dataset. Following the previous works <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref>, we used ResNet-50 pre-trained on ImageNet. For preprocessing, we resize the image to 256 × 256, crop the middle 224 × 224 as input, and perform normalization. We used SGD with a momentum 0.9, a weight decay 10 −3 , and an initial learning rate 0.01, and batch size 32. The learning rate of ResNet-50 is divided by 10 after 5 epoch (for a total 10 epoch), and the learning rate of WN-Net is fixed as 10 −3 .</p><p>The results are summarized in <ref type="table">Table.</ref> 4. which shows that the proposed method achieves the best performance. <ref type="figure" target="#fig_1">Fig. 1(f)</ref> plots the tendency curve of the learned MW-Net function, which reveals abundant data insights. Specifically, when the loss is with relatively small values, the weighting function inclines to increase with loss, meaning that it tends to more emphasize hard margin samples with informative knowledge for classification; while when the loss gradually changes large, the weighting function begins to monotonically decrease, implying that it tends to suppress noise labels samples with relatively large loss values. Such complicated essence cannot be finely delivered by conventional weight functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel meta-learning method for adaptively extracting sample weights to guarantee robust deep learning in the presence of training data bias. Compared with current reweighting methods that require to manually set the form of weight functions, the new method is able to yield a rational one directly from data. The working principle of our algorithm can be well explained and the procedure of our method can be easily reproduced ( Appendix A provide the Pytorch implement of our algorithm (less than 30 lines of codes)), and the completed training code is avriable at https://github.com/xjtushujun/meta-weight-net.). Our empirical results show that the propose method can perform superior in general data bias cases, like class imbalance, corrupted labels, and more complicated real cases. Besides, such an adaptive weight learning approach is hopeful to be employed to other weight setting problems in machine learning, like ensemble methods and multi-view learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Pytorch codes of our algorithm</head><p>The following is the Pytorch codes of our algorithm (core code is less than 30 lines), and the completed training code is avriable at https://github.com/xjtushujun/meta-weight-net. </p><formula xml:id="formula_12">Θ (t+1) = Θ (t) − β 1 m m i=1 ∇ Θ L meta i (ŵ (t) (Θ)) Θ (t) .<label>(9)</label></formula><p>The computation of Eq. (9) in the paper by backpropagation can be understood by the following derivation:</p><formula xml:id="formula_13">1 m m i=1 ∇ Θ L meta i (ŵ (t) (Θ)) Θ (t) = 1 m m i=1 ∂L meta i (ŵ) ∂ŵ(Θ) ŵ (t) n j=1 ∂ŵ (t) (Θ) ∂V(L train j (w (t) ); Θ) ∂V(L train j (w (t) ); Θ) ∂Θ Θ (t) = −α n * m m i=1 ∂L meta i (ŵ) ∂ŵ ŵ (t) n j=1 ∂L train j (w) ∂w w (t) ∂V(L train j (w (t) ); Θ) ∂Θ Θ (t) = −α n n j=1 1 m m i=1 ∂L meta i (ŵ) ∂ŵ T w (t) ∂L train j (w) ∂w w (t) ∂V(L train j (w (t) ); Θ) ∂Θ Θ (t) .<label>(10)</label></formula><p>Let</p><formula xml:id="formula_14">G ij = ∂L meta i (ŵ) ∂ŵ T w (t) ∂L train j (w) ∂w w (t) ,<label>(11)</label></formula><p>by substituting Eq. (10) into Eq. <ref type="formula" target="#formula_12">(9)</ref>, we can get:</p><formula xml:id="formula_15">Θ (t+1) = Θ (t) + αβ n n j=1 1 m m i=1 G ij ∂V(L train j (w (t) ); Θ) ∂Θ Θ (t) .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Convergence Proof of Our Method</head><p>This section provides the proofs for Theorems 1 and 2 in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose that we have a small amount of meta (validation) dataset with</head><formula xml:id="formula_16">M samples {(x (m) i , y (m) i</formula><p>), 1 ≤ i ≤ M } with clean labels, and the overall meta loss is,</p><formula xml:id="formula_17">L meta (w * (Θ)) = 1 M M i=1 L meta i (w * (Θ)),<label>(13)</label></formula><p>where w * is the parameter of the classifier network, and Θ is the parameter of the Meta-Weight-Net. Let's suppose we have another N training data, {(x i , y i ), 1 ≤ i ≤ N }, where M N , and the overall training loss is,</p><formula xml:id="formula_18">L train (w; Θ) = 1 N N i=1 V(L train i (w); Θ)L train i (w).<label>(14)</label></formula><p>Lemma 1. Suppose the meta loss function is Lipschitz smooth with constant L, and V(·) is differential with a δ-bounded gradient and twice differential with its Hessian bounded by B, and the loss function have ρ-bounded gradients with respect to training/meta data. Then the gradient of Θ with respect to meta loss is Lipschitz continuous.</p><p>Proof. The gradient of Θ with respect to meta loss can be written as:</p><formula xml:id="formula_19">∇ Θ L meta i (ŵ (t) (Θ)) Θ (t) = −α n n j=1 ∂L meta i (ŵ) ∂ŵ T w (t) ∂L train j (w) ∂w w (t) ∂V(L train j (w (t) ); Θ) ∂Θ Θ (t) .<label>(15)</label></formula><p>Let V j (Θ) = V(L train j (w (t) ); Θ) and Gij being defined in Eq. <ref type="bibr" target="#b10">(11)</ref>. Taking gradient of Θ in both sides of Eq.(15), we have</p><formula xml:id="formula_20">∇ 2 Θ 2 L meta i (ŵ (t) (Θ)) Θ (t) = −α n n j=1 ∂ ∂Θ (Gij) Θ (t) ∂Vj(Θ) ∂Θ Θ (t) + (Gij) ∂ 2 Vj(Θ) ∂Θ 2 Θ (t) .</formula><p>For the first term in the right hand side, we have that</p><formula xml:id="formula_21">∂ ∂Θ (Gij) Θ (t) ∂Vj(Θ) ∂Θ Θ (t) ≤ δ ∂ ∂ŵ ∂L meta i (ŵ) ∂Θ Θ (t) T w (t) ∂L train j (w) ∂w w (t) = δ ∂ ∂ŵ ∂L meta i (ŵ) ∂ŵ ŵ (t) −α n n k=1 ∂L train k (w) ∂w w (t) ∂V k (Θ) ∂Θ Θ (t) T w (t) ∂L train j (w) ∂w w (t) = δ ∂ 2 L meta i (ŵ) ∂ŵ 2 ŵ (t) −α n n k=1 ∂L train k (w) ∂w w (t) ∂V k (Θ) ∂Θ Θ (t) T w (t) ∂L train j (w) ∂w ≤ αLρ 2 δ 2 , (16) since ∂ 2 L meta i (ŵ) ∂ŵ 2 ŵ (t) ≤ L, ∂L train j (w) ∂w w (t) ≤ ρ, ∂V j (Θ) ∂Θ Θ (t)</formula><p>≤ δ. And for the second term we have (Gij)</p><formula xml:id="formula_22">∂ 2 Vj(Θ) ∂Θ 2 Θ (t) = ∂L meta i (ŵ) ∂ŵ T w (t) ∂L train j (w) ∂w w (t) ∂ 2 Vj(Θ) ∂Θ 2 Θ (t) ≤ Bρ 2 ,<label>(17)</label></formula><formula xml:id="formula_23">since ∂L meta i (ŵ) ∂ŵ T w (t) ≤ρ, ∂ 2 V j (Θ) ∂Θ 2 Θ (t) ≤B.</formula><p>Combining the above two inequalities Eq.(16) (17), we have</p><formula xml:id="formula_24">∇ 2 Θ 2 L meta i (ŵ (t) (Θ)) Θ (t) ≤ αρ 2 (αLδ 2 + B).<label>(18)</label></formula><p>Define LV = αρ 2 (αLδ 2 + B), based on Lagrange mean value theorem, we have:</p><formula xml:id="formula_25">∇L meta (ŵ (t) (Θ1)) − ∇L meta (ŵ (t) (Θ2)) ≤ LV Θ1 − Θ2 , f or all Θ1, Θ2,<label>(19)</label></formula><p>where</p><formula xml:id="formula_26">∇L meta (ŵ (t) (Θ1)) = ∇ΘL meta i (ŵ (t) (Θ)) Θ 1 .</formula><p>Theorem 1. Suppose the loss function is Lipschitz smooth with constant L, and V(·) is differential with a δ-bounded gradient and twice differential with its Hessian bounded by B, and the loss function have ρ-bounded gradients with respect to training/meta data. Let the learning rate α t satisfies α t = min{1, k T }, for some k &gt; 0, such that k T &lt; 1, and β t , 1 ≤ t ≤ N is a monotone descent sequence,</p><formula xml:id="formula_27">β t = min{ 1 L , c σ √ T } for some c &gt; 0, such that σ √ T c ≥ L and ∞ t=1 β t ≤ ∞, ∞ t=1 β 2 t ≤ ∞. Then Meta-Weight-Net can achieve E[ ∇L meta (ŵ (t) (Θ (t) )) 2 2 ] ≤ in O(1/ 2 ) steps. More specifically, min 0≤t≤T E[ ∇L meta (ŵ (t) (Θ (t) )) 2 2 ] ≤ O( C √ T ),<label>(20)</label></formula><p>where C is some constant independent of the convergence process, σ is the variance of drawing uniformly mini-batch sample at random.</p><p>Proof. The update of Θ in each iteration is as follows:</p><formula xml:id="formula_28">Θ (t+1) = Θ (t) − β 1 m m i=1 ∇ Θ L meta i (ŵ (t) (Θ)) Θ (t) .<label>(21)</label></formula><p>This can be written as:</p><formula xml:id="formula_29">Θ (t+1) = Θ (t) − β t ∇L meta (ŵ (t) (Θ (t) )) Ξt .<label>(22)</label></formula><p>Since the mini-batch Ξ t is drawn uniformly from the entire data set, we can rewrite the update equationp as:</p><formula xml:id="formula_30">Θ (t+1) = Θ (t) − β t [∇L meta (ŵ (t) (Θ (t) )) + ξ (t) ],<label>(23)</label></formula><p>where ξ (t) = ∇L meta (ŵ (t) (Θ (t) )) Ξt − ∇L meta (ŵ (t) (Θ (t) )). Note that ξ (t) are i.i.d random variable with finite variance, since Ξ t are drawn i.i.d with a finite number of samples. Furthermore, E[ξ (t) ] = 0, since samples are drawn uniformly at random. Observe that</p><formula xml:id="formula_31">L meta (ŵ (t+1) (Θ (t+1) )) − L meta (ŵ (t) (Θ (t) )) = L meta (ŵ (t+1) (Θ (t+1) )) − L meta (ŵ (t) (Θ (t+1) )) + L meta (ŵ (t) (Θ (t+1) )) − L meta (ŵ (t) (Θ (t) )) .<label>(24)</label></formula><p>By Lipschitz smoothness of meta loss function, we have</p><formula xml:id="formula_32">L meta (ŵ (t+1) (Θ (t+1) )) − L meta (ŵ (t) (Θ (t+1) )) ≤ ∇L meta (ŵ (t) (Θ (t+1) )),ŵ (t+1) (Θ (t+1) ) −ŵ (t) (Θ (t+1) ) + L 2 ŵ (t+1) (Θ (t+1) ) −ŵ (t) (Θ (t+1) ) 2 2 Sinceŵ (t+1) (Θ (t+1) ) −ŵ (t) (Θ (t+1) ) = −α t 1 n n i=1 V(L train i (w (t+1) ); Θ (t+1) )∇w L train i (w) w (t+1) according to Eq.(3),(5), we have L meta (ŵ (t+1) (Θ (t+1) )) − L meta (ŵ (t) (Θ (t+1) )) ≤ α t ρ 2 + Lα 2 t 2 ρ 2 = α t ρ 2 (1 + α t L 2 ) (25) Since ∂L train j (w) ∂w w (t) ≤ ρ, ∂L meta i (ŵ) ∂ŵ T w (t) ≤ρ.</formula><p>By Lipschitz continuity of ∇L meta (ŵ (t) (Θ)) according to Lemma 1, we can obtain the following:</p><formula xml:id="formula_33">L meta (ŵ (t) (Θ (t+1) )) − L meta (ŵ (t) (Θ (t) )) ≤ ∇L meta (ŵ (t) (Θ (t) )), Θ (t+1) − Θ (t) + L 2 Θ (t+1) − Θ (t) 2 2 = ∇L meta (ŵ (t) (Θ (t) )), −β t [∇L meta (ŵ (t) (Θ (t) )) + ξ (t) ] + Lβ 2 t 2 ∇L meta (ŵ (t) (Θ (t) )) + ξ (t) 2 2 = −(β t − Lβ 2 t 2 ) ∇L meta (ŵ (t) (Θ (t) )) 2 2 + Lβ 2 t 2 ξ (t) 2 2 − (β t − Lβ 2 t ) ∇L meta (ŵ (t) (Θ (t) )), ξ (t) .</formula><p>Thus Eq.(35) satifies</p><formula xml:id="formula_34">L meta (ŵ (t+1) (Θ (t+1) )) − L meta (ŵ (t) (Θ (t) )) ≤ α t ρ 2 (1 + α t L 2 ) − (β t − Lβ 2 t 2 ) ∇L meta (ŵ (t) (Θ (t) )) 2 2 + Lβ 2 t 2 ξ (t) 2 2 − (β t − Lβ 2 t ) ∇L meta (ŵ (t) (Θ (t) )), ξ (t) .<label>(26)</label></formula><p>Rearranging the terms, we can obtain</p><formula xml:id="formula_35">(β t − Lβ 2 t 2 ) ∇L meta (ŵ (t) (Θ (t) )) 2 2 ≤ α t ρ 2 (1 + α t L 2 ) + L meta (ŵ (t) (Θ (t) )) − L meta (ŵ (t+1) (Θ (t+1) )) + Lβ 2 t 2 ξ (t) 2 2 − (β t − Lβ 2 t ) ∇L meta (ŵ (t) (Θ (t) )), ξ (t) .<label>(27)</label></formula><p>Summing up the above inequalities and rearranging the terms, we can obtain</p><formula xml:id="formula_36">T t=1 (β t − Lβ 2 t 2 ) ∇L meta (ŵ (t) (Θ (t) )) 2 2 ≤ L meta (ŵ (1) )(Θ (1) ) − L meta (ŵ (T +1) (Θ (T +1) )) + T t=1 α t ρ 2 (1 + α t L 2 ) − T t=1 (β t −Lβ 2 t ) ∇L meta (ŵ (t) (Θ (t) )),ξ (t) + L 2 T t=1 β 2 t ξ (t) 2 2 ≤ L meta (ŵ (1) (Θ (1) ))+ T t=1 α t ρ 2 (1 + α t L 2 ) − T t=1 (β t −Lβ 2 t ) ∇L meta (ŵ (t) (Θ (t) )),ξ (t) + L 2 T t=1 β 2 t ξ (t) 2 2 ,<label>(28)</label></formula><p>Taking expectations with respect to ξ (N ) on both sides of Eq. 28, we can then obtain:</p><formula xml:id="formula_37">T t=1 (β t − Lβ 2 t 2 )E ξ (N ) ∇L meta (ŵ (t) (Θ (t) )) 2 2 ≤ L meta (ŵ (1) (Θ (1) ))+ T t=1 α t ρ 2 (1 + α t L 2 ) + Lσ 2 2 T t=1 β since E ξ (N ) ∇L meta (Θ (t) ), ξ (t) = 0 and E[ ξ (t) 2 2 ] ≤ σ 2 ,</formula><p>where σ 2 is the variance of ξ (t) . Furthermore, we can deduce that</p><formula xml:id="formula_38">min t E[ ∇L meta (ŵ (t) (Θ (t) )) 2 2 ] ≤ T t=1 (β t − Lβ 2 t 2 )E ξ (N ) ∇L meta (ŵ (t) (Θ (t) )) 2 2 T t=1 (β t − Lβ 2 t 2 ) ≤ 1 T t=1 (2β t − Lβ 2 t ) 2L meta (ŵ (1) (Θ (1) ))+ T t=1 α t ρ 2 (2 + α t L) + Lσ 2 T t=1 β 2 t ≤ 1 T t=1 β t 2L meta (ŵ (1) (Θ (1) ))+ T t=1 α t ρ 2 (2 + α t L) + Lσ 2 T t=1 β 2 t ≤ 1 T β t 2L meta (ŵ (1) (Θ (1) ))+ α 1 ρ 2 T (2 + L) + Lσ 2 T t=1 β 2 t = 2L meta (ŵ (1) (Θ (1) )) T 1 β t + 2α 1 ρ 2 (2 + L) β t + Lσ 2 T T t=1 β t ≤ 2L meta (ŵ (1) (Θ (1) )) T 1 β t + 2α 1 ρ 2 (2 + L) β t + Lσ 2 β t = L meta (ŵ (1) (Θ (1) )) T max{L, σ √ T c } + min{1, k T } max{L, σ √ T c }ρ 2 (2 + L) + Lσ 2 min{ 1 L , c σ √ T } ≤ σL meta (ŵ (1) (Θ (1) ) c √ T + kσρ 2 (2 + L) c √ T + Lσc √ T = O( 1 √ T ).<label>(30)</label></formula><p>The third inequlity holds for</p><formula xml:id="formula_39">T t=1 (2β t − Lβ 2 t ) ≥ T t=1 β t .</formula><p>Therefore, we can conclude that our algorithm can always achieve</p><formula xml:id="formula_40">min 0≤t≤T E[ ∇L meta (Θ (t) ) 2 2 ] ≤ O( 1 √ T )</formula><p>in T steps, and this finishes our proof of Theorem 1. <ref type="bibr" target="#b68">[69]</ref>) Let (a n ) n≤1 , (b n ) n≤1 be two non-negative real sequences such that the series ∞ i=1 a n diverges, the series ∞ i=1 a n b n converges, and there exists K &gt; 0 such that |b n+1 − b n | ≤ Ka n . Then the sequences (b n ) n≤1 converges to 0. Theorem 2. Suppose the loss function is Lipschitz smooth with constant L, and V(·) is differential with a δ-bounded gradient and twice differential with its Hessian bounded by B, and the loss function have ρ-bounded gradients with respect to training/meta data. Let the learning rate α t satisfies α t = min{1, k T }, for some k &gt; 0, such that k T &lt; 1, and β t , 1 ≤ t ≤ N is a monotone descent sequence,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2. (Lemma A.5 in</head><formula xml:id="formula_41">β t = min{ 1 L , c σ √ T } for some c &gt; 0, such that σ √ T c ≥ L and ∞ t=1 β t ≤ ∞, ∞ t=1 β 2 t ≤ ∞.Then lim t→∞ E[ ∇L train (w (t) ; Θ (t+1) ) 2 2 ] = 0.<label>(31)</label></formula><p>Proof. It is easy to conclude that α t satisfy</p><formula xml:id="formula_42">∞ t=0 α t = ∞, ∞ t=0 α 2 t &lt; ∞.</formula><p>Recall the update of w in each iteration as follows:</p><formula xml:id="formula_43">w (t+1) = w (t) − α 1 n n i=1 V(L train i (w (t) ); Θ (t+1) )∇ w L train i (w) w (t) .<label>(32)</label></formula><p>It can be written as:</p><formula xml:id="formula_44">w (t+1) = w (t) − α t ∇L train (w (t) ; Θ (t+1) )| Ψt ,<label>(33)</label></formula><p>where ∇L train (w (t) ; Θ) = 1</p><formula xml:id="formula_45">n n i=1 V(L train i (w (t) ); Θ)∇ w L train i (w) w (t) .</formula><p>Since the mini-batch Ψ t is drawn uniformly at random, we can rewrite the update equation as:</p><formula xml:id="formula_46">w (t+1) = w (t) − α t [∇L train (w (t) ; Θ (t+1) ) + ψ (t) ],<label>(34)</label></formula><p>where ψ (t) = ∇L train (w (t) ; Θ (t+1) )| Ψt − ∇L train (w (t) ; Θ (t+1) ). Note that ψ (t) is i.i.d. random variable with finite variance, since Ψ t are drawn i.i.d. with a finite number of samples. Furthermore, E[ψ (t) ] = 0, since samples are drawn uniformly at random, and E[ ψ (t) 2 2 ] ≤ σ 2 . The objective function L train (w; Θ) defined in Eq. 14 can be easily checked to be Lipschitz-smooth with constant L, and have ρ-bounded gradients with respect to training data. Observe that</p><formula xml:id="formula_47">L train (w (t+1) ; Θ (t+2) ) − L train (w (t) ; Θ (t+1) ) = L train (w (t+1) ; Θ (t+2) ) − L train (w (t+1) ; Θ (t+1) ) + L train (w (t+1) ; Θ (t+1) ) − L train (w (t) ; Θ (t+1) ) .<label>(35)</label></formula><p>For the first term,</p><formula xml:id="formula_48">L train (w (t+1) ; Θ (t+2) ) − L train (w (t+1) ; Θ (t+1) ) = 1 n n i=1 V(L train i (w (t) ); Θ (t+2) ) − V(L train i (w (t) ); Θ (t+1) ) L train i (w (t) ) ≤ 1 n n i=1 ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) , Θ (t+1) − Θ (t) + δ 2 Θ (t+1) − Θ (t) 2 2 L train i (w (t) ) = 1 n n i=1 ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) , −βt[∇L meta (ŵ (t) (Θ (t) )) + ξ (t) ] + δβ 2 t 2 ∇L meta (ŵ (t) (Θ (t) )) + ξ (t) 2 2 L train i (w (t) ) = 1 n n i=1 ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) , −βt[∇L meta (ŵ (t) (Θ (t) )) + ξ (t) ] + δβ 2 t 2 ∇L meta (ŵ (t) (Θ (t) )) + ξ (t) 2 2 L train i (w (t) ) = 1 n n i=1 ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) , −βt[∇L meta (ŵ (t) (Θ (t) )) + ξ (t) ] + δβ 2 t 2 ( ∇L meta (ŵ (t) (Θ (t) )) 2 2 + ξ (t) 2 2 + 2 ∇L meta (ŵ (t) (Θ (t) )), ξ (t) ) L train i (w (t) )<label>(36)</label></formula><p>For the second term,</p><formula xml:id="formula_49">L train (w (t+1) ; Θ (t+1) ) − L train (w (t) ; Θ (t+1) ) ≤ ∇L train (w (t) ; Θ (t+1) ), w (t+1) − w (t) + L 2 w (t+1) − w (t) 2 2 = ∇L train (w (t) ; Θ (t+1) ), −αt[∇L train (w (t) ; Θ (t+1) ) + ψ (t) ] + Lα 2 t 2 ∇L train (w (t) ; Θ (t+1) ) + ψ (t) 2 2 = −(αt − Lα 2 t 2 ) ∇L train (w (t) ; Θ (t+1) ) 2 2 + Lα 2 t 2 ψ (t) 2 2 − (αt − Lα 2 t ) ∇L train (w (t) ; Θ (t+1) ), ψ (t) .<label>(37)</label></formula><p>Therefore, we have:</p><formula xml:id="formula_50">L train (w (t+1) ; Θ (t+2) ) − L train (w (t) ; Θ (t+1) ) ≤ 1 n n i=1 ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) , −βt[∇L meta (ŵ (t) (Θ (t) )) + ξ (t) ] + + δβ 2 t 2 ( ∇L meta (ŵ (t) (Θ (t) )) 2 2 + ξ (t) 2 2 + 2 ∇L meta (ŵ (t) (Θ (t) )), ξ (t) ) L train i (w (t) ) − (αt − Lα 2 t 2 ) ∇L train (w (t) ; Θ (t+1) ) 2 2 + Lα 2 t 2 ψ (t) 2 2 − (αt − Lα 2 t ) ∇L train (w (t) ; Θ (t+1) ), ψ (t) .<label>(38)</label></formula><p>Taking expectation of both sides of (37) and since E[ξ (t) ] = 0, E[ψ (t) ] = 0, we have</p><formula xml:id="formula_51">E[L train (w (t+1) ; Θ (t+2) )] − E[L train (w (t) ; Θ (t+1) )] ≤ E 1 n n i=1 L train i (w (t) ) ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) , −β t [∇L meta (ŵ (t) (Θ (t) ))] + + δβ 2 t 2 ( ∇L meta (ŵ (t) (Θ (t) )) 2 2 + ξ (t) 2 2 ) − α t E[ ∇L train (w (t) ; Θ (t+1) ) 2 2 ] + Lα 2 t 2 E[ ∇L train (w (t) ; Θ (t+1) ) 2 2 ] + E[ ψ (t) 2 2 ]</formula><p>Summing up the above inequalities over t = 1, ..., ∞ in both sides, we obtain</p><formula xml:id="formula_52">∞ t=1 α t E[ ∇L train (w (t) ;Θ (t+1) ) 2 2 ] + ∞ t=1 β t E 1 n n i=1 L train i (w (t) ) ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) · ∇L meta (ŵ (t) (Θ (t) )) ≤ ∞ t=1 Lα 2 t 2 E[ ∇L train (w (t) ;Θ (t+1) ) 2 2 ]+E[ ψ (t) 2 2 ] +E[L train (w (1) ; Θ (2) )]− lim T →∞ E[L train (w (t+1)</formula><p>;Θ</p><p>)]</p><formula xml:id="formula_54">+ ∞ t=1 δβ 2 t 2 1 n n i=1 L train i (w (t) ) (E ∇L meta (ŵ (t) (Θ (t) )) 2 2 + E ξ (t) 2 2 ≤ ∞ t=1 Lα 2 t 2 {ρ 2 +σ 2 }+E[L tr (w<label>(1)</label></formula><p>;Θ</p><formula xml:id="formula_55">(2) )] + ∞ t=1 δβ 2 t 2 M (ρ 2 + σ 2 ) ≤∞.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The last inequality holds since</head><formula xml:id="formula_56">∞ t=0 α 2 t &lt; ∞, ∞ t=0 β 2 t &lt; ∞, and 1 n n i=1 L train i (w (t) ) ≤ M for limited number of samples' loss is bounded. Thus we have ∞ t=1 α t E[ ∇L train (w (t) ;Θ (t+1) ) 2 2 ] + ∞ t=1 β t E 1 n n i=1 L train i (w (t) ) ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) · ∇L meta (ŵ (t) (Θ (t) )) ≤ ∞. (39) Since ∞ t=1 β t E 1 n n i=1 L train i (w (t) ) ∂V(L train i (w (t) ); Θ) ∂Θ Θ (t) · ∇L meta (ŵ (t) (Θ (t) )) ≤ M δρ ∞ t=1 β t ≤ ∞,<label>(40)</label></formula><p>which implies that ∞ t=1 α t E[ ∇L train (w (t) ; Θ (t+1) ) 2 2 ] &lt; ∞. By Lemma 2, to substantiate lim t→∞ E[∇L train (w (t) ; Θ (t+1) ) 2 2 ] = 0, since ∞ t=0 α t = ∞, it only needs to prove:</p><formula xml:id="formula_57">E[∇L train (w (t+1) ; Θ (t+2) ) 2 2 ] − E[∇L train (w (t) ; Θ (t+1) ) 2 2 ] ≤ Cα k ,<label>(41)</label></formula><p>for some constant C. Based on the inequality:</p><formula xml:id="formula_58">|( a + b )( a − b )| ≤ a + b a − b ,<label>(42)</label></formula><p>we then have:</p><formula xml:id="formula_59">E[ ∇L train (w (t+1) ; Θ (t+2) ) 2 2 ] − E[ ∇L train (w (t) ; Θ (t+1) ) 2 2 ] = E ( ∇L train (w (t+1) ; Θ (t+2) ) 2 + ∇L train (w (t) ; Θ (t+1) ) 2 )( ∇L train (w (t+1) ; Θ (t+2) ) 2 − ∇L train (w (t) ; Θ (t+1) ) 2 ) ≤ E ∇L train (w (t+1) ; Θ (t+1) ) 2 + ∇L train (w (t) ; Θ (t) ) 2 ) ( ∇L train (w (t+1) ; Θ (t+2) ) 2 − ∇L train (w (t) ; Θ (t+1) ) 2 ) ≤ E ∇L train (w (t+1) ; Θ (t+2) ) + ∇L train (w (t) ; Θ (t+1) ) 2 ∇L train (w (t+1) ; Θ (t+2) ) − ∇L train (w (t) ; Θ (t+1) ) 2 ≤ E ( ∇L train (w (t+1) ; Θ (t+2) ) 2 + ∇L train (w (t) ; Θ (t+1) ) 2 ) ∇L train (w (t+1) ; Θ (t+2) ) − ∇L train (w (t) ; Θ (t+1) ) 2 ≤ 2LρE (w (t+1) , Θ (t+2) ) − (w (t) , Θ (t+1) ) 2 ≤ 2Lρα t β t E ∇L train (w (t) ; Θ (t+1) ) + ψ (t) , ∇L meta (Θ (t+1) ) + ξ (t+1) 2 ≤ 2Lρα t β t E ∇L train (w (t) ; Θ (t+1) ) + ψ (t) 2 2 + ∇L meta (Θ (t+1) ) + ξ (t+1) 2 2 ≤ 2Lρα t β t E ∇L train (w (t) ; Θ (t+1) ) + ψ (t) 2 2 + E ∇L meta (Θ (t+1) ) + ξ (t+1) 2 2 ≤ 2Lρα t β t E ∇L train (w (t) ; Θ (t+1) ) 2 2 + E ψ (t) 2 2 + E ξ (t+1) 2 2 + E ∇L meta (Θ (t+1) ) 2 2 ≤ 2Lρα t β t 2σ 2 + 2ρ 2 ≤ 2 2(σ 2 + ρ 2 )Lρβ 1 α t .<label>(43)</label></formula><p>According to the above inequality, we can conclude that our algorithm can achieve</p><formula xml:id="formula_60">lim t→∞ E[ ∇L train (w (t) ; Θ (t+1) ) 2 2 ] = 0.<label>(44)</label></formula><p>The proof is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimentalp Details on Meta-Weight-Net</head><p>To match the original training step size, in our experiment, we can consider normalizing the weights of all examples in a training batch so that they sum up to one. In other words, we choose to have a hard constraint within the set V( ; Θ) 1 = 1, and the normalized weight</p><formula xml:id="formula_61">η (t) i = V (t) (L i ; Θ) j V (t) (L j ; Θ) + δ( i V (t) (L j ; Θ)) ,<label>(45)</label></formula><p>where the function of δ(·) is to prevent the degeneration case when all V (t) (L j ; Θ) in a mini-batch are zeros, i.e. δ(a) = τ . τ denotes a constant grater than 0, if a = 0; and equal to 0 otherwise. With batch normalization, we effectively cancel the learning rate of Meta-Weight-Net, and it works well with a fixed learning rate.   <ref type="figure">Figure 7</ref>: Training and test accuracy changing curves in uniform noise cases of CIFAR-10 and CIFAR-100 datasets. Solid and dotted curves denote the test and training accuracies, respectively. Our method and L2RW are less prone to overfit label noises, while our method can converge faster at around 40 epoch as shown in <ref type="figure">Fig. 7(a)</ref>. We thus terminate our method in 40 epoch in other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MLP architecture of Meta-Weight-Net</head><p>We actually have tried different MLP architecture settings in experiments. The right table depicts some representative results under 6 different structures, with different depths and widths. It can be seen that varying MLP settings have unsubstantial effects to the final result. We thus prefer to use the simple and shallow one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Complexity Analysis of The Proposed Algorithm</head><p>Our Meta-PGC algorithm can be roughly regarded as that requires an extra full forward and backward passes of the network on training data (step 5 in algorithm) and an extra full forward and backward passes of the network of meta data (step 6 in algorithm) in the presence of the normal classifier network' parameters update (step 7 in algorithm). Therefore compared to regular training, our method needs approximately 3 × training time in each iteration. It is suggest to let batch size of meta data less than or equal to batch size of training data, which avoids GPU memory increase and speeds up the learning process. <ref type="figure">Fig. 7</ref> plots the tendency curves of the mini-batch training accuracy calculated on noisy training set in experiments, as well as those calculated simultaneously on clean test data during learning iterations. From the figure, we can easily find that the BaseModel can easily overfit to the noisy labels contained in the training set, whose test accuracy quickly degrades after the first learning rate decays. While our method and L2RW are less prone to such overfitting issue, and they retain the similar test accuracy until termination. Especially, throughout all our experiments, we find that our method can converge significantly faster than the BaseModel and L2RW methods 1 , as clearly shown in <ref type="figure">Fig.7</ref>, and get the peak performance at around 40 epochs, as compared with 120 epochs required for the other two methods, as shown in <ref type="figure">Fig. 7(a)</ref>. We thus only report our results at 40 epochs in the figure for other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Robustness Towards Label Noise Overfitting Issue</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Confusion Matrices for Class Imbalance and Corrupted Labels</head><p>We demonstrate confusion matrices of Baseline and our algorithm on long-tailed CIFAR-10 dataset for class imbalance and corrupted labels experiments, as shown in <ref type="figure">Fig. 2-4</ref>.         </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Convergence Verification for The Training loss and Meta Loss</head><p>To validate the convergence results obtained in Theorem 1 and 2 in the paper, we plot the changing tendency curves of training and meta losses with the number of epochs in our experiments, as shown in <ref type="figure" target="#fig_1">Fig. 11 -13</ref>. The convergence tendency can be easily observed in the figures, substantiating the properness of the theoretical results in two theorems.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a)-(b) weight functions set in focal loss and self-paced learning (SPL). (c) Meta-Weighting-Net architecture. (d)-(f) Meta-Weighting-Net functions learned in class imbalance (imbalanced factor 100), noisy label (40% uniform noise), and real dataset, respectively, by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Confusion matrices for the Basemodel and ours on long-tailed CIFAR-10 with imbalance factors 200. Performance comparison for different classifier networks (WRN-28-10 and ResNet32) under CIFAR flip noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Sample weight distribution on training data under 40% uniform noise experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Weight variation curves under 40% uniform noise experiment on CIFAR10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>def norm_func ( v_lambda ) : norm_c = torch . sum ( v_lambda ) if norm_c ! = 0 : v_lambda_norm = v_lambda / norm_c else : v_lambda_norm = v_lambda return v_lambda_norm optimizer_a = torch . optim . SGD ( model . params () , args . lr , momentum = args . momentum , nesterov = args . nesterov , weight_decay = args . weight_decay ) optimizer_c = torch . optim . SGD ( vnet . params () , 1e -3 , momentum = args . momentum , nesterov = args . nesterov , weight_decay = args . weight_decay ) for iters in range ( num_iters ) : a d j u s t _ l e a r n i n g _ r a t e ( optimizer_a , iters + 1 ) model . train () data , target = next ( iter ( train_loader ) ) data , target = data . to ( device ) , target . to ( device ) meta_model . load_state_dict ( model . state_dict () ) y_f_hat = meta_model ( data ) cost = F . cross_entropy ( y_f_hat , target_var , reduce = False ) cost_v = torch . reshape ( cost , ( len ( cost ) , 1 ) ) v_lambda = vnet ( cost_v . data ) v_lambda_norm = norm_func ( v_lambda ) l_f_meta = torch . sum ( cost_v * v_lambda_norm ) meta_model . zero_grad () grads = torch . autograd . grad ( l_f_meta ,( meta_model . params () ) , create_graph = True ) meta_model . update_params ( lr_inner = meta_lr , source_params = grads ) data_meta , target_meta = next ( iter ( tr ain _me ta_ lo ade r ) ) data_meta , target_meta = data_meta . to ( device ) , target_meta . to ( device ) y_g_hat = meta_model ( data_meta ) l_g_meta = F . cross_entropy ( y_g_hat , target_meta ) optimizer_c . zero_grad () l_g_meta . backward () optimizer_c . step () y_f = model ( data ) cost_w = F . cross_entropy ( y_f , target , reduce = False ) cost_v = torch . reshape ( cost_w , ( len ( cost_w ) , 1 ) ) with torch . no_grad () : w_new = vnet ( cost_v ) w_v = norm_func ( w_new ) l_f = torch . sum ( cost_v * w_v ) optimizer_a . zero_grad () l_f . backward () optimizer_a . step () B Derivation of the Weighting Scheme in Meta-Weight-Net Recall the update equation of the parameters of Meta-Weight-Net as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>20</head><label>20</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Confusion matrices on long-tailed CIFAR-10 with imbalance factors ranging from 1 to 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Confusion matrices on CIFAR-10 dataset with varying noise rates under uniform noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Confusion matrices on CIFAR-10 dataset with varying noise rates under flip noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Training and meta loss tendency curves on long-tailed CIFAR with imbalance factors ranging from 1 to 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Training and meta loss tendency curves on CIFAR dataset with varying noise rates under uniform noise. Training and meta loss tendency curves on CIFAR dataset with varying noise rates under flip noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) of ResNet-32 on long-tailed CIFAR-10 and CIFAR-100, and the best and the second best results are highlighted in bold and italic bold, respectively.</figDesc><table><row><cell>Dataset Name</cell><cell></cell><cell></cell><cell cols="2">Long-Tailed CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Long-Tailed CIFAR-100</cell><cell></cell><cell></cell></row><row><cell>Imbalance</cell><cell>200</cell><cell>100</cell><cell>50</cell><cell>20</cell><cell>10</cell><cell>1</cell><cell>200</cell><cell>100</cell><cell>50</cell><cell>20</cell><cell>10</cell><cell>1</cell></row><row><cell>BaseModel</cell><cell>65.68</cell><cell>70.36</cell><cell>74.81</cell><cell>82.23</cell><cell>86.39</cell><cell>92.89</cell><cell>34.84</cell><cell>38.32</cell><cell>43.85</cell><cell>51.14</cell><cell>55.71</cell><cell>70.50</cell></row><row><cell>Focal Loss</cell><cell>65.29</cell><cell>70.38</cell><cell>76.71</cell><cell>82.76</cell><cell>86.66</cell><cell>93.03</cell><cell>35.62</cell><cell>38.41</cell><cell>44.32</cell><cell>51.95</cell><cell>55.78</cell><cell>70.52</cell></row><row><cell>Class-Balanced</cell><cell>68.89</cell><cell>74.57</cell><cell>79.27</cell><cell>84.36</cell><cell>87.49</cell><cell>92.89</cell><cell>36.23</cell><cell>39.60</cell><cell>45.32</cell><cell>52.59</cell><cell>57.99</cell><cell>70.50</cell></row><row><cell>Fine-tuning</cell><cell>66.08</cell><cell>71.33</cell><cell>77.42</cell><cell>83.37</cell><cell>86.42</cell><cell>93.23</cell><cell>38.22</cell><cell>41.83</cell><cell>46.40</cell><cell>52.11</cell><cell>57.44</cell><cell>70.72</cell></row><row><cell>L2RW</cell><cell>66.51</cell><cell>74.16</cell><cell>78.93</cell><cell>82.12</cell><cell>85.19</cell><cell>89.25</cell><cell>33.38</cell><cell>40.23</cell><cell>44.44</cell><cell>51.64</cell><cell>53.73</cell><cell>64.11</cell></row><row><cell>Ours</cell><cell>68.91</cell><cell>75.21</cell><cell>80.06</cell><cell>84.94</cell><cell>87.84</cell><cell>92.66</cell><cell>37.91</cell><cell>42.09</cell><cell>46.74</cell><cell>54.37</cell><cell>58.46</cell><cell>70.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy comparison on CIFAR-10 and CIFAR-100 of WRN-28-10 with varying noise rates under uniform noise. Mean accuracy (±std) over 5 repetitions are reported ('-' means the method fails).</figDesc><table><row><cell cols="2">Datasets / Noise Rate</cell><cell>BaseModel</cell><cell>Reed-Hard</cell><cell>S-Model</cell><cell>Self-paced</cell><cell>Focal Loss</cell><cell>Co-teaching</cell><cell>D2L</cell><cell>Fine-tining</cell><cell>MentorNet</cell><cell>L2RW</cell><cell>GLC</cell><cell>Ours</cell></row><row><cell></cell><cell>0%</cell><cell>95.60±0.22</cell><cell>94.38±0.14</cell><cell>83.79±0.11</cell><cell>90.81±0.34</cell><cell>95.70±0.15</cell><cell>88.67±0.25</cell><cell>94.64±0.33</cell><cell>95.65±0.15</cell><cell>94.35±0.42</cell><cell>92.38±0.10</cell><cell>94.30±0.19</cell><cell>94.52±0.25</cell></row><row><cell>CIFAR-10</cell><cell>40%</cell><cell>68.07±1.23</cell><cell>81.26±0.51</cell><cell>79.58±0.33</cell><cell>86.41±0.29</cell><cell>75.96±1.31</cell><cell>74.81±0.34</cell><cell>85.60±0.13</cell><cell>80.47±0.25</cell><cell>87.33±0.22</cell><cell>86.92±0.19</cell><cell>88.28±0.03</cell><cell>89.27±0.28</cell></row><row><cell></cell><cell>60%</cell><cell>53.12±3.03</cell><cell>73.53±1.54</cell><cell>-</cell><cell>53.10±1.78</cell><cell>51.87±1.19</cell><cell>73.06±0.25</cell><cell>68.02±0.41</cell><cell>78.75±2.40</cell><cell>82.80±1.35</cell><cell>82.24±0.36</cell><cell>83.49±0.24</cell><cell>84.07±0.33</cell></row><row><cell></cell><cell>0%</cell><cell>79.95±1.26</cell><cell>64.45±1.02</cell><cell>52.86±0.99</cell><cell>59.79±0.46</cell><cell>81.04±0.24</cell><cell>61.80±0.25</cell><cell>66.17±1.42</cell><cell>80.88±0.21</cell><cell>73.26±1.23</cell><cell>72.99±0.58</cell><cell>73.75±0.51</cell><cell>78.76±0.24</cell></row><row><cell>CIFAR-100</cell><cell>40%</cell><cell>51.11±0.42</cell><cell>51.27±1.18</cell><cell>42.12±0.99</cell><cell>46.31±2.45</cell><cell>51.19±0.46</cell><cell>46.20±0.15</cell><cell>52.10 ±0.97</cell><cell>52.49±0.74</cell><cell>61.39±3.99</cell><cell>60.79±0.91</cell><cell>61.31±0.22</cell><cell>67.73±0.26</cell></row><row><cell></cell><cell>60%</cell><cell>30.92±0.33</cell><cell>26.95±0.98</cell><cell>-</cell><cell>19.08±0.57</cell><cell>27.70±3.77</cell><cell>35.67±1.25</cell><cell>41.11±0.30</cell><cell>38.16±0.38</cell><cell>36.87±1.47</cell><cell>48.15±0.34</cell><cell>50.81±1.00</cell><cell>58.75±0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>9% 0.8% 82.9% 2.9% 2.1% 1.7% 1.7% 6.1% 1.5% 9.0% 70.4% 4.2% 6.7% 1.7% 0.4% 6.8% 0.4% 12.2% 6.2% 70.2% 0.9% 1.1% 2.2% 1.9% 0.5% 13.0% 25.6% 4.0% 53.0% 0.6% 1.4% 3.6% 1.4% 16.0% 11.9% 3.9% 0.6% 62.3% 0.1% 0.2% 11.1% 0.5% 11.9% 10.4% 12.7% 6.5% 0.2% 46.4% 0.1% 0.2% 59.5% 13.7% 2.1% 1.9% 0.4% 0.3% 0.1% 0.1% 21.9% 29.6% 59.4% 1.0% 1.9% 0.3% 0.5% 0.1% 0.4% 0.3% 6.5% 5% 0.2% 83.8% 2.9% 3.7% 2.3% 1.1% 0.4% 2.9% 0.7% 6.0% 74.4% 4.3% 9.9% 0.9% 0.7% 0.1% 2.5% 5.2% 5.7% 82.4% 1.6% 1.2% 1.4% 1.8% 0.4% 6.6% 17.3% 4.1% 68.7% 0.5% 0.6% 3.2% 1.1% 10.5% 11.6% 5.4% 2.0% 65.8% 0.3% 0.1% 4.7% 0.4% 3.7% 10.3% 12.1% 11.5% 0.1% 56.9% 0.2% 51.0% 8.7% 1.8% 2.4% 0.8% 0.1% 0.5% 34.1% 0.5% 22.0% 42.0% 0.6% 2.4% 0.4% 0.2% 0.3% 0.4% 0.2% 31.4%</figDesc><table><row><cell>0 1 2</cell><cell cols="4">96.6% 1.3% 1.6% 0.2% 0.1% 0.1% 0.8% 98.9% 0.2% 7.BaseModel</cell><cell></cell><cell>0.1%</cell><cell>0.1%</cell><cell>800</cell><cell>0 1 2</cell><cell cols="4">97.1% 0.9% 1.2% 0.4% 0.1% 0.1% 0.7% 98.2% 0.2% 0.3% 5.</cell><cell>0.2%</cell><cell>0.1% 0.1%</cell><cell>0.4%</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 5 True label</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>400 600</cell><cell>4 5 True label</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell cols="2">3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row></table><note>1. The learning rate of classifier network is divided by 10 after 36 epoch and 38 epoch (for a total of 40 epoches) in uniform noise, and after 40 epoch and 50 epoch (for a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy comparison on CIFAR-10 and CIFAR-100 of ResNet-32 with varying noise rates under flip noise.</figDesc><table><row><cell cols="2">Datasets / Noise Rate</cell><cell>BaseModel</cell><cell>Reed-Hard</cell><cell>S-Model</cell><cell>Self-paced</cell><cell>Focal Loss</cell><cell>Co-teaching</cell><cell>D2L</cell><cell>Fine-tining</cell><cell>MentorNet</cell><cell>L2RW</cell><cell>GLC</cell><cell>Ours</cell></row><row><cell></cell><cell>0%</cell><cell>92.89±0.32</cell><cell>92.31±0.25</cell><cell>83.61±0.13</cell><cell>88.52±0.21</cell><cell>93.03±0.16</cell><cell>89.87±0.10</cell><cell>92.02±0.14</cell><cell>93.23±0.23</cell><cell>92.13±0.30</cell><cell>89.25±0.37</cell><cell>91.02±0.20</cell><cell>92.04±0.15</cell></row><row><cell>CIFAR-10</cell><cell>20%</cell><cell>76.83±2.30</cell><cell>88.28±0.36</cell><cell>79.25±0.30</cell><cell>87.03±0.34</cell><cell>86.45±0.19</cell><cell>82.83±0.85</cell><cell>87.66±0.40</cell><cell>82.47±3.64</cell><cell>86.36±0.31</cell><cell>87.86±0.36</cell><cell>89.68±0.33</cell><cell>90.33±0.61</cell></row><row><cell></cell><cell>40%</cell><cell>70.77±2.31</cell><cell>81.06±0.76</cell><cell>75.73±0.32</cell><cell>81.63±0.52</cell><cell>80.45±0.97</cell><cell>75.41±0.21</cell><cell>83.89±0.46</cell><cell>74.07±1.56</cell><cell>81.76±0.28</cell><cell>85.66±0.51</cell><cell>88.92±0.24</cell><cell>87.54±0.23</cell></row><row><cell></cell><cell>0%</cell><cell>70.50±0.12</cell><cell>69.02±0.32</cell><cell>51.46±0.20</cell><cell>67.55±0.27</cell><cell>70.02±0.53</cell><cell>63.31±0.05</cell><cell>68.11±0.26</cell><cell>70.72±0.22</cell><cell>70.24±0.21</cell><cell>64.11±1.09</cell><cell>65.42±0.23</cell><cell>70.11±0.33</cell></row><row><cell>CIFAR-100</cell><cell>20%</cell><cell>50.86±0.27</cell><cell>60.27±0.76</cell><cell>45.45±0.25</cell><cell>63.63±0.30</cell><cell>61.87±0.30</cell><cell>54.13±0.55</cell><cell>63.48±0.53</cell><cell>56.98±0.50</cell><cell>61.97±0.47</cell><cell>57.47±1.16</cell><cell>63.07±0.53</cell><cell>64.22±0.28</cell></row><row><cell></cell><cell>40%</cell><cell>43.01±1.16</cell><cell>50.40±1.01</cell><cell>43.81±0.15</cell><cell>53.51±0.53</cell><cell>54.13±0.40</cell><cell>44.85±0.81</cell><cell>51.83±0.33</cell><cell>46.37±0.25</cell><cell>52.66±0.56</cell><cell>50.98±1.55</cell><cell>62.22±0.62</cell><cell>58.64±0.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy (%) of all competing methods on the Clothing1M test set.</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>Accuracy</cell><cell>#</cell><cell>Method</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell>Cross Entropy</cell><cell>68.94</cell><cell>5</cell><cell>Joint Optimization [66]</cell><cell>72.23</cell></row><row><cell>2</cell><cell>Bootstrapping [58]</cell><cell>69.12</cell><cell>6</cell><cell>LCCN [67]</cell><cell>73.07</cell></row><row><cell>3</cell><cell>Forward [65]</cell><cell>69.84</cell><cell>7</cell><cell>MLNT [68]</cell><cell>73.47</cell></row><row><cell>4</cell><cell>S-adaptation [12]</cell><cell>70.36</cell><cell>8</cell><cell>Ours</cell><cell>73.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy on CIFAR-10 and CIFAR-100 of different MW-Nets.</figDesc><table><row><cell>architcture</cell><cell cols="6">Imbalance (factor 100) CIFAR10 CIFAR100 CIFAR10 CIFAR100 CIFAR10 CIFAR100 Uniform noise (40%) Flip noise (40%)</cell></row><row><cell>1-50-1</cell><cell>73.50</cell><cell>41.87</cell><cell>89.01</cell><cell>67.63</cell><cell>87.38</cell><cell>57.83</cell></row><row><cell>1-100-1</cell><cell>75.21</cell><cell>42.09</cell><cell>89.27</cell><cell>67.73</cell><cell>87.54</cell><cell>58.64</cell></row><row><cell>1-200-1</cell><cell>74.70</cell><cell>41.72</cell><cell>89.58</cell><cell>67.84</cell><cell>87.74</cell><cell>58.41</cell></row><row><cell>1-100-100-1</cell><cell>75.01</cell><cell>41.97</cell><cell>89.09</cell><cell>66.48</cell><cell>87.28</cell><cell>57.39</cell></row><row><cell>1-10-10-1</cell><cell>74.71</cell><cell>41.94</cell><cell>89.10</cell><cell>66.53</cell><cell>87.58</cell><cell>57.11</cell></row><row><cell>1-10-10-10-1</cell><cell>74.96</cell><cell>42.31</cell><cell>88.82</cell><cell>66.67</cell><cell>87.36</cell><cell>57.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>4% 12.2% 6.2% 70.2% 0.9% 1.1% 2.2% 1.9% 0.5% 13.0% 25.6% 4.0% 53.0% 0.6% 1.4% 3.6% 1.4% 16.0% 11.9% 3.9% 0.6% 62.3% 0.1% 0.2% 11.1% 0.5% 11.9% 10.4% 12.7% 6.5% 0.2% 46.4% 0.1% 0.2%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BaseModel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BaseModel</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell cols="5">90.0% 0.6% 2.2% 1.1% 0.8%</cell><cell cols="5">0.2% 0.5% 3.0% 1.6%</cell><cell></cell><cell cols="2">0</cell><cell cols="11">94.9% 0.5% 0.9% 0.6% 0.3% 0.1% 0.2% 0.2% 1.5% 0.7%</cell><cell></cell><cell cols="2">0</cell><cell cols="4">95.4% 0.5% 1.4% 0.5% 0.5%</cell><cell>0.1% 0.2% 1.3% 0.1%</cell><cell>0</cell><cell>95.8% 0.6% 1.4% 0.7% 0.1% 0.1% 0.1% 0.1% 0.8% 0.3%</cell></row><row><cell></cell><cell></cell><cell cols="2">0.5% 94.5%</cell><cell></cell><cell></cell><cell cols="2">0.1% 0.1% 0.1%</cell><cell></cell><cell cols="3">0.6% 4.1%</cell><cell>800</cell><cell cols="2">1</cell><cell cols="2">0.4% 97.2%</cell><cell></cell><cell></cell><cell cols="2">0.1% 0.1%</cell><cell>0.2%</cell><cell></cell><cell cols="3">0.4% 1.6%</cell><cell>800</cell><cell cols="2">1</cell><cell cols="2">0.7% 97.7%</cell><cell></cell><cell>0.1%</cell><cell>0.1%</cell><cell>0.6% 0.8%</cell><cell>800</cell><cell>1</cell><cell>0.7% 98.0% 0.1% 0.1%</cell><cell>1.1%</cell><cell>800</cell></row><row><cell></cell><cell></cell><cell cols="10">3.1% 0.1% 82.3% 2.5% 3.6% 2.8% 3.0% 1.5% 0.5% 0.6%</cell><cell></cell><cell cols="2">2</cell><cell>2.6%</cell><cell></cell><cell cols="9">87.9% 2.5% 1.7% 2.0% 2.5% 0.3% 0.3% 0.1%</cell><cell></cell><cell cols="2">2</cell><cell>4.8%</cell><cell></cell><cell cols="2">83.8% 2.4% 2.8% 2.5% 3.3% 0.3% 0.1%</cell><cell>2</cell><cell>3.6%</cell><cell>88.5% 1.6% 2.1% 1.7% 1.9% 0.4% 0.1%</cell></row><row><cell>True label</cell><cell></cell><cell cols="10">1.4% 0.2% 1.9% 78.0% 2.4% 9.7% 2.9% 2.1% 0.8% 0.6% 0.6% 0.2% 0.8% 1.9% 92.8% 0.9% 1.1% 1.3% 0.3% 0.1% 0.5% 0.1% 1.2% 7.9% 1.8% 85.9% 0.8% 1.7% 0.1%</cell><cell>400 600</cell><cell cols="2">3 4 5 True label</cell><cell cols="11">0.9% 0.2% 1.5% 84.6% 1.4% 7.2% 1.8% 1.3% 0.2% 0.8% 0.2% 1.1% 1.5% 94.7% 0.8% 0.6% 1.0% 0.6% 0.8% 6.8% 1.6% 88.3% 0.5% 1.3% 0.1%</cell><cell>400 600</cell><cell cols="2">3 4 5 True label</cell><cell cols="4">2.6% 0.2% 3.6% 77.5% 2.8% 7.4% 4.4% 1.2% 0.2% 0.1% 1.3% 4.0% 2.0% 89.3% 0.8% 1.7% 0.6% 0.3% 1.1% 0.1% 3.3% 14.8% 3.5% 74.8% 0.8% 1.6%</cell><cell>400 600</cell><cell>3 4 5 True label</cell><cell>1.1% 0.1% 2.9% 3.3% 88.2% 1.5% 1.6% 1.2% 1.0% 2.7% 12.2% 2.0% 79.8% 0.6% 1.5% 1.0% 0.3% 4.7% 81.3% 1.9% 6.7% 2.4% 1.0% 0.2% 0.4% 0.1%</cell><cell>400 600</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell></cell><cell cols="8">1.3% 2.0% 0.7% 0.6% 94.5% 0.1% 0.1% 0.2%</cell><cell></cell><cell cols="2">6</cell><cell>0.2%</cell><cell></cell><cell cols="7">1.0% 1.9% 0.7% 0.4% 95.6% 0.1%</cell><cell cols="2">0.1%</cell><cell></cell><cell cols="2">6</cell><cell cols="4">0.8% 0.3% 2.4% 3.2% 0.9% 0.7% 91.4% 0.1% 0.1% 0.1%</cell><cell>6</cell><cell>0.8% 0.1% 4.1% 4.1% 0.8% 0.9% 88.9% 0.1% 0.1%</cell></row><row><cell></cell><cell></cell><cell>0.7%</cell><cell></cell><cell cols="4">0.3% 0.8% 2.1% 2.0%</cell><cell cols="3">94.0% 0.1%</cell><cell></cell><cell>200</cell><cell cols="2">7</cell><cell>0.3%</cell><cell></cell><cell cols="8">0.8% 0.7% 1.3% 1.8% 0.1% 94.7% 0.2%</cell><cell></cell><cell>200</cell><cell cols="2">7</cell><cell>2.4%</cell><cell></cell><cell cols="2">2.6% 4.0% 5.3% 3.2% 0.6% 81.8%</cell><cell>0.1%</cell><cell>200</cell><cell>7</cell><cell>1.4%</cell><cell>2.4% 3.3% 3.5% 3.5% 0.1% 85.6% 0.1%</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell cols="4">2.8% 0.6% 0.3%</cell><cell cols="2">0.2% 0.1%</cell><cell></cell><cell cols="3">94.9% 1.1%</cell><cell></cell><cell cols="2">8</cell><cell cols="5">2.9% 0.7% 0.4% 0.3%</cell><cell></cell><cell>0.1%</cell><cell></cell><cell cols="3">94.7% 0.8%</cell><cell></cell><cell cols="2">8</cell><cell cols="4">8.3% 1.7% 0.9% 0.9% 0.2% 0.3% 0.4%</cell><cell>86.7% 0.6%</cell><cell>8</cell><cell>6.9% 1.8% 0.9% 0.6%</cell><cell>0.2% 0.3% 0.2% 88.6% 0.5%</cell></row><row><cell></cell><cell></cell><cell cols="5">1.2% 2.4% 0.1% 0.3%</cell><cell></cell><cell></cell><cell cols="3">0.9% 95.1%</cell><cell></cell><cell cols="2">9</cell><cell cols="2">0.7% 3.1%</cell><cell></cell><cell></cell><cell>0.5%</cell><cell>0.1%</cell><cell></cell><cell></cell><cell cols="3">0.7% 94.8%</cell><cell></cell><cell cols="2">9</cell><cell cols="4">5.7% 11.6% 0.2% 1.0% 0.1% 0.1% 0.2% 0.2% 1.4% 79.5%</cell><cell>9</cell><cell>4.0% 9.9% 0.1% 0.8% 0.2% 0.1% 0.1% 0.2% 0.7% 83.8%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell></cell><cell>2</cell><cell cols="4">3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell cols="2">2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell cols="10">(a) 1 for Baseline BaseModel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">(b) 1 for Ours Ours</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) 10 for Baseline BaseModel</cell><cell>(d) 10 for Ours Ours</cell></row><row><cell cols="2">0</cell><cell cols="5">95.6% 1.2% 1.3% 0.8% 0.1%</cell><cell cols="5">0.1% 0.1% 0.7% 0.1%</cell><cell></cell><cell cols="2">0</cell><cell cols="6">95.8% 0.5% 1.4% 0.7% 0.3%</cell><cell></cell><cell cols="4">0.1% 1.0% 0.2%</cell><cell></cell><cell cols="2">0</cell><cell cols="4">96.6% 1.1% 1.4% 0.6%</cell><cell>0.1%</cell><cell>0.2%</cell><cell>0</cell><cell>97.6% 0.8% 0.7% 0.3% 0.1%</cell><cell>0.1%</cell><cell>0.4%</cell></row><row><cell cols="2">1</cell><cell cols="2">0.6% 97.8%</cell><cell></cell><cell></cell><cell>0.5%</cell><cell>0.2%</cell><cell></cell><cell cols="3">0.4% 0.5%</cell><cell>800</cell><cell cols="2">1</cell><cell cols="2">0.5% 97.7%</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.1% 0.1%</cell><cell></cell><cell cols="3">0.6% 1.0%</cell><cell>800</cell><cell cols="2">1</cell><cell cols="2">0.6% 98.6%</cell><cell></cell><cell>0.2%</cell><cell>0.1%</cell><cell>0.5%</cell><cell>800</cell><cell>1</cell><cell>0.5% 98.8%</cell><cell>0.1%</cell><cell>0.1%</cell><cell>0.2% 0.3%</cell><cell>800</cell></row><row><cell cols="2">2</cell><cell cols="8">5.1% 0.2% 84.8% 3.8% 2.9% 1.2% 1.2% 0.8%</cell><cell></cell><cell></cell><cell></cell><cell cols="2">2</cell><cell cols="10">4.1% 0.2% 86.2% 2.1% 3.3% 1.4% 2.4% 0.1% 0.1%</cell><cell></cell><cell></cell><cell cols="2">2</cell><cell cols="4">5.9% 0.2% 86.0% 2.5% 1.9% 2.0% 1.0% 0.5%</cell><cell>2</cell><cell>6.0% 0.2% 85.5% 2.8% 2.5% 1.3% 1.4% 0.3%</cell></row><row><cell cols="2">3 4 5 True label</cell><cell cols="10">2.1% 0.8% 3.6% 83.2% 2.5% 4.7% 1.6% 1.1% 0.2% 0.2% 1.9% 0.1% 4.2% 4.6% 85.1% 0.7% 1.0% 2.3% 0.1% 1.1% 0.2% 4.4% 21.0% 2.5% 68.6% 0.2% 2.0%</cell><cell>400 600</cell><cell cols="2">3 4 5 True label</cell><cell cols="11">2.5% 0.9% 4.0% 78.9% 2.8% 7.3% 2.3% 0.5% 0.5% 0.2% 1.5% 0.1% 2.1% 2.8% 89.8% 1.5% 1.1% 1.0% 1.3% 0.2% 3.7% 14.5% 3.2% 75.6% 0.5% 0.8% 0.1%</cell><cell>400 600</cell><cell cols="2">3 4 5 True label</cell><cell cols="4">2.6% 1.1% 6.7% 77.4% 2.0% 7.2% 2.2% 0.5% 0.1% 0.2% 3.4% 0.2% 7.0% 6.5% 79.5% 1.3% 1.1% 1.0% 1.8% 0.1% 5.4% 20.2% 2.4% 68.4% 0.7% 1.0%</cell><cell>400 600</cell><cell>3 4 5 True label</cell><cell>2.3% 0.2% 5.1% 4.3% 83.0% 2.0% 1.5% 1.4% 0.1% 1.6% 0.1% 3.7% 20.3% 2.6% 68.8% 0.8% 1.9% 0.1% 3.1% 0.7% 5.7% 78.4% 2.5% 6.3% 2.0% 0.7% 0.2% 0.4%</cell><cell>400 600</cell></row><row><cell cols="2">6</cell><cell cols="10">1.4% 0.4% 4.9% 8.5% 1.6% 1.2% 81.4% 0.3% 0.1% 0.2%</cell><cell></cell><cell cols="2">6</cell><cell cols="10">1.2% 0.1% 3.3% 5.1% 1.6% 0.9% 87.5% 0.1% 0.2%</cell><cell></cell><cell></cell><cell cols="2">6</cell><cell cols="4">3.2% 0.7% 9.5% 8.3% 1.9% 1.1% 75.1% 0.1% 0.1%</cell><cell>6</cell><cell>2.3% 0.8% 5.9% 6.3% 2.0% 0.6% 81.5% 0.4% 0.1% 0.1%</cell></row><row><cell cols="2">7</cell><cell cols="6">3.3% 0.1% 3.0% 6.1% 4.7% 2.5%</cell><cell cols="2">80.3%</cell><cell></cell><cell></cell><cell></cell><cell cols="2">7</cell><cell cols="10">2.6% 0.1% 2.9% 5.6% 5.5% 5.6% 0.4% 77.3% 0.1%</cell><cell></cell><cell></cell><cell cols="2">7</cell><cell cols="4">6.3% 0.2% 4.9% 9.5% 6.0% 5.9% 0.2% 66.8%</cell><cell>0.2%</cell><cell>7</cell><cell>5.6% 0.3% 3.6% 7.8% 5.8% 6.3% 0.4% 70.2%</cell><cell>0.1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell>200</cell></row><row><cell cols="2">8</cell><cell cols="5">17.0% 4.4% 0.6% 1.3%</cell><cell>0.4% 0.1%</cell><cell></cell><cell cols="3">75.3% 0.9%</cell><cell></cell><cell cols="2">8</cell><cell cols="11">11.6% 2.5% 0.7% 0.9% 0.2% 0.1% 0.2% 0.1% 83.2% 0.4%</cell><cell></cell><cell cols="2">8</cell><cell cols="4">26.0% 6.9% 1.2% 1.6% 0.2% 0.3% 0.2%</cell><cell>63.3% 0.3%</cell><cell>8</cell><cell>20.5% 4.6% 0.9% 1.3% 0.2% 0.1% 0.4%</cell><cell>71.7% 0.2%</cell></row><row><cell cols="2">9</cell><cell cols="5">7.4% 18.4% 0.4% 1.6% 0.1%</cell><cell cols="5">0.2% 0.2% 0.8% 70.9%</cell><cell></cell><cell cols="2">9</cell><cell cols="5">5.1% 12.2% 0.3% 0.5%</cell><cell></cell><cell>0.2%</cell><cell></cell><cell cols="3">1.0% 80.7%</cell><cell></cell><cell cols="2">9</cell><cell cols="4">11.0% 22.2% 0.5% 1.5% 0.2% 0.2% 0.3% 0.2% 0.6% 63.3%</cell><cell>9</cell><cell>9.6% 24.1% 0.4% 0.5% 0.1%</cell><cell>0.7%</cell><cell>1.8% 62.7%</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell></cell><cell>2</cell><cell cols="3">3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell></cell><cell>2</cell><cell cols="4">3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell cols="2">2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell cols="10">(e) 20 for Baseline BaseModel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">(f) 20 for Ours Ours</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(g) 50 for Baseline BaseModel</cell><cell>(h) 50 for Ours Ours</cell></row><row><cell>0</cell><cell cols="6">97.4% 0.7% 0.8% 0.7% 0.1%</cell><cell></cell><cell></cell><cell cols="3">0.2% 0.1%</cell><cell></cell><cell>0</cell><cell cols="6">96.9% 0.6% 1.7% 0.6% 0.1%</cell><cell></cell><cell cols="3">0.1%</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="5">96.6% 1.3% 1.6% 0.2% 0.1% 0.1%</cell><cell>0.1%</cell><cell>0</cell><cell>97.1% 0.9% 1.2% 0.4% 0.1% 0.1%</cell><cell>0.1% 0.1%</cell></row><row><cell>1</cell><cell cols="6">1.2% 98.1% 0.1% 0.1%</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.5%</cell><cell>800</cell><cell>1</cell><cell cols="3">0.7% 98.4%</cell><cell></cell><cell></cell><cell>0.2%</cell><cell cols="2">0.2% 0.1%</cell><cell></cell><cell cols="3">0.2% 0.2%</cell><cell>800</cell><cell>1</cell><cell cols="4">0.8% 98.9% 0.2%</cell><cell>0.1%</cell><cell>800</cell><cell>1</cell><cell>0.7% 98.2% 0.2% 0.3%</cell><cell>0.2%</cell><cell>0.4%</cell><cell>800</cell></row><row><cell>2</cell><cell cols="9">8.5% 0.3% 80.4% 3.2% 4.4% 1.8% 1.2% 0.2%</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell cols="2">4.8%</cell><cell cols="6">86.5% 4.1% 2.0% 1.4% 1.1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell cols="5">7.9% 0.8% 82.9% 2.9% 2.1% 1.7% 1.7%</cell><cell>2</cell><cell>5.5% 0.2% 83.8% 2.9% 3.7% 2.3% 1.1% 0.4%</cell></row><row><cell>3 4 5 6 7 8 True label</cell><cell cols="11">4.0% 1.0% 7.5% 76.8% 4.4% 3.7% 1.7% 0.7% 0.1% 0.1% 3.8% 0.4% 6.5% 6.6% 79.0% 0.6% 1.0% 2.1% 3.0% 0.7% 10.1% 24.2% 3.9% 55.4% 0.2% 2.5% 3.3% 1.0% 14.3% 9.8% 3.6% 0.5% 67.3% 0.2% 7.6% 0.2% 7.3% 10.6% 10.2% 3.0% 0.2% 60.9% 36.3% 6.4% 1.2% 1.6% 0.2% 0.1% 53.3% 0.9%</cell><cell>200 400 600</cell><cell>3 4 5 6 7 8 True label</cell><cell cols="12">3.1% 0.2% 4.0% 81.7% 2.5% 6.7% 1.1% 0.4% 0.2% 2.7% 0.3% 7.8% 7.1% 77.5% 1.3% 1.3% 2.0% 2.4% 0.1% 5.6% 24.8% 1.9% 62.7% 0.6% 1.8% 2.4% 0.2% 8.8% 11.5% 2.4% 1.4% 73.0% 0.1% 0.1% 6.7% 0.2% 4.0% 12.7% 6.4% 9.4% 0.3% 60.2% 32.5% 5.7% 1.5% 2.2% 0.4% 0.1% 0.3% 56.9% 0.4% 0.1%</cell><cell>200 400 600</cell><cell>3 4 5 6 7 8 True label</cell><cell cols="5">6.1% 1.5% 9.0% 70.4% 4.2% 6.7% 1.7% 0.4% 6.8% 0.59.5% 13.7% 2.1% 1.9% 0.4% 0.3% 0.1% 0.1% 21.9%</cell><cell>200 400 600</cell><cell>6 7 8 3 4 5 True label</cell><cell>3.2% 1.1% 10.5% 11.6% 5.4% 2.0% 65.8% 0.3% 0.1% 4.7% 0.4% 3.7% 10.3% 12.1% 11.5% 0.1% 56.9% 51.0% 8.7% 1.8% 2.4% 0.8% 0.1% 0.5% 34.1% 0.5% 0.2% 2.5% 5.2% 5.7% 82.4% 1.6% 1.2% 1.4% 1.8% 0.4% 6.6% 17.3% 4.1% 68.7% 0.5% 0.6% 2.9% 0.7% 6.0% 74.4% 4.3% 9.9% 0.9% 0.7% 0.1%</cell><cell>200 400 600</cell></row><row><cell>9</cell><cell cols="11">16.1% 30.9% 0.7% 2.2% 0.2% 0.2% 0.1% 0.2% 0.6% 48.8%</cell><cell></cell><cell>9</cell><cell cols="6">19.6% 25.5% 0.6% 2.1% 0.1%</cell><cell cols="6">0.2% 0.2% 0.4% 51.3%</cell><cell></cell><cell>9</cell><cell cols="5">29.6% 59.4% 1.0% 1.9% 0.3% 0.5% 0.1% 0.4% 0.3% 6.5%</cell><cell>9</cell><cell>22.0% 42.0% 0.6% 2.4% 0.4% 0.2% 0.3% 0.4% 0.2% 31.4%</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell cols="3">3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell cols="4">3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell></row><row><cell></cell><cell cols="11">(i) 100 for Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">(j) 100 for Ours</cell><cell></cell><cell></cell><cell cols="5">(k) 200 for Baseline</cell><cell>(l) 200 for Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>6% 0.2% 84.6% 1.7% 3.4% 1.9% 2.0% 0.8% 0.4% 0.4% 2.5% 1.8% 12.2% 66.1% 1.8% 6.5% 3.3% 1.9% 0.6% 3.3% 2.7% 0.2% 7.6% 5.0% 74.8% 1.9% 2.4% 4.8% 0.6% 1.1% 0.9% 11.1% 12.0% 1.2% 63.4% 2.4% 6.1% 0.2% 1.6% 0.8% 0.6% 8.2% 3.6% 1.4% 0.3% 84.3% 0.4% 0.3% 0.1% 2.0% 0.6% 5.2% 2.1% 1.8% 1.4% 0.2% 85.5% 2% 0.1% 86.1% 2.8% 2.4% 2.3% 2.4% 0.7% 0.8% 0.2% 0.8% 0.1% 2.4% 78.5% 3.8% 9.3% 2.8% 1.2% 1.1% 0.2% 2.4% 2.1% 91.2% 1.6% 0.9% 1.5% 0.1% 0.4% 0.1% 1.1% 7.7% 2.0% 85.4% 0.7% 2.5% 0.6% 0.2% 1.7% 1.5% 0.4% 0.1% 95.2% 0.1% 0.2% 1% 0.7% 2.0% 1.4% 0.4% 0.6% 0.3% 2.7% 5.4% 4.4% 0.7% 86.8% 0.1% 0.1% 0.5% 0.1% 0.5% 1.1% 10.1% 7.4% 0.6% 66.2% 2.6% 6.1% 6.1% 5.2% 4.3% 0.5% 1.0% 2.8% 0.4% 5.4% 54.2% 2.9% 19.5% 5.7% 6.6% 0.6% 1.9% 4.1% 0.1% 3.6% 4.3% 63.6% 5.6% 1.8% 15.4% 0.2% 1.3% 0.3% 3.7% 7.3% 1.2% 77.9% 1.5% 6.9% 0.3% 0.9% 0.9% 0.2% 3.9% 1.7% 4.7% 4.8% 81.7% 1.3% 0.7% 0.1% 1.0% 0.1% 1.0% 2.5% 0.5% 8.3% 0.5% 85.0% 1.1% 3.9% 1.4% 0.9% 0.9% 0.2% 0.9% 0.1% 0.7% 81.1% 9.9% 6% 0.6% 86.5% 1.2% 2.8% 1.5% 2.0% 0.6% 2.1% 0.1% 1.0% 0.3% 3.3% 72.2% 7.9% 7.5% 2.7% 2.0% 2.4% 0.7% 0.3% 6.1% 0.6% 87.4% 2.9% 0.9% 1.5% 0.2% 0.3% 0.1% 1.3% 6.8% 5.1% 80.0% 0.7% 5.2% 0.3% 0.1% 0.4% 0.2% 3.6% 1.3% 0.4% 0.3% 92.4% 0.2% 0.2% 0.8%</figDesc><table><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell>True label</cell><cell>85.7% 1.6% 5.1% 0.9% 0.3% 0.9% 91.0% 0.3% 4.1.2% 0.3% 0.3% 2.2% 3.6% 0.2% 0.1% 0.4% 7.1% 8.3% 2.3% 2.3% 0.7% 0.1% 0.2% 82.3% 3.8% 1.6% 3.0% 0.7% 0.1% 0.9% 93.7%</cell><cell>200 400 600 800</cell><cell>0 1 2 3 4 5 6 7 8 9 True label</cell><cell cols="7">91.9% 0.3% 1.6% 1.2% 0.7% 0.3% 96.0% 0.1% 0.1% 2.0.4% 0.8% 1.0% 2.6% 2.3% 3.1% 0.2% 0.7% 0.3% 0.2% 0.3% 0.1% 0.3% 0.1% 2.2% 1.7% 0.1% 0.7% 2.7% 92.4% 0.1% 0.3% 94.4% 0.6% 0.7% 2.2% 0.3% 0.2% 0.1% 0.2% 0.1% 0.9% 95.2%</cell><cell>150 300 450 600 750</cell><cell>0 1 2 3 4 5 6 7 8 9 True label</cell><cell cols="4">82.1.3% 2.3% 0.2% 0.3%</cell><cell cols="4">0.4% 0.2% 0.7% 0.9% 93.7%</cell><cell>200 400 600 800</cell><cell>0 2 3 4 5 6 7 8 9 True label 1</cell><cell cols="3">91.4% 0.3% 1.7% 0.8% 0.3% 0.1% 2.0.9% 0.1% 1.3% 0.6% 2.0% 5.6% 3.0% 0.7% 0.3% 0.1% 0.6% 0.3% 0.1% 94.1% 0.8% 0.2% 1.9% 3.2% 88.4% 1.1% 0.6% 2.7% 0.2% 2.0% 1.4% 0.1% 0.9% 92.1% 0.3% 94.3% 1.7% 1.7% 2.0%</cell><cell>150 300 450 600 750</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell cols="2">3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 Predicted label 4 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>0</cell></row><row><cell></cell><cell>(a) 20% for Baseline</cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) 20% for Ours</cell><cell></cell><cell></cell><cell cols="8">(c) 40% for Baseline</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(d) 40% for Ours</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Notice that Θ here is a variable instead of a quantity, which makesŵ t (Θ) a function of Θ and the gradient in Eq. (9) be able to be computed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Derivation can be found in supplementary materials.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We have tried different classifier network architectures as classifier networks under each noise setting to show our algorithm is suitable to different deep learning architectures. We show this effect inFig.4, verifying the consistently good performance of our method in two classifier network settings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">t ,(29)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Since our method is inspired by L2RW<ref type="bibr" target="#b0">[1]</ref>, we compare this method, as well as the BaseModel, for better illustration. The uniform noise experiment setting is the same as L2RW<ref type="bibr" target="#b0">[1]</ref>, and thus the iteration steps of BaseModel and L2RW follows the original setting.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the China NSFC projects under contracts 61661166011, 11690011, 61603292, 61721002,U1811461. The authors would also like to thank anonymous reviewers for their constructive suggestions on improving the paper, especially on the proofs and theoretical analysis of our paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sensitivity and generalization in neural networks: an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Abolafia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edurne</forename><surname>Barrenechea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humberto</forename><surname>Bustince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auxiliary image regularization for deep cnns with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Co-teaching: robust training deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to predict from crowdsourced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to detect concepts from webly-labeled video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attend in groups: a weakly-supervised deep learning framework for learning from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting for classification of imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3358" to="3378" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A framework for robust subspace learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black Mkchael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="142" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Easy samples first: Self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauptmann</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust probabilistic modeling with bayesian data reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Approximation with artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csáji</forename><surname>Balázs Csanád</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">48</biblScope>
			<pubPlace>Hungary</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Sciences, Etvs Lornd University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to teach with dynamic loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><surname>Jian-Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to learn from weak supervision by full supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saverio</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimilano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gradient agreement as an optimization objective for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Amir Erfan Eshratifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massoud</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08178</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Class rectification hard mining for imbalanced deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning and evaluating classifiers under sample selection bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferdous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haw-Shiuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Small sample learning in big data era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04572</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Jaap Kamps, and Bernhard Schölkopf. Fidelity-weighted learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning to teach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Safeguarded dynamic label regression for noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Stochastic majorization-minimization algorithms for large-scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
