<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Effective Domain Adaptive Post-Training Method for BERT in Response Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Whang</surname></persName>
							<email>taesunwhang@wisenut.co.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Wisenut Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyub</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Kakao Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanhee</forename><surname>Lee</surname></persName>
							<email>chanhee0222@korea.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kisu</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsuk</forename><surname>Oh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuiseok</forename><surname>Lim</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Effective Domain Adaptive Post-Training Method for BERT in Response Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Response selection</term>
					<term>Human computer dialog system</term>
					<term>Spoken language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We focus on multi-turn response selection in a retrieval-based dialog system. In this paper, we utilize the powerful pre-trained language model Bi-directional Encoder Representations from Transformer (BERT) for a multi-turn dialog system and propose a highly effective post-training method on domain-specific corpus. Although BERT is easily adopted to various NLP tasks and outperforms previous baselines of each task, it still has limitations if a task corpus is too focused on a certain domain. Posttraining on domain-specific corpus (e.g., Ubuntu Corpus) helps the model to train contextualized representations and words that do not appear in general corpus (e.g., English Wikipedia). Experimental results show that our approach achieves new stateof-the-art on two response selection benchmarks (i.e., Ubuntu Corpus V1, Advising Corpus) performance improvement by 5.9% and 6% on R10@1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human computer conversation system aims to have natural and consistent conversation. In general, dialog systems can be categorized into task-oriented dialog systems and non-task-oriented dialog systems (i.e., chatbot systems). In this paper, we focus on one of non-task-oriented dialog systems, especially retrievalbased dialog systems, in which the task of predicting most likely next utterance from a set of candidates pool. While generative-based models provide a consistent but inaccurate response, retrieval-based models have a advantage of ensuring accurate answer.</p><p>Previous studies of response selection utilized recurrent neural networks <ref type="bibr" target="#b0">[1]</ref> or convolutional neural networks <ref type="bibr" target="#b1">[2]</ref> to represent dialog context and response, and predict their relevance score based on their representations. While these models represent dialog as a sequential sentence, utterance-response matching models build utterance-level encodings and utilize attention mechanism to catch relevant words between two sentences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Although attention-based utterance-response matching models are highly effective for predicting relevant response, self-attention-based matching model based on the work of Vaswani et al. <ref type="bibr" target="#b4">[5]</ref> strengthens encoding sophisticated segment representations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Also, a model based on contextualized language representations is also applied to response selection task in the work of Vig and Ramea <ref type="bibr" target="#b8">[9]</ref> and showed effectiveness of the pre-trained contextual language models. * Work performed while at Korea University Recently, pre-trained contextualized language models, such as ELMo <ref type="bibr" target="#b9">[10]</ref>, BERT <ref type="bibr" target="#b10">[11]</ref>, and XLNet <ref type="bibr" target="#b11">[12]</ref>) are proposed and showed great improvements on a wide range of NLP tasks, such as natural language inference, named entity recognition, and question answering. Despite their huge success, they still have limitations to represent contextual information in dialog corpus, specifically domain-specific corpus, since it is trained on general corpora (e.g., English Wikipedia and Book Corpus). For example, Ubuntu Corpus, which is the most used corpus for evaluating retrieval-based dialog system, contains a number of terminologies and ubuntu manuals that do not usually appear in general corpora (e.g., apt-get, lsmod, and grep). Since the corpus is biased towards a certain domain, pre-trained contextualized language models are not able to fully represent dialog contexts. In addition, conversation corpus, such as Twitter and Reddits, is mainly composed of colloquial expressions and abbreviation which are usually grammatically incorrect. In response selection task, one approach of training a certain domain knowledge embeddings is proposed. Chaudhuri et al. <ref type="bibr" target="#b12">[13]</ref> proposed a method building domain understandable embeddings by incorporating external knowledge (e.g., ubuntu manual description). In other NLP task, such as aspect extraction and review reading comprehension (RRC), there has been an attempt to learn domain specificity based on pre-trained model. Xu et al. <ref type="bibr" target="#b13">[14]</ref> proposed BERT based post-training method for RRC to enhance domain-awareness. Since reviews and opinion-based texts have many differences compared to the original corpus of BERT, therefore, post-training of BERT with two powerful unsupervised objectives (i.e., masked language model (MLM) and next sentence prediction (NSP)) on task-specific corpus enhance to produce task-awareness contextualized representations. Also, additional training on down-stream corpus shows effectiveness and performance improvements in various NLP tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In this work, we propose an effective post-training method for a multi-turn conversational system. To the best of our knowledge, it is the first attempt to adopt BERT base on the most popular response selection benchmark, Ubuntu Corpus V1. We demonstrate that NSP is especially considered as an important task for response selection, since classifying whether given two sentences are IsNext or NotNext is the ultimate objective of response selection. Also, we append [EOT] to the end of utterance that model can learn relationships among utterances during the period of post-training. Furthermore, our approach outperforms previous state-of-the-art performance by 5.9% on R10@1. We also evaluate on the recently released data set in the dialog system technology challenges 7 (DSTC 7) <ref type="bibr" target="#b16">[17]</ref> and outperforms the 1st place of the challenges. arXiv:1908.04812v2 [cs.CL] 27 Jul 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Lowe et al. <ref type="bibr" target="#b0">[1]</ref> proposed a new response selection benchmark dataset, Ubuntu Corpus V1, with a dual encoder baseline model. They utilized a RNN based models, specifically a vanilla RNN, a long short-term memory (LSTM), and a bi-directional LSTM. The sequential representations of a dialog context and a response are encoded by the RNN-based encoder and are utilized to predict the probability score of a given response being the next utterance in a given dialogue context. Kadlec et al. <ref type="bibr" target="#b1">[2]</ref> conducted experiments that apply convolutional neural networks to a dual encoder architecture that offers better performance than that of the vanilla RNN model. Even though each turn of the dialog should be considered in a multi-turn conversation, a given dialog context was regarded as a sequence in the previous baselines. To alleviate this, Zhou et al. <ref type="bibr" target="#b17">[18]</ref> proposed the MultiView model that encodes both word-level and utterance-level representations. However, it does not have the ability to fully capture the relevance of the dialog context and the response.</p><p>Therefore, more recently, the utterance-level dialogresponse matching models were proposed to enable the model to readily catch the relevance of the dialog and the response <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Wu et al. <ref type="bibr" target="#b2">[3]</ref> proposed a sequential matching network, utilizing both word embeddings and their sequential representations encoded by gated recurrent unit (GRU) encoder to build matching matrices between the dialogue context and the response. Zhang et al. <ref type="bibr" target="#b3">[4]</ref> proposed a highly effective turnsaware aggregation methodology and exercised self-matching attention to fuse the representation of each utterance. Another approach to the dialogue-response matching model proposed by Dong and Huang (2018), is designed based on enhanced sequential inference model (ESIM) <ref type="bibr" target="#b18">[19]</ref> model. ESIM achieved a good performance on a natural language inference task and is a proper model for discriminating if given two sentences are relevant or not. As an self-attention-based model <ref type="bibr" target="#b4">(Vaswani et al., 2017</ref>) achieved significant performance improvements on various NLP tasks; it was also adapted for the response selection task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Zhou et al. <ref type="bibr" target="#b5">[6]</ref> used self-attention and crossattention simultaneously so as to capture token-level dependencies as well as relevant segment pairs from a dialog and a response. One of most recent models, interaction-over-interaction model alleviates shallow interaction between dialog-response matching by stacking multiple interaction blocks <ref type="bibr" target="#b19">[20]</ref>. Yuan et al. <ref type="bibr" target="#b7">[8]</ref> pointed out previous dialog-response matching models in terms of excessive use of dialog context information, therefore they proposed multi-hop selector to filter only relevant utterances to response. They achieved state-of-the-art results on three response selection benchmark datasets (i.e., Ubuntu Corpus V1, Douban Corpus, and E-commerce).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Domain Post-Training</head><p>BERT is designed to be easily applied to other nlp tasks with a fine-tuning manner. Since it is pre-trained on general corpus (e.g., Wikipedia Corpora), it is insufficient to have enough supervision of task-specific words and phrases during the period of fine-tuning. To alleviate this issue, we post-train BERT on our task-specific corpora that helps model understand certain domain. The model is trained with two objectives, MLM and NSP, which are highly effective to learn contextual representations from the corpora. One example (Ubuntu Corpus) of domain post-training of BERT for response selection is described in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: BERT for Response Selection</head><p>In the example of Masked LM, model can learn that sud ##o command is needed when trying apt install in Ubuntu system, which is not generally showed from universal corpora. Unlike general sentence, multi-turn dialog system is composed of a set of utterances. We append " End Of Turn " token [EOT] to the end of each turn to make the model catch each utterance is finished at the point. By conducting NSP during the posttraining, model also can train given two sentences are sequential and relevant, which is the common ultimate goal of response selection. To optimize the model domain post-training (DPT) loss is calculated adding mean MLM likelihood and that of NSP, formulated as  </p><formula xml:id="formula_0">LDP T = LMLM + LNSP .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fine-tuning BERT for Response Selection</head><p>Our overall approach is described in <ref type="figure">Figure 1</ref>. We transform the task of ranking responses from the candidates pool into binary classification problem by approaching a pointwise method.</p><p>We denote a training set as a triples D = {(ci, ri, yi)} N i=1 , where c = {u1, u2, ..., um} is a dialog context consists of a set of utterances. An utterance ui = {wi,1, wi,2, ..., w i,l i } is composed of a set of word tokens w i,k , where 1 ≤ k ≤ li and li is the length of i-th utterance. Response is denoted as ri = {r1, r2, ..., rn} (n is the number of tokens in the response), and ground truth yi ∈ {0, 1}. We define BERT input , is utilized to classify whether a given dialog context and response is IsNextUtterance or not. We feed T [CLS] to single-layer perceptron to compute the model prediction score,</p><formula xml:id="formula_1">g(c, r) = σ(W task T [CLS] + b),<label>(2)</label></formula><p>where W task is a task-specific trainable parameter. We use cross entropy loss as the objective function to optimize our model, formulated as Loss = − (c,r,y)∈D ylog(g(c, r))</p><formula xml:id="formula_2">+ (1 − y)log(1 − g(c, r)).<label>(3)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Training Setup</head><p>We evaluate our model on two multi-turn dyadic data sets, Ubuntu IRC (Internet Relay Chat) Corpus V1 <ref type="bibr" target="#b0">[1]</ref> and Advising Corpus 1 <ref type="bibr" target="#b16">[17]</ref>. For the Ubuntu Corpus, training set is composed of 0.5M dialog context containing positive and negative response with the ratio of 1:1. Each validation and test set contains 50k dialog context with one positive response and nine negative responses. Advising Corpus consists of 100k dialogs for training set and 500 for validation and test set. All sets contain one positive response and 99 negative responses. We only use one negative sample for training to make same conditions with Ubuntu Corpus. For an evaluation metric, we use Rn@k, evaluating if the ground truth exists in top k from n candidates <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. We also use another evaluation metric mean reciprocal rank (MRR) <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The models are implemented using the Tensorflow library <ref type="bibr" target="#b21">[22]</ref>. We use the uncased BERT base model 2 as a base code for our experiments. The batch size is set to 32 and the maximum sequence length is set to 320, specifically 280 for a dialog context and 40 for a response. We post-train the model more on Ubuntu Corpus V1 and Advising Corpus, 200,000 steps and 100,000 steps, respectively. The model is optimized using Adam weight decay optimizer with learning rate of 3e-5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">BERT Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance on Ubuntu Corpus V1</head><p>For Ubuntu Corpus V1, we compared our model with the following baseline methods and the results are given in <ref type="table">Table 2</ref>. Dual Encoder is simple dialogue-response matching model based on RNN, CNN, LSTM, and BiLSTM <ref type="bibr" target="#b1">[2]</ref>. Each dialog context and response is represented by each encoder and utilized to obtain relevance score of given two sentences. MultiView model is based on both token-level and utterancelevel representations to catch utterance-level information in a dialog. CNN encoder is firstly used to encode each utterance representation, and Gated recurrent unit (GRU) encoder is used to encode both token-level representations and utterance-level representations <ref type="bibr" target="#b17">[18]</ref>. SMN <ref type="bibr" target="#b2">[3]</ref> proposed utterance-response matching methods, specifically attention matrices created by word embeddings and sequential representations are encoded by CNN and they are fed into GRU encoder to obtain a probability score. AK-DE-biGRU <ref type="bibr" target="#b12">[13]</ref> proposed a method incorporating domain knowledge (i.e., Ubuntu manual description). Domain knowledge embeddings are created based on manual description, Bi-GRU encoder's forward and backward last hidden states are concatenated to build the embeddings. Pre-trained word embeddings and created domain knowledge embeddings are added and fed into an attention module. DUA <ref type="bibr" target="#b3">[4]</ref> is a method of modeling utterance aggregation. By conducting turns-aware aggregation and gated self attention, the model can give weights to more relevant utterance with response. DAM <ref type="bibr" target="#b5">[6]</ref> is based on transformer <ref type="bibr" target="#b4">[5]</ref> encoder and apply both self-attention and cross-attention to obtain matching scores. Each attention matching score is aggregated by 3D matching image. MRFN <ref type="bibr" target="#b19">[20]</ref> proposed multi representation fusion network and highlighted the effectiveness of fusing strategy. They proposed a methodology to fuse multiple representations of words, con-texts, and attention. IoI <ref type="bibr" target="#b6">[7]</ref> build interaction blocks to help model conduct deep interactions between utterances and response. All representations extracted from the blocks are aggregated to obtain a relevance score. MSN <ref type="bibr" target="#b7">[8]</ref> is a previous state-of-the-art model, which proposed multi-hop selector network to control which dialog context information is reflected for matching response.</p><p>In Ubuntu Corpus V1, it is observed that BERT-VFT achieves new state-of-the-art performance and it obtains performance improvement by 5.5%, 2.9%, 0.7% in terms of R10@k, where k={1, 2, 5} respectively, compared to the previous stateof-the-art method (i.e., MSN). Focusing on R10@1 metric, the performance of a vanilla BERT base is 0.817 and comparing our main approach, which is BERT-DPT, shows better result (improvement by 3.4%).</p><p>In addition, we especially point out comparing BERT-VFT with other self-attention-based models, such as DAM, IoI, and MSN, since all models are built with similar model architecture. Especially, domain-specific optimized BERT-VPT model shows performance improvement by 8.8% in terms of R10@1 compared to the general transformer based model (i.e., DAM). In the case of MSN, even though it uses word2vec <ref type="bibr" target="#b22">[23]</ref> as pre-trained word embeddings, it shows comparable results with BERT base model. However, BERT-VFT that understands domain specific corpus shows 5.5% higher results in terms of R10@1, which has significant gap between two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance on Advising Corpus</head><p>As shown in <ref type="table">Table 3</ref>, we compare our approach with two existing baselines on Advising Corpus, proposed by Vig and Remea <ref type="bibr" target="#b8">[9]</ref> and Chen et al. <ref type="bibr" target="#b23">[24]</ref> in DSTC 7. The former baseline evaluate BERT base model on the Advising Corpus, but there is substantial performance difference from what we obtain. We believe that different implementation frameworks and hyperparameters, such as learning rates, are the main reason why performance differences exist between our work and that of Vig and Remea <ref type="bibr" target="#b8">[9]</ref>. The first place of the challenge was the work conducted by Chen et al. <ref type="bibr" target="#b23">[24]</ref>, BERT-VFT outperforms 6% in terms of R10@1. DPT shows its effectiveness on Advising Corpus, leading 3.4% higher results in terms of R10@1 compared to the BERT baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>R 100 @1 R 100 @10 R 100 @50 MRR Vig and Remma <ref type="bibr" target="#b8">[9]</ref>   <ref type="table">Table 3</ref>: Evaluation results on the Advising Corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Variable Fine-tuning</head><p>Houlsby et al. <ref type="bibr" target="#b24">[25]</ref> proposed efficient fine-tuning approach that only updates a few top layers of BERT during fine-tuning period. They suggested that small data sets may be sub-optimal, when fine-tuning the whole BERT layers. Inspired by this assumption, we conduct an experiment (  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Data Augmentation</head><p>We also experiment how effective data augmentation is for both data sets. Formerly, both training sets contain 1:1 ratio of positive and negative responses since the model is trained with a pointwise manner. We change the ratio of the samples to 1:k, where k is the number of negative samples, by increasing k.</p><p>The process of how we select the number of negative samples is heuristic, and the best performance is obtained at ratio of 1:4. <ref type="table" target="#tab_7">Table 5</ref> shows the effectiveness of two unsupervised objectives of BERT, MLM and NSP in Ubuntu Corpus V1. Also, it shows that appending special token [EOT] at the end of each utterance is highly effective for response selection task. During the DPT phase, MLM helps the model learn contextual representations from the target domain corpus and it is trained sequential consistency of the given two sentences by conducting NSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">MLM and NSP in BERT</head><p>In the case of NSP, it does not influence much in improvement of performance because what NSP do during the DPT phase is the same task as the model do during the fine-tuning phase. While conducting only NSP is not helpful for corpus understanding, MLM leads significant improvement in performance regardless of using the special token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-Training</head><p>Special Token R 10 @1 R 10 @2 R 10 @5 MRR  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a highly effective post-training method for a multiturn response selection is proposed and evaluated. Our approach achieved new state-of-the-art results for two response selection benchmark data sets, Ubuntu Corpus V1 and Advising Corpus. For future work, we will utilize external domain knowledge, such as ubuntu manual description in Ubuntu Corpus or curriculum information in Advising Corpus, for enhancing domain understandings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>x = ([CLS], u1, [EOT], u2, [EOT], ..., um, [SEP], r1, ..., rn, [SEP]). Maximum sequence length of the dialog context and response is denoted as q. Position, segment, and token embeddings are added and fed into the BERT layers. The BERT contextual representations of [CLS] token, T [CLS]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>An example of domain post-training in Ubuntu Corpus</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 2 :</head><label>42</label><figDesc>BERT base<ref type="bibr" target="#b10">[11]</ref> is a vanilla BERT base model for response selection task. Uncased BERT base model checkpoint is utilized as initial weights of the model, and single-layer perceptron are trained during the fine-tuning phase.BERT-DPT is a BERT base model that is post-trained on domain-specific corpus, Ubuntu Corpus V1 and Advising Corpus. The unsupervised objectives of the BERT model, MLM and NSP tasks are conducted during the DPT phase. BERT-VFT is a model that selects the number of top layers to fine-tuning in BERT-DPT. We evaluate the model with varying T = {0, 2, 4, 6, 8, 10, 12}, where T is the number of layers which are tuned during training time. When T = 12, all the BERT-DPT layers are fine-tuned and only embedding layers are fine-tuned when T = 0. We fill in the best score based on MRR BERT-VFT(DA) performs data augmentation technique by increasing the number of negative samples. The number of negative samples are decided by several experiments, when the number is 4 shows the best results. Negative responses are randomly resampled for every epoch from response candidates pool. Model comparison on Ubuntu Corpus V1.</figDesc><table><row><cell>Model</cell><cell>R 10 @1</cell><cell>R 10 @2</cell><cell>R 10 @5</cell></row><row><cell>DualEncoder rnn</cell><cell>0.403</cell><cell>0.547</cell><cell>0.819</cell></row><row><cell>DualEncoder cnn</cell><cell>0.549</cell><cell>0.684</cell><cell>0.896</cell></row><row><cell>DualEncoder lstm</cell><cell>0.638</cell><cell>0.784</cell><cell>0.949</cell></row><row><cell>DualEncoder bilstm</cell><cell>0.630</cell><cell>0.780</cell><cell>0.944</cell></row><row><cell>MultiView</cell><cell>0.662</cell><cell>0.801</cell><cell>0.951</cell></row><row><cell>SMN</cell><cell>0.726</cell><cell>0.847</cell><cell>0.961</cell></row><row><cell>AK-DE-biGRU</cell><cell>0.747</cell><cell>0.868</cell><cell>0.972</cell></row><row><cell>DUA</cell><cell>0.752</cell><cell>0.868</cell><cell>0.962</cell></row><row><cell>DAM</cell><cell>0.767</cell><cell>0.874</cell><cell>0.969</cell></row><row><cell>MRFN</cell><cell>0.786</cell><cell>0.886</cell><cell>0.976</cell></row><row><cell>IoI</cell><cell>0.796</cell><cell>0.894</cell><cell>0.974</cell></row><row><cell>MSN</cell><cell>0.800</cell><cell>0.899</cell><cell>0.978</cell></row><row><cell>BERT base</cell><cell>0.817</cell><cell>0.904</cell><cell>0.977</cell></row><row><cell>BERT-DPT</cell><cell>0.851</cell><cell>0.924</cell><cell>0.984</cell></row><row><cell>BERT-VFT</cell><cell>0.855</cell><cell>0.928</cell><cell>0.985</cell></row><row><cell>BERT-VFT(DA)</cell><cell>0.858</cell><cell>0.931</cell><cell>0.985</cell></row></table><note>1 https://github.com/IBM/dstc-noesis2 https://github.com/google-research/bert in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>=4 and T =6, for Ubuntu and Advising, respectively. We demonstrate that utilizing this application is effective on not only small sets but also domain-specific sets. Variable fine-tuning also helps for reducing computational resources (e.g., GPU memory) as well as showing performance improvements.</figDesc><table><row><cell>) varying</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Variable fine-tuning in BERT model. BERT-VFT model is utilized for this experiment and the model is evaluated using MRR metric.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of MLM and NSP on Ubuntu Corpus V1. Experiments are conducted depending on the use of [EOT].</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved deep learning baselines for ubuntu corpus dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03753</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling multi-turn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One time of interaction may not be enough: Go deep with an interactionover-interaction network for response selection in dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-hop selector network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Comparison of transfer-learning approaches for response selection in multi-turn conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in 7th Edition of the Dialog System Technology Challenges at AAAI 2019</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving response selection in multi-turn dialogue systems by incorporating domain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bert post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02232</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DSTC7 task 1: Noetic end-to-end response selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Polymenakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lasecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on NLP for Conversational AI at ACL 2019</title>
		<meeting>the First Workshop on NLP for Conversational AI at ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="60" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view response selection for human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multirepresentation fusion network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The trec-8 question answering track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trec</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
	<note>OSDI 16</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequential attention-based network for noetic end-to-end response selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in 7th Edition of the Dialog System Technology Challenges at AAAI 2019</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parameterefficient transfer learning for nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
