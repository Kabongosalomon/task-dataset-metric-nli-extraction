<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COLBERT: USING BERT SENTENCE EMBEDDING FOR HUMOR DETECTION 1 ColBERT: Using BERT Sentence Embedding for Humor Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issa</forename><surname>Annamoradnejad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gohar</forename><surname>Zoghi</surname></persName>
						</author>
						<title level="a" type="main">COLBERT: USING BERT SENTENCE EMBEDDING FOR HUMOR DETECTION 1 ColBERT: Using BERT Sentence Embedding for Humor Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Computational Humor</term>
					<term>Human-device Interac- tion</term>
					<term>Natural Language Processing</term>
					<term>Sentence Embedding</term>
					<term>Humor Structure</term>
					<term>Dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic humor detection has interesting use cases in modern technologies, such as chatbots and virtual assistants. In this paper, we propose a novel approach for detecting humor in short texts based on the general linguistic structure of humor. Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs of parallel lines of hidden layers in a neural network. These lines are finally concatenated to predict the target value. For evaluation purposes, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive and 100k negative). Experimental results show that our proposed method can determine humor in short texts with accuracy and an F1score of 98.2 percent. Our 8-layer model with 110M parameters outperforms the baseline models with a large margin, showing the importance of utilizing linguistic structure of texts in machine learning models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N Interstellar (2014 movie), a future earth is depicted where robots easily understand and use humor in their connections with their owners and humans can set the level of humor in their personal robots <ref type="bibr" target="#b0">1</ref> . While we may have a long road toward the astral travels, we are very close in reaching high-quality systems injected with adjustable humor.</p><p>Humor, as a potential cause of laughter, is an important part of human communication, which not only makes people feel comfortable, it also creates a cozier environment <ref type="bibr" target="#b0">[1]</ref>. Automatic humor detection in texts has interesting use cases in building human-centered artificial intelligence systems such as chatbots and virtual assistants. An appealing use case <ref type="bibr" target="#b0">[1]</ref> is to identify whether an input text should be taken seriously or not, which is a critical step to understand real motive of users' queries, return appropriate answers, and enhance the overall experience of user with the system. A more advanced outcome would be the injection of humor into computer-generated responses, thus making the human-computer interaction more engaging and interesting <ref type="bibr" target="#b1">[2]</ref>. This is an outcome that is achievable by setting the level of humor in possible answers to a desired level, similar to the mentioned movie.</p><p>The general structure of humor states that a joke consists of a few sentences that concludes with a punchline. The punchline is responsible for bringing contradiction into the story, thus making the whole text laughable. In other words, any sentence in a joke is normally non-humorous in itself, but when we try to comprehend all sentences together in one context or in one line of story, the text become humorous. For this reason, we believe and show that it is required to view and encode each sentence separately and capture this underlying structure of humor in a proper way. As a result, our proposed classification model for the task of humor detection is based on creating parallel paths of neural network hidden layers to extract features for each sentence and combining them at the end.</p><p>Our approach uses BERT model to encode text into a few sentence embeddings which enter into an eight-layered neural network. Sentences are separately encoded and fed into parallel hidden layers of neural network to extract mid-level features for each sentence (related to context, type of sentence, etc). The final layers combine the output of all previous lines of hidden layers in order to predict the final output. In theory, these final layers should determine the congruity of sentences and detect the transformation of reader's viewpoint after reading the punchline.</p><p>In addition to proposing an accurate model, we publish a large dataset for the task of humor detection. Several existing humor detection datasets combined some formal nonhumorous texts with informal humorous short texts. Since the two part have incompatible statistics (text length, words count, etc.), it is more likely to detect humor with simple analytical models and without understanding the underlying latent lingual features and structures.</p><p>We summarize our contributions as follows:</p><p>• We introduce a new dataset for the task of humor detection, entitled "ColBERT dataset", which contains 200k short texts (100k positive and 100k negative). We reduced or completely removed issues of the existing datasets to build a proper dataset for the task at-hand. • For the first time, we propose an automated approach for humor detection that is based on an accepted linguistic structure of humor. We will introduce the model architecture and components in detail. • We evaluate our model on 20% of the dataset, and compare its performance with five baselines. The structure of this article is as follows: Section 2 reviews past works on the task of humor detection with focus on arXiv:2004.12765v5 [cs.CL] 5 Apr 2021 transfer learning methods. Section 3 describes the data collection and preparation techniques, and introduces the new dataset. Section 4 elaborates on the methodology, and section 5 presents our experimental results. Section 6 is the concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LITERATURE REVIEW</head><p>With advances in NLP, researchers applied and evaluated state-of-the-art methods for the task of humor detection. This includes using statistical and N-gram analysis <ref type="bibr" target="#b2">[3]</ref>, Regression Trees <ref type="bibr" target="#b3">[4]</ref>, Word2Vec combined with K-NN Human Centric Features <ref type="bibr" target="#b4">[5]</ref>, and Convolutional Neural Networks <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b6">[7]</ref>.</p><p>With the popularity of transfer learning, some researchers focused on using pre-trained models for several tasks of text classification. Transfer learning in NLP, particularly models like ULMFiT <ref type="bibr" target="#b7">[8]</ref>, Allen AI's ELMO <ref type="bibr" target="#b8">[9]</ref>, and Google's BERT <ref type="bibr" target="#b9">[10]</ref>, focuses on storing knowledge gained from training on one problem and applying it to a different but related problem usually after fine-tuning on a small amount of data.</p><p>Among them, BERT <ref type="bibr" target="#b9">[10]</ref> utilizes a multi-layer bidirectional transformer encoder consisting of several encoders stacked together, which can learn deep bi-directional representations. Similar to previous transfer learning methods, it is pre-trained on unlabeled data to be later fine-tuned for a variety of tasks. It initially came with two model sizes (BERT BASE and BERT LARGE ) and obtained eleven new state-of-the-art results. Since then, it was pre-trained and fine-tuned for several tasks and languages, and several BERT-based architectures and model sizes have been introduced (such as Multilingual BERT, RoBERTa <ref type="bibr" target="#b10">[11]</ref>, ALBERT <ref type="bibr" target="#b11">[12]</ref> and VideoBERT <ref type="bibr" target="#b12">[13]</ref>).</p><p>Ref <ref type="bibr" target="#b6">[7]</ref> focused on the task of detecting whether a joke is humorous by using a Transformer architecture. They approached this problem by building a model that learns to identify humorous jokes based on ratings taken from the popular Reddit r/Jokes thread (13884 negative and 2025 positives).</p><p>There are emerging tasks related to humor detection. Ref <ref type="bibr" target="#b13">[14]</ref> focused on predicting humor by using audio information, hence reached 0.750 AUC by using only audio data. A good number of research is focused on the detecting humor in non-English texts, such as on Spanish <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, Chinese <ref type="bibr" target="#b13">[14]</ref>, and English-Hindi <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA</head><p>Existing humor detection datasets use a combination of formal texts and informal jokes with incompatible statistics (text length, words count, etc.), making it more likely to detect humor with simple analytical models and without understanding the underlying latent connections. Moreover, they are relatively small for the tasks of text classification, making them prone to over-fit models. These problems encouraged us to create a new dataset exclusively for the task of humor detection, where simple feature-based models will not be able to predict without an insight into the linguistic features.</p><p>We begin with a survey of the existing humor detection datasets (binary task), highlighting their size and data source (see <ref type="table" target="#tab_0">Table I</ref> for an overview). There are other datasets focused on similar tasks, for example, on the tasks of punchline detection and success (whether or not a punchline triggers laughter) <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, or on using speak audio and video to detect humor <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this section, we will introduce data sources, data filtering methods, and some general statistics on the new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collection</head><p>We carefully analyzed existing datasets (exclusively on news stories, news headlines, Wikipedia pages, tweets, proverbs and jokes) with regard to table size, character length, word count, and formality of language. Since none of them was compatible with each other as is in the mentioned criteria, first, we selected two datasets with formal texts (one with humor texts and one without) and performed a few preprocessing actions and row cuts to make them syntactically similar.</p><p>News dataset includes 200,853 news headlines (plus links, categories and stories) from 2012-2018 obtained from Huffington Post. Headlines are scattered in all news categories, including politics, wellness, entertainment and parenting.</p><p>Jokes dataset contains 231,657 jokes/humor short texts, crawled from Reddit communities 2 . The dataset is compiled as a single csv file with no additional information about each text (such as the source, date, etc) and is available at Kaggle. Ref <ref type="bibr" target="#b5">[6]</ref> combined this dataset with the WMT162 English news crawl, but did not publicly publish the dataset. Ref <ref type="bibr" target="#b6">[7]</ref> also combined this dataset with extracted sentences from the WMT162 news crawl and made it publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing and Filtering</head><p>First, we realized that there are duplicate texts in both datasets. Dropping duplicate rows removed 1369 rows from the jokes dataset and 1558 rows from the news dataset.</p><p>Then, to make their statistics more similar, we analyzed the number of characters and words, separately for each one of them, and by realizing differences in their distributions, we performed a few cuts. In short, we only kept texts with character length between 30 and 100, and word length between 10 and 18. Resulting data parts have very similar distribution with regard to these statistics.</p><p>In addition, we noticed that headlines in the news dataset use Title Case 3 formatting, which was not the case with the jokes dataset. Thus, we decided to apply Sentence Case 4 formatting to all news headlines by keeping the first character of the sentences in capital and lower-casing the rest. This simple modification helps to prevent simple classifiers from reaching perfect accuracy. Finally, we randomly selected 100k rows from both datasets and merged them together to create an evenly distributed dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset Statistics</head><p>5 Dataset contains 200k labeled short texts, equally distributed between humor and non-humor. It is much larger than the previous datasets <ref type="table" target="#tab_0">(Table I)</ref> and it includes texts with similar textual features. Correlation between character count and the target is insignificant (+0.09), and there is no notable connection between the target value and sentiment features (correlation coefficient of -0.09 and +0.02 for polarity and subjectivity, respectively). <ref type="table" target="#tab_0">Table II</ref> gives details on a few general statistics of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED METHOD</head><p>In this section, we will explore our proposed method for the task of humor detection. From a technical viewpoint, we are proposing a supervised binary classifier that takes a string as input and determines if the given text is humorous or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Humor Structure</head><p>First, we take a look at the general structure of a joke to understand the underlying linguistic features that makes a text laughable.</p><p>There has been a long line of works in linguistics of humor that classify jokes into various categories based on their structure or content. Many believed that humor arises from the sudden transformation of an expectation into nothing <ref type="bibr" target="#b22">[23]</ref>. Therefore, main theories on the structure of a joke involves two or three stages of storytelling that concludes with a punchline <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Punchline is the last part of a joke that destroys the perceiver's previous expectations and bring humor for its incongruity.</p><p>Raskin <ref type="bibr" target="#b25">[26]</ref> presented Semantic Script Theory of Humor (SSTH), a detailed formal semantic theory of humor. The SSTH has the necessary condition that a text has to have two distinct related scripts that are opposite in nature, such as real/unreal, possible/impossible. For example, let us review a typical joke:</p><p>"Is the doctor at home?" the patient asked in his bronchial whisper. "No," the doctor's young and pretty wife whispered in reply. "Come right in." <ref type="bibr" target="#b25">[26]</ref> This is compatible with the two-staged theory which ends with a punchline. The punchline is related to previous sentences but is included as an opposition to previous lines in order to transform the reader's expectation of the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model</head><p>Based on the presented short introduction to the structure of humor, if one reads sentences of a joke separately, they are most-likely to be found as normal and non-humorous texts. On the other hand, if we try to comprehend all sentences together in one context or in one line of story, the text become humorous. Our proposed method utilizes this linguistic characteristic of humor in order to view or encode sentences separately and extract mid-level features using hidden layers.</p><p>Therefore, the classification model uses separate paths of hidden layers especially designed to extract latent features from each sentence. Furthermore, we include another path to extract latent features of the whole text. Hence, our proposed neural network structure includes one parallel path to view text as a whole and several other paths to view each sentence separately. <ref type="figure" target="#fig_0">Figure 1</ref> displays the architecture of the proposed method. It is comprised of a few general steps: 1) First, to assess each sentence separately and extract numerical features, we separate sentences and tokenize them individually. 2) To prepare these textual parts as proper numerical inputs for the neural network, we encode them using BERT sentence embedding. This step is performed individually on each sentence (left side in <ref type="figure" target="#fig_0">Figure 1</ref>) and also on the whole text (right side in <ref type="figure" target="#fig_0">Figure 1</ref>). 3) Now that we have BERT sentence embedding for each sentence, we feed them into parallel hidden layers of neural network to extract mid-level features for each sentence (related to context, type of sentence, etc). The output of this part for each sentence is a vector of size 20. 4) While our main idea is to detect existing relationships between sentences (specifically the punchline's relationship with the rest), it is also required to examine word-level connections in the whole text that may have meaningful impacts in determining congruity of the text. For example, existence of synonyms and antonyms in text could be meaningful. We feed BERT sentence embedding for the whole text into hidden layers of neural network (right side in <ref type="figure" target="#fig_0">Figure 1</ref>). The output of this part is a vector of size 60. 5) Finally, three sequential layers of neural network conclude our model. These final layers combine the output </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Notes</head><p>To achieve clean data, we performed a few textual preprocessing actions on all input texts:</p><p>• Expanding contractions: We replaced all contractions with the expanded version of the expressions. For example, "is not" instead of "isn't". • Cleaning punctuation marks: We separated the punctuation marks 6 from words to achieve cleaner sentences. For example, the sentence "This is' (fun)." is converted to "This is ' ( fun ) ." • Cleaning special characters: We replaced some special characters with an alias. For example, "alpha" instead of "α". Our approach builds on using BERT sentence embedding in a neural network. More specifically, first we obtains token representation using BERT tokenizer with the maximum sequence length of 100 (the maximum sequence length of BERT is 512). Then, we generate BERT sentence embedding by feeding tokens as input into the BERT model (vector size=768). The model will pass BERT embedding vectors of the given text and its sentences as inputs to a neural network with eight layers. For each sentence, We have a separate parallel line of three hidden layers which are concatenated in the fourth layer and continue in a sequential manner to predict the single target value. We use huggingface and keras.tensorflow packages for the BERT model and neural network implementations, respectively.</p><p>It is important to note that we used BERT model to generate sentence embedding. Therefore, training is performed on the neural network and not on the BERT model. BERT comes with two pre-trained general types (the BERT BASE and the BERT LARGE ), both of which are pre-trained from unlabeled data extracted from BooksCorpus <ref type="bibr" target="#b26">[27]</ref> with 800M words and English Wikipedia with 2,500M words <ref type="bibr" target="#b9">[10]</ref>. In our proposed method, we use the smaller sized (BERT BASE ) with 12 layers, 768-hidden, 12-heads, 110M parameters, which is pre-trained on lower-cased English text (uncased).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we compare the performance of the Col-BERT model with a few baselines. For the purposes of this section, the data is split into 80% (160k) train and 20% (40k) test parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baselines</head><p>We chose five baselines to evaluate the performance of our proposed method. The baseline models are: 1) Decision Tree: A methodology that is commonly used as a data mining method for establishing classification systems based on multiple covariates or for developing prediction algorithms for a target variable. The method uses the train dataset to generate a branch-like segments that construct an inverted tree with a root node, internal nodes, and leaf nodes <ref type="bibr" target="#b27">[28]</ref>. For our evaluation, we used CountVectorizer to generate numerical word representations. 2) SVM: A supervised model that achieved robust results for many classification and regression tasks. For this baseline, we applied TfidfVectorizer to generate  <ref type="bibr" target="#b28">[29]</ref>. We applied XG-Boost on numerical word representations generated by CountVectorizer which resulted in better accuracy than TfidfVectorizer. 5) XLNet: A generalized language model that aims to mitigate the issues related to BERT model and previous autoregressive language models. For the task of text classification (and some other NLP tasks), XLNet outperforms BERT on several benchmark datasets <ref type="bibr" target="#b29">[30]</ref>. We used xlnet-large-cased that has 24 layers and 340M parameters. We trained these baselines for 5 epochs on the complete 160K rows of the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Our experiments on the ColBERT dataset found the proposed model's accuracy and F1 score to be 98.2% which outperforms all baselines with a large margin <ref type="table" target="#tab_0">(Table III)</ref>. This is a 7 percent jump from the recent state-of-the-art XLNet model (with 340M parameters). Traditional models of Decision Tree, SVM and Multinomial naïve Bayes gained less than 90% accuracy, still an acceptable performance for a general baseline. XGBoost did not perform very well, achieving 81% F1score based on the selected word representations. XLNet Large , which required less optimization, was the strongest among the baselines, reaching close to 92% accuracy, 4 percent higher than Multinomial NB.</p><p>Based on the results, we can see two important factors in achieving high accuracy in the current task. First, can clearly see the power of sentence embeddings. XLNet and the proposed model both use their own embeddings and achieve much better results than other baselines. Traditional methods of word representations such as TF-IDF could not break a limit even with the use of latest classification boosting models (such as XGBoost). Second, it is interesting to note that our model with 110M parameters and 8 layers was able to outperform XLNet with 340 parameters and 24 layers. This is a direct result of utilizing the linguistic structure of humor in designing the proposed model.</p><p>For timing and performance, it took close to 2 hours in average to perform one epoch of training on 100k rows of the dataset on a computer with NVIDIA TESLA P100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>For many years, human beings have been fantasizing about (or terrified of) humanoid robots indistinguishable from humans. In making that a reality, humor cannot be missed as a major human feature, which for its subjectivity, ambiguity and semantic intricacies has been a difficult problem for researchers to tackle. This work contributes to this old human fantasy and is paving the way for creating high-quality artificial intelligence systems (such as chatbots, virtual assistants and even robots) injected with adjustable humor.</p><p>Based on the results of this study, we identified two important factors in achieving high accuracy in the current task: 1) usage of sentence embeddings, and 2) utilizing the linguistic structure of humor in designing the proposed model.</p><p>Our technical approach consists of injecting BERT sentence embedding into a neural network model that processes sentences separately in parallel hidden layers. Moreover, we built a novel and large dataset consisting of 200k formal short texts for the task of humor detection. Our method with much less parameters and layers obtained an accuracy of 0.982 and outperforms traditional and state-of-the-art models. Results showed that our hypothesis on the structure of humor is valid and can be utilized to create very accurate systems. In addition to the task of humor detection, the proposed combined method of neural network can be used in future studies examining a wider range of tasks of text classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Components of the proposed method of all previous paths of hidden layers in order to predict the final output. In theory, these final layers should determine the congruity of sentences and detect the transformation of reader's viewpoint after reading the punchline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DATASETS</head><label>I</label><figDesc>FOR THE BINARY TASK OF HUMOR CLASSIFICATION</figDesc><table><row><cell></cell><cell>Parts</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">#Positive #Negative</cell></row><row><cell>16000 One-Liners [22]</cell><cell>16,000</cell><cell>16,002</cell></row><row><cell>Pun of the Day [5]</cell><cell>2,423</cell><cell>2,403</cell></row><row><cell>PTT Jokes [6]</cell><cell>1,425</cell><cell>2,551</cell></row><row><cell>English-Hindi [18]</cell><cell>1,755</cell><cell>1,698</cell></row><row><cell>ColBERT</cell><cell>100,000</cell><cell>100,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II GENERAL</head><label>II</label><figDesc>STATISTICS OF THE COLBERT DATASET (100K POSITIVE, 100K NEGATIVE)</figDesc><table><row><cell></cell><cell cols="3">#chars #words #unique</cell><cell>#punctuation</cell><cell>#duplicate</cell><cell cols="2">#sentences sentiment</cell><cell>sentiment sub-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>words</cell><cell></cell><cell>words</cell><cell></cell><cell>polarity</cell><cell>jectivity</cell></row><row><cell>mean</cell><cell cols="2">71.561 12.811</cell><cell>12.371</cell><cell>2.378</cell><cell>0.440</cell><cell>1.180</cell><cell>0.051</cell><cell>0.317</cell></row><row><cell>std</cell><cell cols="2">12.305 2.307</cell><cell>2.134</cell><cell>1.941</cell><cell>0.794</cell><cell>0.448</cell><cell>0.288</cell><cell>0.327</cell></row><row><cell>min</cell><cell>36</cell><cell>10</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>-1.000</cell><cell>0.000</cell></row><row><cell>median</cell><cell>71</cell><cell>12</cell><cell>12</cell><cell>2</cell><cell>0</cell><cell>1</cell><cell>0.000</cell><cell>0.268</cell></row><row><cell>max</cell><cell>99</cell><cell>22</cell><cell>22</cell><cell>37</cell><cell>13</cell><cell>2</cell><cell>1.000</cell><cell>1.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF DIFFERENT METHODS ON THE COLBERT DATASET</figDesc><table><row><cell>Method</cell><cell>Configuration</cell><cell cols="4">Accuracy Precision Recall F1</cell></row><row><cell>Decision Tree</cell><cell></cell><cell>0.786</cell><cell>0.769</cell><cell>0.821</cell><cell>0.794</cell></row><row><cell>SVM</cell><cell>sigmoid, gamma=1.0</cell><cell>0.872</cell><cell>0.869</cell><cell>0.880</cell><cell>0.874</cell></row><row><cell cols="2">Multinomial NB alpha=0.2</cell><cell>0.876</cell><cell>0.863</cell><cell>0.902</cell><cell>0.882</cell></row><row><cell>XGBoost</cell><cell></cell><cell>0.720</cell><cell>0.753</cell><cell>0.777</cell><cell>0.813</cell></row><row><cell>XLNet</cell><cell>XLNet-Large-Cased</cell><cell>0.916</cell><cell>0.872</cell><cell>0.973</cell><cell>0.920</cell></row><row><cell>Proposed</cell><cell></cell><cell>0.982</cell><cell>0.990</cell><cell>0.974</cell><cell>0.982</cell></row><row><cell cols="2">numerical word representations with some optimization</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>on hyper-parameters.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3) Multinomial naïve Bayes: The model is suited when</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">we deal with discrete integer features, such as word</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">counts in a text. Here, we used CountVectorizer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">to generate numerical word representations.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4) XGBoost: XGBoost is the latest step in the evolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">of tree-based algorithms that include decision trees,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">boosting, random forests, boosting and gradient boost-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ing. It is an optimized distributed gradient boosting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">that provides fast and accurate results, which achieves</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accurate results in less time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Mostly from /r/jokes and /r/cleanjokes subreddits.<ref type="bibr" target="#b2">3</ref> All words are capitalized, except non-initial articles like "a, the, and", etc.<ref type="bibr" target="#b3">4</ref> Capitalization as in a standard English sentence, e.g., "Witchcraft is real.".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The dataset is available at: https://github.com/Moradnejad/ColBERT-Using-BERT-Sentence-Embedding-for-Humor-Detection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The punctuation marks are: period, comma, question mark, hyphen, dash, parentheses, apostrophe, ellipsis, quotation mark, colon, semicolon, exclamation point.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is this a joke? detecting humor in spanish tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cubero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moncecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ibero-American Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making social robots more attractive: the effects of voice pitch, humor and empathy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>See</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of social robotics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="191" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computationally recognizing wordplay in jokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Mazlack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Humor: Prosody analysis and automatic recognition for f* r* i* e* n* d* s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purandare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Humor recognition and humor anchor extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2367" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Humor recognition using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-W</forename><surname>Soo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="113" to="117" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Humor detection: A transformer gets the last laugh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seppi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00252</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting humor by learning from time-aligned comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="496" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of haha at iberlef 2019: Humor analysis based on human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chiruzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Etcheverry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Iberian Languages Evaluation Forum</title>
		<meeting>the Iberian Languages Evaluation Forum<address><addrLine>Bilbao, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings, CEUR-WS</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Humor analysis based on human annotation challenge at iberlef 2019: First-place solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ismailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Iberian Languages Evaluation Forum</title>
		<meeting>the Iberian Languages Evaluation Forum<address><addrLine>Bilbao, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aspie96 at haha (iberlef 2019): Humor detection in spanish tweets with character-level convolutional rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Giudice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Iberian Languages Evaluation Forum</title>
		<meeting>the Iberian Languages Evaluation Forum<address><addrLine>Bilbao, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings, CEUR-WS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Humor detection in english-hindi code-mixed social media content: Corpus and baseline system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05513</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting audience&apos;s laughter during presentations using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 12th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ur-funny: A multimodal language dataset for understanding humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06618</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning of audio and language features for humor prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="496" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Making computers laugh: Investigations in automatic humor recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kritik der urteilskraft. Meiner</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The appreciation of humour: an experimental and theoretical study 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Eysenck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology. General Section</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="309" />
			<date type="published" when="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A two-stage model for the appreciation of jokes and cartoons: An information-processing analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Suls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The psychology of humor: Theoretical perspectives and empirical issues</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="100" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semantic mechanisms of humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decision tree methods: applications for classification and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shanghai archives of psychiatry</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">130</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
