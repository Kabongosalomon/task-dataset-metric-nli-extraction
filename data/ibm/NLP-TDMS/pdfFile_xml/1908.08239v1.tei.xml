<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Face Super-Resolution via Attention to Facial Landmark Dae-Shik Kim</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deokyun</forename><surname>Kim</surname></persName>
							<email>deokyunkim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseon</forename><surname>Kim</surname></persName>
							<email>minseonkim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gihyun</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Face Super-Resolution via Attention to Facial Landmark Dae-Shik Kim</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>D. KIM, M. KIM, G. KWON ET AL.: PROGRESSIVE FACE SUPER-RESOLUTION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face Super-Resolution (SR) is a subfield of the SR domain that specifically targets the reconstruction of face images. The main challenge of face SR is to restore essential facial features without distortion. We propose a novel face SR method that generates photo-realistic 8× super-resolved face images with fully retained facial details. To that end, we adopt a progressive training method, which allows stable training by splitting the network into successive steps, each producing output with a progressively higher resolution. We also propose a novel facial attention loss and apply it at each step to focus on restoring facial attributes in greater details by multiplying the pixel difference and heatmap values. Lastly, we propose a compressed version of the state-of-the-art face alignment network (FAN) for landmark heatmap extraction. With the proposed FAN, we can extract the heatmaps suitable for face SR and also reduce the overall training time. Experimental results verify that our method outperforms state-of-the-art methods in both qualitative and quantitative measurements, especially in perceptual quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face Super-Resolution (SR) is a domain-specific SR which aims to reconstruct High Resolution (HR) face images from Low Resolution (LR) face images while restoring facial details. When enlarging the LR face images to high-resolution images, the HR images suffer from face distortion. The finer details of faces disappear incurring misperception of facial attributes on faces. In an attempt to address this problem, the previous studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> embedded additional facial attribute vectors into network feature maps to reflect facial attributes in super-resolved face images. These approaches require prior information for face SR; however, the additional information is difficult to obtain in the wild. Other studies incorporate facial landmark information by employing auxiliary networks such as face alignment network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>, and prior estimation network <ref type="bibr" target="#b4">[5]</ref>. However, these approaches tend to concentrate on the localization of facial landmarks without sufficient consideration of the facial attributes in the areas around landmarks.</p><p>Different from the previous works, we propose a face SR method which restores original facial details more precisely by giving strong constraints to the landmark areas. To stably generate photo-realistic 8× upscaled images, we adopt a progressive training method <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> which grows both generator and discriminator progressively. We also introduce a novel facial attention loss which makes our SR network to restore the accurate facial details. The attention loss is applied in both the intermediate and the last step of our progressive training.</p><p>Constraining the outputs by applying the attention loss at each step, the output images of each step reflect more accurate facial details. To obtain the attention loss, we extract the heatmaps from the pre-trained Face Alignment Network (FAN). The extracted heatmaps are used as weights of the pixel difference of the adjacent areas to the landmarks. Instead of using the state-of-the-art FAN <ref type="bibr" target="#b3">[4]</ref>, we suggest a compressed network of FAN, called distilled FAN, which is trained by a hint-based method <ref type="bibr" target="#b16">[17]</ref>. The distilled FAN delivers comparable performance to the original FAN while being much more compact. With our approach, we obtain SR-oriented landmark heatmaps as well as significantly reduce the overall training time. Therefore, our method generates super-resolved face images which successfully reflect accurate details of facial components.</p><p>For the evaluation, we measure the performance of our method on both aligned and unaligned face images from CelebA <ref type="bibr" target="#b13">[14]</ref> and AFLW <ref type="bibr" target="#b9">[10]</ref> datasets. To compare the quality of our results, we calculate the conventional measurements of the average Peak Signal to Noise Ratio (PSNR), Structural SIMilarity (SSIM) <ref type="bibr" target="#b21">[22]</ref>, and Multi-Scale Structural SIMilarity (MS-SSIM) <ref type="bibr" target="#b20">[21]</ref>. By conducting an ablation study, we verify that the proposed loss function is beneficial to super-resolving LR face images; we demonstrate the superiority of our method by comparing the results with those of previous studies. We further conduct Mean-Opinion-Score (MOS) <ref type="bibr" target="#b11">[12]</ref> test to measure the perceptual quality. The experimental results show that our network successfully generates high-fidelity face images, accurately preserving the original features around the facial landmarks. In summary, our contributions are as follows:</p><p>1. To the best of our knowledge, progressive training method is used in natural image SR, but this is the first method which leverages the progressive training method for face SR. We give constraints to each step of the SR network and generate high-quality face images reflecting details of facial components. 2. Facial attention loss makes the SR network learn to restore facial details with the method of focusing on the adjacent area of facial landmarks, which is verified by our super-resolved results. 3. We compress the state-of-the-art FAN into a smaller network using hint-based method.</p><p>With the distilled FAN, we are able to extract meaningful landmark heatmaps which are more suitable for a face SR task and reduce the overall training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Utilizing facial information, such as facial attributes and spatial configuration of facial components, is the key factor in face SR. Yu et al. <ref type="bibr" target="#b25">[26]</ref> interweave multiple spatial transformer networks to satisfy the requirement of face alignment as well as embeds facial attribute vec- tors to lower the ambiguity in facial attributes. Lee et al. <ref type="bibr" target="#b12">[13]</ref> fuses the information of both image domain and attribute domain in order to reflect facial attributes in super-resolved images. These methods preserve facial attributes indicated by facial attribute vectors. However, attribute vectors are not only difficult to acquire in the wild but also limited to describing partial facial attributes. While preserving the facial landmarks location, Chen et al. <ref type="bibr" target="#b4">[5]</ref> propose the face shape prior estimation network, which provides a solution for accurate geometry estimation obtained from coarse HR face images. Yu et al. <ref type="bibr" target="#b24">[25]</ref> estimate the spatial position of their facial components to preserve the original spatial structure in upscaled face images. In addition, Bulat et al. <ref type="bibr" target="#b0">[1]</ref> propose a heatmap loss to localize landmarks in super-resolved images so as to upscale input face images 4×. Although these methods preserve the spatial configuration of facial components, they fail to fully reflect accurate facial attributes. In contrast to the previous works, our method carefully considers the facial attributes around the landmarks to restore the facial details without prior information as well as preserves landmark location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we describe our methods for the enhanced face SR. To generate the highfidelity super-resolved face images that reflect the facial attributes of target face images, three main approaches are used: progressive training, facial attention loss, and distillation of Face Alignment Network (FAN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Progressive Face SR Network</head><p>The overview of our network architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. To incorporate the adversarial loss, our architecture is composed of the generator network, which is our face SR network, and the discriminator network. To train the generator and the discriminator stably, we construct both the networks which consist of layers stacked by steps. Our generator network consists of three residual blocks <ref type="bibr" target="#b5">[6]</ref> with batch normalization layers (BatchNorm), transpose convolution layers (ConvTrans), and Rectifier Linear Unit (ReLU) as an activation function. The discriminator network has a corresponding architecture to the generator network, which is comprised of convolution layers (Conv), average pooling layers (AvgPool), and Leaky ReLU. To improve the discriminator performance, we calculate the standard deviation of input batch, then replicate the value into a one-dimensional feature map, and concatenate it to the end of the discriminator <ref type="bibr" target="#b18">[19]</ref>. We use additional convolution layers in each step in order to convert the intermediate feature maps into RGB images, and vice versa.</p><p>In Step 1, each network employs one block for training and learns to upscale images 2×. These 2× upscaled outputs from the generator go through the corresponding part of the discriminator, and the outputs are then compared with target images. In Step 2, the 2× upscaled outputs from Step 1 are upscaled 2× again by nearest-neighbor interpolation, and then the interpolated outputs are added to 4× upscaled outputs from Step 2. This process is expressed as follows:</p><formula xml:id="formula_0">(1 − α) * f (G N−1 (I)) + α * G N (I),</formula><p>where G is our SR network, f is nearest neighbor (NN) interpolation, I and N ∈ {2, 3} denote input images and number of step, respectively. A weight scale α increases linearly from zero to one. The upscaled outputs are compared to the corresponding target images through the discriminator. The same procedure above is implemented in Step 3 (8×). The method allow the network learn super-resolving face images with different loss in each step effectively and stably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Facial Attention Loss</head><p>We propose the facial attention loss to restore the attributes of the adjacent area to the facial landmarks. This facial attention loss makes the face SR network focus on the facial details around the predicted landmark area by element-wise multiplying landmark attention heatmaps M * , and the L1 distance between the upscaled images and the corresponding target images. To achieve this, we employ facial landmark heatmaps which contain landmark location information. The facial attention loss is defined as:</p><formula xml:id="formula_1">L attention = 1 r 2 W H rW ∑ x=1 rH ∑ y=1 (M * x,y · |I HR x,y − G(I LR ) x,y |)<label>(1)</label></formula><p>where G is face SR network, I HR and I LR are target face images and input LR images, respectively. The landmark attention heatmap M * is channel-wise max values of the target heatmap M generated from target face images. To compensate for the variance between the landmarks, the heatmap M is min-max normalized into [0,1]. The heatmap M has the dimension of N × rW × rH, where N is number of landmarks, W and H are width and height of the input image. The upscale factor r is set to be 4 and 8 in Step 2 and Step 3, respectively. To give attention at images with enough information, we adjust facial attention loss at upscaled outputs with size of 64×64 and 128×128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distilled Face Alignment Network</head><p>The state-of-the-art FAN <ref type="bibr" target="#b3">[4]</ref> predicts the location of all landmarks including occluded landmarks exploiting multiple-scale feature maps from four-stacked Hourglass architecture which consists of encoder-decoder and skip-layer <ref type="bibr" target="#b14">[15]</ref>. This approach predicts landmark locations based on heatmap values, which are highly concentrated around the landmark points. However, for face SR, we aim to give attention to the overall facial landmark area in the images except for the occluded landmark area. To generate heatmaps suitable for giving attention to accurate facial landmark area, we construct the network with neither encoder-decoder architecture nor skip-layer so as to predict landmarks based on single-scale feature maps. Also, in order to reduce overall training time and achieve comparable performance to state-of-the-art FAN, we compress the FAN into the network shown in <ref type="figure" target="#fig_1">Figure 2</ref> based on a hintbased training method <ref type="bibr" target="#b16">[17]</ref>. We train the distilled FAN to minimize ∑ (F d (I) − F t (I)) 2 , where F d and F t represents our distilled FAN and original FAN, I denotes input face images. This approach has two advantages: it provides face SR-oriented heatmaps, and it reduces the overall training time from ∼ 3 days to ∼ 1 days in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall Training Loss</head><p>MSE loss We use the pixel-wise Mean-Square-Error (MSE) loss to minimize the distance between the HR target image and the super-resolved image.</p><formula xml:id="formula_2">L pixel = 1 r 2 W H rW ∑ x=1 rH ∑ y=1 I HR x,y − G I LR x,y 2<label>(2)</label></formula><p>Perceptual loss A perceptual loss <ref type="bibr" target="#b6">[7]</ref> is proposed to prevent generating blurry and unrealistic face images, and to obtain more realistic HR images. The loss over the pre-trained VGG16 <ref type="bibr" target="#b17">[18]</ref> features at a given layer i is defined as:</p><formula xml:id="formula_3">L f eat/i = 1 W i H i W i ∑ x=1 H i ∑ y=1 φ i I HR x,y − φ i G I LR x,y 2<label>(3)</label></formula><p>where φ i denotes the feature map obtained after the last convolutional layer of the i−th block. Adversarial loss We use the WGAN Loss <ref type="bibr" target="#b2">[3]</ref> to stabilize the training process. In WGAN, the loss function is defined as the Wasserstein distance between the distribution of target I HR ∼ P r and those of the generated imagesĨ ∼ P g . For further improvement of training stability, we apply the Gradient Penalty term proposed in WGAN-GP <ref type="bibr" target="#b22">[23]</ref>, which enforces the Lipschitz -1 condition of the discriminator.Î is a randomly sampled image among the samples from P r and P g . Therefore, the loss function is as follows:</p><formula xml:id="formula_4">L W GAN = E I HR ∼P r [D(I HR )] − EĨ ∼P g [D(Ĩ)] + λ EÎ ∼PÎ [||∇ÎD(Î) 2 − 1|| 2 ]<label>(4)</label></formula><p>Heatmap Loss As proposed by <ref type="bibr" target="#b0">[1]</ref>, the heatmap loss improves the structural consistency of face images by minimizing the distance between the heatmaps of both generated images and target ones. The heatmap loss function is described as:</p><formula xml:id="formula_5">L heatmap = 1 r 2 NW H N ∑ n=1 rW ∑ x=1 rH ∑ y=1 (M n x,y −M n x,y ) 2<label>(5)</label></formula><p>where N is the number of landmarks, M andM are calculated as M = F d (I HR ) andM = F d (G I LR ).  Overall training loss Since the landmark losses are applied to Step 2 &amp; 3, we intend not to include the L heatmap and L attention in Step 1. The final loss term is shown as:</p><formula xml:id="formula_6">L Ours = αL pixel + β L f eat + γL W GAN , at step 1 L Ours = αL pixel + β L f eat + γL W GAN + λ L heatmap + ηL attention , at step 2 &amp; step3<label>(6)</label></formula><p>where α, β , γ, λ and η are the corresponding weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Datasets In our experiments, we use two different datasets: aligned dataset and unaligned one. The aligned CelebA dataset <ref type="bibr" target="#b13">[14]</ref> is used to test how accurately the facial details can be restored. The unaligned CelebA and AFLW <ref type="bibr" target="#b9">[10]</ref> datasets are used to verify the applicability of our face SR network in real world. The aligned face images are cropped into square. The face images of the unaligned dataset are cropped based on the bounding box areas. The cropped images are resized into 128×128 pixels to be used as targets of <ref type="bibr">Step</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distilled FAN Results</head><p>In this section, we compare our distilled FAN to the original FAN <ref type="bibr" target="#b3">[4]</ref>. To verify how similarly the distilled FAN predicts landmark location compared to the original FAN, we use Normalized Mean Error (NME) metric <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref> between the predicted landmark locations from the distilled and the original FAN. The NME is calculated as NME = 1 N ∑ N k=1 g k −p k 2 d , where g denotes the landmark from original FAN, p is the corresponding prediction from the distilled model, and d is the facial image size. The NME evaluation results and the number of parameters are shown in <ref type="table" target="#tab_2">Table 1</ref>. The results show that our distilled FAN predicts facial landmarks with comparable performance, and it has much fewer parameters than the original FAN.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3(b)</ref> and (c), the output heatmaps of the original FAN have high values around the landmark points even in the occluded area, but the output heatmaps of distilled FAN have relatively low values in the occluded landmark area. The heatmaps of our distilled FAN are suitable for facial attention weights. <ref type="figure" target="#fig_3">Figure 4</ref>(a) shows the problem of using the heatmap from the original FAN as attention weights. There is no significant distortion in facial attributes, but it leads to some artifacts because the attention is applied only to the distinct points of facial landmarks. As shown in <ref type="table" target="#tab_2">Table 1</ref>, the distilled FAN improves SR performance with a small number of parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In order to observe the effects of each element of our method, we conduct an ablation study using the conventional measurements of the average PSNR, SSIM, and MS-SSIM; Minimizing the MSE maximize the PSNR, which is commonly used to evaluate the SR results. Since PSNR is defined based only on pixel-wise differences, the value of PSNR has limitation to represent perceptually relevant differences <ref type="bibr" target="#b11">[12]</ref>. Therefore, we further measure SSIM and MS-SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of loss functions</head><p>We conduct three experiments to estimate the effect of perceptual loss, heatmap loss, and proposed facial attention loss on the aligned dataset and the unaligned dataset. <ref type="figure" target="#fig_4">Figure 5</ref> shows the results of using different loss functions on both aligned and unaligned datasets. The result images without perceptual loss have severely deteriorated texture of face images. Moreover, the result images without heatmap loss have unclear shapes and distortion around the eyes and mouths. As the facial attention loss uses the landmark heatmaps as guidance, it is helpful for face images to restore facial details. As shown in <ref type="table" target="#tab_4">Table 2</ref>, using our facial attention loss shows the highest value in PSNR, SSIM and MS-SSIM. These results verify that our facial attention loss is helpful to generate more structurally meaningful face images. Effects of progressive training To verify that the progressive training method is helpful to super-resolve face images, we train our face SR network without the progressive training   method. The qualitative comparison of outputs is shown in <ref type="figure" target="#fig_4">Figure 5(d)</ref>. There is some degradation of facial details in super-resolved images from our non-progressively trained network, which is trained by minimizing L ours . As shown in <ref type="table" target="#tab_4">Table 2</ref>, the measurement values are also increased by progressive training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-Art</head><p>We compare our face SR method to the state-of-the-art SR methods both quantitatively and qualitatively. VDSR <ref type="bibr" target="#b7">[8]</ref> employs a pixel-wise L2 loss in training. FSRNet and FSRGAN <ref type="bibr" target="#b4">[5]</ref> employ a facial parsing map in training. URDGN <ref type="bibr" target="#b23">[24]</ref> employs a spatial convolution layer and a discriminator. <ref type="figure" target="#fig_5">Figure 6</ref> provides results of various models, and <ref type="table" target="#tab_6">Table 3</ref> presents the quantitative comparisons on the test set. The images from VDSR achieve the highest PSNR, but the results   We also conduct a MOS test to quantify image quality based on human vision. We asked 26 raters to assign a score from 1 (bad quality) to 5 (excellent quality) to all the superresolved images and the high-resolution target images. The raters were calibrated on the 20 images of Nearest Neighbor (score 1) and HR (score 5) before the main test. The raters rated 8 versions of each image on aligned and unaligned dataset: Nearest neighbor, Bilinear, URDGN, FSRNet, FSRGAN, VDSR, Ours, and HR images (GT). Each rater rated randomly presented 240 images with each dataset (total 480 images). More details are explained in the  <ref type="figure" target="#fig_6">Figure 7</ref>, our method shows overwhelming performance on MOS test, which indicates that our results are perceptually superior to other results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel face SR method which fully reflects facial details. To achieve this, we adopt progressive training method to generate photo-realistic face images and learn restoration of facial details with different guidance in each step. In addition, we propose a new facial attention loss which gives large weights to facial features in the adjacent area of landmarks. Therefore, the facial details are well expressed in super-resolved images. However, the original FAN produces landmark heatmaps including occluded landmark area, which results in degradation of super-resolution performance. Therefore, we suggest distillation of face alignment network to produce more suitable heatmaps for the SR. Besides, our distilled face alignment network has relatively light-weight architecture, so the overall training time is reduced from ∼ 3 days to ∼ 1 day. Our experiments demonstrate that our proposed method restore more accurate facial details. In particular, our method produces high-quality face images which are perceptually similar to the real images. As a summary, the proposed method allows our face SR network to super-resolve face images with more precise facial details.</p><p>We give attention to specific areas on the faces and propose a method to obtain the heatmaps suitable for face SR. If a better method is developed to obtain the heatmaps which well represent facial landmark areas, we will be able to achieve even better performance through our proposed method. Since our approach restores lost information by focusing on specific areas, we will further be able to restore the desired information by applying our mechanism to any task, such as super-resolution on the medical image, satellite image, and microscopic image, which requires restoration of lost information using super-resolution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our network architecture overview. (k : kernel size, n : output channel, s : stride)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Distilled face alignment network hint based training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Distilled FAN results(Ours) comparison with FAN results<ref type="bibr" target="#b3">[4]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Our image results (a) with the original FAN, (b) with the distilled FAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Ablation study results on aligned and unaligned datasets.(a) L pixel + L W GAN (b) L pixel + L W GAN + L f eat (c) L pixel + L W GAN + L f eat + L hm (d) L Ours -no progressive (e) L Ours .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison with aligned and unaligned datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>MOS result with aligned and unaligned datasets Supplementary Materials. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>This work was supported by Engineering Research Center of Excellence (ERC) Program supported by National Research Foundation (NRF), Korean Ministry of Science &amp; ICT (MSIT) (Grant No. NRF-2017R1A5A1014708), Institute for Information &amp; communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No.2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion) and Hyundai Motor Company through HYUNDAI-TECHNION-KAIST Consortium.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3, and bilinearly downsampled into 64×64 pixels as targets of Step 2, 32×32 pixels as targets of Step 1, and 16×16 pixels as LR inputs. We use all 162,770 images as a training set, and 19,867 images as a test set from aligned CelebA dataset. For the unaligned dataset, we use 80,000 cropped images from unaligned CelebA, and 20,000 from AFLW for training. As a test set, 5,000 images from CelebA and 4,384 images from AFLW are used.</figDesc><table /><note>Training details We implement our face SR network using PyTorch [16]. We train our networks using the Adam optimizer [9] with a learning rate of 1 × 10 −3 , and the mini-batch size of 16. The training iteration of each step is set by hyperparameter. In our model, we train our model 50K, 50K and 100K iterations, empirically. Running totally 200K iterations takes ∼1 day on single Titan X GPU. In addition, we train the distilled FAN using the Adam optimizer with a learning rate of 1 × 10 −4 , mini-batch size of 16, and 100K iterations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Parameters, NME evaluation, PSNR, SSIM, and MS-SSIM comparison results</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>PSNR, SSIM and MS-SSIM values for the ablation study results on aligned and unaligned datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>PSNR, SSIM and MS-SSIM values for the baseline experimental results on aligned and unaligned datasets. are significantly blurred. As we can see, the results of FSRNet and FSRGAN have realistic features in facial details, but they have artifacts and partially blurred facial components. The URDGN produces relatively clear images but generates distorted face images. The results show that our method outperforms other methods especially on SSIM and MS-SSIM, and generates photo-realistic face images with restoring accurate facial attributes. More image results are shown in the Supplementary Materials.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Super-fan: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image super-resolution via progressive cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fsrnet: End-to-end learning face super-resolution with facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attribute augmented convolutional neural network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Samuli Laine Jaakko Lehtinen Tero Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fully progressive approach to single-image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-scale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving the improved training of wasserstein gans: A consistency term and its dual effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Murat Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face superresolution guided by facial component heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Super-resolving very low-resolution face images with supplementary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
