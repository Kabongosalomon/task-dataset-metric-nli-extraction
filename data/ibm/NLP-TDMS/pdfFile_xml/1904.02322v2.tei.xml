<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modified Distribution Alignment for Domain Adaptation with Pre-trained Inception ResNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youshan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">Lehigh University Bethlehem</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
							<email>davison@cse.lehigh.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">Lehigh University</orgName>
								<address>
									<settlement>Bethlehem</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modified Distribution Alignment for Domain Adaptation with Pre-trained Inception ResNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain Adaptation</term>
					<term>Pre-trained Inception ResNet</term>
					<term>Distribution Alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have been widely used in computer vision. There are several well trained deep neural networks for the ImageNet classification challenge, which has played a significant role in image recognition. However, little work has explored pre-trained neural networks for image recognition in domain adaption. In this paper, we are the first to extract better-represented features from a pre-trained Inception ResNet model for domain adaptation. We then present a modified distribution alignment method for classification using the extracted features. We test our model using three benchmark datasets (Office+Caltech-10, Office-31 and Office-Home). Extensive experiments demonstrate significant improvements (4.8%, 5.5%, and 10%) in classification accuracy over the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the rapid development of social media and content sharing applications, data grows much faster than we can make sense of it. There is great demand for automatic classification and analysis for text, images, and other multimedia data <ref type="bibr" target="#b0">[1]</ref>. However, it is time-consuming and expensive to acquire enough labeled data to train models. Therefore, it is valuable to learn a model for a new target domain from a different domain with abundant labeled samples. Mechanisms for learning feature representations of a continuous intermediate space from one domain to another has been widely used in many fields such as machine learning <ref type="bibr" target="#b1">[2]</ref>, language processing <ref type="bibr" target="#b2">[3]</ref>, and computer vision <ref type="bibr" target="#b3">[4]</ref>. There are several techniques to address this problem; a prominent one is domain adaption <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. There have been efforts for both semi-supervised <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and unsupervised <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> domain adaptation. In the first case, the target domain contains a small amount of labeled data; for the latter case, the target domain is entirely unlabeled. Usually the labeled target data alone is insufficient to construct a good classifier. Thus, how to effectively leverage sufficient label source data to facilitate unlabeled target data is key to domain adaptation.</p><p>However, a critical challenge remains: to find and identify useful features that span the representations of two domains.</p><p>The quality of such features will directly affect classification accuracy. We cannot expect to train a high-quality classifier if the learned features are poor. Therefore, it is essential to find a proper way to represent the source and target data. One useful working model for feature representation is based on manifold learning, which learns the intermediate features between the source and the target domain via a Grassmannian manifold. Gopalan et al. <ref type="bibr" target="#b3">[4]</ref> proposed a sampling geodesic flow (SGF) method to learn the intermediate features between the source and the target domain via the geodesic (shortest path) on Grassmannian manifold. However, Gong et al. <ref type="bibr" target="#b4">[5]</ref> have noted that it is difficult to choose an optimal sampling strategy. Moreover, SGF has high time complexity making sampling slow when many points are needed. Gong et al. <ref type="bibr" target="#b4">[5]</ref> proposed a geodesic flow kernel (GFK) model to overcome the limitations of unknown sampling size in SGF. They integrated all samples along the "geodesic", which is calculated from Gopalan et al. <ref type="bibr" target="#b3">[4]</ref>. We show that the "geodesic" is not the true geodesic. Several works have addressed the alignment of marginal distribution and conditional distribution of data in domain adaption. Wang and Mahadevan aligned the source and target domain by preserving the neighborhood structure of the data points <ref type="bibr" target="#b13">[14]</ref>. Wang et al. proposed a manifold embedding distribution alignment method (based on work of Gong et al. <ref type="bibr" target="#b4">[5]</ref>) to align both the degenerate feature transformation and the unevaluated distributions of both domains <ref type="bibr" target="#b14">[15]</ref>. However, none of these models explore the quality of the learned features.</p><p>Deep learning models are also widely applied to domain adaptation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Stacked Denoising Autoencoders is one of the first deep models for domain adaptation, and aims to find the common features between the source and target domain via denoising autoencoders <ref type="bibr" target="#b21">[22]</ref>. The deep neural network for domain adaptation can be majorly classified in four types: discrepancy-based methods, adversarial discriminative models, adversarial generative models, and data reconstruction-based models. One of the first discrepancy-based methods is Deep Domain Confusion (DDC), which considers the discrepancy in different layers and the network is fine-tuned based on maximum mean discrepancy (MMD) <ref type="bibr" target="#b6">[7]</ref>. Later Long et al. <ref type="bibr" target="#b22">[23]</ref> proposed a Deep Adaptation Network (DAN) that considered the sum of MMD from several layers with several kernels of MMD functions. The Domain adaptive neural network also embedded MMD as a regularization <ref type="bibr" target="#b23">[24]</ref>. Adversarial discriminative based models aim to define a domain confusion objective to identify the domains via a domain discriminator. The Domain-Adversarial Neural Networks (DANN) consider a minimax loss to integrate a gradient reversal layer to promote the discrimination of source and target domain <ref type="bibr" target="#b24">[25]</ref>. The Adversarial Discriminative Domain Adaptation (ADDA) uses an inverted label GAN loss to split the source and target domain, and features can be learned separately <ref type="bibr" target="#b16">[17]</ref>. The adversarial generative models combine the discriminative model with generative components based on Generative Adversarial Networks (GANs) <ref type="bibr" target="#b25">[26]</ref>. The Coupled Generative Adversarial Networks <ref type="bibr" target="#b26">[27]</ref> consists of a series of GANs, and each of them can represent one of the domains. Data reconstruction-based methods jointly learn source label predictions and unsupervised target data reconstruction <ref type="bibr" target="#b27">[28]</ref>.</p><p>However, training of deep neural networks consume time and require much effort to tune the parameters. We are inspired by Zhang et al. <ref type="bibr" target="#b28">[29]</ref>, which extracted features from the well-trained Alexnet, and then trained an SVM using the deep features to facilitate improvements in classification accuracy. Also, other work indicated that the features extracted from the activation layers of a well-trained deep neural network could be re-used for different tasks even when the new tasks are different from the original tasks used to train the model <ref type="bibr" target="#b29">[30]</ref>.</p><p>In this paper, we first extract features from a well-trained Inception ResNet-v2 (IR) model; we then classify these features based on a modified distribution alignment. Our contributions are three-fold: 1) We create three datasets for domain adaptation based on better extracted features, which can be of significant value in future research for the community. 2) We show the shortcomings of the original manifold embedded distribution alignment method, and propose a modified distribution alignment for classification, which enhances the accuracy for classification. 3) We test these improvements using three benchmark datasets. Extensive experiments demonstrate significant improvements (4.8%, 5.5%, and 10%) in classification accuracy over the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM STATEMENT</head><p>To avoid the complex and time-consuming process of handtuning parameters for training a deep neural network, we present the extraction of features from a well-trained deep neural network, so that we are able to learn a better feature representation of source and target domain data. Also, we want to further align the distribution from both source and target domain.</p><p>Given training data (source domain): X S , with its labels</p><formula xml:id="formula_0">Y S = {y i } N1 i=1 ∈ {1, 2, 3</formula><p>, · · · , C}, denoting the C categories, and the test data (target domain):</p><formula xml:id="formula_1">X T with its labels Y T = {y i } N2 i=1 ∈ {1, 2, 3</formula><p>, · · · , C} and N 2 ≤ N 1 , that implies that we might not have all labels for testing data. If N 2 = N 1 , which means we have sufficient labels for X T , we aim to get a higher predictive accuracy. If N 2 &lt; N 1 , we not only want to get a high enough predictive accuracy, but also to predict the labels for the unlabeled data. We have two concerns: 1) how to generate better source X S and target X T features for the image recognition problem; 2) how to improve prediction accuracy using the features of step 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction</head><p>Feature extraction is a relatively easy and fast way to take advantage of deep learning without investing time and much effort into training a full neural network. Feature extraction will be especially useful if we do not use GPUs since it only requires a single pass over the input images. Kornblith et al. indicated that ResNets are often the best feature extractors, independently of their ImageNet accuracies <ref type="bibr" target="#b30">[31]</ref>. In this paper, we use Inception-ResNet-v2 as the pre-trained model from which to extract features. Inception-ResNet-v2 is a powerful convolution neural network, which is trained on more than one million images from the ImageNet datasets. This network consists of 164 layers (the largest number of convolutional and fully connected layers from the input layer to the output layer). IR model can predict 1000 categories of images, such as cup, smart phone, backpack, and many animals. Therefore, IR model has learned rich feature representations with a wide range of images. The image input size of IR model is 299-by-299-by-3. Please refer to <ref type="bibr" target="#b31">[32]</ref> for details of Inception-ResNet-v2 model.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we compare the number of parameters and top-1 accuracy of several well-trained deep neural networks (SqueezeNet <ref type="bibr" target="#b32">[33]</ref> , AlexNet <ref type="bibr" target="#b33">[34]</ref>, VGG16 <ref type="bibr" target="#b34">[35]</ref>, VGG19 <ref type="bibr" target="#b34">[35]</ref>, GoogLeNet <ref type="bibr" target="#b35">[36]</ref>, ResNet18 <ref type="bibr" target="#b36">[37]</ref>, ResNet50, ResNet101, ResNet152 <ref type="bibr" target="#b36">[37]</ref>, DenseNet201 <ref type="bibr" target="#b37">[38]</ref>, Inceptionv3 <ref type="bibr" target="#b38">[39]</ref>, Inception-Resent-V2 <ref type="bibr" target="#b31">[32]</ref>). There are two essential reasons why we choose the IR model as the deep neural network to extract features. First, the top-1 accuracy of Inception-ResNet-v2 model is higher than other models. Secondly, the IR model uses fewer parameters compared with several lower accuracy networks (e.g. VGG-16).</p><p>We assume that extracted features from the IR model contain more detailed information than other features, which will enable a classifier to achieve higher accuracy. We then compare extracted IR features with three commonly used sets of features (SURF, Resnet-50, and DeCAF), which is shown in Sec. IV-B. In addition, the extracted features from different layers will have different effects on final recognition results, which is also shown in Sec. IV-B.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distribution Alignment</head><p>To train a robust classifier for features, which were extracted in the previous section, we perform dynamic distribution alignment to quantitatively account for the relative importance of marginal and conditional distribution to address the challenge of unevaluated distribution alignment.</p><p>Manifold Embedded Distribution Alignment (MEDA) is proposed by Wang et al. <ref type="bibr" target="#b14">[15]</ref> to align learned features from manifold learning. It has three fundamental steps: 1) learn features from the manifold based on Gong et al. <ref type="bibr" target="#b4">[5]</ref>; 2) use dynamic distribution alignment to estimate the marginal and conditional distributions of data; and, 3) update the classified labels based on estimated parameters. Please refer to Wang et al. <ref type="bibr" target="#b14">[15]</ref> for more details. The classifier (f r) is defined as:</p><formula xml:id="formula_2">f r = arg min f r∈H k N1 i=1 l(f r(g(X Si )), Y Si ) + η||f r|| 2 K + λD f r (X S , X T ) + ρR f r (X s , X T )<label>(1)</label></formula><p>where H k represents kernel Hilbert space; l(·, ·) is the loss function; g(·) is a feature learning function in Grassmannian manifold <ref type="bibr" target="#b4">[5]</ref>; X S is the learned features from IR model, ||f r|| 2 K is the squared norm of f r; D f r (·, ·) represents the dynamic distribution alignment; R f r (·, ·) is a Laplacian regularization; η, λ, and ρ are regularization parameters. Here, the term arg</p><formula xml:id="formula_3">min f r∈H k N1 i=1 l(f r(g(X Si )), Y Si ) + η||f r|| 2 K</formula><p>is the structure risk minimization (SRM). We can only employ the SRM on X S , since there are few labels (perhaps no labels) for X T . By training the classifier from Eq. 1, we can predict labels of test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Weaknesses of MEDA</head><p>The first step of the MEDA method is learning the kernel mapping g(·) from Grassmannian manifold based on GFK model. However, the calculation of "geodesic" in GFK model is originally from SGF method, which is a unevaluated geodesic <ref type="bibr" target="#b3">[4]</ref>. Then GFk considered all samples points on "geodesic" for constructing a kernel function. It is a "kernel trick"; but it cannot maintain the true information from a manifold since geodesic is not correctly estimated. We design two experiments to show the defects of GFK.</p><p>Given two points P 1 and P 2 on the sphere, we want to recover all other points between them. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, sampled points of the SGF method (yellow curve is not able to recover the true points on a geodesic (cyan curve). Therefore, the GFK model will lose feature information if it integrates all pseudo samples from wrong geodesic (yellow curve), which is calculated using the SGF method.</p><p>We design another experiment to show shape deformation using SGF model. As shown in <ref type="figure" target="#fig_7">Fig. 5</ref>, the source image is a square (the leftmost of <ref type="figure" target="#fig_7">Fig. 5a</ref>), and the target image is a circle (the rightmost of <ref type="figure" target="#fig_7">Fig. 5a</ref>). The progress of sampled images of the SGF model are shown in <ref type="figure" target="#fig_7">Fig. 5b</ref>. To evaluate  However, the sampled images of SGF model are far from the source and target images when t = 0.05 and t = 0.95, respectively. There are two issues in the sampled images of the SGF method: first, its background is dark; this is caused by the Log map not being correctly calculated in Gopalan et al. <ref type="bibr" target="#b3">[4]</ref> (there are some negations of the estimated velocity v between the source image and target image). The second is that the shape is never unified, and this is caused by the Exp does not approach the target at t = 1 <ref type="bibr" target="#b3">[4]</ref>.</p><p>The second shortcoming of GFK is that the dimensionality is difficult to determine. The first step of GFK is to project the original source and target data into a subspace since the number of instances in the original space is not the same (N 2 ≤ N 1 ). The reduced dimensionality will lead to information loss of original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Modified Distribution Alignment</head><p>To resolve the issues mentioned above, we use the original features instead of features from the GFK model. These are two essential reasons: 1) we want to maintain the  Obviously, the SGF model does not generate a correct sample in (b). For reference, the source image is the far left at t = 0 and the target image is far away at t = 1 in <ref type="figure" target="#fig_7">Fig. 5a</ref>. information of original features, and we want to avoid the undetermined dimensionality in the GFK model; 2) the extracted IR features contain enough detailed information for the classification problem 1 . Therefore, we have the following objective function:</p><formula xml:id="formula_4">f = arg min f ∈H k N1 i=1 l(f (X Si ), Y Si ) + η||f || 2 K + λD f r (X S , X T ) + ρR f (X s , X T )<label>(2)</label></formula><p>We only need to replace the manifold learning feature Z in line 1 of Alg.1 in Wang et al. <ref type="bibr" target="#b14">[15]</ref> with our extracted IR features to get the modified distribution alignment model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Description of Datasets</head><p>In this experiment, we show how our MDAIR method can enhance image recognition accuracy. We test our model using three public image datasets: Office+Caltech-10 (we combine Office-10 and Caltech-10 as one dataset), Office-31, and Office-Home <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b40">[41]</ref>. These datasets are widely used in many publications <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and are the benchmarking data for evaluating the performance of    <ref type="figure" target="#fig_8">Fig. 6</ref> shows example images from three benchmark datasets. Amazon and Caltech images are mostly from online merchants, while DSLR and Webcam images are mostly from offices <ref type="bibr" target="#b4">[5]</ref>. We also combine Office-10 and Caltech-10 to be one dataset, and we perform twelve tasks in this dataset: C A, C W, · · · , D W. In Office-31 dataset, we have another six tasks: A W, A D, · · · , D W. For Office-Home datasets, we have another twelve tasks: A C, A P, · · · , R P. Therefore, we have a total of 30 tasks in our experiment.  <ref type="figure">Figure 7</ref>: Differences in accuracy of extracted features three layers (average pooling, fully connected, and classification) for the Office+Caltech-10 dataset tasks, where the baseline is the fully connected layer. The accuracy from the fully connected layer is better than other layers-all accuracies from the classification layer are below the fully connected layer, and most accuracies of the average pooling layer are below the fully connected layer. Therefore, we suggest extracting features from the last fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Comparison</head><p>To determine the best layer for feature extraction, we first explore the effect of different layers in final accuracy. We list the number of features from three layers in Tab. II. Based on an experiment using the Office+Caltech-10 dataset, we choose the optimal layer. <ref type="figure">Fig. 7</ref> shows the accuracy of different tasks from different layers. Accuracy from the fully connected layer is typically higher than the other two layers.   Therefore, we suggest that last fully connected layer is the best layer to extract features in domain adaption problem.</p><p>We then examine the quality of our IR features in the last fully connected layer. We visualize the three domains from three datasets using the t-SNE technique. T-SNE (t-distributed Stochastic Neighbor Embedding) <ref type="bibr" target="#b41">[42]</ref> is an algorithm for visualizing high-dimensional data by re-representing it in a lower dimensional space. t-SNE generates a low-dimensional representation in which points near each other are similar in the high-dimensional space and vice versa. The better that clusters are separated in the t-SNE view, the better the extracted features are likely to be. The loss function of the t-SNE method is Kullback-Leibler divergence, which measures the difference between the two distributions <ref type="bibr" target="#b41">[42]</ref>. Typically, lower losses correspond to better features.</p><p>As shown in <ref type="figure" target="#fig_9">Fig. 8</ref>, our extracted IR features produce better clearer and better separated clusters than SURF, DeCAF, and Resnet-50 features. Therefore, we can assume that our IR features will lead to better classification result than the others. Similarly, the visualizations based on our IR features have the lowest loss values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison to State-of-the-art Methods</head><p>We compare the performance of our MDAIR model with 25 state-of-the-art (both traditional and deep learning) methods: Transfer Component Analysis (TCA) <ref type="bibr" target="#b7">[8]</ref>; Global and Local Metrics for Domain Adaptation (IGLDA also called ITCA) <ref type="bibr" target="#b18">[19]</ref>; Semi-supervised TCA (SSTCA) <ref type="bibr" target="#b7">[8]</ref>; Transfer Joint Matching (TJM) <ref type="bibr" target="#b42">[43]</ref>; Balanced distribution adaptation (BDA) <ref type="bibr" target="#b43">[44]</ref>; Joint distribution alignment (JDA) <ref type="bibr" target="#b5">[6]</ref>; Support Vector Machine (SVM) <ref type="bibr" target="#b9">[10]</ref>; Geodesic Flow Kernel (GFK) <ref type="bibr" target="#b4">[5]</ref>; Adaptation Regularization (ARTL) <ref type="bibr" target="#b44">[45]</ref>; Joint Geometrical and Statistical Alignment (JGSA) <ref type="bibr" target="#b45">[46]</ref>; Manifold Embedded Distribution Alignment (MEDA) <ref type="bibr" target="#b14">[15]</ref>; AlexNet <ref type="bibr" target="#b33">[34]</ref>; VGG-16 <ref type="bibr" target="#b34">[35]</ref>; Deep Adaptation Networks (DAN) <ref type="bibr" target="#b22">[23]</ref>; Deep Domain Confusion (DDC) <ref type="bibr" target="#b6">[7]</ref>; Deep Correlation Alignment (DCORAL) <ref type="bibr" target="#b15">[16]</ref>; Joint Adaptation Networks (JAN) <ref type="bibr" target="#b17">[18]</ref>; Residual Transfer Networks (RTN) <ref type="bibr" target="#b46">[47]</ref>; Domain Adaptive Neural Networks (DANN) <ref type="bibr" target="#b47">[48]</ref>; Domain Adaptive Hashing (DAH) <ref type="bibr" target="#b48">[49]</ref>; Minimum Discrepancy Deep Adaptation (MDDA) <ref type="bibr" target="#b40">[41]</ref>; Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b16">[17]</ref>; Collaborative Adversarial Network (CAN) <ref type="bibr" target="#b20">[21]</ref>, Joint Discriminative Domain Adaptation (JDDA) <ref type="bibr" target="#b19">[20]</ref>, and Conditional Domain Adversarial Networks (CDAN-RM, CDAN-M) <ref type="bibr" target="#b49">[50]</ref>.</p><p>From <ref type="table" target="#tab_1">Tables III, IV</ref> and V, we can observe that the accuracy of MDAIR model is ahead of all other methods in most tasks <ref type="bibr">(23/30)</ref>. Notably, our model always achieves the best performance in Office-Home dataset. Regarding all three datasets, the overall average performance is significantly improved over the best state-of-the-art baseline methods. The results of using SURF feature are too low to compare with DeCAF and IR features and are omitted.</p><p>To illustrate the effectiveness of our model, we consider the case in which all models use our IR features, and view the prediction results using t-SNE. Focusing on the A D task in which the accuracy of our MDAIR is 100% (and thus identical to ground truth), <ref type="figure" target="#fig_10">Fig. 9</ref> shows that all other conventional methods contained mixed colors in the t-SNE view. These results indicate that our modified distribution alignment is better than several baseline methods even using the same features. In addition, we test our IR features using the original MEDA method (MEDA-IR in <ref type="table" target="#tab_1">Tables III, IV  and V)</ref>; results still turn out that our modified distribution alignment is better than the previous MEDA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameter Settings</head><p>In our experiments, the optimal parameters for different tasks might be different. To more easily reproduce our results, we use consistent parameters: λ = 10, ρ = 1.0, p = 10, and η = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>We list the improvement of our model based on the best state-of-the-art methods in <ref type="table" target="#tab_1">Table VI</ref>. For three datasets (Office+Caltech-10, Office-31, and Office-Home), our method improves the absolute accuracy by 4.8%, 5.5%, and 10% respectively. Therefore, the quality of our model exceeds that of all the state-of-the-art methods. There are two prominent reasons for the success of our model. First of all, our model takes advantage of deep features from the Inception-ResNet-v2 model, which produces better features than SURF and DeCAF features. And better features reduce the difference between the source and target domains. Secondly, the modified distribution alignment facilitate the alignment of the distribution of features which leads to higher accuracy.</p><p>In addition, our experiments imply that the last fully connected layer is the best layer for feature extraction. A likely reason is that the layer collects all features from the previous layer; hence it will form better features than previous layer. Although the last classification layer can be used for feature extraction from the IR model, the performance is worse than the last fully connected layer since features from classification layer will be affected by original trained classes. We observe that our model is compromised in some tasks (A W in Office+Caltech-10 and D A in Office-31 dataset). This caused by the intrinsic differences of various datasets, and so we cannot guarantee that our model always beats all other methods.</p><p>However, one shallow weakness of our model is that feature extraction affects the results significantly. We suggest that extracting feature from higher top-1 accuracy deep neural networks will further improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we are the first to extract features from a pretrained Inception-ResNet-v2 model for the domain adaption problem. The experiment shows that the last fully connected layer is the best layer to extract features and the extracted features are better than DeCAF and Resnet-50 features. The modified distribution alignment model has a better performance than other models. We also test our model using three benchmark datasets. Extensive experiments demonstrate significant improvements in classification accuracy over the state-of-the-art.</p><p>There are some obvious areas for follow-up work. Extracting features from another well-trained deep neural network might generate a better input for the modified distribution alignment than the IR model. Testing on a broader set of unsupervised learning tasks will improve the applicability of our model. Also, a new distribution method will be beneficial for increasing the predictive accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The scheme of MDAIR model. 1) We first extract the feature from the last fully connected layer in Inception-ResNet-v2 model. The learned features are slightly more aligned than the raw features; 2) We then align the distribution of learned features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 3is an example of extracting features using the well-trained Inception-ResNet-v2 model. The left ofFig. 3ais the input image,Fig. 3bis the extracted features from the first conventional layer in the IR model; the right ofFig. 3ais strongest channel feature inFig. 3b; and Fig. 3cis extracted feature from last fully connected layer. Alg. 1 describes the procedures of extracting features from the pre-trained IR model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The top 1 accuracy and number of parameters of different pre-trained deep neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Extracting features from IR model Input: Raw images and pre-trained Inception-ResNet-v2 model Output: Extracted features from IR model 1: Prepare the images (rescale the size of images to be 299 × 299 × 3) 2: Select one layer to extract features3:  Apply the raw features of a datapoint as input, and use activation functions to extract the feature using IR model in the selected layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a) Original image and strongest channel (b) The activation of first convolutional layer (c) Final extracted feature Original image and extracted features of first convolutional layer in Inception-ResNet-V2 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The comparison of SGF samples and ground truth. Two black points are the given points; the cyan curve highlights the true geodesic points; the yellow curve is the sampling results of SGF. Sampled points are away from the true geodesic in SGF model. the quality of samples, there are two criteria. The sample should be similar to the source image when t = 0, and the sample should be similar to the target image when t = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) The progress of true samples (b) The progress of SGF samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>The comparison of sampling results between the two images (square and circle) with t = 0, 0.05, 0.5, 0.95, 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Some example images from three benchmark datasets. (a) is from the DSLR domain in Office+Caltech-10 dataset; (b) is from the Amazon domain in Office-31 dataset, and (c) is from Art domain in Office-Home dataset. domain adaptation algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>The t-SNE view of the comparison of our IR features (a, d and g) with DeCAF (b and e), Resnet-50 (h), and SURF features (c and f). Different color means different classes. The first row is from DSLR domain in Office+Caltech-10 datasets, and the second row is from the Webcam domain in the Office-31 dataset, and (g) and (h) are from the Art domain in Office-Home dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>T-SNE view of the comparison of baseline methods and the proposed MDAIR model in the A D in Office+Caltech-10 dataset. The proposed MDAIR model has the highest accuracy, while all other methods have some mixed colors, which implied the classes are wrongly classified (as colors correspond to labels).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Statistics of extracted IR features for four benchmark datasets</figDesc><table><row><cell cols="5">Dataset # Sample # Feature # Class Domain(s)</cell></row><row><cell>Office-10</cell><cell>1410</cell><cell>1000</cell><cell>10</cell><cell>A, W, D</cell></row><row><cell>Caltech-10</cell><cell>1123</cell><cell>1000</cell><cell>10</cell><cell>C</cell></row><row><cell>Office-31</cell><cell>1330</cell><cell>1000</cell><cell>31</cell><cell>A, W, D</cell></row><row><cell cols="2">Office-Home 15588</cell><cell>1000</cell><cell>65</cell><cell>A, C, P, R</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>Statistics of extracted IR features from different layers.</figDesc><table><row><cell cols="4">Layers Last average pooling Last fully connected Classification</cell></row><row><cell># Feature</cell><cell>1536</cell><cell>1000</cell><cell>1000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table III :</head><label>III</label><figDesc>Accuracy (%) on Office + Caltech-10 datasets</figDesc><table><row><cell>Task</cell><cell>C</cell><cell>A C</cell><cell cols="2">W C</cell><cell cols="2">D A</cell><cell cols="2">C A</cell><cell cols="2">W A</cell><cell cols="2">D W</cell><cell>C W</cell><cell>A W</cell><cell>D D</cell><cell>C D</cell><cell>A D</cell><cell>W Average</cell></row><row><cell>TCA</cell><cell>77</cell><cell></cell><cell>80.7</cell><cell cols="2">84.7</cell><cell cols="2">82.2</cell><cell cols="2">68.1</cell><cell cols="2">72.6</cell><cell>79.3</cell><cell cols="2">86.4</cell><cell>88.5</cell><cell>82.2</cell><cell>86.4</cell><cell>84.7</cell><cell>81.1</cell></row><row><cell>ITCA</cell><cell>81</cell><cell></cell><cell>65.8</cell><cell cols="2">79.6</cell><cell cols="2">82.9</cell><cell cols="2">70.8</cell><cell>79</cell><cell></cell><cell>78.2</cell><cell cols="2">85.5</cell><cell>92.4</cell><cell>77.9</cell><cell>82.5</cell><cell>90.5</cell><cell>80.5</cell></row><row><cell>SSTCA</cell><cell cols="2">79.6</cell><cell>70.5</cell><cell cols="2">80.9</cell><cell cols="2">76.5</cell><cell cols="2">72.5</cell><cell cols="2">83.4</cell><cell>69.9</cell><cell cols="2">79.5</cell><cell>90.4</cell><cell>78.7</cell><cell>85.2</cell><cell>87.8</cell><cell>79.6</cell></row><row><cell>TJM</cell><cell cols="2">86.7</cell><cell>84.7</cell><cell>86</cell><cell></cell><cell cols="2">82.8</cell><cell cols="2">78.3</cell><cell>86</cell><cell></cell><cell>82</cell><cell>86</cell><cell>100</cell><cell>83.8</cell><cell>89.6</cell><cell>99.3</cell><cell>87.1</cell></row><row><cell>BDA</cell><cell cols="2">89.5</cell><cell>78.6</cell><cell cols="2">81.5</cell><cell cols="2">79.6</cell><cell cols="2">73.2</cell><cell cols="2">84.7</cell><cell>78.1</cell><cell cols="2">83.3</cell><cell>100</cell><cell>79.7</cell><cell>88.5</cell><cell>98.6</cell><cell>84.6</cell></row><row><cell>JDA</cell><cell cols="2">88.4</cell><cell>84.4</cell><cell cols="2">85.4</cell><cell cols="2">81.6</cell><cell cols="2">80.7</cell><cell cols="2">81.5</cell><cell>82.2</cell><cell cols="2">89.8</cell><cell>100</cell><cell>86</cell><cell>91.5</cell><cell>99.3</cell><cell>87.6</cell></row><row><cell>SVM</cell><cell>91</cell><cell></cell><cell>78</cell><cell cols="2">85.4</cell><cell cols="2">83.3</cell><cell cols="2">72.5</cell><cell cols="2">83.4</cell><cell>62.9</cell><cell cols="2">72.1</cell><cell>99.4</cell><cell>65</cell><cell>78.2</cell><cell>96.6</cell><cell>80.7</cell></row><row><cell>GFK</cell><cell cols="2">88.8</cell><cell>77.3</cell><cell>86</cell><cell></cell><cell cols="2">77.4</cell><cell cols="2">66.8</cell><cell>79</cell><cell></cell><cell>72</cell><cell cols="2">76.5</cell><cell>100</cell><cell>75.5</cell><cell>84.7</cell><cell>99</cell><cell>81.9</cell></row><row><cell>JGSA</cell><cell cols="2">91.4</cell><cell>86.8</cell><cell cols="2">93.6</cell><cell cols="2">84.9</cell><cell cols="2">81.0</cell><cell cols="2">88.5</cell><cell>85.0</cell><cell cols="2">90.7</cell><cell>100</cell><cell>86.2</cell><cell>92.0</cell><cell>99.7</cell><cell>90.0</cell></row><row><cell>ARTL</cell><cell cols="2">92.4</cell><cell>87.8</cell><cell cols="2">86.6</cell><cell cols="2">87.4</cell><cell cols="2">88.5</cell><cell cols="2">85.4</cell><cell>88.2</cell><cell cols="2">92.3</cell><cell>100</cell><cell>87.3</cell><cell>92.7</cell><cell>100</cell><cell>90.7</cell></row><row><cell>MEDA</cell><cell>93</cell><cell></cell><cell>91.2</cell><cell cols="2">89.8</cell><cell>89</cell><cell></cell><cell cols="2">90.8</cell><cell cols="2">88.5</cell><cell>89</cell><cell cols="2">92.2</cell><cell>99.4</cell><cell>88.6</cell><cell>93.2</cell><cell>98.6</cell><cell>91.9</cell></row><row><cell>AlexNet</cell><cell cols="2">91.9</cell><cell>83.7</cell><cell cols="2">87.1</cell><cell>83</cell><cell></cell><cell cols="2">79.5</cell><cell cols="2">87.4</cell><cell>73</cell><cell cols="2">83.8</cell><cell>100</cell><cell>79</cell><cell>87.1</cell><cell>97.7</cell><cell>86.1</cell></row><row><cell>DAN</cell><cell>92</cell><cell></cell><cell>90.6</cell><cell cols="2">89.3</cell><cell cols="2">84.1</cell><cell cols="2">91.8</cell><cell cols="2">91.7</cell><cell>81.2</cell><cell cols="2">92.1</cell><cell>100</cell><cell>80.3</cell><cell>90</cell><cell>98.5</cell><cell>90.1</cell></row><row><cell>DDC</cell><cell cols="2">91.9</cell><cell>85.4</cell><cell cols="2">88.8</cell><cell>85</cell><cell></cell><cell cols="2">86.1</cell><cell>89</cell><cell></cell><cell>78</cell><cell cols="2">83.8</cell><cell>100</cell><cell>79</cell><cell>87.1</cell><cell>97.7</cell><cell>86.1</cell></row><row><cell cols="3">DCORAL 89.8</cell><cell>97.3</cell><cell>91</cell><cell></cell><cell cols="2">91.9</cell><cell cols="2">100</cell><cell cols="2">90.5</cell><cell>83.7</cell><cell cols="2">81.5</cell><cell>90.1</cell><cell>88.6</cell><cell>80.1</cell><cell>92.3</cell><cell>89.7</cell></row><row><cell cols="3">MEDA-IR 96.2</cell><cell>95.9</cell><cell cols="2">96.2</cell><cell cols="2">95.2</cell><cell>98</cell><cell></cell><cell cols="2">96.8</cell><cell>94.5</cell><cell cols="2">96.2</cell><cell>99.4</cell><cell>93.8</cell><cell>95.5</cell><cell>98.6</cell><cell>96.4</cell></row><row><cell>MDAIR</cell><cell cols="2">96.1</cell><cell>94.9</cell><cell cols="2">96.2</cell><cell cols="2">94.2</cell><cell cols="2">98.6</cell><cell>100</cell><cell></cell><cell>94.9</cell><cell cols="2">96.3</cell><cell>100</cell><cell>94.2</cell><cell>95.8</cell><cell>98.6</cell><cell>96.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">: Accuracy (%) on Office-31 datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Task</cell><cell cols="10">TCA SSTCA MEDA DAN RTN DANN ADDA CAN JDDA JAN</cell><cell cols="2">MEDA-IR MDAIR</cell></row><row><cell>A</cell><cell>W</cell><cell>82.6</cell><cell>81</cell><cell>83.3</cell><cell>80.5</cell><cell>84.5</cell><cell>82</cell><cell>86.2</cell><cell>81.5</cell><cell>82.6</cell><cell>85.4</cell><cell>90.8</cell><cell>94</cell></row><row><cell>A</cell><cell>D</cell><cell>84.1</cell><cell>78.7</cell><cell>83.3</cell><cell>78.6</cell><cell>77.5</cell><cell>79.7</cell><cell>77.8</cell><cell>65.9</cell><cell>79.8</cell><cell>84.7</cell><cell>91.4</cell><cell>92.6</cell></row><row><cell>W</cell><cell>A</cell><cell>69.1</cell><cell>68.9</cell><cell>66.2</cell><cell>62.8</cell><cell>64.8</cell><cell>67.4</cell><cell>68.9</cell><cell>98.2</cell><cell>66.7</cell><cell>70.0</cell><cell>74.6</cell><cell>77.6</cell></row><row><cell>W</cell><cell>D</cell><cell>99.6</cell><cell>99.6</cell><cell>96</cell><cell>99.6</cell><cell>99.4</cell><cell>99.1</cell><cell>98.4</cell><cell>85.5</cell><cell>99.7</cell><cell>99.8</cell><cell>97.2</cell><cell>99.2</cell></row><row><cell>D</cell><cell>A</cell><cell>66.1</cell><cell>66.6</cell><cell>66.7</cell><cell>63.6</cell><cell>66.2</cell><cell>68.2</cell><cell>69.5</cell><cell>99.7</cell><cell>57.4</cell><cell>68.6</cell><cell>75.4</cell><cell>78.7</cell></row><row><cell>D</cell><cell>W</cell><cell>97</cell><cell>97.4</cell><cell>91.7</cell><cell>97.1</cell><cell>96.8</cell><cell>96.9</cell><cell>96.2</cell><cell>63.4</cell><cell>95.2</cell><cell>97.4</cell><cell>96</cell><cell>96.9</cell></row><row><cell cols="3">Average 83.1</cell><cell>82.0</cell><cell>81.2</cell><cell>80.4</cell><cell>81.6</cell><cell>82.2</cell><cell>82.9</cell><cell>82.4</cell><cell>80.2</cell><cell>84.3</cell><cell>87.5</cell><cell>89.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table V :</head><label>V</label><figDesc>Accuracy (%) on Office-Home datasets</figDesc><table><row><cell>Task</cell><cell>A</cell><cell cols="2">C A</cell><cell cols="2">P A</cell><cell cols="2">R C</cell><cell cols="2">A C</cell><cell cols="2">P C</cell><cell cols="2">R P</cell><cell>A P</cell><cell>C P</cell><cell>R R</cell><cell>A R</cell><cell>C R</cell><cell>P Average</cell></row><row><cell>AlexNet</cell><cell cols="2">26.4</cell><cell cols="2">32.6</cell><cell cols="2">41.3</cell><cell cols="2">22.1</cell><cell cols="2">41.7</cell><cell cols="2">42.1</cell><cell cols="2">20.5</cell><cell>20.3</cell><cell>51.1</cell><cell>31</cell><cell>27.9</cell><cell>54.9</cell><cell>34.3</cell></row><row><cell>VGG16</cell><cell cols="2">30.4</cell><cell cols="2">45.9</cell><cell cols="2">57.5</cell><cell cols="2">35.4</cell><cell cols="2">48.7</cell><cell cols="2">50.8</cell><cell cols="2">35.8</cell><cell>30.5</cell><cell>60.2</cell><cell>49.6</cell><cell>34.5</cell><cell>64.0</cell><cell>45.3</cell></row><row><cell>D-CORAL</cell><cell cols="2">32.2</cell><cell cols="2">40.5</cell><cell cols="2">54.5</cell><cell cols="2">31.5</cell><cell cols="2">45.8</cell><cell cols="2">47.3</cell><cell cols="2">30.0</cell><cell>32.3</cell><cell>55.3</cell><cell>44.7</cell><cell>42.8</cell><cell>59.4</cell><cell>42.8</cell></row><row><cell>RTN</cell><cell cols="2">31.3</cell><cell cols="2">40.2</cell><cell cols="2">54.6</cell><cell cols="2">32.5</cell><cell cols="2">46.6</cell><cell cols="2">48.3</cell><cell cols="2">28.2</cell><cell>32.9</cell><cell>56.4</cell><cell>45.5</cell><cell>44.8</cell><cell>61.3</cell><cell>43.5</cell></row><row><cell>DAH</cell><cell cols="2">31.6</cell><cell cols="2">40.8</cell><cell cols="2">51.7</cell><cell cols="2">34.7</cell><cell cols="2">51.9</cell><cell cols="2">52.8</cell><cell cols="2">29.9</cell><cell>39.6</cell><cell>60.7</cell><cell>45.0</cell><cell>45.1</cell><cell>62.5</cell><cell>45.5</cell></row><row><cell>MDDA</cell><cell cols="2">35.2</cell><cell cols="2">44.4</cell><cell cols="2">57.2</cell><cell cols="2">36.8</cell><cell cols="2">52.5</cell><cell cols="2">53.7</cell><cell cols="2">34.8</cell><cell>37.2</cell><cell>62.2</cell><cell>50.0</cell><cell>46.3</cell><cell>66.1</cell><cell>48.0</cell></row><row><cell>ResNet-50</cell><cell cols="2">34.9</cell><cell>50</cell><cell></cell><cell>58</cell><cell></cell><cell cols="2">37.4</cell><cell cols="2">41.9</cell><cell cols="2">46.2</cell><cell cols="2">38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9</cell><cell>46.1</cell></row><row><cell>DAN</cell><cell cols="2">43.6</cell><cell>57</cell><cell></cell><cell cols="2">67.9</cell><cell cols="2">45.8</cell><cell cols="2">56.5</cell><cell cols="2">60.4</cell><cell>44</cell><cell>43.6</cell><cell>67.7</cell><cell>63.1</cell><cell>51.5</cell><cell>74.3</cell><cell>56.3</cell></row><row><cell>DANN</cell><cell cols="2">45.6</cell><cell cols="2">59.3</cell><cell cols="2">70.1</cell><cell>47</cell><cell></cell><cell cols="2">58.5</cell><cell cols="2">60.9</cell><cell cols="2">46.1</cell><cell>43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8</cell><cell>57.6</cell></row><row><cell>JAN</cell><cell cols="2">45.9</cell><cell cols="2">61.2</cell><cell cols="2">68.9</cell><cell cols="2">50.4</cell><cell cols="2">59.7</cell><cell>61</cell><cell></cell><cell cols="2">45.8</cell><cell>43.4</cell><cell>70.3</cell><cell>63.9</cell><cell>52.4</cell><cell>76.8</cell><cell>58.3</cell></row><row><cell cols="3">CDAN-RM 49.2</cell><cell cols="2">64.8</cell><cell cols="2">72.9</cell><cell cols="2">53.8</cell><cell cols="2">62.4</cell><cell cols="2">62.9</cell><cell cols="2">49.8</cell><cell>48.8</cell><cell>71.5</cell><cell>65.8</cell><cell>56.4</cell><cell>79.2</cell><cell>61.5</cell></row><row><cell>CDAN-M</cell><cell cols="2">50.6</cell><cell cols="2">65.9</cell><cell cols="2">73.4</cell><cell cols="2">55.7</cell><cell cols="2">62.7</cell><cell cols="2">64.2</cell><cell cols="2">51.8</cell><cell>49.1</cell><cell>74.5</cell><cell>68.2</cell><cell>56.9</cell><cell>80.7</cell><cell>62.8</cell></row><row><cell>MEDA-IR</cell><cell cols="2">52.9</cell><cell cols="2">79.3</cell><cell cols="2">78.9</cell><cell cols="2">67.3</cell><cell cols="2">78.8</cell><cell cols="2">78.8</cell><cell cols="2">68.2</cell><cell>53.4</cell><cell>79.8</cell><cell>71.8</cell><cell>56.3</cell><cell>83</cell><cell>70.7</cell></row><row><cell>MDAIR</cell><cell cols="2">55.6</cell><cell cols="2">80.4</cell><cell cols="2">81.6</cell><cell cols="2">70.2</cell><cell cols="2">80.7</cell><cell cols="2">80.8</cell><cell>71</cell><cell>55.6</cell><cell>82.5</cell><cell>73.5</cell><cell>57.7</cell><cell>83.9</cell><cell>72.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table VI :</head><label>VI</label><figDesc>Comparison of average accuracy of the best baseline method and our MDAIR model</figDesc><table><row><cell>Task</cell><cell cols="3">Best baseline MDAIR Improvement</cell></row><row><cell>Office+Caltech-10</cell><cell>91.9</cell><cell>96.7</cell><cell>4.8%</cell></row><row><cell>Office-31</cell><cell>84.3</cell><cell>89.8</cell><cell>5.5%</cell></row><row><cell>Office-Home</cell><cell>62.8</cell><cell>72.8</cell><cell>10%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code is available at: https://github.com/heaventian93/MDAIR.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition using semanticspreserving adversarial embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 45th Annual Meeting of the Assoc. of Computational Linguistics</title>
		<meeting>45th Annual Meeting of the Assoc. of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning bounds for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.3430</idno>
		<title level="m">Domain adaptation: Learning bounds and algorithms</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Impossibility theorems for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dávid</forename><surname>Pál</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Manifold alignment without correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1273" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integration of global and local metrics for domain adaptation learning via dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary G</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="51" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint domain alignment and discriminative feature learning for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09347</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated identification of hookahs (waterpipes) on instagram: an application in feature extraction using convolutional neural network and support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon-Patrick</forename><surname>Allem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Beth</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess Boley</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08974</idno>
		<title level="m">Do better imagenet models transfer better</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On minimum discrepancy estimation for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Mohammad Mahfujur Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00282</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Balanced distribution adaptation for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1129" to="1134" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaptation regularization: A general framework for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1076" to="1089" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint geometrical and statistical alignment for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1859" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Domain adaptive neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim international conference on artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="898" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
