<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Poly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
							<email>samuelhumeau@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
							<email>kshuster@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
							<email>malachaux@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Poly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of deep pre-trained transformers has led to remarkable progress in a number of applications . For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks. * Joint First Authors.</p><p>Published as a conference paper at ICLR 2020 to Wikipedia/Toronto Books (i.e., BERT). We obtain a new state-of-the-art on all four datasets with our best architectures and pre-training strategies, as well as providing practical implementations for real-time use. Our code and models will be released open-source.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, substantial improvements to state-of-the-art benchmarks on a variety of language understanding tasks have been achieved through the use of deep pre-trained language models followed by fine-tuning . In this work we explore improvements to this approach for the class of tasks that require multi-sentence scoring: given an input context, score a set of candidate labels, a setup common in retrieval and dialogue tasks, amongst others. Performance in such tasks has to be measured via two axes: prediction quality and prediction speed, as scoring many candidates can be prohibitively slow.</p><p>The current state-of-the-art focuses on using BERT models for pre-training , which employ large text corpora on general subjects: Wikipedia and the Toronto Books Corpus <ref type="bibr" target="#b30">(Zhu et al., 2015)</ref>. Two classes of fine-tuned architecture are typically built on top: Bi-encoders and Cross-encoders. Cross-encoders <ref type="bibr" target="#b22">(Wolf et al., 2019;</ref><ref type="bibr" target="#b21">Vig &amp; Ramea, 2019)</ref>, which perform full (cross) self-attention over a given input and label candidate, tend to attain much higher accuracies than their counterparts, Bi-encoders <ref type="bibr" target="#b18">(Mazaré et al., 2018;</ref>, which perform self-attention over the input and candidate label separately and combine them at the end for a final representation. As the representations are separate, Bi-encoders are able to cache the encoded candidates, and reuse these representations for each input resulting in fast prediction times. Cross-encoders must recompute the encoding for each input and label; as a result, they are prohibitively slow at test time.</p><p>In this work, we provide novel contributions that improve both the quality and speed axes over the current state-of-the-art. We introduce the Poly-encoder, an architecture with an additional learnt attention mechanism that represents more global features from which to perform self-attention, resulting in performance gains over Bi-encoders and large speed gains over Cross-Encoders. To pre-train our architectures, we show that choosing abundant data more similar to our downstream task also brings significant gains over BERT pre-training. This is true across all different architecture choices and downstream tasks we try.</p><p>We conduct experiments comparing the new approaches, in addition to analysis of what works best for various setups of existing methods, on four existing datasets in the domains of dialogue and information retrieval (IR), with pre-training strategies based on Reddit <ref type="bibr" target="#b18">(Mazaré et al., 2018</ref>) compared 2 Related Work The task of scoring candidate labels given an input context is a classical problem in machine learning. While multi-class classification is a special case, the more general task involves candidates as structured objects rather than discrete classes; in this work we consider the inputs and the candidate labels to be sequences of text.</p><p>There is a broad class of models that map the input and a candidate label separately into a common feature space wherein typically a dot product, cosine or (parameterized) non-linearity is used to measure their similarity. We refer to these models as Bi-encoders. Such methods include vector space models <ref type="bibr" target="#b19">(Salton et al., 1975)</ref>, LSI <ref type="bibr" target="#b5">(Deerwester et al., 1990)</ref>, supervised embeddings <ref type="bibr" target="#b0">(Bai et al., 2009;</ref><ref type="bibr" target="#b23">Wu et al., 2018)</ref> and classical siamese networks <ref type="bibr" target="#b1">(Bromley et al., 1994)</ref>. For the next utterance prediction tasks we consider in this work, several Bi-encoder neural approaches have been considered, in particular Memory Networks <ref type="bibr" target="#b28">(Zhang et al., 2018a)</ref> and Transformer Memory networks  as well as LSTMs <ref type="bibr" target="#b17">(Lowe et al., 2015)</ref> and CNNs <ref type="bibr" target="#b13">(Kadlec et al., 2015)</ref> which encode input and candidate label separately. A major advantage of Bi-encoder methods is their ability to cache the representations of a large, fixed candidate set. Since the candidate encodings are independent of the input, Bi-encoders are very efficient during evaluation.</p><p>Researchers have also studied a more rich class of models we refer to as Cross-encoders, which make no assumptions on the similarity scoring function between input and candidate label. Instead, the concatenation of the input and a candidate serve as a new input to a nonlinear function that scores their match based on any dependencies it wants. This has been explored with Sequential Matching Network CNN-based architectures <ref type="bibr" target="#b24">(Wu et al., 2017)</ref>, Deep Matching Networks <ref type="bibr" target="#b25">(Yang et al., 2018)</ref>, Gated Self-Attention <ref type="bibr" target="#b29">(Zhang et al., 2018b)</ref>, and most recently transformers <ref type="bibr" target="#b22">(Wolf et al., 2019;</ref><ref type="bibr" target="#b21">Vig &amp; Ramea, 2019;</ref><ref type="bibr" target="#b20">Urbanek et al., 2019)</ref>. For the latter, concatenating the two sequences of text results in applying self-attention at every layer. This yields rich interactions between the input context and the candidate, as every word in the candidate label can attend to every word in the input context, and vice-versa. <ref type="bibr" target="#b20">Urbanek et al. (2019)</ref> employed pre-trained BERT models, and fine-tuned both Bi-and Cross-encoders, explicitly comparing them on dialogue and action tasks, and finding that Cross-encoders perform better. However, the performance gains come at a steep computational cost. Cross-encoder representations are much slower to compute, rendering some applications infeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks</head><p>We consider the tasks of sentence selection in dialogue and article search in IR. The former is a task extensively studied and recently featured in two competitions: the Neurips ConvAI2 competition <ref type="bibr" target="#b8">(Dinan et al., 2020)</ref>, and the DSTC7 challenge, Track 1 <ref type="bibr" target="#b27">(Yoshino et al., 2019;</ref><ref type="bibr">Jonathan K. Kummerfeld &amp; Lasecki, 2018;</ref>. We compare on those two tasks and in addition, we also test on the popular Ubuntu V2 corpus <ref type="bibr" target="#b17">(Lowe et al., 2015)</ref>. For IR, we use the Wikipedia Article Search task of <ref type="bibr" target="#b23">Wu et al. (2018)</ref>.</p><p>The ConvAI2 task is based on the Persona-Chat dataset <ref type="bibr" target="#b28">(Zhang et al., 2018a)</ref> which involves dialogues between pairs of speakers. Each speaker is given a persona, which is a few sentences that describe a character they will imitate, e.g. "I love romantic movies", and is instructed to get to know the other. Models should then condition their chosen response on the dialogue history and the lines of persona. As an automatic metric in the competition, for each response, the model has to pick the correct annotated utterance from a set of 20 choices, where the remaining 19 were other randomly chosen utterances from the evaluation set. Note that in a final system however, one would retrieve from the entire training set of over 100k utterances, but this is avoided for speed reasons in common evaluation setups. The best performing competitor out of 23 entrants in this task achieved 80.7% accuracy on the test set utilizing a pre-trained Transformer fine-tuned for this task <ref type="bibr" target="#b22">(Wolf et al., 2019)</ref>.</p><p>The DSTC7 challenge (Track 1) consists of conversations extracted from Ubuntu chat logs, where one partner receives technical support for various Ubuntu-related problems from the other. The best performing competitor (with 20 entrants in Track 1) in this task achieved 64.5% R@1 <ref type="bibr" target="#b3">(Chen &amp; Wang, 2019)</ref>. Ubuntu V2 is a similar but larger popular corpus, created before the competition <ref type="bibr" target="#b17">(Lowe et al., 2015)</ref>; we report results for this dataset as well, as there are many existing results on it.</p><p>Finally, we evaluate on Wikipedia Article Search <ref type="bibr" target="#b23">(Wu et al., 2018)</ref>. Using the 2016-12-21 dump of English Wikipedia (∼5M articles), the task is given a sentence from an article as a search query, find the article it came from. Evaluation ranks the true article (minus the sentence) against 10,000 other articles using retrieval metrics. This mimics a web search like scenario where one would like to search for the most relevant articles (web documents). The best reported method is the learningto-rank embedding model, StarSpace, which outperforms fastText, SVMs, and other baselines.</p><p>We summarize all four datasets and their statistics in <ref type="table" target="#tab_1">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>In this section we describe the various models and methods that we explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transformers and Pre-training Strategies</head><p>Transformers Our Bi-, Cross-, and Poly-encoders, described in sections 4.2, 4.3 and 4.4 respectively, are based on large pre-trained transformer models with the same architecture and dimension as BERT-base , which has 12 layers, 12 attention heads, and a hidden size of 768. As well as considering the BERT pre-trained weights, we also explore our own pre-training schemes. Specifically, we pre-train two more transformers from scratch using the exact same architecture as BERT-base.  <ref type="bibr" target="#b18">(Mazaré et al., 2018)</ref>, which is a dataset more adapted to dialogue. The former is performed to verify that reproducing a BERT-like setting gives us the same results as reported previously, while the latter tests whether pre-training on data more similar to the downstream tasks of interest helps. For training both new setups we used XLM <ref type="bibr" target="#b15">(Lample &amp; Conneau, 2019)</ref>.</p><p>Input Representation Our pre-training input is the concatenation of input and label [IN-PUT,LABEL], where both are surrounded with the special token [S], following <ref type="bibr" target="#b15">Lample &amp; Conneau (2019)</ref>. When pre-training on Reddit, the input is the context, and the label is the next utterance. When pre-training on Wikipedia and Toronto Books, as in , the input is one sentence and the label the next sentence in the text. Each input token is represented as the sum of three embeddings: the token embedding, the position (in the sequence) embedding and the segment embedding. Segments for input tokens are 0, and for label tokens are 1.</p><p>Pre-training Procedure Our pre-training strategy involves training with a masked language model (MLM) task identical to the one in . In the pre-training on Wikipedia and Toronto Books we add a next-sentence prediction task identical to BERT training. In the pre-training on Reddit, we add a next-utterance prediction task, which is slightly different from the previous one as an utterance can be composed of several sentences. During training 50% of the time the candidate is the actual next sentence/utterance and 50% of the time it is a sentence/utterance randomly taken from the dataset. We alternate between batches of the MLM task and the next-sentence/nextutterance prediction task. Like in <ref type="bibr" target="#b15">Lample &amp; Conneau (2019)</ref> we use the Adam optimizer with learning rate of 2e-4, β 1 = 0.9, β 2 = 0.98, no L2 weight decay, linear learning rate warmup, and inverse square root decay of the learning rate. We use a dropout probability of 0.1 on all layers, and a batch of 32000 tokens composed of concatenations [INPUT, LABEL] with similar lengths. We train the model on 32 GPUs for 14 days.</p><p>Fine-tuning After pre-training, one can then fine-tune for the multi-sentence selection task of choice, in our case one of the four tasks from Section 3. We consider three architectures with which we fine-tune the transformer: the Bi-encoder, Cross-encoder and newly proposed Poly-encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bi-encoder</head><p>In a Bi-encoder, both the input context and the candidate label are encoded into vectors:</p><formula xml:id="formula_0">y ctxt = red(T 1 (ctxt)) y cand = red(T 2 (cand))</formula><p>where T 1 and T 2 are two transformers that have been pre-trained following the procedure described in 4.1; they initially start with the same weights, but are allowed to update separately during finetuning. T (x) = h 1 , .., h N is the output of a transformer T and red(·) is a function that reduces that sequence of vectors into one vector. As the input and the label are encoded separately, segment tokens are 0 for both. To resemble what is done during our pre-training, both the input and label are surrounded by the special token [S] and therefore h 1 corresponds to <ref type="bibr">[S]</ref>.</p><p>We considered three ways of reducing the output into one representation via red(·): choose the first output of the transformer (corresponding to the special token [S]), compute the average over all outputs or the average over the first m ≤ N outputs. We compare them in <ref type="table" target="#tab_8">Table 7</ref> in the Appendix. We use the first output of the transformer in our experiments as it gives slightly better results.</p><p>Scoring The score of a candidate cand i is given by the dot-product s(ctxt, cand i ) = y ctxt ·y cand i . The network is trained to minimize a cross-entropy loss in which the logits are y ctxt · y cand 1 , ..., y ctxt · y cand n , where cand 1 is the correct label and the others are chosen from the training set. Similar to <ref type="bibr" target="#b18">Mazaré et al. (2018)</ref>, during training we consider the other labels in the batch as negatives. This allows for much faster training, as we can reuse the embeddings computed for each candidate, and also use a larger batch size; e.g., in our experiments on ConvAI2, we were able to use batches of 512 elements.</p><p>Inference speed In the setting of retrieval over known candidates, a Bi-encoder allows for the precomputation of the embeddings of all possible candidates of the system. After the context embedding y ctxt is computed, the only operation remaining is a dot product between y ctxt and every candidate embedding, which can scale to millions of candidates on a modern GPU, and potentially billions using nearest-neighbor libraries such as FAISS <ref type="bibr" target="#b11">(Johnson et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross-encoder</head><p>The Cross-encoder allows for rich interactions between the input context and candidate label, as they are jointly encoded to obtain a final representation. Similar to the procedure in pre-training, the context and candidate are surrounded by the special token [S] and concatenated into a single vector, which is encoded using one transformer. We consider the first output of the transformer as the context-candidate embedding:</p><formula xml:id="formula_1">y ctxt,cand = h 1 = f irst(T (ctxt, cand))</formula><p>where f irst is the function that takes the first vector of the sequence of vectors produced by the transformer. By using a single transformer, the Cross-encoder is able to perform self-attention between the context and candidate, resulting in a richer extraction mechanism than the Bi-encoder. As the candidate label can attend to the input context during the layers of the transformer, the Crossencoder can produce a candidate-sensitive input representation, which the Bi-encoder cannot. For example, this allows it to select useful input features per candidate.</p><p>Scoring To score one candidate, a linear layer W is applied to the embedding y ctxt,cand to reduce it from a vector to a scalar: s(ctxt, cand i ) = y ctxt,cand i W Similarly to what is done for the Bi-encoder, the network is trained to minimize a cross entropy loss where the logits are s(ctxt, cand 1 ), ..., s(ctxt, cand n ), where cand 1 is the correct candidate and the The Poly-encoder combines the strengths of the Bi-encoder and Cross-encoder by both allowing for caching of candidate representations and adding a final attention mechanism between global features of the input and a given candidate to give richer interactions before computing a final score.</p><p>others are negatives taken from the training set. Unlike in the Bi-encoder, we cannot recycle the other labels of the batch as negatives, so we use external negatives provided in the training set. The Cross-encoder uses much more memory than the Bi-encoder, resulting in a much smaller batch size.</p><p>Inference speed Unfortunately, the Cross-encoder does not allow for precomputation of the candidate embeddings. At inference time, every candidate must be concatenated with the input context and must go through a forward pass of the entire model. Thus, this method cannot scale to a large amount of candidates. We discuss this bottleneck further in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Poly-encoder</head><p>The Poly-encoder architecture aims to get the best of both worlds from the Bi-and Cross-encoder. A given candidate label is represented by one vector as in the Bi-encoder, which allows for caching candidates for fast inference time, while the input context is jointly encoded with the candidate, as in the Cross-encoder, allowing the extraction of more information.</p><p>The Poly-encoder uses two separate transformers for the context and label like a Bi-encoder, and the candidate is encoded into a single vector y cand i . As such, the Poly-encoder method can be implemented using a precomputed cache of encoded responses. However, the input context, which is typically much longer than a candidate, is represented with m vectors (y 1 ctxt ..y m ctxt ) instead of just one as in the Bi-encoder, where m will influence the inference speed. To obtain these m global features that represent the input, we learn m context codes (c 1 , ..., c m ), where c i extracts representation y i ctxt by attending over all the outputs of the previous layer. That is, we obtain y i ctxt using:</p><formula xml:id="formula_2">y i ctxt = j w c i j h j where (w c i 1 , .., w c i N ) = softmax(c i · h 1 , .., c i · h N )</formula><p>The m context codes are randomly initialized, and learnt during finetuning.</p><p>Finally, given our m global context features, we attend over them using y cand i as the query:</p><formula xml:id="formula_3">y ctxt = i w i y i ctxt where (w 1 , .</formula><p>., w m ) = softmax(y cand i · y 1 ctxt , .., y cand i · y m ctxt )</p><p>The final score for that candidate label is then y ctxt · y cand i as in a Bi-encoder. As m &lt; N, where N is the number of tokens, and the context-candidate attention is only performed at the top layer, this is far faster than the Cross-encoder's full self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We perform a variety of experiments to test our model architectures and training strategies over four tasks. For metrics, we measure Recall@k where each test example has C possible candidates to select from, abbreviated to R@k/C, as well as mean reciprocal rank (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Bi-encoders and Cross-encoders</head><p>We first investigate fine-tuning the Bi-and Cross-encoder architectures initialized with the weights provided by , studying the choice of other hyperparameters (we explore our own pre-training schemes in section 5.3). In the case of the Bi-encoder, we can use a large number of negatives by considering the other batch elements as negative training samples, avoiding recomputation of their embeddings. On 8 Nvidia Volta v100 GPUs and using half-precision operations (i.e. float16 operations), we can reach batches of 512 elements on ConvAI2. <ref type="table">Table 2</ref> shows that in this setting, we obtain higher performance with a larger batch size, i.e. more negatives, where 511 negatives yields the best results. For the other tasks, we keep the batch size at 256, as the longer sequences in those datasets uses more memory. The Cross-encoder is more computationally intensive, as the embeddings for the (context, candidate) pair must be recomputed each time. We thus limit its batch size to 16 and provide negatives random samples from the training set. For DSTC7 and Ubuntu V2, we choose 15 such negatives; For ConvAI2, the dataset provides 19 negatives.</p><p>Negatives 31 63 127 255 511 R@1/20 81.0 81.7 82.3 83.0 83.3 <ref type="table">Table 2</ref>: Validation performance on ConvAI2 after fine-tuning a Bi-encoder pre-trained with BERT, averaged over 5 runs. The batch size is the number of training negatives + 1 as we use the other elements of the batch as negatives during training.</p><p>The above results are reported with Bi-encoder aggregation based on the first output. Choosing the average over all outputs instead is very similar but slightly worse (83.1, averaged over 5 runs). We also tried to add further non-linearities instead of the inner product of the two representations, but could not obtain improved results over the simpler architecture (results not shown).</p><p>We tried two optimizers: Adam <ref type="bibr" target="#b14">(Kingma &amp; Ba, 2015)</ref> with weight decay of 0.01 (as recommended by ) and Adamax (Kingma &amp; Ba, 2015) without weight decay; based on validation set performance, we choose to fine-tune with Adam when using the BERT weights. The learning rate is initialized to 5e-5 with a warmup of 100 iterations for Bi-and Poly-encoders, and 1000 iterations for the Cross-encoder. The learning rate decays by a factor of 0.4 upon plateau of the loss evaluated on the valid set every half epoch. In <ref type="table" target="#tab_4">Table 3</ref> we show validation performance when fine-tuning various layers of the weights provided by , using Adam with decay optimizer. Fine-tuning the entire network is important, with the exception of the word embeddings.</p><p>With the setups described above, we fine-tune the Bi-and Cross-encoders on the datasets, and report the results in  test set is not part of the pre-training for that dataset. In addition, Cross-encoders are also too slow to evaluate on the evaluation setup of that task, which has 10k candidates. 83.3 ± 0.1 65.8 ± 0.7 73.5 ± 0.5 83.4 ± 0.1 89.9 ± 0.0 -Poly-encoder 360 83.8 ± 0.1 65.8 ± 0.7 73.6 ± 0.6 83.7 ± 0.0 90.1 ± 0.0 -Cross-encoder 84.9 ± 0.3 65.3 ± 1.0 73.8 ± 0.6 83.1 ± 0.7 89.7 ± 0.5 -Our pre-training on Reddit Bi-encoder 84.8 ± 0.1 70.9 ± 0.5 78.1 ± 0.3 83.6 ± 0.7 90.1 ± 0.4 71.0 Poly-encoder 16 86.3 ± 0.3 71.6 ± 0.6 78.4 ± 0.4 86.0 ± 0.1 91.5 ± 0.1 71.5 Poly-encoder 64 86.5 ± 0.2 71.2 ± 0.8 78.2 ± 0.7 85.9 ± 0.1 91.5 ± 0.1 71.3 Poly-encoder 360 86.8 ± 0.1 71.4 ± 1.0 78.3 ± 0.7 85.9 ± 0.1 91.5 ± 0.0 71.8 Cross-encoder 87.9 ± 0.2 71.7 ± 0.3 79.0 ± 0.2 86.5 ± 0.1 91.9 ± 0.0 - <ref type="table" target="#tab_3">Table 4</ref>: Test performance of Bi-, Poly-and Cross-encoders on our selected tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Poly-encoders</head><p>We train the Poly-encoder using the same batch sizes and optimizer choices as in the Bi-encoder experiments. Results are reported in <ref type="table" target="#tab_3">Table 4</ref> for various values of m context vectors.</p><p>The Poly-encoder outperforms the Bi-encoder on all the tasks, with more codes generally yielding larger improvements. Our recommendation is thus to use as large a code size as compute time allows (see Sec. 5.4). On DSTC7, the Poly-encoder architecture with BERT pretraining reaches 68.9% R1 with 360 intermediate context codes; this actually outperforms the Cross-encoder result (67.4%) and is noticeably better than our Bi-encoder result (66.8%). Similar conclusions are found on Ubuntu V2 and ConvAI2, although in the latter Cross-encoders give slightly better results.</p><p>We note that since reporting our results, the authors of <ref type="bibr" target="#b16">Li et al. (2019)</ref> have conducted a human evaluation study on ConvAI2, in which our Poly-encoder architecture outperformed all other models compared against, both generative and retrieval based, including the winners of the competition. <ref type="table" target="#tab_1">Candidates  1k  100k  1k 100k  Bi-encoder  115  160  19  22  Poly-encoder 16 122  678  18  38  Poly-encoder 64 126  692  23  46  Poly-encoder 360 160  837  57</ref> 88 Cross-encoder 21.7k 2.2M* 2.6k 266k* <ref type="table">Table 5</ref>: Average time in milliseconds to predict the next dialogue utterance from C possible candidates on ConvAI2. * are inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring time (ms) CPU GPU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Domain-specific Pre-training</head><p>We fine-tune our Reddit-pre-trained transformer on all four tasks; we additionally fine-tune a transformer that was pre-trained on the same datasets as BERT, specifically Toronto Books + Wikipedia. When using our pre-trained weights, we use the Adamax optimizer and optimize all the layers of the transformer including the embeddings. As we do not use weight decay, the weights of the final layer are much larger than those in the final layer of BERT; to avoid saturation of the attention layer in the Poly-encoder, we re-scaled the last linear layer so that the standard deviation of its output matched that of BERT, which we found necessary to achieve good results. We report results of fine-tuning with our pre-trained weights in <ref type="table" target="#tab_3">Table 4</ref>. We show that pre-training on Reddit gives further state-ofthe-art performance over our previous results with BERT, a finding that we see for all three dialogue tasks, and all three architectures.</p><p>The results obtained with fine-tuning on our own transformers pre-trained on Toronto Books + Wikipedia are very similar to those obtained with the original BERT weights, indicating that the choice of dataset used to pre-train the models impacts the final results, not some other detail in our training. Indeed, as the two settings pre-train with datasets of similar size, we can conclude that choosing a pre-training task (e.g. dialogue data) that is similar to the downstream tasks of interest (e.g. dialogue) is a likely explanation for these performance gains, in line with previous results showing multi-tasking with similar tasks is more useful than with dissimilar ones <ref type="bibr" target="#b2">(Caruana, 1997)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Inference Speed</head><p>An important motivation for the Poly-encoder architecture is to achieve better results than the Biencoder while also performing at a reasonable speed. Though the Cross-encoder generally yields strong results, it is prohibitively slow. We perform speed experiments to determine the trade-off of improved performance from the Poly-encoder. Specifically, we predict the next utterance for 100 dialogue examples in the ConvAI2 validation set, where the model scores C candidates (in this case, chosen from the training set). We perform these experiments on both CPU-only and GPU setups. CPU computations were run on an 80 core Intel Xeon processor CPU E5-2698. GPU computations were run on a single Nvidia Quadro GP100 using cuda 10.0 and cudnn 7.4.</p><p>We show the average time per example for each architecture in <ref type="table">Table 5</ref>. The difference in timing between the Bi-encoder and the Poly-encoder architectures is rather minimal when there are only 1000 candidates for the model to consider. The difference is more pronounced when considering 100k candidates, a more realistic setup, as we see a 5-6x slowdown for the Poly-encoder variants. Nevertheless, both models are still tractable. The Cross-encoder, however, is 2 orders of magnitude slower than the Bi-encoder and Poly-encoder, rendering it intractable for real-time inference, e.g. when interacting with a dialogue agent, or retrieving from a large set of documents. Thus, Polyencoders, given their desirable performance and speed trade-off, are the preferred method.</p><p>We additionally report training times in the Appendix, <ref type="table" target="#tab_7">Table 6</ref>. Poly-encoders also have the benefit of being 3-4x faster to train than Cross-encoders (and are similar in training time to Bi-encoders).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we present new architectures and pre-training strategies for deep bidirectional transformers in candidate selection tasks. We introduced the Poly-encoder method, which provides a mechanism for attending over the context using the label candidate, while maintaining the ability to precompute each candidate's representation, which allows for fast real-time inference in a production setup, giving an improved trade off between accuracy and speed. We provided an experimental analysis of those trade-offs for Bi-, Poly-and Cross-encoders, showing that Poly-encoders are more accurate than Bi-encoders, while being far faster than Cross-encoders, which are impractical for real-time use. In terms of training these architectures, we showed that pre-training strategies more closely related to the downstream task bring strong improvements. In particular, pre-training from scratch on Reddit allows us to outperform the results we obtain with BERT, a result that holds for all three model architectures and all three dialogue datasets we tried. However, the methods introduced in this work are not specific to dialogue, and can be used for any task where one is scoring a set of candidates, which we showed for an information retrieval task as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Time</head><p>We report the training time on 8 GPU Volta 100 for the 3 datasets considered and for 4 types of models in <ref type="table" target="#tab_7">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>ConvAI2 <ref type="formula">DSTC7</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Alternative Choices for Context Vectors</head><p>We considered a few other ways to derive the context vectors (y 1 ctxt , ..., y m ctxt ) of the Poly-encoder from the output (h 1 ctxt , ..., h N ctxt ) of the underlying transformer:</p><p>• Learn m codes (c 1 , ..., c m ), where c i extracts representation y i ctxt by attending over all the outputs (h 1 ctxt , ..., h N ctxt ). This method is denoted "Poly-encoder (Learnt-codes)" or "Polyencoder (Learnt-m)", and is the method described in section 4.4 • Consider the first m outputs (h 1 ctxt , ..., h m ctxt ). This method is denoted "Poly-encoder (First m outputs)" or "Poly-encoder (First-m)". Note that when N &lt; m, only m vectors are considered.</p><p>• Consider the last m outputs.</p><p>• Consider the last m outputs concatenated with the first one, h 1 ctxt which plays a particular role in BERT as it corresponds to the special token [S].</p><p>The performance of those four methods is evaluated on the validation set of Convai2 and DSTC7 and reported on <ref type="table" target="#tab_9">Table 8</ref>. The first two methods are shown in <ref type="figure" target="#fig_2">Figure 2</ref>. We additionally provide the inference time for a given number of candidates coming from the Convai2 dataset on <ref type="table" target="#tab_8">Table 9.  Dataset  ConvAI2  DSTC 7  split  dev  test  dev  test  metric</ref> R@1/20 R@1/20 R@1/100 R@1/100 <ref type="bibr" target="#b22">(Wolf et al., 2019)</ref> 82   <ref type="table">Table 9</ref>: Average time in milliseconds to predict the next dialogue utterance from N possible candidates. * are inferred.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagrams of the three model architectures we consider. (a) The Bi-encoder encodes the context and candidate separately, allowing for the caching of candidate representations during inference. (b) The Cross-encoder jointly encodes the context and candidate in a single transformer, yielding richer interactions between context and candidate at the cost of slower computation. (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>0.2 82.2 ± 0.5 56.5 ± 0.5 66.8 ± 0.7 First m outputs 83.4 ± 0.2 81.6 ± 0.1 56.9 ± 0.5 67.2 ± 1.3 Last m outputs 82.8 ± 0.2 81.3 ± 0.4 56.0 ± 0.5 65.8 ± 0.5 Last m outputs and h 1 ctxt 82.9 ± 0.1 81.4 ± 0.2 55.8 ± 0.3 66.1 ± 0.8 16 Attention Codes Learnt-codes 84.4 ± 0.1 83.2 ± 0.1 57.7 ± 0.2 67.8 ± 0.3 First m outputs 85.2 ± 0.1 83.9 ± 0.2 56.1 ± 1.7 66.8 ± 1.1 Last m outputs 83.9 ± 0.2 82.0 ± 0.4 56.1 ± 0.3 66.2 ± 0.7 Last m outputs and h 1 ctxt 83.8 ± 0.3 81.7 ± 0.3 56.1 ± 0.3 66.6 ± 0.2 64 Attention Codes Learnt-codes 84.9 ± 0.1 83.7 ± 0.2 58.3 ± 0.4 67.0 ± 0.9 First m outputs 86.0 ± 0.2 84.2 ± 0.2 57.7 ± 0.6 67.1 ± 0.1 Last m outputs 84.9 ± 0.3 82.9 ± 0.2 57.0 ± 0.2 66.5 ± 0.5 Last m outputs and h 1 ctxt 85.0 ± 0.2 83.2 ± 0.2 57.3 ± 0.3 67.1 ± 0.5 360 Attention Codes Learnt-codes 85.3 ± 0.3 83.7 ± 0.2 57.7 ± 0.3 68.9 ± 0.4 First m outputs 86.3 ± 0.1 84.6 ± 0.3 58.1 ± 0.4 66.8 ± 0.7 Last m outputs 86.3 ± 0.1 84.7 ± 0.3 58.0 ± 0.4 68.1 ± 0.5 Last m outputs and h 1 ctxt 86.2 ± 0.3 84.5 ± 0.4 58.3 ± 0.4 68.0 ± 0.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) The Bi-encoder (b) The Cross-encoder (c) The Poly-encoder with first m vectors. (d) The Poly-encoder with m learnt codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Datasets used in this paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Fine-tuned parameters Bi-encoder Cross-encoder</cell></row><row><cell>Top layer</cell><cell>74.2</cell><cell>80.6</cell></row><row><cell>Top 4 layers</cell><cell>82.0</cell><cell>86.3</cell></row><row><cell>All but Embeddings</cell><cell>83.3</cell><cell>87.3</cell></row><row><cell>Every Layer</cell><cell>83.0</cell><cell>86.6</cell></row></table><note>. On the first three tasks, our Bi-encoders and Cross-encoders outperform the best existing approaches in the literature when we fine-tune from BERT weights. E.g., the Bi- encoder reaches 81.7% R@1 on ConvAI2 and 66.8% R@1 on DSTC7, while the Cross-encoder achieves higher scores of 84.8% R@1 on ConvAI2 and 67.4% R@1 on DSTC7. Overall, Cross- encoders outperform all previous approaches on the three dialogue tasks, including our Bi-encoders (as expected). We do not report fine-tuning of BERT for Wikipedia IR as we cannot guarantee the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Validation performance (R@1/20) on ConvAI2 using pre-trained weights of BERT-base with different parameters fine-tuned. Average over 5 runs (Bi-encoders) or 3 runs (Cross-encoders).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Training time in hours.B Reduction layer in Bi-encoderWe provide inTable 7the results obtained for different types of reductions on top of the Bi-encoder. Specifically we compare the Recall@1/20 on the ConvAI2 validation set when taking the first output of BERT, the average of the first 16 outputs, the average of the first 64 outputs and all of them except the first one ([S]).</figDesc><table><row><cell>Setup</cell><cell>ConvAI2 valid Recall@1/20</cell></row><row><cell>First output</cell><cell>83.3</cell></row><row><cell>Avg first 16 outputs</cell><cell>82.9</cell></row><row><cell>Avg first 64 outputs</cell><cell>82.7</cell></row><row><cell>Avg all outputs</cell><cell>83.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Bi-encoder results on the ConvAI2 valid set for different choices of function red(·).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Validation and test performance of Poly-encoder variants, with weights initialized from. Scores are shown for ConvAI2 and DSTC 7 Track 1. Bold numbers indicate the highest performing variant within that number of codes.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Scoring time (ms)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CPU</cell><cell cols="2">GPU</cell></row><row><cell>Candidates</cell><cell>1k</cell><cell>100k</cell><cell>1k</cell><cell>100k</cell></row><row><cell>Bi-encoder</cell><cell>115</cell><cell>160</cell><cell>19</cell><cell>22</cell></row><row><cell>Poly-encoder (First m outputs) 16</cell><cell>119</cell><cell>551</cell><cell>17</cell><cell>37</cell></row><row><cell>Poly-encoder (First m outputs) 64</cell><cell>124</cell><cell>570</cell><cell>17</cell><cell>39</cell></row><row><cell>Poly-encoder (First m outputs) 360</cell><cell>120</cell><cell>619</cell><cell>17</cell><cell>45</cell></row><row><cell>Poly-encoder (Learnt-codes) 16</cell><cell>122</cell><cell>678</cell><cell>18</cell><cell>38</cell></row><row><cell>Poly-encoder (Learnt-codes) 64</cell><cell>126</cell><cell>692</cell><cell>23</cell><cell>46</cell></row><row><cell>Poly-encoder (Learnt-codes) 360</cell><cell>160</cell><cell>837</cell><cell>57</cell><cell>88</cell></row><row><cell>Cross-encoder</cell><cell cols="4">21.7k 2.2M* 2.6k 266k*</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Supervised semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiko</forename><surname>Sadamasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sequential attention-based network for noetic end-to-end response selection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1901.02609" />
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dstc7 task 1: Noetic end-to-end response selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazaros Polymenakos Chulaka</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
		</author>
		<ptr target="http://workshop.colips.org/dstc7/papers/dstc7_task1_final_report.pdf" />
	</analytic>
	<monogr>
		<title level="m">7th Edition of the Dialog System Technology Challenges at AAAI 2019</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wizard of Wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The second conversational intelligence challenge (convai2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Burtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>978-3-030-29135-8</idno>
	</analytic>
	<monogr>
		<title level="m">The NeurIPS &apos;18 Competition</title>
		<editor>Sergio Escalera and Ralf Herbrich</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="187" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Enhance word representation for out-of-vocabulary on ubuntu dialogue corpus. CoRR, abs/1802.02614</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1802.02614" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Building sequential inference models for end-to-end response selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ping</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1812.00686</idno>
		<ptr target="http://arxiv.org/abs/1812.00686" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jgou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2019.2921572</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Analyzing assumptions in conversation disentanglement research through the lens of a new dataset and model</title>
		<ptr target="https://arxiv.org/pdf/1810.11118.pdf" />
		<editor>Joseph Peper Vignesh Athreya Chulaka Gunasekara Jatin Ganhotra Siva Sankalp Patel Lazaros Polymenakos Jonathan K. Kummerfeld, Sai R. Gouravajhala and Walter S. Lasecki</editor>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved deep learning baselines for ubuntu corpus dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno>abs/1510.03753</idno>
		<ptr target="http://arxiv.org/abs/1510.03753" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03087</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training millions of personalized dialogue agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Shu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to speak and act in a fantasy text adventure game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saachi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="673" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparison of transfer-learning approaches for response selection in multi-turn conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalai</forename><surname>Ramea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on DSTC7</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transfertransfo: A transfer learning approach for neural network based conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08149</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Ledell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<title level="m">Starspace: Embed all the things! In Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Chung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Response ranking with deep matching networks and external knowledge in information-seeking conversation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to rank question-answer pairs using hierarchical recurrent encoder with latent topic clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongbo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyomin</forename><surname>Jung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1575" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazaros</forename><surname>Polymenakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Chulaka</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno>abs/1901.03461</idno>
	</analytic>
	<monogr>
		<title level="j">Dialog system technology challenge</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Personalizing dialogue agents: I have a dog, do you have pets too?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling multi-turn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Toronto Books + Wikipedia Bi-. encoder 83.3 ± 0.2 81.7 ± 0.2 56.5 ± 0.4 66.8 ± 0.7 89.0 ± 1.0 74.6 ± 0.5 80.9 ± 0.6 80.6 ± 0.4 98.2 ± 0.1 88.0 ± 0.3</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>First-m) 16 85.2 ± 0.1 83.9 ± 0.2 56.7 ± 0.2 67.0 ± 0.9 88.8 ± 0.3 74.6 ± 0.6 81.7 ± 0.5 81.4 ± 0.6 98.2 ± 0.1 88.5 ± 0.4</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Learnt-m) 16 84.4 ± 0.1 83.2 ± 0.1 57.7 ± 0.2 67.8 ± 0.3 88.6 ± 0.2 75.1 ± 0.2 81.5 ± 0.1 81.2 ± 0.2 98.2 ± 0.0 88.3 ± 0.1</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>First-m) 64 86.0 ± 0.2 84.2 ± 0.2 57.1 ± 0.2 66.9 ± 0.7 89.1 ± 0.2 74.7 ± 0.4 82.2 ± 0.6 81.9 ± 0.5 98.4 ± 0.0 88.8 ± 0.3</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Learnt-m) 64 84.9 ± 0.1 83.7 ± 0.2 58.3 ± 0.4 67.0 ± 0.9 89.2 ± 0.2 74.7 ± 0.6 81.8 ± 0.1 81.3 ± 0.2 98.2 ± 0.1 88.4 ± 0.1</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>First-m) 360 86.3 ± 0.1 84.6 ± 0.3 57.8 ± 0.5 67.0 ± 0.5 89.6 ± 0.9 75.0 ± 0.6 82.7 ± 0.4 82.2 ± 0.6 98.4 ± 0.1 89.0 ± 0.4</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">360</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">Our pre-training on Toronto Books + Wikipedia Bi</title>
		<imprint/>
	</monogr>
	<note>encoder 84.6 ± 0.1 82.0 ± 0.1 54.9 ± 0.5 64.5 ± 0.5 88.1 ± 0.2 72.6 ± 0.4 80.9 ± 0.5 80.8 ± 0.5 98.4 ± 0.1 88.2 ± 0.4</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>First-m) 16 84.1 ± 0.2 81.4 ± 0.2 53.9 ± 2.7 63.3 ± 2.9 87.2 ± 1.5 71.6 ± 2.4 80.8 ± 0.5 80.6 ± 0.4 98.4 ± 0.1 88.1 ± 0.3</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>First-m) 64 86.1 ± 0.4 83.9 ± 0.3 55.6 ± 0.9 64.3 ± 1.5 87.8 ± 0.4 72.5 ± 1.0 80.9 ± 0.6 80.7 ± 0.6 98.4 ± 0.0 88.2 ± 0.4</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>First-m) 360 86.6 ± 0.3 84.4 ± 0.2 57.5 ± 0.4 66.5 ± 1.2 89.0 ± 0.5 74.4 ± 0.7 81.3 ± 0.6 81.1 ± 0.4 98.4 ± 0.2 88.4 ± 0.3</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">360</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<idno>First-m) 16 89.0 ± 0.1 86.4 ± 0.3 60.4 ± 0.3 70.7 ± 0.7 91.0 ± 0.4 78.0 ± 0.5 84.3 ± 0.3 84.3 ± 0.2 98.9 ± 0.0 90.5 ± 0.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Learnt-m</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>First-m) 64 89.5 ± 0.1 87.3 ± 0.2 61.0 ± 0.4 70.9 ± 0.6 91.5 ± 0.5 78.0 ± 0.3 84.0 ± 0.4 83.9 ± 0.4 98.8 ± 0.0 90.3 ± 0.3</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Learnt-m) 64 89.0 ± 0.1 86.5 ± 0.2 60.9 ± 0.6 71.2 ± 0.8 91.3 ± 0.4 78.2 ± 0.7 86.2 ± 0.1 85.9 ± 0.1 99.1 ± 0.0 91.5 ± 0.1</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<idno>First-m) 360 90.0 ± 0.1</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
	<note>3 ± 0.1 61.1 ± 1.9 70.9 ± 2.1 91.5 ± 0.9 77.9 ± 1.6 84.8 ± 0.5 84.6 ± 0.5 98.9 ± 0.1 90.7 ± 0.3</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poly-Encoder</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">360</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">Table 10: Validation and test performances of Bi-, Poly-and Cross-encoders. Scores are shown for ConvAI2, DSTC7 Track 1 and Ubuntu v2</title>
		<imprint/>
	</monogr>
	<note>and the previous state-of-the-art models in the literature</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
